REPO_URL,REPO_NAME,REPO_OWNER,ISSUE_NUMBER,ISSUE_CREATION_DATE,ISSUE_AUTHOR,ISSUE_TITLE,ISSUE_URL,ISSUE_TEXT,COMMENT_CREATION_DATE,COMMENT_AUTHOR,COMMENT_URL,COMMENT_TEXT,COMMENT_TEXT_LEMMATIZED,NUM_APOLOGY_LEMMAS,IS_APOLOGY
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,2,2019-08-02T14:03:14Z,nemanjavuk,Can't start terminus on macOS 10.14.6,https://github.com/terminusdb/terminusdb/issues/2,"I've been following the instructions from the README.md, installed swi-prolog, installed hdt-cpp but whe I try to run ./start.pl i get the following:

ERROR: /Users/nemanjav/dev/terminusdb/library/triplestore.pl:18:
source_sink library(hdt)' does not exist Warning: /Users/nemanjav/dev/terminusdb/library/triplestore.pl:18: Goal (directive) failed: triplestore:use_module(library(hdt)) ERROR: /Users/nemanjav/dev/terminusdb/library/triplestore.pl:625:20: Syntax error: Operator expected ERROR: /Users/nemanjav/dev/terminusdb/library/triplestore.pl:627:20: Syntax error: Operator expected ERROR: /Users/nemanjav/dev/terminusdb/library/json_ld.pl:34: source_sink library(mavis)' does not exist
Warning: /Users/nemanjav/dev/terminusdb/library/json_ld.pl:34:
Goal (directive) failed: json_ld:use_module(library(mavis))
ERROR: /Users/nemanjav/dev/terminusdb/library/api.pl:90:7: Syntax error: Operator expected
ERROR: /Users/nemanjav/dev/terminusdb/library/api.pl:103:7: Syntax error: Operator expected
Warning: /Users/nemanjav/dev/terminusdb/library/api.pl:122:
Singleton variables: [DB]
Warning: /Users/nemanjav/dev/terminusdb/library/api.pl:128:
Singleton variables: [Auth]
ERROR: /Users/nemanjav/dev/terminusdb/library/api.pl:128:
Full stop in clause-body?  Cannot redefine ,/2
Warning: /Users/nemanjav/dev/terminusdb/library/api.pl:131:
Singleton variables: [DB]
Warning: /Users/nemanjav/dev/terminusdb/library/api.pl:137:
Singleton variables: [Data]
ERROR: /Users/nemanjav/dev/terminusdb/library/api.pl:137:
Full stop in clause-body?  Cannot redefine ,/2
Warning: /Users/nemanjav/dev/terminusdb/library/api.pl:160:
Singleton variables: [Request,Method,DB,Docid]
% Started server at http://localhost:6363/
** Syncing schema in collection 'http://localhost/capability'
ERROR: /Users/nemanjav/dev/terminusdb/start.pl:21: Initialization goal raised exception:
ERROR: ''/1: Undefined procedure: triplestore:hdt_open/2
Welcome to SWI-Prolog (threaded, 64 bits, version 8.0.3)
SWI-Prolog comes with ABSOLUTELY NO WARRANTY. This is free software.
Please run ?- license. for legal details.
For online help and background, visit http://www.swi-prolog.org
For built-in help, use ?- help(Topic). or ?- apropos(Word).
?-

What am I doing wrong? I also don't see hdt.pl in the library folder. Is that maybe the cause of the problem?",2019-08-02T16:14:46Z,GavinMendelGleason,https://github.com/terminusdb/terminusdb/issues/2#issuecomment-517759826,"Hello!

Thanks for the message, stupid bug on my part. There are two additional
dependencies that I had forgotten about. Swipl needs to have mavis and hdt
libraries installed. You can get them with the swipl package manager. I'll
fix this in the documentation as soon as possible.

This gives an example of package installation and available packages.

https://www.swi-prolog.org/pack/list

We've just thrown this up in the last couple of weeks and have not really
launched the beta yet, it's scheduled for October, so this is really
bleeding edge!

Gavin
…
On Fri, 2 Aug 2019 at 15:03, Nemanja Vukosavljević ***@***.***> wrote:
 I've been following the instructions from the README.md, installed
 swi-prolog, installed hdt-cpp but whe I try to run ./start.pl i get the
 following:

 ERROR: /Users/nemanjav/dev/terminusdb/library/triplestore.pl:18:
 source_sink library(hdt)' does not exist
 Warning: /Users/nemanjav/dev/terminusdb/library/triplestore.pl:18:
 Goal (directive) failed: triplestore:use_module(library(hdt))
 ERROR: /Users/nemanjav/dev/terminusdb/library/triplestore.pl:625:20:
 Syntax error: Operator expected
 ERROR: /Users/nemanjav/dev/terminusdb/library/triplestore.pl:627:20:
 Syntax error: Operator expected
 ERROR: /Users/nemanjav/dev/terminusdb/library/json_ld.pl:34:
 source_sink `library(mavis)' does not exist
 Warning: /Users/nemanjav/dev/terminusdb/library/json_ld.pl:34:
 Goal (directive) failed: json_ld:use_module(library(mavis))
 ERROR: /Users/nemanjav/dev/terminusdb/library/api.pl:90:7: Syntax error:
 Operator expected
 ERROR: /Users/nemanjav/dev/terminusdb/library/api.pl:103:7: Syntax error:
 Operator expected
 Warning: /Users/nemanjav/dev/terminusdb/library/api.pl:122:
 Singleton variables: [DB]
 Warning: /Users/nemanjav/dev/terminusdb/library/api.pl:128:
 Singleton variables: [Auth]
 ERROR: /Users/nemanjav/dev/terminusdb/library/api.pl:128:
 Full stop in clause-body? Cannot redefine ,/2
 Warning: /Users/nemanjav/dev/terminusdb/library/api.pl:131:
 Singleton variables: [DB]
 Warning: /Users/nemanjav/dev/terminusdb/library/api.pl:137:
 Singleton variables: [Data]
 ERROR: /Users/nemanjav/dev/terminusdb/library/api.pl:137:
 Full stop in clause-body? Cannot redefine ,/2
 Warning: /Users/nemanjav/dev/terminusdb/library/api.pl:160:
 Singleton variables: [Request,Method,DB,Docid]
 % Started server at http://localhost:6363/

 ** Syncing schema in collection 'http://localhost/capability'
 ERROR: /Users/nemanjav/dev/terminusdb/start.pl:21: Initialization goal
 raised exception:
 ERROR: ''/1: Undefined procedure: triplestore:hdt_open/2
 Welcome to SWI-Prolog (threaded, 64 bits, version 8.0.3)
 SWI-Prolog comes with ABSOLUTELY NO WARRANTY. This is free software.
 Please run ?- license. for legal details.

 For online help and background, visit http://www.swi-prolog.org
 For built-in help, use ?- help(Topic). or ?- apropos(Word).

 ?-`

 What am I doing wrong? On a side note, I don't see hdt.pl in the library
 folder. Is that maybe the cause of the problem?

 —
 You are receiving this because you are subscribed to this thread.
 Reply to this email directly, view it on GitHub
 <#2?email_source=notifications&email_token=AAPGVKSLVVBJWAIPU3D3XJTQCQ5CFA5CNFSM4II6BZXKYY3PNVWWK3TUL52HS4DFUVEXG43VMWVGG33NNVSW45C7NFSM4HDCFTKA>,
 or mute the thread
 <https://github.com/notifications/unsubscribe-auth/AAPGVKX533SZO6DNVDV4QZLQCQ5CFANCNFSM4II6BZXA>
 .",hello thank for the message stupid bug on -PRON- part there be two additional dependency that i have forget about swipl need to have mavi and hdt library instal -PRON- can get -PRON- with the swipl package manager -PRON- will fix this in the documentation as soon as possible this give an example of package installation and available package https//wwwswi - prologorg / pack / list -PRON- have just throw this up in the last couple of week and have not really launch the beta yet -PRON- be schedule for october so this be really bleed edge gavin … on fri 2 aug 2019 at 1503 nemanja vukosavljević * * * @ * * * * * * > write -PRON- have be follow the instruction from the readmemd instal swi - prolog instal hdt - cpp but whe i try to run /startpl i get the follow error /users / nemanjav / dev / terminusdb / library / triplestorepl18 source_sink library(hdt ) ' do not exist warning /users / nemanjav / dev / terminusdb / library / triplestorepl18 goal ( directive ) fail triplestoreuse_module(library(hdt ) ) error /users / nemanjav / dev / terminusdb / library / triplestorepl62520 syntax error operator expect error /users / nemanjav / dev / terminusdb / library / triplestorepl62720 syntax error operator expect error /users / nemanjav / dev / terminusdb / library / json_ldpl34 source_sink ` library(mavi ) ' do not exist warning /users / nemanjav / dev / terminusdb / library / json_ldpl34 goal ( directive ) fail json_lduse_module(library(mavis ) ) error /users / nemanjav / dev / terminusdb / library / apipl907 syntax error operator expect error /users / nemanjav / dev / terminusdb / library / apipl1037 syntax error operator expect warning /users / nemanjav / dev / terminusdb / library / apipl122 singleton variable [ db ] warning /users / nemanjav / dev / terminusdb / library / apipl128 singleton variable [ auth ] error /users / nemanjav / dev / terminusdb / library / apipl128 full stop in clause - body can not redefine /2 warning /users / nemanjav / dev / terminusdb / library / apipl131 singleton variable [ db ] warning /users / nemanjav / dev / terminusdb / library / apipl137 singleton variable [ data ] error /users / nemanjav / dev / terminusdb / library / apipl137 full stop in clause - body can not redefine /2 warning /users / nemanjav / dev / terminusdb / library / apipl160 singleton variable [ requestmethoddbdocid ] % start server at http//localhost6363/ * * syncing schema in collection ' http//localhost / capability ' error /users / nemanjav / dev / terminusdb / startpl21 initialization goal raise exception error ' ' /1 undefined procedure triplestorehdt_open/2 welcome to swi - prolog ( threaded 64 bit version 803 ) swi - prolog come with absolutely no warranty this be free software please run - license for legal detail for online help and background visit http//wwwswi - prologorg for build - in help use - help(topic ) or - apropos(word ) - ` what be i do wrong on a side note i do not see hdtpl in the library folder be that maybe the cause of the problem — -PRON- be receive this because -PRON- be subscribe to this thread reply to this email directly view -PRON- on github < # 2email_source = notifications&email_token = aapgvkslvvbjwaipu3d3xjtqcq5cfa5cnfsm4ii6bzxkyy3pnvwwk3tul52hs4dfuvexg43vmwvgg33nnvsw45c7nfsm4hdcftka > or mute the thread < https//githubcom / notification / unsubscribe - auth / aapgvkx533szo6dnvdv4qzlqcq5cfancnfsm4ii6bzxa >,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,2,2019-08-02T14:03:14Z,nemanjavuk,Can't start terminus on macOS 10.14.6,https://github.com/terminusdb/terminusdb/issues/2,"I've been following the instructions from the README.md, installed swi-prolog, installed hdt-cpp but whe I try to run ./start.pl i get the following:

ERROR: /Users/nemanjav/dev/terminusdb/library/triplestore.pl:18:
source_sink library(hdt)' does not exist Warning: /Users/nemanjav/dev/terminusdb/library/triplestore.pl:18: Goal (directive) failed: triplestore:use_module(library(hdt)) ERROR: /Users/nemanjav/dev/terminusdb/library/triplestore.pl:625:20: Syntax error: Operator expected ERROR: /Users/nemanjav/dev/terminusdb/library/triplestore.pl:627:20: Syntax error: Operator expected ERROR: /Users/nemanjav/dev/terminusdb/library/json_ld.pl:34: source_sink library(mavis)' does not exist
Warning: /Users/nemanjav/dev/terminusdb/library/json_ld.pl:34:
Goal (directive) failed: json_ld:use_module(library(mavis))
ERROR: /Users/nemanjav/dev/terminusdb/library/api.pl:90:7: Syntax error: Operator expected
ERROR: /Users/nemanjav/dev/terminusdb/library/api.pl:103:7: Syntax error: Operator expected
Warning: /Users/nemanjav/dev/terminusdb/library/api.pl:122:
Singleton variables: [DB]
Warning: /Users/nemanjav/dev/terminusdb/library/api.pl:128:
Singleton variables: [Auth]
ERROR: /Users/nemanjav/dev/terminusdb/library/api.pl:128:
Full stop in clause-body?  Cannot redefine ,/2
Warning: /Users/nemanjav/dev/terminusdb/library/api.pl:131:
Singleton variables: [DB]
Warning: /Users/nemanjav/dev/terminusdb/library/api.pl:137:
Singleton variables: [Data]
ERROR: /Users/nemanjav/dev/terminusdb/library/api.pl:137:
Full stop in clause-body?  Cannot redefine ,/2
Warning: /Users/nemanjav/dev/terminusdb/library/api.pl:160:
Singleton variables: [Request,Method,DB,Docid]
% Started server at http://localhost:6363/
** Syncing schema in collection 'http://localhost/capability'
ERROR: /Users/nemanjav/dev/terminusdb/start.pl:21: Initialization goal raised exception:
ERROR: ''/1: Undefined procedure: triplestore:hdt_open/2
Welcome to SWI-Prolog (threaded, 64 bits, version 8.0.3)
SWI-Prolog comes with ABSOLUTELY NO WARRANTY. This is free software.
Please run ?- license. for legal details.
For online help and background, visit http://www.swi-prolog.org
For built-in help, use ?- help(Topic). or ?- apropos(Word).
?-

What am I doing wrong? I also don't see hdt.pl in the library folder. Is that maybe the cause of the problem?",2019-08-02T16:26:12Z,GavinMendelGleason,https://github.com/terminusdb/terminusdb/issues/2#issuecomment-517763601,README updated.,readme update,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,2,2019-08-02T14:03:14Z,nemanjavuk,Can't start terminus on macOS 10.14.6,https://github.com/terminusdb/terminusdb/issues/2,"I've been following the instructions from the README.md, installed swi-prolog, installed hdt-cpp but whe I try to run ./start.pl i get the following:

ERROR: /Users/nemanjav/dev/terminusdb/library/triplestore.pl:18:
source_sink library(hdt)' does not exist Warning: /Users/nemanjav/dev/terminusdb/library/triplestore.pl:18: Goal (directive) failed: triplestore:use_module(library(hdt)) ERROR: /Users/nemanjav/dev/terminusdb/library/triplestore.pl:625:20: Syntax error: Operator expected ERROR: /Users/nemanjav/dev/terminusdb/library/triplestore.pl:627:20: Syntax error: Operator expected ERROR: /Users/nemanjav/dev/terminusdb/library/json_ld.pl:34: source_sink library(mavis)' does not exist
Warning: /Users/nemanjav/dev/terminusdb/library/json_ld.pl:34:
Goal (directive) failed: json_ld:use_module(library(mavis))
ERROR: /Users/nemanjav/dev/terminusdb/library/api.pl:90:7: Syntax error: Operator expected
ERROR: /Users/nemanjav/dev/terminusdb/library/api.pl:103:7: Syntax error: Operator expected
Warning: /Users/nemanjav/dev/terminusdb/library/api.pl:122:
Singleton variables: [DB]
Warning: /Users/nemanjav/dev/terminusdb/library/api.pl:128:
Singleton variables: [Auth]
ERROR: /Users/nemanjav/dev/terminusdb/library/api.pl:128:
Full stop in clause-body?  Cannot redefine ,/2
Warning: /Users/nemanjav/dev/terminusdb/library/api.pl:131:
Singleton variables: [DB]
Warning: /Users/nemanjav/dev/terminusdb/library/api.pl:137:
Singleton variables: [Data]
ERROR: /Users/nemanjav/dev/terminusdb/library/api.pl:137:
Full stop in clause-body?  Cannot redefine ,/2
Warning: /Users/nemanjav/dev/terminusdb/library/api.pl:160:
Singleton variables: [Request,Method,DB,Docid]
% Started server at http://localhost:6363/
** Syncing schema in collection 'http://localhost/capability'
ERROR: /Users/nemanjav/dev/terminusdb/start.pl:21: Initialization goal raised exception:
ERROR: ''/1: Undefined procedure: triplestore:hdt_open/2
Welcome to SWI-Prolog (threaded, 64 bits, version 8.0.3)
SWI-Prolog comes with ABSOLUTELY NO WARRANTY. This is free software.
Please run ?- license. for legal details.
For online help and background, visit http://www.swi-prolog.org
For built-in help, use ?- help(Topic). or ?- apropos(Word).
?-

What am I doing wrong? I also don't see hdt.pl in the library folder. Is that maybe the cause of the problem?",2019-08-02T16:31:07Z,nemanjavuk,https://github.com/terminusdb/terminusdb/issues/2#issuecomment-517765164,I see that it’s still nascent. I saw a couple of videos from the Knowledge Graph Conference and I wanted to see what you’ve done. I’m also interested in WOQL. Do you have any document that describes it? :),i see that -PRON- ’ still nascent i see a couple of video from the knowledge graph conference and i want to see what -PRON- have do -PRON- be also interested in woql do -PRON- have any document that describe -PRON- ),0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,2,2019-08-02T14:03:14Z,nemanjavuk,Can't start terminus on macOS 10.14.6,https://github.com/terminusdb/terminusdb/issues/2,"I've been following the instructions from the README.md, installed swi-prolog, installed hdt-cpp but whe I try to run ./start.pl i get the following:

ERROR: /Users/nemanjav/dev/terminusdb/library/triplestore.pl:18:
source_sink library(hdt)' does not exist Warning: /Users/nemanjav/dev/terminusdb/library/triplestore.pl:18: Goal (directive) failed: triplestore:use_module(library(hdt)) ERROR: /Users/nemanjav/dev/terminusdb/library/triplestore.pl:625:20: Syntax error: Operator expected ERROR: /Users/nemanjav/dev/terminusdb/library/triplestore.pl:627:20: Syntax error: Operator expected ERROR: /Users/nemanjav/dev/terminusdb/library/json_ld.pl:34: source_sink library(mavis)' does not exist
Warning: /Users/nemanjav/dev/terminusdb/library/json_ld.pl:34:
Goal (directive) failed: json_ld:use_module(library(mavis))
ERROR: /Users/nemanjav/dev/terminusdb/library/api.pl:90:7: Syntax error: Operator expected
ERROR: /Users/nemanjav/dev/terminusdb/library/api.pl:103:7: Syntax error: Operator expected
Warning: /Users/nemanjav/dev/terminusdb/library/api.pl:122:
Singleton variables: [DB]
Warning: /Users/nemanjav/dev/terminusdb/library/api.pl:128:
Singleton variables: [Auth]
ERROR: /Users/nemanjav/dev/terminusdb/library/api.pl:128:
Full stop in clause-body?  Cannot redefine ,/2
Warning: /Users/nemanjav/dev/terminusdb/library/api.pl:131:
Singleton variables: [DB]
Warning: /Users/nemanjav/dev/terminusdb/library/api.pl:137:
Singleton variables: [Data]
ERROR: /Users/nemanjav/dev/terminusdb/library/api.pl:137:
Full stop in clause-body?  Cannot redefine ,/2
Warning: /Users/nemanjav/dev/terminusdb/library/api.pl:160:
Singleton variables: [Request,Method,DB,Docid]
% Started server at http://localhost:6363/
** Syncing schema in collection 'http://localhost/capability'
ERROR: /Users/nemanjav/dev/terminusdb/start.pl:21: Initialization goal raised exception:
ERROR: ''/1: Undefined procedure: triplestore:hdt_open/2
Welcome to SWI-Prolog (threaded, 64 bits, version 8.0.3)
SWI-Prolog comes with ABSOLUTELY NO WARRANTY. This is free software.
Please run ?- license. for legal details.
For online help and background, visit http://www.swi-prolog.org
For built-in help, use ?- help(Topic). or ?- apropos(Word).
?-

What am I doing wrong? I also don't see hdt.pl in the library folder. Is that maybe the cause of the problem?",2019-08-02T16:38:31Z,GavinMendelGleason,https://github.com/terminusdb/terminusdb/issues/2#issuecomment-517767499,"Previously our database was proprietary. We're repackaging everything up
and launching as 100% open source. We expect to have the WOQL documentation
ready in a few weeks.

Thanks for the questions!

Gavin
…
On Fri, 2 Aug 2019 at 17:31, Nemanja Vukosavljević ***@***.***> wrote:
 I see that it’s still nascent. I saw a couple of videos from the Knowledge
 Graph Conference and I wanted to see what you’ve done. I’m also interested
 in WOQL. Do you have any document that describes it? :)

 —
 You are receiving this because you modified the open/close state.
 Reply to this email directly, view it on GitHub
 <#2?email_source=notifications&email_token=AAPGVKU3UVZLHZ4L5JH2BS3QCROMXA5CNFSM4II6BZXKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD3OHQLA#issuecomment-517765164>,
 or mute the thread
 <https://github.com/notifications/unsubscribe-auth/AAPGVKV5O5ZX2CRYGGAWZL3QCROMXANCNFSM4II6BZXA>
 .",previously -PRON- database be proprietary -PRON- be repackage everything up and launch as 100 % open source -PRON- expect to have the woql documentation ready in a few week thank for the question gavin … on fri 2 aug 2019 at 1731 nemanja vukosavljević * * * @ * * * * * * > write i see that -PRON- ’ still nascent i see a couple of video from the knowledge graph conference and i want to see what -PRON- have do -PRON- be also interested in woql do -PRON- have any document that describe -PRON- ) — -PRON- be receive this because -PRON- modify the open / close state reply to this email directly view -PRON- on github < # 2email_source = notifications&email_token = aapgvku3uvzlhz4l5jh2bs3qcromxa5cnfsm4ii6bzxkyy3pnvwwk3tul52hs4dfvrexg43vmvbw63lnmvxhjktdn5ww2zloorpwszgod3ohqla#issuecomment-517765164 > or mute the thread < https//githubcom / notification / unsubscribe - auth / aapgvkv5o5zx2cryggawzl3qcromxancnfsm4ii6bzxa >,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,4,2019-09-04T16:17:07Z,GavinMendelGleason,Server Endpoint management fixes,https://github.com/terminusdb/terminusdb/issues/4,"Currently there are two places in which it is necessary to specify the server name. One is in the configuration script which creates the initial terminus database. The other is a legacy config/config.pl which allowed some configuration of predicates.
We should be trying to move as much as possible to the terminus database. This means the initial startup script should probably use a configuration file to draw from - perhaps config.pl in order to set defaults.  Much better that this information resides in one place than two.
The configuration script also doesn't handle different protocols (https versus http) and just takes the server name. This is obviously wrong. We need to either increase the number of parameters to the script, or draw them from configuration.
We may also need to think about how we deal with slight difference of server. Apache has virtual servers to handle this issue. This is easy enough in swipl, but not actually achieved by the current code.",2019-09-13T14:29:07Z,GavinMendelGleason,https://github.com/terminusdb/terminusdb/issues/4#issuecomment-531259123,We are punting on virtual servers for now. The rest should have already been done by @rrooij,-PRON- be punt on virtual server for now the rest should have already be do by @rrooij,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,5,2019-09-13T09:22:34Z,GavinMendelGleason,xrdf needs to throw error on non-existent graphs. ,https://github.com/terminusdb/terminusdb/issues/5,It is very confusing to get silent failure when attempting to access graphs which do not exist. It is impossible to distinguish an empty query from an incorrect query.,2020-07-09T13:57:23Z,rrooij,https://github.com/terminusdb/terminusdb/issues/5#issuecomment-656143513,Is this still a problem @GavinMendelGleason ?,be this still a problem @gavinmendelgleason,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,5,2019-09-13T09:22:34Z,GavinMendelGleason,xrdf needs to throw error on non-existent graphs. ,https://github.com/terminusdb/terminusdb/issues/5,It is very confusing to get silent failure when attempting to access graphs which do not exist. It is impossible to distinguish an empty query from an incorrect query.,2020-07-09T14:15:33Z,GavinMendelGleason,https://github.com/terminusdb/terminusdb/issues/5#issuecomment-656153848,"Yes, this is still a live issue.
WOQL.using(""_system"").quad(""v:X"", ""v:Y"", ""v:Z"", ""instance/nonsense"")","yes this be still a live issue woqlusing(""_system"")quad(""vx "" "" vy "" "" vz "" "" instance / nonsense "" )",0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,5,2019-09-13T09:22:34Z,GavinMendelGleason,xrdf needs to throw error on non-existent graphs. ,https://github.com/terminusdb/terminusdb/issues/5,It is very confusing to get silent failure when attempting to access graphs which do not exist. It is impossible to distinguish an empty query from an incorrect query.,2020-10-01T15:46:22Z,GavinMendelGleason,https://github.com/terminusdb/terminusdb/issues/5#issuecomment-702225984,This now throws an error.,this now throw an error,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,6,2019-09-13T16:39:01Z,rrooij,rapper used in some rules,https://github.com/terminusdb/terminusdb/issues/6,"Rapper is used in checkpoint_to_turtle in library/file_utils.pl.
It is not listed as a dependency.",2019-09-13T17:35:45Z,rrooij,https://github.com/terminusdb/terminusdb/issues/6#issuecomment-531326278,Fixed in commit 761723a,fix in commit 761723a,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,7,2019-09-13T18:04:59Z,rrooij,travis-ci should run api tests,https://github.com/terminusdb/terminusdb/issues/7,Travis CI should run the api tests and not only the initialize_database script.,2019-09-25T07:17:16Z,rrooij,https://github.com/terminusdb/terminusdb/issues/7#issuecomment-534888357,It runs the API tests already since 894a1b3,-PRON- run the api test already since 894a1b3,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,8,2019-09-26T14:42:31Z,GavinMendelGleason,start.pl should die with suitably frightening error message if initialise has never run.,https://github.com/terminusdb/terminusdb/issues/8,We should not allow the user to start until templates and schemas have been properly set up with the initiliasation script.,2019-09-26T15:09:33Z,rrooij,https://github.com/terminusdb/terminusdb/issues/8#issuecomment-535549531,"c95088a
Is this sufficient?",c95088a be this sufficient,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,9,2019-09-27T09:32:16Z,GavinMendelGleason,JSON Errors need to be lifted to schema driven JSON-LD,https://github.com/terminusdb/terminusdb/issues/9,"We are currently returning ad-hoc javascript objects. These really need to conform to the ontology to which they refer, together with a context object so they can be statelessly interpreted, with a valid '@type'.  Should there be a seperate API ontology for talking about interchange messages? Should it all live in ""vio""? Or is ""vio"" meant only for data conformance checking.",2020-10-01T15:44:55Z,GavinMendelGleason,https://github.com/terminusdb/terminusdb/issues/9#issuecomment-702225075,These are now correctly documented in the API and VIO ontologies,these be now correctly document in the api and vio ontology,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,10,2019-09-27T09:38:55Z,GavinMendelGleason,All unrecoverable errors should be http_reply exceptions,https://github.com/terminusdb/terminusdb/issues/10,"Instead of throwing for instance, type errors which yield 500s, we need to throw more informative status messages with JSON-LD objects. This will require scouring the code-base for throw and determining if this is a recoverable exception or not, or by making recovery catch http replies if it's impossible to disambiguate.",2020-10-01T15:44:26Z,GavinMendelGleason,https://github.com/terminusdb/terminusdb/issues/10#issuecomment-702224804,Most of this cleanup occurred during our summer refactor.,most of this cleanup occur during -PRON- summer refactor,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,11,2019-09-27T12:47:31Z,GavinMendelGleason,Schema updates require instance checking,https://github.com/terminusdb/terminusdb/issues/11,Updates are not checking instance data compatibility. The simplest approach is a global check. Further optimisations can be made later,2020-10-01T15:43:51Z,GavinMendelGleason,https://github.com/terminusdb/terminusdb/issues/11#issuecomment-702224417,This is being done correctly now.,this be be do correctly now,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,14,2019-09-27T18:39:45Z,pmoura,Possible bug in setof/3 call,https://github.com/terminusdb/terminusdb/issues/14,"In the following predicate definition:
https://github.com/terminusdb/terminus-server/blob/152b2f58c869523f2ad5ffeecbb702045d3a84d2/library/validate_schema.pl#L476-L482
the Logtalk linter reports:
*     Suspicious call: xrdf(A,B,C,D,E) in setof/3 goal contains singleton variable D
*       while compiling object validate_schema
*       in file /Users/pmoura/terminus-server/library/validate_schema.pl between lines 476-482
The reported singleton variable can result in multiple solutions (one per binding of the variable) where a single solution is expected.",2019-09-27T18:51:50Z,GavinMendelGleason,https://github.com/terminusdb/terminusdb/issues/14#issuecomment-536057408,I think this is actually correct - the point is to check if there are multiple answers.,i think this be actually correct - the point be to check if there be multiple answer,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,14,2019-09-27T18:39:45Z,pmoura,Possible bug in setof/3 call,https://github.com/terminusdb/terminusdb/issues/14,"In the following predicate definition:
https://github.com/terminusdb/terminus-server/blob/152b2f58c869523f2ad5ffeecbb702045d3a84d2/library/validate_schema.pl#L476-L482
the Logtalk linter reports:
*     Suspicious call: xrdf(A,B,C,D,E) in setof/3 goal contains singleton variable D
*       while compiling object validate_schema
*       in file /Users/pmoura/terminus-server/library/validate_schema.pl between lines 476-482
The reported singleton variable can result in multiple solutions (one per binding of the variable) where a single solution is expected.",2019-09-27T18:56:25Z,pmoura,https://github.com/terminusdb/terminusdb/issues/14#issuecomment-536058930,"Yes, I noticed the soft-cut control construct. But, theoretically, that call can generate multiple solutions for the bindings of the Collection, Schema, X, and CE variables. But the use of an anonymous variable in the fourth argument suggests that you don't want multiple solutions for the bindings of that particular argument. If so, then you will need to existentially qualify that variable.",yes i notice the soft - cut control construct but theoretically that call can generate multiple solution for the binding of the collection schema x and ce variable but the use of an anonymous variable in the fourth argument suggest that -PRON- do not want multiple solution for the binding of that particular argument if so then -PRON- will need to existentially qualify that variable,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,14,2019-09-27T18:39:45Z,pmoura,Possible bug in setof/3 call,https://github.com/terminusdb/terminusdb/issues/14,"In the following predicate definition:
https://github.com/terminusdb/terminus-server/blob/152b2f58c869523f2ad5ffeecbb702045d3a84d2/library/validate_schema.pl#L476-L482
the Logtalk linter reports:
*     Suspicious call: xrdf(A,B,C,D,E) in setof/3 goal contains singleton variable D
*       while compiling object validate_schema
*       in file /Users/pmoura/terminus-server/library/validate_schema.pl between lines 476-482
The reported singleton variable can result in multiple solutions (one per binding of the variable) where a single solution is expected.",2019-09-27T18:58:59Z,GavinMendelGleason,https://github.com/terminusdb/terminusdb/issues/14#issuecomment-536059776,"Indeed, but we want to be able to check length(L,1) - so we're intending to find if there are duplicates.",indeed but -PRON- want to be able to check length(l1 ) - so -PRON- be intend to find if there be duplicate,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,14,2019-09-27T18:39:45Z,pmoura,Possible bug in setof/3 call,https://github.com/terminusdb/terminusdb/issues/14,"In the following predicate definition:
https://github.com/terminusdb/terminus-server/blob/152b2f58c869523f2ad5ffeecbb702045d3a84d2/library/validate_schema.pl#L476-L482
the Logtalk linter reports:
*     Suspicious call: xrdf(A,B,C,D,E) in setof/3 goal contains singleton variable D
*       while compiling object validate_schema
*       in file /Users/pmoura/terminus-server/library/validate_schema.pl between lines 476-482
The reported singleton variable can result in multiple solutions (one per binding of the variable) where a single solution is expected.",2019-12-05T11:34:12Z,pmoura,https://github.com/terminusdb/terminusdb/issues/14#issuecomment-562091792,"Still confused by this predicate definition. Given the comment ""Exactly one reference to this class"" shouldn't the predicate call bagof/3 instead of setof/3? But assuming setof/3 is correct, why not simplifying it to:
anonymousEquivalentClass(C,CE,Database) :-
    database_schema(Database,Schema),
    equivalentClass(C,CE,Database),
    % Exactly one reference to this class, or everything will go to hell.
    setof(X,xrdf(Database,Schema,X,_,CE),[_]).","still confuse by this predicate definition give the comment "" exactly one reference to this class "" should not the predicate call bagof/3 instead of setof/3 but assume setof/3 be correct why not simplify -PRON- to anonymousequivalentclass(ccedatabase ) - database_schema(databaseschema ) equivalentclass(ccedatabase ) % exactly one reference to this class or everything will go to hell setof(xxrdf(databaseschemax_ce ) [ _ ] )",0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,14,2019-09-27T18:39:45Z,pmoura,Possible bug in setof/3 call,https://github.com/terminusdb/terminusdb/issues/14,"In the following predicate definition:
https://github.com/terminusdb/terminus-server/blob/152b2f58c869523f2ad5ffeecbb702045d3a84d2/library/validate_schema.pl#L476-L482
the Logtalk linter reports:
*     Suspicious call: xrdf(A,B,C,D,E) in setof/3 goal contains singleton variable D
*       while compiling object validate_schema
*       in file /Users/pmoura/terminus-server/library/validate_schema.pl between lines 476-482
The reported singleton variable can result in multiple solutions (one per binding of the variable) where a single solution is expected.",2019-12-05T13:13:41Z,GavinMendelGleason,https://github.com/terminusdb/terminusdb/issues/14#issuecomment-562123505,"Yikes, indeed, this can't be setof/3, but must be bagof/3.",yike indeed this can not be setof/3 but must be bagof/3,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,14,2019-09-27T18:39:45Z,pmoura,Possible bug in setof/3 call,https://github.com/terminusdb/terminusdb/issues/14,"In the following predicate definition:
https://github.com/terminusdb/terminus-server/blob/152b2f58c869523f2ad5ffeecbb702045d3a84d2/library/validate_schema.pl#L476-L482
the Logtalk linter reports:
*     Suspicious call: xrdf(A,B,C,D,E) in setof/3 goal contains singleton variable D
*       while compiling object validate_schema
*       in file /Users/pmoura/terminus-server/library/validate_schema.pl between lines 476-482
The reported singleton variable can result in multiple solutions (one per binding of the variable) where a single solution is expected.",2021-03-12T12:15:37Z,GavinMendelGleason,https://github.com/terminusdb/terminusdb/issues/14#issuecomment-797454078,closed due to age,close due to age,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,15,2019-09-27T18:43:22Z,pmoura,Redundant call/1 wrapper,https://github.com/terminusdb/terminusdb/issues/15,"Also found by the Logtalk linter, a redundant call/1 wrapper in the following line:
https://github.com/terminusdb/terminus-server/blob/152b2f58c869523f2ad5ffeecbb702045d3a84d2/library/sdk.pl#L83",2019-09-27T18:57:50Z,GavinMendelGleason,https://github.com/terminusdb/terminusdb/issues/15#issuecomment-536059420,Fixed.,fix,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,16,2019-09-27T19:06:41Z,GavinMendelGleason,Frames need to be genuine JSON-LD,https://github.com/terminusdb/terminusdb/issues/16,Frames are currently a well defined but undocumented and ontology free json structure. This isn't good enough! Needs to be ontologised and made into genuine JSON-LD return object.,2021-03-12T12:21:13Z,GavinMendelGleason,https://github.com/terminusdb/terminusdb/issues/16#issuecomment-797456967,No plans to do this at the moment.,no plan to do this at the moment,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,17,2019-09-27T19:10:14Z,pmoura,Generate diagrams for the application,https://github.com/terminusdb/terminusdb/issues/17,"I cannot do this myself yet as I'm still missing the correct version of the serd dependency on my macOS laptop. But you may have fun generating diagrams for the application.
First, start Logtalk using either the swilgt script or the pack:
?- pack_install(logtalk).
...

?- use_module(library(logtalk)).
...

Then, load the application and try the next queries:
?- [start].
...

?- {diagrams(loader)}.
...

?- working_directory(TerminusDB, TerminusDB),
   file_search_path(swi, SWIHome),
   atom_concat(SWIHome, '/library', Library),
   git_hash(Hash, []),
   atomic_list_concat(['https://github.com/terminusdb/terminus-server/tree/',Hash,'/'], Git),
   diagrams::rdirectory(
       terminusdb,
       '.',
       [
           zoom(true),
           path_url_prefixes(TerminusDB, Git, ''),
           path_url_prefixes(Library, 'https://github.com/SWI-Prolog/swipl-devel/tree/master/library/', ''),
           omit_path_prefixes([TerminusDB, Library]),
           title('TerminusDB'),
           layout(top_to_bottom)
        ]
   ).

The last query assumes that [start] loads all the application files. If that's not the case, load any missing files first.",2019-09-27T19:44:19Z,GavinMendelGleason,https://github.com/terminusdb/terminusdb/issues/17#issuecomment-536072819,Brilliant. I'll give it a go. We'll be getting rid of the serd dependency as soon as we can. Hopefully within the next month.,brilliant -PRON- will give -PRON- a go -PRON- will be get rid of the serd dependency as soon as -PRON- can hopefully within the next month,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,17,2019-09-27T19:10:14Z,pmoura,Generate diagrams for the application,https://github.com/terminusdb/terminusdb/issues/17,"I cannot do this myself yet as I'm still missing the correct version of the serd dependency on my macOS laptop. But you may have fun generating diagrams for the application.
First, start Logtalk using either the swilgt script or the pack:
?- pack_install(logtalk).
...

?- use_module(library(logtalk)).
...

Then, load the application and try the next queries:
?- [start].
...

?- {diagrams(loader)}.
...

?- working_directory(TerminusDB, TerminusDB),
   file_search_path(swi, SWIHome),
   atom_concat(SWIHome, '/library', Library),
   git_hash(Hash, []),
   atomic_list_concat(['https://github.com/terminusdb/terminus-server/tree/',Hash,'/'], Git),
   diagrams::rdirectory(
       terminusdb,
       '.',
       [
           zoom(true),
           path_url_prefixes(TerminusDB, Git, ''),
           path_url_prefixes(Library, 'https://github.com/SWI-Prolog/swipl-devel/tree/master/library/', ''),
           omit_path_prefixes([TerminusDB, Library]),
           title('TerminusDB'),
           layout(top_to_bottom)
        ]
   ).

The last query assumes that [start] loads all the application files. If that's not the case, load any missing files first.",2019-09-27T19:57:54Z,pmoura,https://github.com/terminusdb/terminusdb/issues/17#issuecomment-536076777,"Forgot a step. After the queries, exit Prolog and type:
$ lgt2svg

If you used the pack to load Logtalk, you will likely need to dig the path to the lgt2svg script. In my case, it's stored at ~/lib/swipl/pack/logtalk/logtalk-3.30.0/tools/diagrams/lgt2svg.sh. Requires a recent Graphviz installation.",forget a step after the query exit prolog and type $ lgt2svg if -PRON- use the pack to load logtalk -PRON- will likely need to dig the path to the lgt2svg script in -PRON- case -PRON- be store at ~/lib / swipl / pack / logtalk / logtalk-3300 / tool / diagram / lgt2svgsh require a recent graphviz installation,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,17,2019-09-27T19:10:14Z,pmoura,Generate diagrams for the application,https://github.com/terminusdb/terminusdb/issues/17,"I cannot do this myself yet as I'm still missing the correct version of the serd dependency on my macOS laptop. But you may have fun generating diagrams for the application.
First, start Logtalk using either the swilgt script or the pack:
?- pack_install(logtalk).
...

?- use_module(library(logtalk)).
...

Then, load the application and try the next queries:
?- [start].
...

?- {diagrams(loader)}.
...

?- working_directory(TerminusDB, TerminusDB),
   file_search_path(swi, SWIHome),
   atom_concat(SWIHome, '/library', Library),
   git_hash(Hash, []),
   atomic_list_concat(['https://github.com/terminusdb/terminus-server/tree/',Hash,'/'], Git),
   diagrams::rdirectory(
       terminusdb,
       '.',
       [
           zoom(true),
           path_url_prefixes(TerminusDB, Git, ''),
           path_url_prefixes(Library, 'https://github.com/SWI-Prolog/swipl-devel/tree/master/library/', ''),
           omit_path_prefixes([TerminusDB, Library]),
           title('TerminusDB'),
           layout(top_to_bottom)
        ]
   ).

The last query assumes that [start] loads all the application files. If that's not the case, load any missing files first.",2019-11-11T16:20:15Z,pmoura,https://github.com/terminusdb/terminusdb/issues/17#issuecomment-552511287,"With my recent work on GitHub actions and workflows for Logtalk and Prolog repos, if you enable actions tab in this repo settings, I'm happy to submit a PR with a workflow that will (re)generate and publish the diagrams automatically on push events.",with -PRON- recent work on github action and workflow for logtalk and prolog repos if -PRON- enable action tab in this repo setting -PRON- be happy to submit a pr with a workflow that will ( re)generate and publish the diagram automatically on push event,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,17,2019-09-27T19:10:14Z,pmoura,Generate diagrams for the application,https://github.com/terminusdb/terminusdb/issues/17,"I cannot do this myself yet as I'm still missing the correct version of the serd dependency on my macOS laptop. But you may have fun generating diagrams for the application.
First, start Logtalk using either the swilgt script or the pack:
?- pack_install(logtalk).
...

?- use_module(library(logtalk)).
...

Then, load the application and try the next queries:
?- [start].
...

?- {diagrams(loader)}.
...

?- working_directory(TerminusDB, TerminusDB),
   file_search_path(swi, SWIHome),
   atom_concat(SWIHome, '/library', Library),
   git_hash(Hash, []),
   atomic_list_concat(['https://github.com/terminusdb/terminus-server/tree/',Hash,'/'], Git),
   diagrams::rdirectory(
       terminusdb,
       '.',
       [
           zoom(true),
           path_url_prefixes(TerminusDB, Git, ''),
           path_url_prefixes(Library, 'https://github.com/SWI-Prolog/swipl-devel/tree/master/library/', ''),
           omit_path_prefixes([TerminusDB, Library]),
           title('TerminusDB'),
           layout(top_to_bottom)
        ]
   ).

The last query assumes that [start] loads all the application files. If that's not the case, load any missing files first.",2019-11-14T11:58:16Z,rrooij,https://github.com/terminusdb/terminusdb/issues/17#issuecomment-553856351,"@pmoura We got rid of the serd and hdt dependency in the dev branch, so might be easier for you to work with now.",@pmoura -PRON- get rid of the serd and hdt dependency in the dev branch so may be easy for -PRON- to work with now,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,17,2019-09-27T19:10:14Z,pmoura,Generate diagrams for the application,https://github.com/terminusdb/terminusdb/issues/17,"I cannot do this myself yet as I'm still missing the correct version of the serd dependency on my macOS laptop. But you may have fun generating diagrams for the application.
First, start Logtalk using either the swilgt script or the pack:
?- pack_install(logtalk).
...

?- use_module(library(logtalk)).
...

Then, load the application and try the next queries:
?- [start].
...

?- {diagrams(loader)}.
...

?- working_directory(TerminusDB, TerminusDB),
   file_search_path(swi, SWIHome),
   atom_concat(SWIHome, '/library', Library),
   git_hash(Hash, []),
   atomic_list_concat(['https://github.com/terminusdb/terminus-server/tree/',Hash,'/'], Git),
   diagrams::rdirectory(
       terminusdb,
       '.',
       [
           zoom(true),
           path_url_prefixes(TerminusDB, Git, ''),
           path_url_prefixes(Library, 'https://github.com/SWI-Prolog/swipl-devel/tree/master/library/', ''),
           omit_path_prefixes([TerminusDB, Library]),
           title('TerminusDB'),
           layout(top_to_bottom)
        ]
   ).

The last query assumes that [start] loads all the application files. If that's not the case, load any missing files first.",2019-11-16T10:34:54Z,pmoura,https://github.com/terminusdb/terminusdb/issues/17#issuecomment-554625137,Great! But the required terminus_store_prolog pack cannot currently be installed on macOS (which I'm using). I'm looking into this issue first.,great but the required terminus_store_prolog pack can not currently be instal on macos ( which -PRON- be use ) -PRON- be look into this issue first,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,17,2019-09-27T19:10:14Z,pmoura,Generate diagrams for the application,https://github.com/terminusdb/terminusdb/issues/17,"I cannot do this myself yet as I'm still missing the correct version of the serd dependency on my macOS laptop. But you may have fun generating diagrams for the application.
First, start Logtalk using either the swilgt script or the pack:
?- pack_install(logtalk).
...

?- use_module(library(logtalk)).
...

Then, load the application and try the next queries:
?- [start].
...

?- {diagrams(loader)}.
...

?- working_directory(TerminusDB, TerminusDB),
   file_search_path(swi, SWIHome),
   atom_concat(SWIHome, '/library', Library),
   git_hash(Hash, []),
   atomic_list_concat(['https://github.com/terminusdb/terminus-server/tree/',Hash,'/'], Git),
   diagrams::rdirectory(
       terminusdb,
       '.',
       [
           zoom(true),
           path_url_prefixes(TerminusDB, Git, ''),
           path_url_prefixes(Library, 'https://github.com/SWI-Prolog/swipl-devel/tree/master/library/', ''),
           omit_path_prefixes([TerminusDB, Library]),
           title('TerminusDB'),
           layout(top_to_bottom)
        ]
   ).

The last query assumes that [start] loads all the application files. If that's not the case, load any missing files first.",2019-11-19T21:23:10Z,pmoura,https://github.com/terminusdb/terminusdb/issues/17#issuecomment-555721520,Can you create a gh-pages branch and then go into the repo settings and set GitHub Pages to build from this branch? That will allow the diagrams to be published and browsed without disturbing  anything.,can -PRON- create a gh - page branch and then go into the repo setting and set github page to build from this branch that will allow the diagram to be publish and browse without disturb anything,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,17,2019-09-27T19:10:14Z,pmoura,Generate diagrams for the application,https://github.com/terminusdb/terminusdb/issues/17,"I cannot do this myself yet as I'm still missing the correct version of the serd dependency on my macOS laptop. But you may have fun generating diagrams for the application.
First, start Logtalk using either the swilgt script or the pack:
?- pack_install(logtalk).
...

?- use_module(library(logtalk)).
...

Then, load the application and try the next queries:
?- [start].
...

?- {diagrams(loader)}.
...

?- working_directory(TerminusDB, TerminusDB),
   file_search_path(swi, SWIHome),
   atom_concat(SWIHome, '/library', Library),
   git_hash(Hash, []),
   atomic_list_concat(['https://github.com/terminusdb/terminus-server/tree/',Hash,'/'], Git),
   diagrams::rdirectory(
       terminusdb,
       '.',
       [
           zoom(true),
           path_url_prefixes(TerminusDB, Git, ''),
           path_url_prefixes(Library, 'https://github.com/SWI-Prolog/swipl-devel/tree/master/library/', ''),
           omit_path_prefixes([TerminusDB, Library]),
           title('TerminusDB'),
           layout(top_to_bottom)
        ]
   ).

The last query assumes that [start] loads all the application files. If that's not the case, load any missing files first.",2019-11-23T16:56:50Z,pmoura,https://github.com/terminusdb/terminusdb/issues/17#issuecomment-557814320,"Meanwhile I have upload the diagrams for the dev branch at 6e079ec to:
https://logtalk.org/files/terminus-server-diagrams/
There are two sets of diagrams:

directory and file diagrams
entity and xref diagrams

All diagrams are linked to the source code. The zoom icon allows looking into a sub-diagram with details (e.g. going from directory to its files or from a module interface to its xref details). Most nodes and edges have live links to the directory, files, and file lines in the GitHub repos. Some entry points are:

https://logtalk.org/files/terminus-server-diagrams/terminus_server_directory_load_diagram.svg
https://logtalk.org/files/terminus-server-diagrams/terminus_server_directory_dependency_diagram.svg
https://logtalk.org/files/terminus-server-diagrams/terminus_server_entity_diagram.svg
https://logtalk.org/files/terminus-server-diagrams/terminus_server_xref_diagram.svg

The diagrams were generated using the query:
logtalk_load(diagrams(loader),
working_directory(TerminusStore,TerminusStore), getenv('HOME', Home), file_search_path(swi, SWIHome),
     atom_concat(SWIHome, '/library', Library),
     atom_concat(SWIHome, '/library/http', SWIhttp),
     atom_concat(SWIHome, '/library/pldoc', SWIpldoc),
     atom_concat(SWIHome, '/library/semweb', SWIsemweb),
     git_hash(Hash, []),
     atomic_list_concat(['https://github.com/terminusdb/terminus-server/tree/',Hash,'/'], Git),
     diagrams::rdirectory(terminus_server, '.', [
         title('terminus-server - open source model driven graph database for knowledge graph representation'),
        zoom(true),
        path_url_prefixes(TerminusStore, Git, ''),
        path_url_prefixes(SWIhttp, 'https://github.com/SWI-Prolog/packages-http/tree/master/', ''),
        path_url_prefixes(SWIpldoc, 'https://github.com/SWI-Prolog/packages-pldoc/tree/master/', ''),
        path_url_prefixes(SWIsemweb, 'https://github.com/SWI-Prolog/packages-semweb/tree/master/', ''),
        path_url_prefixes(Library, 'https://github.com/SWI-Prolog/swipl-devel/tree/master/library/', ''),
        omit_path_prefixes([TerminusStore, SWIhttp, SWIpldoc, SWIsemweb, Library, Home]),
        directory_paths(true),
        exclude_directories([examples, storage, 'terminus-schema', 'terminus-server-diagrams', tmp, utils]),
        layout(top_to_bottom)
    ]).
The query is run after loading a modified start.pl file where the single change is in its first line:
#!/usr/bin/env swilgt",meanwhile i have upload the diagram for the dev branch at 6e079ec to https//logtalkorg / file / terminus - server - diagrams/ there be two set of diagram directory and file diagram entity and xref diagram all diagram be link to the source code the zoom icon allow look into a sub - diagram with detail ( eg go from directory to -PRON- file or from a module interface to -PRON- xref detail ) most node and edge have live link to the directory file and file line in the github repos some entry point be https//logtalkorg / file / terminus - server - diagram / terminus_server_directory_load_diagramsvg https//logtalkorg / file / terminus - server - diagram / terminus_server_directory_dependency_diagramsvg https//logtalkorg / file / terminus - server - diagram / terminus_server_entity_diagramsvg https//logtalkorg / file / terminus - server - diagram / terminus_server_xref_diagramsvg the diagram be generate use the query logtalk_load(diagrams(loader ) working_directory(terminusstoreterminusstore ) getenv('home ' home ) file_search_path(swi swihome ) atom_concat(swihome ' /library ' library ) atom_concat(swihome ' /library / http ' swihttp ) atom_concat(swihome ' /library / pldoc ' swipldoc ) atom_concat(swihome ' /library / semweb ' swisemweb ) git_hash(hash [ ] ) atomic_list_concat(['https//githubcom / terminusdb / terminus - server / tree/'hash'/ ' ] git ) diagramsrdirectory(terminus_server '' [ title('terminus - server - open source model drive graph database for knowledge graph representation ' ) zoom(true ) path_url_prefixes(terminusstore git '' ) path_url_prefixes(swihttp ' https//githubcom / swi - prolog / package - http / tree / master/ ' '' ) path_url_prefixes(swipldoc ' https//githubcom / swi - prolog / package - pldoc / tree / master/ ' '' ) path_url_prefixes(swisemweb ' https//githubcom / swi - prolog / package - semweb / tree / master/ ' '' ) path_url_prefixes(library ' https//githubcom / swi - prolog / swipl - devel / tree / master / library/ ' '' ) omit_path_prefixes([terminusstore swihttp swipldoc swisemweb library home ] ) directory_paths(true ) exclude_directories([examples storage ' terminus - schema ' ' terminus - server - diagram ' tmp util ] ) layout(top_to_bottom ) ] ) the query be run after load a modified startpl file where the single change be in -PRON- first line # /usr / bin / env swilgt,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,17,2019-09-27T19:10:14Z,pmoura,Generate diagrams for the application,https://github.com/terminusdb/terminusdb/issues/17,"I cannot do this myself yet as I'm still missing the correct version of the serd dependency on my macOS laptop. But you may have fun generating diagrams for the application.
First, start Logtalk using either the swilgt script or the pack:
?- pack_install(logtalk).
...

?- use_module(library(logtalk)).
...

Then, load the application and try the next queries:
?- [start].
...

?- {diagrams(loader)}.
...

?- working_directory(TerminusDB, TerminusDB),
   file_search_path(swi, SWIHome),
   atom_concat(SWIHome, '/library', Library),
   git_hash(Hash, []),
   atomic_list_concat(['https://github.com/terminusdb/terminus-server/tree/',Hash,'/'], Git),
   diagrams::rdirectory(
       terminusdb,
       '.',
       [
           zoom(true),
           path_url_prefixes(TerminusDB, Git, ''),
           path_url_prefixes(Library, 'https://github.com/SWI-Prolog/swipl-devel/tree/master/library/', ''),
           omit_path_prefixes([TerminusDB, Library]),
           title('TerminusDB'),
           layout(top_to_bottom)
        ]
   ).

The last query assumes that [start] loads all the application files. If that's not the case, load any missing files first.",2019-11-28T12:39:57Z,rrooij,https://github.com/terminusdb/terminusdb/issues/17#issuecomment-559479529,"@pmoura Thanks a lot for the nice diagrams, they look great! I especially like the xref diagram as a quick way of showing people the current architecture.",@pmoura thank a lot for the nice diagram -PRON- look great i especially like the xref diagram as a quick way of show people the current architecture,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,17,2019-09-27T19:10:14Z,pmoura,Generate diagrams for the application,https://github.com/terminusdb/terminusdb/issues/17,"I cannot do this myself yet as I'm still missing the correct version of the serd dependency on my macOS laptop. But you may have fun generating diagrams for the application.
First, start Logtalk using either the swilgt script or the pack:
?- pack_install(logtalk).
...

?- use_module(library(logtalk)).
...

Then, load the application and try the next queries:
?- [start].
...

?- {diagrams(loader)}.
...

?- working_directory(TerminusDB, TerminusDB),
   file_search_path(swi, SWIHome),
   atom_concat(SWIHome, '/library', Library),
   git_hash(Hash, []),
   atomic_list_concat(['https://github.com/terminusdb/terminus-server/tree/',Hash,'/'], Git),
   diagrams::rdirectory(
       terminusdb,
       '.',
       [
           zoom(true),
           path_url_prefixes(TerminusDB, Git, ''),
           path_url_prefixes(Library, 'https://github.com/SWI-Prolog/swipl-devel/tree/master/library/', ''),
           omit_path_prefixes([TerminusDB, Library]),
           title('TerminusDB'),
           layout(top_to_bottom)
        ]
   ).

The last query assumes that [start] loads all the application files. If that's not the case, load any missing files first.",2019-11-28T14:55:13Z,pmoura,https://github.com/terminusdb/terminusdb/issues/17#issuecomment-559523562,"As you noticed, the nodes and edges URLs link to exact locations in the source code at GitHub. That works by linking to a specific commit (as you can see in the query above that generates the diagrams). As a consequence, the locations will become invalid as new commits are pushed. But it's easy to define an automatic workflow that updates the diagrams everytime a commit is pushed (e.g. to the dev branch) and that publishes the diagrams (e.g. to the gh-pages if you're not using it so that you can browse the diagrams at https://terminusdb.github.io/terminus-server/. Let me know if you want me to submit a pull request with the workflow (it uses GitHub actions and thus you'll need to enable both GitHub Actions and GitHub Pages in the repo settings).",as -PRON- notice the node and edge urls link to exact location in the source code at github that work by link to a specific commit ( as -PRON- can see in the query above that generate the diagram ) as a consequence the location will become invalid as new commit be push but -PRON- be easy to define an automatic workflow that update the diagram everytime a commit be push ( eg to the dev branch ) and that publish the diagram ( eg to the gh - page if -PRON- be not use -PRON- so that -PRON- can browse the diagram at https//terminusdbgithubio / terminus - server/ let -PRON- know if -PRON- want -PRON- to submit a pull request with the workflow ( -PRON- use github action and thus -PRON- will need to enable both github action and github page in the repo setting ),0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,17,2019-09-27T19:10:14Z,pmoura,Generate diagrams for the application,https://github.com/terminusdb/terminusdb/issues/17,"I cannot do this myself yet as I'm still missing the correct version of the serd dependency on my macOS laptop. But you may have fun generating diagrams for the application.
First, start Logtalk using either the swilgt script or the pack:
?- pack_install(logtalk).
...

?- use_module(library(logtalk)).
...

Then, load the application and try the next queries:
?- [start].
...

?- {diagrams(loader)}.
...

?- working_directory(TerminusDB, TerminusDB),
   file_search_path(swi, SWIHome),
   atom_concat(SWIHome, '/library', Library),
   git_hash(Hash, []),
   atomic_list_concat(['https://github.com/terminusdb/terminus-server/tree/',Hash,'/'], Git),
   diagrams::rdirectory(
       terminusdb,
       '.',
       [
           zoom(true),
           path_url_prefixes(TerminusDB, Git, ''),
           path_url_prefixes(Library, 'https://github.com/SWI-Prolog/swipl-devel/tree/master/library/', ''),
           omit_path_prefixes([TerminusDB, Library]),
           title('TerminusDB'),
           layout(top_to_bottom)
        ]
   ).

The last query assumes that [start] loads all the application files. If that's not the case, load any missing files first.",2021-03-08T15:23:35Z,matko,https://github.com/terminusdb/terminusdb/issues/17#issuecomment-792828620,Closing due to age of issue.,close due to age of issue,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,18,2019-09-28T09:50:19Z,pmoura,Possible bug in setof/3 call in frame module,https://github.com/terminusdb/terminusdb/issues/18,"In the setof/3 call in the following clause:
https://github.com/terminusdb/terminus-server/blob/82871e8522e9cf993b0f53cad8ada3994a007901/library/frame.pl#L730-L750
the Logtalk linter reports:
*     Suspicious call: A^(inferredEdge(B,C,A,D),(schema:document(A,D)->(E=<1->F=[A];G is E-1,document_object(A,D,G,H),H=[I,J|F]);realiser(A,K,D,E,F))) in setof/3 goal contains singleton variables [I,J]
*       while compiling object frame
*       in file /Users/pmoura/terminus-server/library/frame.pl between lines 730-750
The reported variables are _Type  and _Id  in the sub-goal:
Object = [_Type,_Id|New_Realiser]

This may be a false positive as in #14. Still using complex goals in all-solutions predicate calls is not ideal for performance and readability. Here, a suitably named auxiliary predicate would improve the code and simplify the setof/3 call by also eliminating the need of existentially qualify any variables.",2019-09-28T11:02:27Z,GavinMendelGleason,https://github.com/terminusdb/terminusdb/issues/18#issuecomment-536176929,Indeed this is ugly and needs a rewrite. I'm going to have a go at a complete overhaul of frames after release.,indeed this be ugly and need a rewrite -PRON- be go to have a go at a complete overhaul of frame after release,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,19,2019-09-28T11:01:33Z,GavinMendelGleason,GET string Auth obviously wrong,https://github.com/terminusdb/terminusdb/issues/19,"Needs to be in headers as:
Authorization: Basic 
So that HTTPS has an opportunity to hide the credentials.",2019-09-28T13:09:01Z,GavinMendelGleason,https://github.com/terminusdb/terminusdb/issues/19#issuecomment-536188886,Now using basic auth.,now use basic auth,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,20,2019-09-30T19:00:42Z,GavinMendelGleason,Capabilities analyser for woql,https://github.com/terminusdb/terminusdb/issues/20,"WOQL queries need to be analysed to assess which capabilities need to be present for execution, and then checked against auth_action_scope/3.",2020-10-01T15:46:59Z,GavinMendelGleason,https://github.com/terminusdb/terminusdb/issues/20#issuecomment-702226417,We are now checking capabilities pervasively.,-PRON- be now check capability pervasively,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,23,2019-09-30T23:18:06Z,GavinMendelGleason,WOQL transaction failures with nonsense graphs,https://github.com/terminusdb/terminusdb/issues/23,WOQL updates suffer from silent failures when the graphs referred to for update are nonsense. This should instead return a vio witness.,2020-10-01T15:06:52Z,GavinMendelGleason,https://github.com/terminusdb/terminusdb/issues/23#issuecomment-702200318,This is now throwing errors in all known cases.,this be now throw error in all know case,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,24,2019-10-01T12:13:04Z,GavinMendelGleason,Schema not checking labels or comment datatype,https://github.com/terminusdb/terminusdb/issues/24,Labels and comments types not being checked for type correctness on schema update.,2019-10-01T16:20:11Z,GavinMendelGleason,https://github.com/terminusdb/terminusdb/issues/24#issuecomment-537113929,This no longer appears to be a problem.,this no long appear to be a problem,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,27,2019-10-01T14:56:04Z,padraic7a,Text missing / truncated from the WOQL section in README.md,https://github.com/terminusdb/terminusdb/issues/27,"""The syntax itself is in JSON-LD, making the syntax a native data-structure in JavaScript, Python, Prolog or any other language which has the"" .....",2019-10-01T16:22:12Z,GavinMendelGleason,https://github.com/terminusdb/terminusdb/issues/27#issuecomment-537114661,Fixed.,fix,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,28,2019-10-06T12:53:13Z,KarlLevik,Build instructions for Fedora/Redhat,https://github.com/terminusdb/terminusdb/issues/28,"BUILD.md has instructions for Debian, but not Fedora.
So for fun I tried building on Fedora release 30 (Thirty), but no luck. I did this (as root):
dnf install pl
swipl
pack_install('https://github.com/GavinMendelGleason/hdt.git').


Create directory for packages
(1) * /root/lib/swipl/pack
(2)   /usr/lib64/swipl-8.0.3/pack
(3)   Cancel
Your choice?
% Cloning into '/usr/lib64/swipl-8.0.3/pack/hdt'...
Verify package status (anonymously)
at ""https://www.swi-prolog.org/pack/query"" Y/n?
% Contacting server at https://www.swi-prolog.org/pack/query ...
ERROR: Server reported the following error:
ERROR: No permission to register pack(hdt) `'https://github.com/GavinMendelGleason/hdt.git''
Package:                hdt
Title:                  Access RDF HDT files
Installed version:      0.5
Author:                 Jan Wielemaker J.Wielemaker@vu.nl
Maintainer:             Jan Wielemaker J.Wielemaker@vu.nl
Packager:               Jan Wielemaker J.Wielemaker@vu.nl
Home page:              https://github.com/GavinMendelGleason/hdt
Download URL:           https://github.com/GavinMendelGleason/hdt.git
Run post installation scripts for pack ""hdt"" Y/n?
% git submodule update --init
% Submodule 'hdt-cpp' (https://github.com/rdfhdt/hdt-cpp.git) registered for path 'hdt-cpp'
% Cloning into '/usr/lib64/swipl-8.0.3/pack/hdt/hdt-cpp'...
% Submodule path 'hdt-cpp': checked out 'a8aafe32e0209a50a7671ddd5376b5c0d6934496'
% cd hdt-cpp && ./autogen.sh
% aclocal: installing 'm4/pkg.m4' from '/usr/share/aclocal/pkg.m4'
% configure.ac:16: installing 'build/ar-lib'
% configure.ac:12: installing 'build/compile'
% configure.ac:5: installing 'build/install-sh'
% configure.ac:5: installing 'build/missing'
ERROR: libcds/Makefile.am:8: error: Libtool library used but 'LIBTOOL' is undefined
ERROR: libcds/Makefile.am:8:   The usual way to define 'LIBTOOL' is to add 'LT_INIT'
ERROR: libcds/Makefile.am:8:   to 'configure.ac' and run 'aclocal' and 'autoconf' again.
ERROR: libcds/Makefile.am:8:   If 'LT_INIT' is in 'configure.ac', make sure
ERROR: libcds/Makefile.am:8:   its definition is in aclocal's search path.
% libcds/Makefile.am: installing 'build/depcomp'
% parallel-tests: installing 'build/test-driver'
ERROR: libhdt/Makefile.am:23: error: Libtool library used but 'LIBTOOL' is undefined
ERROR: libhdt/Makefile.am:23:   The usual way to define 'LIBTOOL' is to add 'LT_INIT'
ERROR: libhdt/Makefile.am:23:   to 'configure.ac' and run 'aclocal' and 'autoconf' again.
ERROR: libhdt/Makefile.am:23:   If 'LT_INIT' is in 'configure.ac', make sure
ERROR: libhdt/Makefile.am:23:   its definition is in aclocal's search path.
% autoreconf: automake failed with exit status: 1
% make: *** [Makefile:23: hdt-cpp/libhdt/.make-senitel] Error 1
ERROR: Process ""process(path(make),[all])"": exit status: 2
ERROR: In:
ERROR:   [18] throw(error(process_error(...,...),_28682))
ERROR:   [16] '$apply':forall(prolog_pack:member(all,...),prolog_pack:run_process(...,...,...)) at /usr/lib64/swipl-8.0.3/boot/apply.pl:52
ERROR:   [12] prolog_pack:pack_post_install(hdt,'/usr/lib64/swipl-8.0.3/pack/hdt',[git(true),...|...]) at /usr/lib64/swipl-8.0.3/library/prolog_pack.pl:1017
ERROR:    [7] 
ERROR:
ERROR: Note: some frames are missing due to last-call optimization.
ERROR: Re-run your program in debug mode (:- debug.) to get more detail.",2019-10-06T13:09:48Z,rrooij,https://github.com/terminusdb/terminusdb/issues/28#issuecomment-538745724,"Did you install hdt-cpp and its requirements?
See: https://github.com/rdfhdt/hdt-cpp#prerequisites
Nevertheless, build instructions for other distributions are a good idea.",do -PRON- install hdt - cpp and -PRON- requirement see https//githubcom / rdfhdt / hdt - cpp#prerequisite nevertheless build instruction for other distribution be a good idea,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,28,2019-10-06T12:53:13Z,KarlLevik,Build instructions for Fedora/Redhat,https://github.com/terminusdb/terminusdb/issues/28,"BUILD.md has instructions for Debian, but not Fedora.
So for fun I tried building on Fedora release 30 (Thirty), but no luck. I did this (as root):
dnf install pl
swipl
pack_install('https://github.com/GavinMendelGleason/hdt.git').


Create directory for packages
(1) * /root/lib/swipl/pack
(2)   /usr/lib64/swipl-8.0.3/pack
(3)   Cancel
Your choice?
% Cloning into '/usr/lib64/swipl-8.0.3/pack/hdt'...
Verify package status (anonymously)
at ""https://www.swi-prolog.org/pack/query"" Y/n?
% Contacting server at https://www.swi-prolog.org/pack/query ...
ERROR: Server reported the following error:
ERROR: No permission to register pack(hdt) `'https://github.com/GavinMendelGleason/hdt.git''
Package:                hdt
Title:                  Access RDF HDT files
Installed version:      0.5
Author:                 Jan Wielemaker J.Wielemaker@vu.nl
Maintainer:             Jan Wielemaker J.Wielemaker@vu.nl
Packager:               Jan Wielemaker J.Wielemaker@vu.nl
Home page:              https://github.com/GavinMendelGleason/hdt
Download URL:           https://github.com/GavinMendelGleason/hdt.git
Run post installation scripts for pack ""hdt"" Y/n?
% git submodule update --init
% Submodule 'hdt-cpp' (https://github.com/rdfhdt/hdt-cpp.git) registered for path 'hdt-cpp'
% Cloning into '/usr/lib64/swipl-8.0.3/pack/hdt/hdt-cpp'...
% Submodule path 'hdt-cpp': checked out 'a8aafe32e0209a50a7671ddd5376b5c0d6934496'
% cd hdt-cpp && ./autogen.sh
% aclocal: installing 'm4/pkg.m4' from '/usr/share/aclocal/pkg.m4'
% configure.ac:16: installing 'build/ar-lib'
% configure.ac:12: installing 'build/compile'
% configure.ac:5: installing 'build/install-sh'
% configure.ac:5: installing 'build/missing'
ERROR: libcds/Makefile.am:8: error: Libtool library used but 'LIBTOOL' is undefined
ERROR: libcds/Makefile.am:8:   The usual way to define 'LIBTOOL' is to add 'LT_INIT'
ERROR: libcds/Makefile.am:8:   to 'configure.ac' and run 'aclocal' and 'autoconf' again.
ERROR: libcds/Makefile.am:8:   If 'LT_INIT' is in 'configure.ac', make sure
ERROR: libcds/Makefile.am:8:   its definition is in aclocal's search path.
% libcds/Makefile.am: installing 'build/depcomp'
% parallel-tests: installing 'build/test-driver'
ERROR: libhdt/Makefile.am:23: error: Libtool library used but 'LIBTOOL' is undefined
ERROR: libhdt/Makefile.am:23:   The usual way to define 'LIBTOOL' is to add 'LT_INIT'
ERROR: libhdt/Makefile.am:23:   to 'configure.ac' and run 'aclocal' and 'autoconf' again.
ERROR: libhdt/Makefile.am:23:   If 'LT_INIT' is in 'configure.ac', make sure
ERROR: libhdt/Makefile.am:23:   its definition is in aclocal's search path.
% autoreconf: automake failed with exit status: 1
% make: *** [Makefile:23: hdt-cpp/libhdt/.make-senitel] Error 1
ERROR: Process ""process(path(make),[all])"": exit status: 2
ERROR: In:
ERROR:   [18] throw(error(process_error(...,...),_28682))
ERROR:   [16] '$apply':forall(prolog_pack:member(all,...),prolog_pack:run_process(...,...,...)) at /usr/lib64/swipl-8.0.3/boot/apply.pl:52
ERROR:   [12] prolog_pack:pack_post_install(hdt,'/usr/lib64/swipl-8.0.3/pack/hdt',[git(true),...|...]) at /usr/lib64/swipl-8.0.3/library/prolog_pack.pl:1017
ERROR:    [7] 
ERROR:
ERROR: Note: some frames are missing due to last-call optimization.
ERROR: Re-run your program in debug mode (:- debug.) to get more detail.",2019-10-06T16:40:42Z,KarlLevik,https://github.com/terminusdb/terminusdb/issues/28#issuecomment-538764651,"Ah, I didn't do that - I just followed the order of the instructions in the BUILD.md file, and the HDT library is next. OK, so I've now installed that. pack_install('https://github.com/GavinMendelGleason/hdt.git'). now gets much further, but stops after a while with this error:

% gcc  -fPIC -pthread -I""/usr/lib64/swipl-8.0.3/include"" -Ihdt-cpp/libhdt/include -g -c -o c/hdt4pl.o c/hdt4pl.cpp
ERROR: c/hdt4pl.cpp:36:10: fatal error: SWI-Stream.h: No such file or directory
ERROR:    36 | #include <SWI-Stream.h>
ERROR:       |          ^~~~~~~~~~~~~~
ERROR: compilation terminated.","ah i do not do that - i just follow the order of the instruction in the buildmd file and the hdt library be next ok so -PRON- have now instal that pack_install('https//githubcom / gavinmendelgleason / hdtgit ' ) now get much further but stop after a while with this error % gcc -fpic -pthread -i""/usr / lib64 / swipl-803 / include "" -ihdt - cpp / libhdt / include -g -c -o c / hdt4plo c / hdt4plcpp error c / hdt4plcpp3610 fatal error swi - streamh no such file or directory error 36 | # include < swi - streamh > error | ^~~~~~~~~~~~~~ error compilation terminate",0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,28,2019-10-06T12:53:13Z,KarlLevik,Build instructions for Fedora/Redhat,https://github.com/terminusdb/terminusdb/issues/28,"BUILD.md has instructions for Debian, but not Fedora.
So for fun I tried building on Fedora release 30 (Thirty), but no luck. I did this (as root):
dnf install pl
swipl
pack_install('https://github.com/GavinMendelGleason/hdt.git').


Create directory for packages
(1) * /root/lib/swipl/pack
(2)   /usr/lib64/swipl-8.0.3/pack
(3)   Cancel
Your choice?
% Cloning into '/usr/lib64/swipl-8.0.3/pack/hdt'...
Verify package status (anonymously)
at ""https://www.swi-prolog.org/pack/query"" Y/n?
% Contacting server at https://www.swi-prolog.org/pack/query ...
ERROR: Server reported the following error:
ERROR: No permission to register pack(hdt) `'https://github.com/GavinMendelGleason/hdt.git''
Package:                hdt
Title:                  Access RDF HDT files
Installed version:      0.5
Author:                 Jan Wielemaker J.Wielemaker@vu.nl
Maintainer:             Jan Wielemaker J.Wielemaker@vu.nl
Packager:               Jan Wielemaker J.Wielemaker@vu.nl
Home page:              https://github.com/GavinMendelGleason/hdt
Download URL:           https://github.com/GavinMendelGleason/hdt.git
Run post installation scripts for pack ""hdt"" Y/n?
% git submodule update --init
% Submodule 'hdt-cpp' (https://github.com/rdfhdt/hdt-cpp.git) registered for path 'hdt-cpp'
% Cloning into '/usr/lib64/swipl-8.0.3/pack/hdt/hdt-cpp'...
% Submodule path 'hdt-cpp': checked out 'a8aafe32e0209a50a7671ddd5376b5c0d6934496'
% cd hdt-cpp && ./autogen.sh
% aclocal: installing 'm4/pkg.m4' from '/usr/share/aclocal/pkg.m4'
% configure.ac:16: installing 'build/ar-lib'
% configure.ac:12: installing 'build/compile'
% configure.ac:5: installing 'build/install-sh'
% configure.ac:5: installing 'build/missing'
ERROR: libcds/Makefile.am:8: error: Libtool library used but 'LIBTOOL' is undefined
ERROR: libcds/Makefile.am:8:   The usual way to define 'LIBTOOL' is to add 'LT_INIT'
ERROR: libcds/Makefile.am:8:   to 'configure.ac' and run 'aclocal' and 'autoconf' again.
ERROR: libcds/Makefile.am:8:   If 'LT_INIT' is in 'configure.ac', make sure
ERROR: libcds/Makefile.am:8:   its definition is in aclocal's search path.
% libcds/Makefile.am: installing 'build/depcomp'
% parallel-tests: installing 'build/test-driver'
ERROR: libhdt/Makefile.am:23: error: Libtool library used but 'LIBTOOL' is undefined
ERROR: libhdt/Makefile.am:23:   The usual way to define 'LIBTOOL' is to add 'LT_INIT'
ERROR: libhdt/Makefile.am:23:   to 'configure.ac' and run 'aclocal' and 'autoconf' again.
ERROR: libhdt/Makefile.am:23:   If 'LT_INIT' is in 'configure.ac', make sure
ERROR: libhdt/Makefile.am:23:   its definition is in aclocal's search path.
% autoreconf: automake failed with exit status: 1
% make: *** [Makefile:23: hdt-cpp/libhdt/.make-senitel] Error 1
ERROR: Process ""process(path(make),[all])"": exit status: 2
ERROR: In:
ERROR:   [18] throw(error(process_error(...,...),_28682))
ERROR:   [16] '$apply':forall(prolog_pack:member(all,...),prolog_pack:run_process(...,...,...)) at /usr/lib64/swipl-8.0.3/boot/apply.pl:52
ERROR:   [12] prolog_pack:pack_post_install(hdt,'/usr/lib64/swipl-8.0.3/pack/hdt',[git(true),...|...]) at /usr/lib64/swipl-8.0.3/library/prolog_pack.pl:1017
ERROR:    [7] 
ERROR:
ERROR: Note: some frames are missing due to last-call optimization.
ERROR: Re-run your program in debug mode (:- debug.) to get more detail.",2019-10-06T16:52:08Z,GavinMendelGleason,https://github.com/terminusdb/terminusdb/issues/28#issuecomment-538765663,"Oh dear! Both good things for us to know! I'll have to edit the README.md to reflect this.
I'd imagine the problem is caused by needing header files for swi-prolog. If there is a dev package you can install for RedHat, that will probably fix it. Otherwise you'll have to get the swipl source: https://github.com/SWI-Prolog/swipl
Easiest way to get the correct location for header files is probably to build and install swipl unfortunately.
We are actively moving the back-end storage to our own terminus-store which should simplify the entire build process significantly.",oh dear both good thing for -PRON- to know -PRON- will have to edit the readmemd to reflect this -PRON- 'd imagine the problem be cause by need header file for swi - prolog if there be a dev package -PRON- can install for redhat that will probably fix -PRON- otherwise -PRON- will have to get the swipl source https//githubcom / swi - prolog / swipl easy way to get the correct location for header file be probably to build and install swipl unfortunately -PRON- be actively move the back - end storage to -PRON- own terminus - store which should simplify the entire build process significantly,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,28,2019-10-06T12:53:13Z,KarlLevik,Build instructions for Fedora/Redhat,https://github.com/terminusdb/terminusdb/issues/28,"BUILD.md has instructions for Debian, but not Fedora.
So for fun I tried building on Fedora release 30 (Thirty), but no luck. I did this (as root):
dnf install pl
swipl
pack_install('https://github.com/GavinMendelGleason/hdt.git').


Create directory for packages
(1) * /root/lib/swipl/pack
(2)   /usr/lib64/swipl-8.0.3/pack
(3)   Cancel
Your choice?
% Cloning into '/usr/lib64/swipl-8.0.3/pack/hdt'...
Verify package status (anonymously)
at ""https://www.swi-prolog.org/pack/query"" Y/n?
% Contacting server at https://www.swi-prolog.org/pack/query ...
ERROR: Server reported the following error:
ERROR: No permission to register pack(hdt) `'https://github.com/GavinMendelGleason/hdt.git''
Package:                hdt
Title:                  Access RDF HDT files
Installed version:      0.5
Author:                 Jan Wielemaker J.Wielemaker@vu.nl
Maintainer:             Jan Wielemaker J.Wielemaker@vu.nl
Packager:               Jan Wielemaker J.Wielemaker@vu.nl
Home page:              https://github.com/GavinMendelGleason/hdt
Download URL:           https://github.com/GavinMendelGleason/hdt.git
Run post installation scripts for pack ""hdt"" Y/n?
% git submodule update --init
% Submodule 'hdt-cpp' (https://github.com/rdfhdt/hdt-cpp.git) registered for path 'hdt-cpp'
% Cloning into '/usr/lib64/swipl-8.0.3/pack/hdt/hdt-cpp'...
% Submodule path 'hdt-cpp': checked out 'a8aafe32e0209a50a7671ddd5376b5c0d6934496'
% cd hdt-cpp && ./autogen.sh
% aclocal: installing 'm4/pkg.m4' from '/usr/share/aclocal/pkg.m4'
% configure.ac:16: installing 'build/ar-lib'
% configure.ac:12: installing 'build/compile'
% configure.ac:5: installing 'build/install-sh'
% configure.ac:5: installing 'build/missing'
ERROR: libcds/Makefile.am:8: error: Libtool library used but 'LIBTOOL' is undefined
ERROR: libcds/Makefile.am:8:   The usual way to define 'LIBTOOL' is to add 'LT_INIT'
ERROR: libcds/Makefile.am:8:   to 'configure.ac' and run 'aclocal' and 'autoconf' again.
ERROR: libcds/Makefile.am:8:   If 'LT_INIT' is in 'configure.ac', make sure
ERROR: libcds/Makefile.am:8:   its definition is in aclocal's search path.
% libcds/Makefile.am: installing 'build/depcomp'
% parallel-tests: installing 'build/test-driver'
ERROR: libhdt/Makefile.am:23: error: Libtool library used but 'LIBTOOL' is undefined
ERROR: libhdt/Makefile.am:23:   The usual way to define 'LIBTOOL' is to add 'LT_INIT'
ERROR: libhdt/Makefile.am:23:   to 'configure.ac' and run 'aclocal' and 'autoconf' again.
ERROR: libhdt/Makefile.am:23:   If 'LT_INIT' is in 'configure.ac', make sure
ERROR: libhdt/Makefile.am:23:   its definition is in aclocal's search path.
% autoreconf: automake failed with exit status: 1
% make: *** [Makefile:23: hdt-cpp/libhdt/.make-senitel] Error 1
ERROR: Process ""process(path(make),[all])"": exit status: 2
ERROR: In:
ERROR:   [18] throw(error(process_error(...,...),_28682))
ERROR:   [16] '$apply':forall(prolog_pack:member(all,...),prolog_pack:run_process(...,...,...)) at /usr/lib64/swipl-8.0.3/boot/apply.pl:52
ERROR:   [12] prolog_pack:pack_post_install(hdt,'/usr/lib64/swipl-8.0.3/pack/hdt',[git(true),...|...]) at /usr/lib64/swipl-8.0.3/library/prolog_pack.pl:1017
ERROR:    [7] 
ERROR:
ERROR: Note: some frames are missing due to last-call optimization.
ERROR: Re-run your program in debug mode (:- debug.) to get more detail.",2019-10-06T17:43:45Z,rrooij,https://github.com/terminusdb/terminusdb/issues/28#issuecomment-538770142,"@KarlLevik You probably need to install pl-devel as well.
https://apps.fedoraproject.org/packages/pl-devel
And indeed as Gavin said, we hope to make the process a lot easier by moving to our own storage backend written in Rust.",@karllevik -PRON- probably need to install pl - devel as well https//appsfedoraprojectorg / package / pl - devel and indeed as gavin say -PRON- hope to make the process a lot easy by move to -PRON- own storage backend write in rust,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,28,2019-10-06T12:53:13Z,KarlLevik,Build instructions for Fedora/Redhat,https://github.com/terminusdb/terminusdb/issues/28,"BUILD.md has instructions for Debian, but not Fedora.
So for fun I tried building on Fedora release 30 (Thirty), but no luck. I did this (as root):
dnf install pl
swipl
pack_install('https://github.com/GavinMendelGleason/hdt.git').


Create directory for packages
(1) * /root/lib/swipl/pack
(2)   /usr/lib64/swipl-8.0.3/pack
(3)   Cancel
Your choice?
% Cloning into '/usr/lib64/swipl-8.0.3/pack/hdt'...
Verify package status (anonymously)
at ""https://www.swi-prolog.org/pack/query"" Y/n?
% Contacting server at https://www.swi-prolog.org/pack/query ...
ERROR: Server reported the following error:
ERROR: No permission to register pack(hdt) `'https://github.com/GavinMendelGleason/hdt.git''
Package:                hdt
Title:                  Access RDF HDT files
Installed version:      0.5
Author:                 Jan Wielemaker J.Wielemaker@vu.nl
Maintainer:             Jan Wielemaker J.Wielemaker@vu.nl
Packager:               Jan Wielemaker J.Wielemaker@vu.nl
Home page:              https://github.com/GavinMendelGleason/hdt
Download URL:           https://github.com/GavinMendelGleason/hdt.git
Run post installation scripts for pack ""hdt"" Y/n?
% git submodule update --init
% Submodule 'hdt-cpp' (https://github.com/rdfhdt/hdt-cpp.git) registered for path 'hdt-cpp'
% Cloning into '/usr/lib64/swipl-8.0.3/pack/hdt/hdt-cpp'...
% Submodule path 'hdt-cpp': checked out 'a8aafe32e0209a50a7671ddd5376b5c0d6934496'
% cd hdt-cpp && ./autogen.sh
% aclocal: installing 'm4/pkg.m4' from '/usr/share/aclocal/pkg.m4'
% configure.ac:16: installing 'build/ar-lib'
% configure.ac:12: installing 'build/compile'
% configure.ac:5: installing 'build/install-sh'
% configure.ac:5: installing 'build/missing'
ERROR: libcds/Makefile.am:8: error: Libtool library used but 'LIBTOOL' is undefined
ERROR: libcds/Makefile.am:8:   The usual way to define 'LIBTOOL' is to add 'LT_INIT'
ERROR: libcds/Makefile.am:8:   to 'configure.ac' and run 'aclocal' and 'autoconf' again.
ERROR: libcds/Makefile.am:8:   If 'LT_INIT' is in 'configure.ac', make sure
ERROR: libcds/Makefile.am:8:   its definition is in aclocal's search path.
% libcds/Makefile.am: installing 'build/depcomp'
% parallel-tests: installing 'build/test-driver'
ERROR: libhdt/Makefile.am:23: error: Libtool library used but 'LIBTOOL' is undefined
ERROR: libhdt/Makefile.am:23:   The usual way to define 'LIBTOOL' is to add 'LT_INIT'
ERROR: libhdt/Makefile.am:23:   to 'configure.ac' and run 'aclocal' and 'autoconf' again.
ERROR: libhdt/Makefile.am:23:   If 'LT_INIT' is in 'configure.ac', make sure
ERROR: libhdt/Makefile.am:23:   its definition is in aclocal's search path.
% autoreconf: automake failed with exit status: 1
% make: *** [Makefile:23: hdt-cpp/libhdt/.make-senitel] Error 1
ERROR: Process ""process(path(make),[all])"": exit status: 2
ERROR: In:
ERROR:   [18] throw(error(process_error(...,...),_28682))
ERROR:   [16] '$apply':forall(prolog_pack:member(all,...),prolog_pack:run_process(...,...,...)) at /usr/lib64/swipl-8.0.3/boot/apply.pl:52
ERROR:   [12] prolog_pack:pack_post_install(hdt,'/usr/lib64/swipl-8.0.3/pack/hdt',[git(true),...|...]) at /usr/lib64/swipl-8.0.3/library/prolog_pack.pl:1017
ERROR:    [7] 
ERROR:
ERROR: Note: some frames are missing due to last-call optimization.
ERROR: Re-run your program in debug mode (:- debug.) to get more detail.",2019-10-06T20:56:37Z,KarlLevik,https://github.com/terminusdb/terminusdb/issues/28#issuecomment-538787312,"Yes, it seems dnf install pl-devel does the trick!
I've run into some other issues, but that's probably for another ticket.",yes -PRON- seem dnf install pl - devel do the trick -PRON- have run into some other issue but that be probably for another ticket,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,28,2019-10-06T12:53:13Z,KarlLevik,Build instructions for Fedora/Redhat,https://github.com/terminusdb/terminusdb/issues/28,"BUILD.md has instructions for Debian, but not Fedora.
So for fun I tried building on Fedora release 30 (Thirty), but no luck. I did this (as root):
dnf install pl
swipl
pack_install('https://github.com/GavinMendelGleason/hdt.git').


Create directory for packages
(1) * /root/lib/swipl/pack
(2)   /usr/lib64/swipl-8.0.3/pack
(3)   Cancel
Your choice?
% Cloning into '/usr/lib64/swipl-8.0.3/pack/hdt'...
Verify package status (anonymously)
at ""https://www.swi-prolog.org/pack/query"" Y/n?
% Contacting server at https://www.swi-prolog.org/pack/query ...
ERROR: Server reported the following error:
ERROR: No permission to register pack(hdt) `'https://github.com/GavinMendelGleason/hdt.git''
Package:                hdt
Title:                  Access RDF HDT files
Installed version:      0.5
Author:                 Jan Wielemaker J.Wielemaker@vu.nl
Maintainer:             Jan Wielemaker J.Wielemaker@vu.nl
Packager:               Jan Wielemaker J.Wielemaker@vu.nl
Home page:              https://github.com/GavinMendelGleason/hdt
Download URL:           https://github.com/GavinMendelGleason/hdt.git
Run post installation scripts for pack ""hdt"" Y/n?
% git submodule update --init
% Submodule 'hdt-cpp' (https://github.com/rdfhdt/hdt-cpp.git) registered for path 'hdt-cpp'
% Cloning into '/usr/lib64/swipl-8.0.3/pack/hdt/hdt-cpp'...
% Submodule path 'hdt-cpp': checked out 'a8aafe32e0209a50a7671ddd5376b5c0d6934496'
% cd hdt-cpp && ./autogen.sh
% aclocal: installing 'm4/pkg.m4' from '/usr/share/aclocal/pkg.m4'
% configure.ac:16: installing 'build/ar-lib'
% configure.ac:12: installing 'build/compile'
% configure.ac:5: installing 'build/install-sh'
% configure.ac:5: installing 'build/missing'
ERROR: libcds/Makefile.am:8: error: Libtool library used but 'LIBTOOL' is undefined
ERROR: libcds/Makefile.am:8:   The usual way to define 'LIBTOOL' is to add 'LT_INIT'
ERROR: libcds/Makefile.am:8:   to 'configure.ac' and run 'aclocal' and 'autoconf' again.
ERROR: libcds/Makefile.am:8:   If 'LT_INIT' is in 'configure.ac', make sure
ERROR: libcds/Makefile.am:8:   its definition is in aclocal's search path.
% libcds/Makefile.am: installing 'build/depcomp'
% parallel-tests: installing 'build/test-driver'
ERROR: libhdt/Makefile.am:23: error: Libtool library used but 'LIBTOOL' is undefined
ERROR: libhdt/Makefile.am:23:   The usual way to define 'LIBTOOL' is to add 'LT_INIT'
ERROR: libhdt/Makefile.am:23:   to 'configure.ac' and run 'aclocal' and 'autoconf' again.
ERROR: libhdt/Makefile.am:23:   If 'LT_INIT' is in 'configure.ac', make sure
ERROR: libhdt/Makefile.am:23:   its definition is in aclocal's search path.
% autoreconf: automake failed with exit status: 1
% make: *** [Makefile:23: hdt-cpp/libhdt/.make-senitel] Error 1
ERROR: Process ""process(path(make),[all])"": exit status: 2
ERROR: In:
ERROR:   [18] throw(error(process_error(...,...),_28682))
ERROR:   [16] '$apply':forall(prolog_pack:member(all,...),prolog_pack:run_process(...,...,...)) at /usr/lib64/swipl-8.0.3/boot/apply.pl:52
ERROR:   [12] prolog_pack:pack_post_install(hdt,'/usr/lib64/swipl-8.0.3/pack/hdt',[git(true),...|...]) at /usr/lib64/swipl-8.0.3/library/prolog_pack.pl:1017
ERROR:    [7] 
ERROR:
ERROR: Note: some frames are missing due to last-call optimization.
ERROR: Re-run your program in debug mode (:- debug.) to get more detail.",2019-10-07T08:12:30Z,rrooij,https://github.com/terminusdb/terminusdb/issues/28#issuecomment-538887390,"I have added Fedora instructions in the dev branch:
https://github.com/terminusdb/terminus-server/blob/dev/BUILD.md#fedora-or-red-hat
Do you think they are sufficient @KarlLevik ?",i have add fedora instruction in the dev branch https//githubcom / terminusdb / terminus - server / blob / dev / buildmd#fedora - or - red - hat do -PRON- think -PRON- be sufficient @karllevik,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,29,2019-10-08T11:46:08Z,rrooij,Editing documents on the master database not possible,https://github.com/terminusdb/terminusdb/issues/29,"Editing a document on the master database is currently not possible and gives the error:
Error: API Error terminus:message Unable to guess a valid document store for database: database('http://localhost:6363/terminus',['http://localhost:6363/terminus/document'],['http://localhost:6363/terminus/inference'],['http://localhost:6363/terminus/schema'],[],[]) terminus:status terminus:failure Code: 404, url: http://localhost:6363/terminus/document/server

Steps to reproduce:

Run start.pl
Go to the dashboard on localhost:6363/dashboard
Find the document id server and change the comment in something else
Try to press save.",2019-10-08T13:59:50Z,rrooij,https://github.com/terminusdb/terminusdb/issues/29#issuecomment-539527485,"database_record_instance_list in library/database.pl seems to be the culprit.
?- database:database_record_instance_list('http://localhost:6363/testdb', Instances).
Instances = ['http://localhost:6363/testdb/document'] .

?- database:database_record_instance_list('http://localhost:6363/terminus', Instances).
Instances = [] .",database_record_instance_list in library / databasepl seem to be the culprit - databasedatabase_record_instance_list('http//localhost6363 / testdb ' instance ) instance = [ ' http//localhost6363 / testdb / document ' ] - databasedatabase_record_instance_list('http//localhost6363 / terminus ' instance ) instance = [ ],0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,29,2019-10-08T11:46:08Z,rrooij,Editing documents on the master database not possible,https://github.com/terminusdb/terminusdb/issues/29,"Editing a document on the master database is currently not possible and gives the error:
Error: API Error terminus:message Unable to guess a valid document store for database: database('http://localhost:6363/terminus',['http://localhost:6363/terminus/document'],['http://localhost:6363/terminus/inference'],['http://localhost:6363/terminus/schema'],[],[]) terminus:status terminus:failure Code: 404, url: http://localhost:6363/terminus/document/server

Steps to reproduce:

Run start.pl
Go to the dashboard on localhost:6363/dashboard
Find the document id server and change the comment in something else
Try to press save.",2019-10-08T14:15:56Z,rrooij,https://github.com/terminusdb/terminusdb/issues/29#issuecomment-539534660,Fixed in the referenced commit.,fix in the referenced commit,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,30,2019-10-13T03:35:37Z,ansarizafar,Single binary without any dependencies.,https://github.com/terminusdb/terminusdb/issues/30,"Developers are eagerly waiting for a modern, easy to use and manage database. It would be great If Terminusdb is released as a single binary without any other dependencies or requiring developers to install Docker.  New storage engine is written in Rust lang, why not just also develop Terminusdb server in Rust lang and release it as a single binary.
I would also like to suggest a twitter account and discord server for community support.",2019-10-14T07:20:12Z,rrooij,https://github.com/terminusdb/terminusdb/issues/30#issuecomment-541529036,"I agree with the need for a single binary or an easier installation option. I've already been researching the possibility to create a PPA repo and a RPM repo which makes installing easier. The advantage over a static binary is that you get updates from your package manager.
However, a single binary would be nice to try things out. We will definitely look into it.
The chance of a Rust port becoming a thing is very slim though. Prolog is used specifically because the predicate logic programming is a very good match for our database software (for instance, constraint checking with logical rules). So it probably won't be ported to Rust.",i agree with the need for a single binary or an easy installation option -PRON- have already be research the possibility to create a ppa repo and a rpm repo which make instal easy the advantage over a static binary be that -PRON- get update from -PRON- package manager however a single binary would be nice to try thing out -PRON- will definitely look into -PRON- the chance of a rust port become a thing be very slim though prolog be use specifically because the predicate logic programming be a very good match for -PRON- database software ( for instance constraint check with logical rule ) so -PRON- probably will not be port to rust,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,30,2019-10-13T03:35:37Z,ansarizafar,Single binary without any dependencies.,https://github.com/terminusdb/terminusdb/issues/30,"Developers are eagerly waiting for a modern, easy to use and manage database. It would be great If Terminusdb is released as a single binary without any other dependencies or requiring developers to install Docker.  New storage engine is written in Rust lang, why not just also develop Terminusdb server in Rust lang and release it as a single binary.
I would also like to suggest a twitter account and discord server for community support.",2019-10-14T08:30:51Z,ansarizafar,https://github.com/terminusdb/terminusdb/issues/30#issuecomment-541552823,"I don't know anything about Prolog but in my opinion a database is a critical piece and must be written in a system language for better performance and efficient CPU/Memory usage.
I would definitely like to play with Terminusdb, When a single binary or an easier installation option will be available. Terminusdb has unusual Schema/query language syntax so a step by step tutorial for designing database schema/queries, access permissions, migrations and Back/restore  for a common use case like a blogging app will really help developers new to Terminusdb. I know Terminusdb is different than other databases but developers needs a simpler schema/query language like https://edgedb.com/roadmap or Prisma2 https://github.com/prisma/prisma2",i do not know anything about prolog but in -PRON- opinion a database be a critical piece and must be write in a system language for well performance and efficient cpu / memory usage i would definitely like to play with terminusdb when a single binary or an easy installation option will be available terminusdb have unusual schema / query language syntax so a step by step tutorial for design database schema / query access permission migration and back / restore for a common use case like a blogge app will really help developer new to terminusdb i know terminusdb be different than other database but developer need a simple schema / query language like https//edgedbcom / roadmap or prisma2 https//githubcom / prisma / prisma2,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,30,2019-10-13T03:35:37Z,ansarizafar,Single binary without any dependencies.,https://github.com/terminusdb/terminusdb/issues/30,"Developers are eagerly waiting for a modern, easy to use and manage database. It would be great If Terminusdb is released as a single binary without any other dependencies or requiring developers to install Docker.  New storage engine is written in Rust lang, why not just also develop Terminusdb server in Rust lang and release it as a single binary.
I would also like to suggest a twitter account and discord server for community support.",2019-10-29T04:08:15Z,ansarizafar,https://github.com/terminusdb/terminusdb/issues/30#issuecomment-547247504,Is there any new update/ETA regarding Single binary for Terminusdb?,be there any new update / eta regard single binary for terminusdb,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,30,2019-10-13T03:35:37Z,ansarizafar,Single binary without any dependencies.,https://github.com/terminusdb/terminusdb/issues/30,"Developers are eagerly waiting for a modern, easy to use and manage database. It would be great If Terminusdb is released as a single binary without any other dependencies or requiring developers to install Docker.  New storage engine is written in Rust lang, why not just also develop Terminusdb server in Rust lang and release it as a single binary.
I would also like to suggest a twitter account and discord server for community support.",2020-01-08T09:32:29Z,ansarizafar,https://github.com/terminusdb/terminusdb/issues/30#issuecomment-571965668,A new version of Terminusdb is just released. Is there any chance of Single binary release for Ubuntu?,a new version of terminusdb be just release be there any chance of single binary release for ubuntu,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,30,2019-10-13T03:35:37Z,ansarizafar,Single binary without any dependencies.,https://github.com/terminusdb/terminusdb/issues/30,"Developers are eagerly waiting for a modern, easy to use and manage database. It would be great If Terminusdb is released as a single binary without any other dependencies or requiring developers to install Docker.  New storage engine is written in Rust lang, why not just also develop Terminusdb server in Rust lang and release it as a single binary.
I would also like to suggest a twitter account and discord server for community support.",2020-02-27T02:39:53Z,ansarizafar,https://github.com/terminusdb/terminusdb/issues/30#issuecomment-591749359,@rrooij I am still waiting for a Single binary release to try out amazing Terminusdb.,@rrooij i be still wait for a single binary release to try out amazing terminusdb,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,30,2019-10-13T03:35:37Z,ansarizafar,Single binary without any dependencies.,https://github.com/terminusdb/terminusdb/issues/30,"Developers are eagerly waiting for a modern, easy to use and manage database. It would be great If Terminusdb is released as a single binary without any other dependencies or requiring developers to install Docker.  New storage engine is written in Rust lang, why not just also develop Terminusdb server in Rust lang and release it as a single binary.
I would also like to suggest a twitter account and discord server for community support.",2020-03-19T21:59:36Z,rrooij,https://github.com/terminusdb/terminusdb/issues/30#issuecomment-601436835,"@ansarizafar Hey, sorry for the late reply. A lot of work has to be done and the single binary is definitely one of them. It is on the list we don't have a ETA yet.
Meanwhile, the easiest way to try terminusdb is probably the heroku deploy.
https://github.com/terminusdb/terminus-heroku",@ansarizafar hey sorry for the late reply a lot of work have to be do and the single binary be definitely one of -PRON- -PRON- be on the list -PRON- do not have a eta yet meanwhile the easy way to try terminusdb be probably the heroku deploy https//githubcom / terminusdb / terminus - heroku,1,1
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,30,2019-10-13T03:35:37Z,ansarizafar,Single binary without any dependencies.,https://github.com/terminusdb/terminusdb/issues/30,"Developers are eagerly waiting for a modern, easy to use and manage database. It would be great If Terminusdb is released as a single binary without any other dependencies or requiring developers to install Docker.  New storage engine is written in Rust lang, why not just also develop Terminusdb server in Rust lang and release it as a single binary.
I would also like to suggest a twitter account and discord server for community support.",2020-04-02T04:16:47Z,ansarizafar,https://github.com/terminusdb/terminusdb/issues/30#issuecomment-607611255,@rrooij Single binary or an easier installation option will allow more developers to try terminusdb and more users means possibly more contributors. I request you to place this higher on your priorities list.,@rrooij single binary or an easy installation option will allow more developer to try terminusdb and more user mean possibly more contributor i request -PRON- to place this high on -PRON- priority list,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,30,2019-10-13T03:35:37Z,ansarizafar,Single binary without any dependencies.,https://github.com/terminusdb/terminusdb/issues/30,"Developers are eagerly waiting for a modern, easy to use and manage database. It would be great If Terminusdb is released as a single binary without any other dependencies or requiring developers to install Docker.  New storage engine is written in Rust lang, why not just also develop Terminusdb server in Rust lang and release it as a single binary.
I would also like to suggest a twitter account and discord server for community support.",2020-09-05T18:46:37Z,rrooij,https://github.com/terminusdb/terminusdb/issues/30#issuecomment-687647465,"This has been solved by the release of terminusdb-desktop. See:
https://terminusdb.com/hub/download",this have be solve by the release of terminusdb - desktop see https//terminusdbcom / hub / download,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,31,2019-10-22T11:15:32Z,GavinMendelGleason,xrdf_db type checking and subsumption,https://github.com/terminusdb/terminusdb/issues/31,"When queries are made in which the type of the input query differs from the stored literal result only by the type, the type should be checked for subsumption by the type given in the query.
For example:
ask(DB, 
        t(URI, some/predicate, Value^^(xsd/string))
       )

Should also get you anything of Value^^(xdd/email) etc.",2020-10-01T14:46:41Z,GavinMendelGleason,https://github.com/terminusdb/terminusdb/issues/31#issuecomment-702186465,"This is now going to be done with a ""typeof"" word.","this be now go to be do with a "" typeof "" word",0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,32,2019-10-22T14:15:18Z,rrooij,Creating a database twice results in internal server error,https://github.com/terminusdb/terminusdb/issues/32,"Creating a database with the API succeeds when trying it once:
curl -X POST 'localhost:6363/testdb' --user ':root' -H ""Content-Type: application/json"" -d@create_new_db.json

However, when running it a second time:
{
  ""code"":500,
  ""message"":""Type error: `json_term' expected, found `{'@value':'http://localhost:6363/testdb','@type':'xdd:url'}' (a compound)""
It should return an error message with a message that the database already exists.
To reproduce:

Save this as create_new_db.json

{
    ""@context"": {
        ""rdfs"": ""http://www.w3.org/2000/01/rdf-schema#"",
        ""terminus"": ""http://terminusdb.com/schema/terminus#"",
        ""_"": ""http://localhost:6363/testdb/""
    },
    ""terminus:document"": {
        ""@type"": ""terminus:Database"",
        ""rdfs:label"": {
            ""@language"": ""en"",
            ""@value"": ""test""
        },
        ""rdfs:comment"": {
            ""@language"": ""en"",
            ""@value"": ""Test database""
        },
        ""terminus:allow_origin"": {
            ""@type"": ""xsd:string"",
            ""@value"": ""*""
        },
        ""@id"": ""http://localhost:6363/testdb""
    },
    ""@type"": ""terminus:APIUpdate""
}

Run the curl command (see the top of the issue)",2019-10-22T14:37:25Z,rrooij,https://github.com/terminusdb/terminusdb/issues/32#issuecomment-544994352,Fixed in the dev branch.,fix in the dev branch,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,33,2019-10-24T08:49:18Z,rrooij,Creating huge schema results in bad performance,https://github.com/terminusdb/terminusdb/issues/33,"Steps to reproduce:

Use the latest terminus-server
Create a new database called testdb
Click in the dashboard on schema -> edit schema -> and paste the attachment in the editor
big_turtle.ttl.txt
Save the edited schema

Results
Terminus gets very slow. It is definitely busy indexing the schema and the like, as my CPU usage gets crazy, but the dashboard UI makes it seem like everything is finished and ready.
Expected results
An indicator that the database is busy with indexing and the like and therefore will be slow. Or better performance for this in general. (this might be good to test again when terminus-store is implemented as storage backend)",2019-10-24T13:35:44Z,matko,https://github.com/terminusdb/terminusdb/issues/33#issuecomment-545921015,I created an issue in terminus-store for this: terminusdb/terminusdb-store#4.,i create an issue in terminus - store for this terminusdb / terminusdb - store#4,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,33,2019-10-24T08:49:18Z,rrooij,Creating huge schema results in bad performance,https://github.com/terminusdb/terminusdb/issues/33,"Steps to reproduce:

Use the latest terminus-server
Create a new database called testdb
Click in the dashboard on schema -> edit schema -> and paste the attachment in the editor
big_turtle.ttl.txt
Save the edited schema

Results
Terminus gets very slow. It is definitely busy indexing the schema and the like, as my CPU usage gets crazy, but the dashboard UI makes it seem like everything is finished and ready.
Expected results
An indicator that the database is busy with indexing and the like and therefore will be slow. Or better performance for this in general. (this might be good to test again when terminus-store is implemented as storage backend)",2020-10-01T14:48:22Z,GavinMendelGleason,https://github.com/terminusdb/terminusdb/issues/33#issuecomment-702187606,This is now significantly faster.,this be now significantly fast,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,34,2019-10-24T14:56:14Z,rrooij,Internal server error when changing schema with API,https://github.com/terminusdb/terminusdb/issues/34,"Error
The following error appears when trying to change the schema from the terminus-server API:
{
  ""code"":500,
  ""message"":""Type error: `json_term' expected, found `base64_char(_9532,58)' (a compound)""
}

Expected result
A clear error message or a changed schema.
How to reproduce

Create a database called testdb.
Run the folllowing python3 script:

#!/usr/bin/env python3
import requests


headers = { 'Authorization' : 'Basic %s' %  "":root"" }

small_turtle = """"""
@base <http://example.org/> .
@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
@prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> .
@prefix foaf: <http://xmlns.com/foaf/0.1/> .
@prefix rel: <http://www.perceive.net/schemas/relationship/> .

<#green-goblin>
    rel:enemyOf <#spiderman> ;
    a foaf:Person ;    # in the context of the Marvel universe
    foaf:name ""Green Goblin"" .

<#spiderman>
    rel:enemyOf <#green-goblin> ;
    a foaf:Person ;
    foaf:name ""Spiderman"" .
""""""
payload = {
    ""terminus:turtle"": small_turtle,
    ""terminus:schema"": ""http://localhost:6363/testdb/schema"",
    ""@type"": ""terminus:APIUpdate""
}
response = requests.post(""http://localhost:6363/testdb/schema"", json=payload, headers=headers)
print(response.text)",2019-10-24T15:09:42Z,rrooij,https://github.com/terminusdb/terminusdb/issues/34#issuecomment-545963843,"Can be related to missing nested json encoding or the like? As uploading this with curl works fine.
curl 'http://localhost:6363/testdb/schema' -X POST -H ""Content-Type: application/json"" --user ':root' -d@payload.json
payload.json:

{""terminus:turtle"": ""\n@base <http://example.org/> .\n@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .\n@prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> .\n@prefix foaf: <http://xmlns.com/foaf/0.1/> .\n@prefix rel: <http://www.perceive.net/schemas/relationship/> .\n\n<#green-goblin>\n    rel:enemyof <#spiderman> ;\n    a foaf:person ;    # in the context of the marvel universe\n    foaf:name \""green goblin\"" .\n\n<#spiderman>\n    rel:enemyof <#green-goblin> ;\n    a foaf:person ;\n    foaf:name \""spiderman\"" .\n"", ""terminus:schema"": ""http://localhost:6363/testdb/schema"", ""@type"": ""terminus:apiupdate""}","can be relate to miss nest json encoding or the like as upload this with curl work fine curl ' http//localhost6363 / testdb / schema ' -x post -h "" content - type application / json "" --user ' root ' -d@payloadjson payloadjson { "" terminusturtle "" "" \n@base < http//exampleorg/ > \n@prefix rdf < http//wwww3org/1999/02/22-rdf - syntax - ns # > \n@prefix rdfs < http//wwww3org/2000/01 / rdf - schema # > \n@prefix foaf < http//xmlnscom / foaf/01/ > \n@prefix rel < http//wwwperceivenet / schemas / relationship/ > \n\n<#green - goblin>\n relenemyof < # spiderman > \n a foafperson # in the context of the marvel universe\n foafname \""green goblin\ "" \n\n<#spiderman>\n relenemyof < # green - goblin > \n a foafperson \n foafname \""spiderman\ "" \n "" "" terminusschema "" "" http//localhost6363 / testdb / schema "" "" @type "" "" terminusapiupdate "" }",0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,34,2019-10-24T14:56:14Z,rrooij,Internal server error when changing schema with API,https://github.com/terminusdb/terminusdb/issues/34,"Error
The following error appears when trying to change the schema from the terminus-server API:
{
  ""code"":500,
  ""message"":""Type error: `json_term' expected, found `base64_char(_9532,58)' (a compound)""
}

Expected result
A clear error message or a changed schema.
How to reproduce

Create a database called testdb.
Run the folllowing python3 script:

#!/usr/bin/env python3
import requests


headers = { 'Authorization' : 'Basic %s' %  "":root"" }

small_turtle = """"""
@base <http://example.org/> .
@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
@prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> .
@prefix foaf: <http://xmlns.com/foaf/0.1/> .
@prefix rel: <http://www.perceive.net/schemas/relationship/> .

<#green-goblin>
    rel:enemyOf <#spiderman> ;
    a foaf:Person ;    # in the context of the Marvel universe
    foaf:name ""Green Goblin"" .

<#spiderman>
    rel:enemyOf <#green-goblin> ;
    a foaf:Person ;
    foaf:name ""Spiderman"" .
""""""
payload = {
    ""terminus:turtle"": small_turtle,
    ""terminus:schema"": ""http://localhost:6363/testdb/schema"",
    ""@type"": ""terminus:APIUpdate""
}
response = requests.post(""http://localhost:6363/testdb/schema"", json=payload, headers=headers)
print(response.text)",2020-01-08T14:51:41Z,GavinMendelGleason,https://github.com/terminusdb/terminusdb/issues/34#issuecomment-572104083,Anyone know the status of this issue?,anyone know the status of this issue,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,34,2019-10-24T14:56:14Z,rrooij,Internal server error when changing schema with API,https://github.com/terminusdb/terminusdb/issues/34,"Error
The following error appears when trying to change the schema from the terminus-server API:
{
  ""code"":500,
  ""message"":""Type error: `json_term' expected, found `base64_char(_9532,58)' (a compound)""
}

Expected result
A clear error message or a changed schema.
How to reproduce

Create a database called testdb.
Run the folllowing python3 script:

#!/usr/bin/env python3
import requests


headers = { 'Authorization' : 'Basic %s' %  "":root"" }

small_turtle = """"""
@base <http://example.org/> .
@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
@prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> .
@prefix foaf: <http://xmlns.com/foaf/0.1/> .
@prefix rel: <http://www.perceive.net/schemas/relationship/> .

<#green-goblin>
    rel:enemyOf <#spiderman> ;
    a foaf:Person ;    # in the context of the Marvel universe
    foaf:name ""Green Goblin"" .

<#spiderman>
    rel:enemyOf <#green-goblin> ;
    a foaf:Person ;
    foaf:name ""Spiderman"" .
""""""
payload = {
    ""terminus:turtle"": small_turtle,
    ""terminus:schema"": ""http://localhost:6363/testdb/schema"",
    ""@type"": ""terminus:APIUpdate""
}
response = requests.post(""http://localhost:6363/testdb/schema"", json=payload, headers=headers)
print(response.text)",2020-10-05T21:41:09Z,kevinchekovfeeney,https://github.com/terminusdb/terminusdb/issues/34#issuecomment-703904552,This no longer applies - problem is in original script :root should be base64 encoded,this no long apply - problem be in original script root should be base64 encode,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,35,2019-10-29T04:09:55Z,ansarizafar,Graphql support,https://github.com/terminusdb/terminusdb/issues/35,Is there any plan to support Graphql like https://www.topquadrant.com/technology/graphql/?,2019-12-25T00:11:12Z,marvin-hansen,https://github.com/terminusdb/terminusdb/issues/35#issuecomment-568812963,"GraphQL support is paramount for industry adoption. Take an inspiration from DGraph that generates it's GraphQL API from the DB schema:
https://graphql.dgraph.io/",graphql support be paramount for industry adoption take an inspiration from dgraph that generate -PRON- be graphql api from the db schema https//graphqldgraphio/,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,35,2019-10-29T04:09:55Z,ansarizafar,Graphql support,https://github.com/terminusdb/terminusdb/issues/35,Is there any plan to support Graphql like https://www.topquadrant.com/technology/graphql/?,2020-10-05T21:52:02Z,kevinchekovfeeney,https://github.com/terminusdb/terminusdb/issues/35#issuecomment-703909144,No plans to implement this - GraphQL is more of a document templating language / means of organizing APIs than a query language - DGraph have had to extend it so that it's no longer actually GraphQL removing all the interoperability benefits and just leaving them with an awkward query language adapted from a REST API templating thing.  Not impossible that we might one day produce document APIs in vanilla GraphQL but not on the roadmap,no plan to implement this - graphql be more of a document templating language / mean of organize apis than a query language - dgraph have have to extend -PRON- so that -PRON- be no long actually graphql remove all the interoperability benefit and just leave -PRON- with an awkward query language adapt from a rest api templating thing not impossible that -PRON- may one day produce document apis in vanilla graphql but not on the roadmap,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,36,2019-11-05T11:03:16Z,rrooij,Memory leaks,https://github.com/terminusdb/terminusdb/issues/36,"Problem
When querying a database, the memory usage of terminus-server keeps growing. This occurs on the dev branch with the terminus-store backend.
How to reproduce

Start a system monitor (for instance gnome-system-monitor)
Play around in the dashboard (click on schema, query etc.) and see the memory size of terminus-server getting bigger.

Additional information
Some valgrind logs of querying and the like:
==1408== 39,312 bytes in 1 blocks are still reachable in loss record 4,049 of 4,104
==1408==    at 0x4C2BBAF: malloc (vg_replace_malloc.c:299)
==1408==    by 0x4E63358: PL_malloc_atomic_unmanaged (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E5E9AE: do_init_atoms (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E6515C: PL_new_atom (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E5CD68: PL_register_blob_type (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EFE9FF: initReservedSymbols (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E5EB38: do_init_atoms (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E6515C: PL_new_atom (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4F2C681: setPrologFlag (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E6DFAD: PL_set_prolog_flag (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EF42AD: PL_initialise (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x108718: main (in /usr/lib/swipl/bin/x86_64-linux/swipl)
==1408== 
==1408== 39,312 bytes in 1 blocks are possibly lost in loss record 4,050 of 4,104
==1408==    at 0x4C2BBAF: malloc (vg_replace_malloc.c:299)
==1408==    by 0x4E63358: PL_malloc_atomic_unmanaged (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E5E9AE: do_init_atoms (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E6515C: PL_new_atom (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4F2C681: setPrologFlag (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E6DFAD: PL_set_prolog_flag (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EF42AD: PL_initialise (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x108718: main (in /usr/lib/swipl/bin/x86_64-linux/swipl)
==1408== 
==1408== 39,936 bytes in 18 blocks are still reachable in loss record 4,051 of 4,104
==1408==    at 0x4C2BBAF: malloc (vg_replace_malloc.c:299)
==1408==    by 0x4E60CCD: allocHeapOrHalt (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4F1F10A: htable_alloc_kvs (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4F1F47F: htable_put (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4F1F891: addHTable (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EB923A: lookupProcedure (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EDB3F5: loadXRc (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EDBE20: loadPredicate (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EDC5CC: loadPart (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EDCA4D: loadStatement (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EDC5CC: loadPart (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EDCA4D: loadStatement (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408== 
==1408== 41,400 bytes in 1,035 blocks are still reachable in loss record 4,052 of 4,104
==1408==    at 0x4C2BBAF: malloc (vg_replace_malloc.c:299)
==1408==    by 0x4E60CCD: allocHeapOrHalt (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E9BB28: lookupFunctorDef (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EAAB96: pl_functor3_va (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E79C62: PL_next_solution (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EB6C2B: callProlog (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4F08067: pl_with_mutex (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E79F63: PL_next_solution (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EB6C2B: callProlog (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EB6E3C: pl_notrace1_va (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E79C62: PL_next_solution (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EBB0F1: trapUndefined (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408== 
==1408== 41,472 bytes in 432 blocks are still reachable in loss record 4,053 of 4,104
==1408==    at 0x4C2BBAF: malloc (vg_replace_malloc.c:299)
==1408==    by 0x4E60CCD: allocHeapOrHalt (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EB9186: lookupProcedure (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EBB445: pl_get_predicate_attribute (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E79FA5: PL_next_solution (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EB675A: query_loop (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EB6F8A: prologToplevel (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E6D8AC: PL_toplevel (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x108724: main (in /usr/lib/swipl/bin/x86_64-linux/swipl)
==1408== 
==1408== 41,760 bytes in 1,740 blocks are still reachable in loss record 4,054 of 4,104
==1408==    at 0x4C2BBAF: malloc (vg_replace_malloc.c:299)
==1408==    by 0x4E60CCD: allocHeapOrHalt (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EB8B75: newClauseRef (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EB8C11: assertProcedure (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EDBE90: loadPredicate (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EDC5CC: loadPart (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EDCA4D: loadStatement (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EDC5CC: loadPart (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EDCA4D: loadStatement (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EDC5CC: loadPart (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EDCA4D: loadStatement (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EDD25B: loadWicFromStream (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408== 
==1408== 41,984 bytes in 8 blocks are still reachable in loss record 4,055 of 4,104
==1408==    at 0x4C2BBAF: malloc (vg_replace_malloc.c:299)
==1408==    by 0x4E60CCD: allocHeapOrHalt (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4F1F10A: htable_alloc_kvs (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4F1F47F: htable_put (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4F1F891: addHTable (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EB923A: lookupProcedure (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E8DCC5: compileSubClause (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E9399F: compileBody (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E94FFC: compileClause (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E95BCE: assert_term (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E96136: record_clause (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E961C9: pl_record_clause3_va (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408== 
==1408== 42,744 bytes in 137 blocks are possibly lost in loss record 4,056 of 4,104
==1408==    at 0x4C2BBAF: malloc (vg_replace_malloc.c:299)
==1408==    by 0xA25005B: alloc::alloc::alloc::he71f6672ff69332a (alloc.rs:84)
==1408==    by 0xA24FFCB: alloc::alloc::exchange_malloc::h46527e14373cbbb6 (alloc.rs:206)
==1408==    by 0xA1844FF: new<terminus_store::layer::child::ChildSubjectLookup<terminus_store::storage::directory::SharedMmap>> (boxed.rs:121)
==1408==    by 0xA1844FF: _$LT$terminus_store..layer..child..ChildLayer$LT$M$GT$$u20$as$u20$terminus_store..layer..layer..Layer$GT$::lookup_subject::h0c968e5564fb6aac (child.rs:378)
==1408==    by 0xA34C2AA: _$LT$terminus_store..store..StoreLayer$u20$as$u20$terminus_store..layer..layer..Layer$GT$::lookup_subject::hc31873af51cae373 (mod.rs:205)
==1408==    by 0xA5BEC30: _$LT$terminus_store..store..sync..SyncStoreLayer$u20$as$u20$terminus_store..layer..layer..Layer$GT$::lookup_subject::hf2365060bef0b9b4 (sync.rs:163)
==1408==    by 0xA284D7D: layer_lookup_subject (lib.rs:351)
==1408==    by 0x9EB2A6B: pl_layer_lookup_subject (terminus_store.c:589)
==1408==    by 0x4E79FA5: PL_next_solution (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EB6C2B: callProlog (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EE5A3B: start_thread (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x60384A3: start_thread (pthread_create.c:456)
==1408== 
==1408== 44,032 bytes in 10 blocks are still reachable in loss record 4,057 of 4,104
==1408==    at 0x4C2BBAF: malloc (vg_replace_malloc.c:299)
==1408==    by 0x4E60CCD: allocHeapOrHalt (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4F1F10A: htable_alloc_kvs (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4F1F47F: htable_put (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4F1F891: addHTable (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EB923A: lookupProcedure (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E8DCC5: compileSubClause (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E9399F: compileBody (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E9396F: compileBody (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E94FFC: compileClause (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E95BCE: assert_term (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E96136: record_clause (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408== 
==1408== 44,032 bytes in 56 blocks are still reachable in loss record 4,058 of 4,104
==1408==    at 0x4C2BBAF: malloc (vg_replace_malloc.c:299)
==1408==    by 0x4E60CCD: allocHeapOrHalt (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4F1F10A: htable_alloc_kvs (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4F1F47F: htable_put (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4F1F901: updateHTable (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EA9135: exportProcedure (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EA92D8: export_pi (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EA92F0: pl_export1_va (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E79C62: PL_next_solution (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EB675A: query_loop (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EB6F8A: prologToplevel (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EF4ACF: PL_initialise (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408== 
==1408== 46,608 bytes in 1,942 blocks are still reachable in loss record 4,059 of 4,104
==1408==    at 0x4C2BBAF: malloc (vg_replace_malloc.c:299)
==1408==    by 0x4E60CCD: allocHeapOrHalt (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EB8B75: newClauseRef (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E616D2: addClauseBucket (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E618AD: addClauseToIndex (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E61BC0: hashDefinition (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E62368: first_clause_guarded (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E636FA: firstClause (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E7B4D8: PL_next_solution (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EB675A: query_loop (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EB6F8A: prologToplevel (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EF4ACF: PL_initialise (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408== 
==1408== 47,104 bytes in 7 blocks are still reachable in loss record 4,060 of 4,104
==1408==    at 0x4C2BBAF: malloc (vg_replace_malloc.c:299)
==1408==    by 0x4E60CCD: allocHeapOrHalt (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4F1F10A: htable_alloc_kvs (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4F1F47F: htable_put (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4F1F891: addHTable (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EB923A: lookupProcedure (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E95E06: assert_term (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E96136: record_clause (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E961C9: pl_record_clause3_va (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E79C62: PL_next_solution (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EB675A: query_loop (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EB6F8A: prologToplevel (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408== 
==1408== 49,152 bytes in 1 blocks are possibly lost in loss record 4,061 of 4,104
==1408==    at 0x4C2BBAF: malloc (vg_replace_malloc.c:299)
==1408==    by 0x4E63358: PL_malloc_atomic_unmanaged (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E5C628: allocateAtomBlock.part.6 (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E5D474: lookupBlob (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E5D6F4: lookupAtom (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E9A5DD: registerBuiltins (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E9A908: initBuildIns (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4ED0CCC: setupProlog (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EF42B2: PL_initialise (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x108718: main (in /usr/lib/swipl/bin/x86_64-linux/swipl)
==1408== 
==1408== 49,248 bytes in 2 blocks are still reachable in loss record 4,062 of 4,104
==1408==    at 0x4C2BBAF: malloc (vg_replace_malloc.c:299)
==1408==    by 0x4E60CCD: allocHeapOrHalt (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E61ACD: hashDefinition (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E62368: first_clause_guarded (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E636FA: firstClause (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E7B4D8: PL_next_solution (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EBB0F1: trapUndefined (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E72C10: PL_next_solution (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EB675A: query_loop (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EB6F8A: prologToplevel (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EF4ACF: PL_initialise (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x108718: main (in /usr/lib/swipl/bin/x86_64-linux/swipl)
==1408== 
==1408== 49,440 bytes in 56 blocks are still reachable in loss record 4,063 of 4,104
==1408==    at 0x4C2BBAF: malloc (vg_replace_malloc.c:299)
==1408==    by 0x4E60CCD: allocHeapOrHalt (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E61ACD: hashDefinition (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E62368: first_clause_guarded (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E636FA: firstClause (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E7B4D8: PL_next_solution (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EB675A: query_loop (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EB6F8A: prologToplevel (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EF4ACF: PL_initialise (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x108718: main (in /usr/lib/swipl/bin/x86_64-linux/swipl)
==1408== 
==1408== 54,643 bytes in 1 blocks are still reachable in loss record 4,064 of 4,104
==1408==    at 0x4C2BBAF: malloc (vg_replace_malloc.c:299)
==1408==    by 0x4E63358: PL_malloc_atomic_unmanaged (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E8D2C8: initWamTable (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4ED0B9E: setupProlog (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EF42B2: PL_initialise (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x108718: main (in /usr/lib/swipl/bin/x86_64-linux/swipl)
==1408== 
==1408== 56,952 bytes in 113 blocks are still reachable in loss record 4,065 of 4,104
==1408==    at 0x4C2BBAF: malloc (vg_replace_malloc.c:299)
==1408==    by 0xA66B22B: alloc::alloc::alloc::h5720529faf6a4289 (alloc.rs:84)
==1408==    by 0xA66B19B: alloc::alloc::exchange_malloc::h96e4bb19607190fc (alloc.rs:206)
==1408==    by 0xA677F65: new<crossbeam_queue::seg_queue::Block<alloc::sync::Arc<tokio_threadpool::task::Task>>> (boxed.rs:121)
==1408==    by 0xA677F65: crossbeam_queue::seg_queue::SegQueue$LT$T$GT$::push::h60912bed1aa34d4c (seg_queue.rs:201)
==1408==    by 0xA68D264: tokio_threadpool::worker::entry::WorkerEntry::remotely_complete_task::hb324e0f074062ccf (entry.rs:266)
==1408==    by 0xA69BA66: tokio_threadpool::worker::Worker::run_task::h1ecf2a8dd4e7574e (mod.rs:515)
==1408==    by 0xA69AEDF: tokio_threadpool::worker::Worker::try_steal_task::hbf0e24cb88f05cbd (mod.rs:416)
==1408==    by 0xA69A4B9: tokio_threadpool::worker::Worker::try_run_task::h54601ba776355230 (mod.rs:301)
==1408==    by 0xA69A2F5: tokio_threadpool::worker::Worker::run::ha620171d3f5df792 (mod.rs:241)
==1408==    by 0xA61BA9B: tokio::runtime::threadpool::builder::Builder::build::_$u7b$$u7b$closure$u7d$$u7d$::_$u7b$$u7b$closure$u7d$$u7d$::_$u7b$$u7b$closure$u7d$$u7d$::_$u7b$$u7b$closure$u7d$$u7d$::hf1a0f86368d24d90 (builder.rs:390)
==1408==    by 0xA629E3B: tokio_timer::timer::handle::with_default::_$u7b$$u7b$closure$u7d$$u7d$::h7f92321d998b87fc (handle.rs:101)
==1408==    by 0xA627C63: std::thread::local::LocalKey$LT$T$GT$::try_with::h0591efca88fdf8c2 (local.rs:262)
==1408== 
==1408== 58,544 bytes in 3,659 blocks are still reachable in loss record 4,066 of 4,104
==1408==    at 0x4C2BBAF: malloc (vg_replace_malloc.c:299)
==1408==    by 0x4E60CCD: allocHeapOrHalt (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EEE291: addProcedureSourceFile (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E95CB0: assert_term (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E96136: record_clause (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E961C9: pl_record_clause3_va (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E79C62: PL_next_solution (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EB675A: query_loop (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EB6F8A: prologToplevel (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EF4ACF: PL_initialise (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x108718: main (in /usr/lib/swipl/bin/x86_64-linux/swipl)
==1408== 
==1408== 59,920 bytes in 1,498 blocks are still reachable in loss record 4,067 of 4,104
==1408==    at 0x4C2BBAF: malloc (vg_replace_malloc.c:299)
==1408==    by 0x4E60CCD: allocHeapOrHalt (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E9BB28: lookupFunctorDef (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EC05F9: build_term (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EC5B14: simple_term (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EC6FFB: complex_term (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EC7A2A: read_term (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EC90E0: read_clause (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EC9374: pl_read_clause3_va (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E79C62: PL_next_solution (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EB675A: query_loop (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EB6F8A: prologToplevel (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408== 
==1408== 64,416 bytes in 671 blocks are still reachable in loss record 4,068 of 4,104
==1408==    at 0x4C2BBAF: malloc (vg_replace_malloc.c:299)
==1408==    by 0x4E60CCD: allocHeapOrHalt (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EB9186: lookupProcedure (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E8DCC5: compileSubClause (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E9399F: compileBody (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E94FFC: compileClause (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E95BCE: assert_term (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E96136: record_clause (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E961C9: pl_record_clause3_va (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E79C62: PL_next_solution (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EB675A: query_loop (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EB6F8A: prologToplevel (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408== 
==1408== 64,524 bytes in 6,007 blocks are still reachable in loss record 4,069 of 4,104
==1408==    at 0x4C2BBAF: malloc (vg_replace_malloc.c:299)
==1408==    by 0x4E63358: PL_malloc_atomic_unmanaged (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E5D4D8: lookupBlob (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E5D6F4: lookupAtom (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EC448C: get_token__LD.part.37 (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EC4EBB: simple_term (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EC6FFB: complex_term (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EC7A2A: read_term (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EC90E0: read_clause (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EC9374: pl_read_clause3_va (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E79C62: PL_next_solution (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EB675A: query_loop (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408== 
==1408== 64,800 bytes in 675 blocks are still reachable in loss record 4,070 of 4,104
==1408==    at 0x4C2BBAF: malloc (vg_replace_malloc.c:299)
==1408==    by 0x4E60CCD: allocHeapOrHalt (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EB9186: lookupProcedure (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E95E06: assert_term (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E96136: record_clause (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E961C9: pl_record_clause3_va (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E79C62: PL_next_solution (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EB675A: query_loop (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EB6F8A: prologToplevel (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EF4ACF: PL_initialise (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x108718: main (in /usr/lib/swipl/bin/x86_64-linux/swipl)
==1408== 
==1408== 65,536 bytes in 1 blocks are still reachable in loss record 4,071 of 4,104
==1408==    at 0x4C2BBAF: malloc (vg_replace_malloc.c:299)
==1408==    by 0x4E60CCD: allocHeapOrHalt (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4F1F10A: htable_alloc_kvs (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4F1F47F: htable_put (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4F1F891: addHTable (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4F1F931: addNewHTable (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EB7F06: importDefinitionModule (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EDC5CC: loadPart (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EDCA4D: loadStatement (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EDC5CC: loadPart (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EDCA4D: loadStatement (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EDD25B: loadWicFromStream (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408== 
==1408== 65,536 bytes in 1 blocks are still reachable in loss record 4,072 of 4,104
==1408==    at 0x4C2BBAF: malloc (vg_replace_malloc.c:299)
==1408==    by 0x4E60CCD: allocHeapOrHalt (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E9B755: rehashFunctors.part.0 (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E9BADE: lookupFunctorDef (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EB4E49: pl_univ2_va (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E79C62: PL_next_solution (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EB675A: query_loop (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EB6F8A: prologToplevel (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E6D8AC: PL_toplevel (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x108724: main (in /usr/lib/swipl/bin/x86_64-linux/swipl)
==1408== 
==1408== 65,536 bytes in 1 blocks are definitely lost in loss record 4,073 of 4,104
==1408==    at 0x4C2BBAF: malloc (vg_replace_malloc.c:299)
==1408==    by 0x4E63358: PL_malloc_atomic_unmanaged (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E9B94F: registerFunctor (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E9BC9A: lookupFunctorDef (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EC05F9: build_term (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EC5B14: simple_term (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EC6FFB: complex_term (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EC7A2A: read_term (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EC90E0: read_clause (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EC9374: pl_read_clause3_va (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E79C62: PL_next_solution (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EB675A: query_loop (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408== 
==1408== 65,544 bytes in 1 blocks are still reachable in loss record 4,074 of 4,104
==1408==    at 0x4C2DDCF: realloc (vg_replace_malloc.c:785)
==1408==    by 0x4ED0708: stack_realloc (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E9FCA3: growStacks (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4ECF053: pl_trim_stacks0_va (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E79C62: PL_next_solution (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EB675A: query_loop (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EB6F8A: prologToplevel (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E6D8AC: PL_toplevel (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x108724: main (in /usr/lib/swipl/bin/x86_64-linux/swipl)
==1408== 
==1408== 69,360 bytes in 68 blocks are still reachable in loss record 4,075 of 4,104
==1408==    at 0x4C2BBAF: malloc (vg_replace_malloc.c:299)
==1408==    by 0x111EA983: ??? (in /usr/lib/x86_64-linux-gnu/libedit.so.2.0.55)
==1408==    by 0x111EB281: ??? (in /usr/lib/x86_64-linux-gnu/libedit.so.2.0.55)
==1408==    by 0x111EB4D7: ??? (in /usr/lib/x86_64-linux-gnu/libedit.so.2.0.55)
==1408==    by 0x111E0CA9: el_init_fd (in /usr/lib/x86_64-linux-gnu/libedit.so.2.0.55)
==1408==    by 0x111E0DC4: el_init (in /usr/lib/x86_64-linux-gnu/libedit.so.2.0.55)
==1408==    by 0x10FD2378: pl_wrap (in /usr/lib/swipl/lib/x86_64-linux/libedit4pl.so)
==1408==    by 0x4E79FEC: PL_next_solution (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EB675A: query_loop (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EB6F8A: prologToplevel (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E6D8AC: PL_toplevel (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x108724: main (in /usr/lib/swipl/bin/x86_64-linux/swipl)
==1408== 
==1408== 69,360 bytes in 68 blocks are still reachable in loss record 4,076 of 4,104
==1408==    at 0x4C2BBAF: malloc (vg_replace_malloc.c:299)
==1408==    by 0x111EAA23: ??? (in /usr/lib/x86_64-linux-gnu/libedit.so.2.0.55)
==1408==    by 0x111EB281: ??? (in /usr/lib/x86_64-linux-gnu/libedit.so.2.0.55)
==1408==    by 0x111EB4D7: ??? (in /usr/lib/x86_64-linux-gnu/libedit.so.2.0.55)
==1408==    by 0x111E0CA9: el_init_fd (in /usr/lib/x86_64-linux-gnu/libedit.so.2.0.55)
==1408==    by 0x111E0DC4: el_init (in /usr/lib/x86_64-linux-gnu/libedit.so.2.0.55)
==1408==    by 0x10FD2378: pl_wrap (in /usr/lib/swipl/lib/x86_64-linux/libedit4pl.so)
==1408==    by 0x4E79FEC: PL_next_solution (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EB675A: query_loop (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EB6F8A: prologToplevel (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E6D8AC: PL_toplevel (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x108724: main (in /usr/lib/swipl/bin/x86_64-linux/swipl)
==1408== 
==1408== 75,376 bytes in 1,346 blocks are possibly lost in loss record 4,077 of 4,104
==1408==    at 0x4C2BBAF: malloc (vg_replace_malloc.c:299)
==1408==    by 0xA25005B: alloc::alloc::alloc::he71f6672ff69332a (alloc.rs:84)
==1408==    by 0xA24FFCB: alloc::alloc::exchange_malloc::h46527e14373cbbb6 (alloc.rs:206)
==1408==    by 0xA281C29: new<terminus_store::store::sync::SyncStoreLayer> (boxed.rs:121)
==1408==    by 0xA281C29: named_graph_get_head (lib.rs:92)
==1408==    by 0x9EB194C: pl_head (terminus_store.c:76)
==1408==    by 0x4E79F63: PL_next_solution (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EB6C2B: callProlog (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EE5A3B: start_thread (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x60384A3: start_thread (pthread_create.c:456)
==1408== 
==1408== 77,066 bytes in 1,562 blocks are possibly lost in loss record 4,078 of 4,104
==1408==    at 0x4C2BBAF: malloc (vg_replace_malloc.c:299)
==1408==    by 0xA6FF59B: alloc::alloc::alloc::hd4aea4183502f317 (alloc.rs:84)
==1408==    by 0xA6FF381: _$LT$alloc..alloc..Global$u20$as$u20$core..alloc..Alloc$GT$::alloc::h480641302563a5de (alloc.rs:172)
==1408==    by 0xA6FD4F1: alloc::raw_vec::RawVec$LT$T$C$A$GT$::allocate_in::hdf0c932391516013 (raw_vec.rs:98)
==1408==    by 0xA6FD315: alloc::raw_vec::RawVec$LT$T$GT$::with_capacity::he59c15e7bbc32409 (raw_vec.rs:142)
==1408==    by 0xA6FB7D3: alloc::vec::Vec$LT$T$GT$::with_capacity::h01034cb82ca16857 (vec.rs:356)
==1408==    by 0xA6FBEE0: alloc::slice::hack::to_vec::ha47e2e0d5411d604 (slice.rs:158)
==1408==    by 0xA6FF714: alloc::slice::_$LT$impl$u20$$u5b$T$u5d$$GT$::to_vec::h4998449c8f1d41db (slice.rs:379)
==1408==    by 0xA6D8684: alloc::slice::_$LT$impl$u20$alloc..borrow..ToOwned$u20$for$u20$$u5b$T$u5d$$GT$::to_owned::h46d730f63c69ab9c (slice.rs:716)
==1408==    by 0xA519399: alloc::str::_$LT$impl$u20$alloc..borrow..ToOwned$u20$for$u20$str$GT$::to_owned::h6c513b92cb4e973a (str.rs:207)
==1408==    by 0xA42AE95: terminus_store::storage::directory::get_label_from_file::hfe4de0f255ba6d3f (directory.rs:185)
==1408==    by 0xA42C7B0: _$LT$terminus_store..storage..directory..DirectoryLabelStore$u20$as$u20$terminus_store..storage..label..LabelStore$GT$::get_label::h3bc90d719ba2e3c2 (directory.rs:257)
==1408== 
==1408== 83,136 bytes in 866 blocks are still reachable in loss record 4,079 of 4,104
==1408==    at 0x4C2BBAF: malloc (vg_replace_malloc.c:299)
==1408==    by 0x4E60CCD: allocHeapOrHalt (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EB9186: lookupProcedure (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EDB3F5: loadXRc (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EDBE20: loadPredicate (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EDC5CC: loadPart (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EDCA4D: loadStatement (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EDC5CC: loadPart (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EDCA4D: loadStatement (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EDC5CC: loadPart (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EDCA4D: loadStatement (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EDD25B: loadWicFromStream (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408== 
==1408== 86,880 bytes in 905 blocks are still reachable in loss record 4,080 of 4,104
==1408==    at 0x4C2BBAF: malloc (vg_replace_malloc.c:299)
==1408==    by 0x4E60CCD: allocHeapOrHalt (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EB9186: lookupProcedure (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E8DCC5: compileSubClause (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E9399F: compileBody (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E9396F: compileBody (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E94FFC: compileClause (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E95BCE: assert_term (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E96136: record_clause (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E961C9: pl_record_clause3_va (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E79C62: PL_next_solution (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EB675A: query_loop (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408== 
==1408== 89,536 bytes in 293 blocks are still reachable in loss record 4,081 of 4,104
==1408==    at 0x4C2BBAF: malloc (vg_replace_malloc.c:299)
==1408==    by 0x4E63358: PL_malloc_atomic_unmanaged (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E95587: compileClause (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E95BCE: assert_term (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E96136: record_clause (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E961C9: pl_record_clause3_va (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E79C62: PL_next_solution (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EBB0F1: trapUndefined (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E72C10: PL_next_solution (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EB675A: query_loop (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EB6F8A: prologToplevel (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EF4ACF: PL_initialise (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408== 
==1408== 98,304 bytes in 1 blocks are still reachable in loss record 4,082 of 4,104
==1408==    at 0x4C2BBAF: malloc (vg_replace_malloc.c:299)
==1408==    by 0x4E63358: PL_malloc_atomic_unmanaged (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E5C628: allocateAtomBlock.part.6 (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E5D474: lookupBlob (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E5D6F4: lookupAtom (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4ED8292: getAtom (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EDB304: loadXRc (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EDB370: loadXRc (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EDC786: loadPart (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EDCA4D: loadStatement (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EDC5CC: loadPart (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EDCA4D: loadStatement (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408== 
==1408== 98,304 bytes in 8 blocks are still reachable in loss record 4,083 of 4,104
==1408==    at 0x4C2BBAF: malloc (vg_replace_malloc.c:299)
==1408==    by 0xA6EE06B: alloc::alloc::alloc::ha26ad29b2718f601 (alloc.rs:84)
==1408==    by 0xA6EDE51: _$LT$alloc..alloc..Global$u20$as$u20$core..alloc..Alloc$GT$::alloc::hd4fce9f3a1e51fd6 (alloc.rs:172)
==1408==    by 0xA6E9FF1: alloc::raw_vec::RawVec$LT$T$C$A$GT$::allocate_in::h4992af088d19145b (raw_vec.rs:98)
==1408==    by 0xA6E9E65: alloc::raw_vec::RawVec$LT$T$GT$::with_capacity::h40203dd55ac5499a (raw_vec.rs:142)
==1408==    by 0xA6F5033: alloc::vec::Vec$LT$T$GT$::with_capacity::ha16120202ccaaf0f (vec.rs:356)
==1408==    by 0xA6F0728: mio::sys::unix::epoll::Events::with_capacity::h7ee83912fc62963e (epoll.rs:184)
==1408==    by 0xA6E4C37: mio::poll::Events::with_capacity::h76433514da4913ed (poll.rs:1362)
==1408==    by 0xA63B1CD: tokio_reactor::Reactor::new::hded064c06aa038fb (lib.rs:256)
==1408==    by 0xA61AF56: tokio::runtime::threadpool::builder::Builder::build::h5c3ebe8c09353878 (builder.rs:356)
==1408==    by 0xA61A374: tokio::runtime::threadpool::Runtime::new::h1069aba4c726ff08 (mod.rs:145)
==1408==    by 0xA4BC54D: __static_ref_initialize (sync.rs:18)
==1408==    by 0xA4BC54D: core::ops::function::FnOnce::call_once::h343c611514ba4125 (function.rs:235)
==1408== 
==1408== 99,968 bytes in 1,562 blocks are possibly lost in loss record 4,084 of 4,104
==1408==    at 0x4C2BBAF: malloc (vg_replace_malloc.c:299)
==1408==    by 0xA25005B: alloc::alloc::alloc::he71f6672ff69332a (alloc.rs:84)
==1408==    by 0xA24FFCB: alloc::alloc::exchange_malloc::h46527e14373cbbb6 (alloc.rs:206)
==1408==    by 0xA281811: new<terminus_store::store::sync::SyncNamedGraph> (boxed.rs:121)
==1408==    by 0xA281811: open_named_graph (lib.rs:70)
==1408==    by 0x9EB18A6: pl_open_named_graph (terminus_store.c:56)
==1408==    by 0x4E79FA5: PL_next_solution (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EB6C2B: callProlog (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EE5A3B: start_thread (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x60384A3: start_thread (pthread_create.c:456)
==1408== 
==1408== 115,536 bytes in 574 blocks are still reachable in loss record 4,085 of 4,104
==1408==    at 0x4C2BBAF: malloc (vg_replace_malloc.c:299)
==1408==    by 0x4E63358: PL_malloc_atomic_unmanaged (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E95587: compileClause (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E95BCE: assert_term (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E95FE5: pl_assertz11_va (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E79C62: PL_next_solution (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EB675A: query_loop (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EB6F8A: prologToplevel (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EF4ACF: PL_initialise (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x108718: main (in /usr/lib/swipl/bin/x86_64-linux/swipl)
==1408== 
==1408== 118,848 bytes in 7,428 blocks are still reachable in loss record 4,086 of 4,104
==1408==    at 0x4C2BBAF: malloc (vg_replace_malloc.c:299)
==1408==    by 0x4E60CCD: allocHeapOrHalt (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EA8621: import (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EA8B70: pl_import2_va (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E79C62: PL_next_solution (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EB675A: query_loop (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EB6F8A: prologToplevel (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EF4ACF: PL_initialise (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x108718: main (in /usr/lib/swipl/bin/x86_64-linux/swipl)
==1408== 
==1408== 121,344 bytes in 1,264 blocks are still reachable in loss record 4,087 of 4,104
==1408==    at 0x4C2BBAF: malloc (vg_replace_malloc.c:299)
==1408==    by 0x4E60CCD: allocHeapOrHalt (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EB9186: lookupProcedure (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EA9293: export_pi (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EA92F0: pl_export1_va (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E79C62: PL_next_solution (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EB675A: query_loop (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EB6F8A: prologToplevel (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EF4ACF: PL_initialise (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x108718: main (in /usr/lib/swipl/bin/x86_64-linux/swipl)
==1408== 
==1408== 131,072 bytes in 1 blocks are still reachable in loss record 4,088 of 4,104
==1408==    at 0x4C2BBAF: malloc (vg_replace_malloc.c:299)
==1408==    by 0x4E60CCD: allocHeapOrHalt (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E5D154: lookupBlob (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E5D6F4: lookupAtom (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EC448C: get_token__LD.part.37 (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EC4EBB: simple_term (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EC6FFB: complex_term (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EC7A2A: read_term (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EC90E0: read_clause (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EC9374: pl_read_clause3_va (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E79C62: PL_next_solution (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EB675A: query_loop (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408== 
==1408== 135,264 bytes in 1,409 blocks are still reachable in loss record 4,089 of 4,104
==1408==    at 0x4C2BBAF: malloc (vg_replace_malloc.c:299)
==1408==    by 0x4E60CCD: allocHeapOrHalt (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EB9186: lookupProcedure (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EBB445: pl_get_predicate_attribute (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E79FA5: PL_next_solution (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EB675A: query_loop (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EB6F8A: prologToplevel (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EF4ACF: PL_initialise (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x108718: main (in /usr/lib/swipl/bin/x86_64-linux/swipl)
==1408== 
==1408== 151,216 bytes in 610 blocks are still reachable in loss record 4,090 of 4,104
==1408==    at 0x4C2BBAF: malloc (vg_replace_malloc.c:299)
==1408==    by 0x4E63358: PL_malloc_atomic_unmanaged (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EDBC36: loadPredicate (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EDC5CC: loadPart (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EDCA4D: loadStatement (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EDD25B: loadWicFromStream (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EF4983: PL_initialise (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x108718: main (in /usr/lib/swipl/bin/x86_64-linux/swipl)
==1408== 
==1408== 162,352 bytes in 1,274 blocks are still reachable in loss record 4,091 of 4,104
==1408==    at 0x4C2BBAF: malloc (vg_replace_malloc.c:299)
==1408==    by 0x4E63358: PL_malloc_atomic_unmanaged (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E95587: compileClause (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E95BCE: assert_term (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E95FE5: pl_assertz11_va (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E79C62: PL_next_solution (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EB6C2B: callProlog (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4F08067: pl_with_mutex (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E79F63: PL_next_solution (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EB6C2B: callProlog (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EB6E3C: pl_notrace1_va (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E79C62: PL_next_solution (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408== 
==1408== 163,840 bytes in 3 blocks are still reachable in loss record 4,092 of 4,104
==1408==    at 0x4C2DDCF: realloc (vg_replace_malloc.c:785)
==1408==    by 0xA66B2FC: alloc::alloc::realloc::h8a506f3cbb6b31d2 (alloc.rs:128)
==1408==    by 0xA66B085: _$LT$alloc..alloc..Global$u20$as$u20$core..alloc..Alloc$GT$::realloc::hb3182a071ab63438 (alloc.rs:187)
==1408==    by 0xA68B4FC: alloc::raw_vec::RawVec$LT$T$C$A$GT$::reserve_internal::h831f47b119d22fb7 (raw_vec.rs:670)
==1408==    by 0xA68C321: alloc::raw_vec::RawVec$LT$T$C$A$GT$::reserve::h0ab4df7f50cf5bac (raw_vec.rs:495)
==1408==    by 0xA67AB59: alloc::vec::Vec$LT$T$GT$::reserve::h52b1de304f27808a (vec.rs:458)
==1408==    by 0xA67A9D8: alloc::vec::Vec$LT$T$GT$::push::ha3ca665daa1a1f25 (vec.rs:1103)
==1408==    by 0xA6A52CA: slab::Slab$LT$T$GT$::insert_at::h368e1806d6c9a02c (lib.rs:632)
==1408==    by 0xA6A4F8D: slab::Slab$LT$T$GT$::insert::h00760eb0289b4efd (lib.rs:593)
==1408==    by 0xA68D0FD: tokio_threadpool::worker::entry::WorkerEntry::register_task::h2dc22e18db88036e (entry.rs:246)
==1408==    by 0xA69B3C2: tokio_threadpool::worker::Worker::run_task::h1ecf2a8dd4e7574e (mod.rs:456)
==1408==    by 0xA69AEDF: tokio_threadpool::worker::Worker::try_steal_task::hbf0e24cb88f05cbd (mod.rs:416)
==1408== 
==1408== 199,848 bytes in 711 blocks are still reachable in loss record 4,093 of 4,104
==1408==    at 0x4C2BBAF: malloc (vg_replace_malloc.c:299)
==1408==    by 0x4E63358: PL_malloc_atomic_unmanaged (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E95587: compileClause (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E95BCE: assert_term (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E96136: record_clause (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E961C9: pl_record_clause3_va (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E79C62: PL_next_solution (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EB675A: query_loop (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EB6F8A: prologToplevel (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E6D8AC: PL_toplevel (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x108724: main (in /usr/lib/swipl/bin/x86_64-linux/swipl)
==1408== 
==1408== 320,912 bytes in 160 blocks are still reachable in loss record 4,094 of 4,104
==1408==    at 0x4C2DDCF: realloc (vg_replace_malloc.c:785)
==1408==    by 0xA6D97FC: alloc::alloc::realloc::h44faf8366b2dd8f5 (alloc.rs:128)
==1408==    by 0xA6D95C5: _$LT$alloc..alloc..Global$u20$as$u20$core..alloc..Alloc$GT$::realloc::h1401f2a0cb8d81be (alloc.rs:187)
==1408==    by 0xA6E2A2C: alloc::raw_vec::RawVec$LT$T$C$A$GT$::reserve_internal::h4c6160995cb76387 (raw_vec.rs:670)
==1408==    by 0xA6E2DF1: alloc::raw_vec::RawVec$LT$T$C$A$GT$::reserve::h02e7114964f068bf (raw_vec.rs:495)
==1408==    by 0xA6DAD09: alloc::vec::Vec$LT$T$GT$::reserve::h3e715be6561b28cf (vec.rs:458)
==1408==    by 0xA439AD7: std::io::read_to_end_with_reservation::ha4f7121a3e92b640 (mod.rs:369)
==1408==    by 0xA4399D9: std::io::read_to_end::h26ee9a584052e32d (mod.rs:356)
==1408==    by 0xA439F54: std::io::Read::read_to_end::h2b0c8315939ac5b6 (mod.rs:648)
==1408==    by 0xA5248F2: _$LT$tokio_io..io..read_to_end..ReadToEnd$LT$A$GT$$u20$as$u20$futures..future..Future$GT$::poll::hb08d089e542610dd (read_to_end.rs:56)
==1408==    by 0xA364B9E: _$LT$futures..future..map..Map$LT$A$C$F$GT$$u20$as$u20$futures..future..Future$GT$::poll::h6f8cdb8e2e4ee012 (map.rs:30)
==1408==    by 0xA394429: _$LT$futures..future..either..Either$LT$A$C$B$GT$$u20$as$u20$futures..future..Future$GT$::poll::h5e4dca6ff650928d (either.rs:36)
==1408== 
==1408== 330,120 bytes in 655 blocks are still reachable in loss record 4,095 of 4,104
==1408==    at 0x4C2BBAF: malloc (vg_replace_malloc.c:299)
==1408==    by 0xA66B22B: alloc::alloc::alloc::h5720529faf6a4289 (alloc.rs:84)
==1408==    by 0xA66B19B: alloc::alloc::exchange_malloc::h96e4bb19607190fc (alloc.rs:206)
==1408==    by 0xA677F65: new<crossbeam_queue::seg_queue::Block<alloc::sync::Arc<tokio_threadpool::task::Task>>> (boxed.rs:121)
==1408==    by 0xA677F65: crossbeam_queue::seg_queue::SegQueue$LT$T$GT$::push::h60912bed1aa34d4c (seg_queue.rs:201)
==1408==    by 0xA68D264: tokio_threadpool::worker::entry::WorkerEntry::remotely_complete_task::hb324e0f074062ccf (entry.rs:266)
==1408==    by 0xA69BA66: tokio_threadpool::worker::Worker::run_task::h1ecf2a8dd4e7574e (mod.rs:515)
==1408==    by 0xA69AA56: tokio_threadpool::worker::Worker::try_run_owned_task::hd80973516daca83e (mod.rs:390)
==1408==    by 0xA69A49C: tokio_threadpool::worker::Worker::try_run_task::h54601ba776355230 (mod.rs:297)
==1408==    by 0xA69A2F5: tokio_threadpool::worker::Worker::run::ha620171d3f5df792 (mod.rs:241)
==1408==    by 0xA61BA9B: tokio::runtime::threadpool::builder::Builder::build::_$u7b$$u7b$closure$u7d$$u7d$::_$u7b$$u7b$closure$u7d$$u7d$::_$u7b$$u7b$closure$u7d$$u7d$::_$u7b$$u7b$closure$u7d$$u7d$::hf1a0f86368d24d90 (builder.rs:390)
==1408==    by 0xA629E3B: tokio_timer::timer::handle::with_default::_$u7b$$u7b$closure$u7d$$u7d$::h7f92321d998b87fc (handle.rs:101)
==1408==    by 0xA627C63: std::thread::local::LocalKey$LT$T$GT$::try_with::h0591efca88fdf8c2 (local.rs:262)
==1408== 
==1408== 330,752 bytes in 125 blocks are still reachable in loss record 4,096 of 4,104
==1408==    at 0x4C2BBAF: malloc (vg_replace_malloc.c:299)
==1408==    by 0x4E60CCD: allocHeapOrHalt (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4F1F10A: htable_alloc_kvs (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4F1F47F: htable_put (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4F1F891: addHTable (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EA867E: import (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EA8B70: pl_import2_va (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E79C62: PL_next_solution (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EB675A: query_loop (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EB6F8A: prologToplevel (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EF4ACF: PL_initialise (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x108718: main (in /usr/lib/swipl/bin/x86_64-linux/swipl)
==1408== 
==1408== 360,448 bytes in 5 blocks are still reachable in loss record 4,097 of 4,104
==1408==    at 0x4C2DDCF: realloc (vg_replace_malloc.c:785)
==1408==    by 0xA66B2FC: alloc::alloc::realloc::h8a506f3cbb6b31d2 (alloc.rs:128)
==1408==    by 0xA66B085: _$LT$alloc..alloc..Global$u20$as$u20$core..alloc..Alloc$GT$::realloc::hb3182a071ab63438 (alloc.rs:187)
==1408==    by 0xA68B4FC: alloc::raw_vec::RawVec$LT$T$C$A$GT$::reserve_internal::h831f47b119d22fb7 (raw_vec.rs:670)
==1408==    by 0xA68C321: alloc::raw_vec::RawVec$LT$T$C$A$GT$::reserve::h0ab4df7f50cf5bac (raw_vec.rs:495)
==1408==    by 0xA67AB59: alloc::vec::Vec$LT$T$GT$::reserve::h52b1de304f27808a (vec.rs:458)
==1408==    by 0xA67A9D8: alloc::vec::Vec$LT$T$GT$::push::ha3ca665daa1a1f25 (vec.rs:1103)
==1408==    by 0xA6A52CA: slab::Slab$LT$T$GT$::insert_at::h368e1806d6c9a02c (lib.rs:632)
==1408==    by 0xA6A4F8D: slab::Slab$LT$T$GT$::insert::h00760eb0289b4efd (lib.rs:593)
==1408==    by 0xA68D0FD: tokio_threadpool::worker::entry::WorkerEntry::register_task::h2dc22e18db88036e (entry.rs:246)
==1408==    by 0xA69B3C2: tokio_threadpool::worker::Worker::run_task::h1ecf2a8dd4e7574e (mod.rs:456)
==1408==    by 0xA69AA56: tokio_threadpool::worker::Worker::try_run_owned_task::hd80973516daca83e (mod.rs:390)
==1408== 
==1408== 441,456 bytes in 18,394 blocks are still reachable in loss record 4,098 of 4,104
==1408==    at 0x4C2BBAF: malloc (vg_replace_malloc.c:299)
==1408==    by 0x4E60CCD: allocHeapOrHalt (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EB8B75: newClauseRef (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EB8C11: assertProcedure (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E95CC7: assert_term (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E96136: record_clause (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E961C9: pl_record_clause3_va (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E79C62: PL_next_solution (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EB675A: query_loop (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EB6F8A: prologToplevel (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EF4ACF: PL_initialise (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x108718: main (in /usr/lib/swipl/bin/x86_64-linux/swipl)
==1408== 
==1408== 460,440 bytes in 1,740 blocks are still reachable in loss record 4,099 of 4,104
==1408==    at 0x4C2BBAF: malloc (vg_replace_malloc.c:299)
==1408==    by 0x4E63358: PL_malloc_atomic_unmanaged (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EDBC36: loadPredicate (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EDC5CC: loadPart (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EDCA4D: loadStatement (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EDC5CC: loadPart (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EDCA4D: loadStatement (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EDC5CC: loadPart (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EDCA4D: loadStatement (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EDD25B: loadWicFromStream (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EF4983: PL_initialise (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x108718: main (in /usr/lib/swipl/bin/x86_64-linux/swipl)
==1408== 
==1408== 589,824 bytes in 2 blocks are possibly lost in loss record 4,100 of 4,104
==1408==    at 0x4C2BBAF: malloc (vg_replace_malloc.c:299)
==1408==    by 0x4E63358: PL_malloc_atomic_unmanaged (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E5C628: allocateAtomBlock.part.6 (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E5D474: lookupBlob (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E5D6F4: lookupAtom (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4F2191C: PL_unify_text (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EB4603: atomic_list_concat (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E79C62: PL_next_solution (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EB675A: query_loop (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EB6F8A: prologToplevel (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EF4ACF: PL_initialise (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x108718: main (in /usr/lib/swipl/bin/x86_64-linux/swipl)
==1408== 
==1408== 786,432 bytes in 1 blocks are still reachable in loss record 4,101 of 4,104
==1408==    at 0x4C2BBAF: malloc (vg_replace_malloc.c:299)
==1408==    by 0x4E63358: PL_malloc_atomic_unmanaged (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E5C628: allocateAtomBlock.part.6 (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E5D474: lookupBlob (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E5D6F4: lookupAtom (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EC448C: get_token__LD.part.37 (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EC4EBB: simple_term (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EC6FFB: complex_term (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EC7A2A: read_term (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EC90E0: read_clause (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EC9374: pl_read_clause3_va (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E79C62: PL_next_solution (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408== 
==1408== 1,532,416 bytes in 11,972 blocks are still reachable in loss record 4,102 of 4,104
==1408==    at 0x4C2BBAF: malloc (vg_replace_malloc.c:299)
==1408==    by 0xA66B22B: alloc::alloc::alloc::h5720529faf6a4289 (alloc.rs:84)
==1408==    by 0xA66B19B: alloc::alloc::exchange_malloc::h96e4bb19607190fc (alloc.rs:206)
==1408==    by 0xA6A6DBD: alloc::sync::Arc$LT$T$GT$::new::h39278ecbb3715592 (sync.rs:306)
==1408==    by 0xA6992FF: _$LT$$RF$tokio_threadpool..sender..Sender$u20$as$u20$tokio_executor..executor..Executor$GT$::spawn::he41a8e947bc04f02 (sender.rs:168)
==1408==    by 0xA4B9EE5: tokio_threadpool::sender::Sender::spawn::hff27a00b7db63eee (sender.rs:84)
==1408==    by 0xA4E555D: _$LT$tokio_threadpool..sender..Sender$u20$as$u20$futures..future..Executor$LT$T$GT$$GT$::execute::heb0eb5a86d418f85 (sender.rs:207)
==1408==    by 0xA4B84B1: _$LT$tokio..runtime..threadpool..task_executor..TaskExecutor$u20$as$u20$futures..future..Executor$LT$T$GT$$GT$::execute::hba082e0ddfdb4670 (task_executor.rs:65)
==1408==    by 0xA38E4BB: futures::sync::oneshot::spawn::h1836a491ca64c44b (oneshot.rs:517)
==1408==    by 0xA5BF4BB: terminus_store::store::sync::SyncStore::open::hdc8943dc25014f5b (sync.rs:282)
==1408==    by 0xA28170E: open_named_graph (lib.rs:67)
==1408==    by 0x9EB18A6: pl_open_named_graph (terminus_store.c:56)
==1408== 
==1408== 1,532,416 bytes in 11,972 blocks are still reachable in loss record 4,103 of 4,104
==1408==    at 0x4C2BBAF: malloc (vg_replace_malloc.c:299)
==1408==    by 0xA66B22B: alloc::alloc::alloc::h5720529faf6a4289 (alloc.rs:84)
==1408==    by 0xA66B19B: alloc::alloc::exchange_malloc::h96e4bb19607190fc (alloc.rs:206)
==1408==    by 0xA6A6DBD: alloc::sync::Arc$LT$T$GT$::new::h39278ecbb3715592 (sync.rs:306)
==1408==    by 0xA6992FF: _$LT$$RF$tokio_threadpool..sender..Sender$u20$as$u20$tokio_executor..executor..Executor$GT$::spawn::he41a8e947bc04f02 (sender.rs:168)
==1408==    by 0xA4B9DC5: tokio_threadpool::sender::Sender::spawn::hedd6cb8ecb813ff8 (sender.rs:84)
==1408==    by 0xA4E538D: _$LT$tokio_threadpool..sender..Sender$u20$as$u20$futures..future..Executor$LT$T$GT$$GT$::execute::hc30fa031c999c532 (sender.rs:207)
==1408==    by 0xA4B838F: _$LT$tokio..runtime..threadpool..task_executor..TaskExecutor$u20$as$u20$futures..future..Executor$LT$T$GT$$GT$::execute::h78ab25535157aa3b (task_executor.rs:65)
==1408==    by 0xA38F6A5: futures::sync::oneshot::spawn::hbf18f4b4bf54e1ba (oneshot.rs:517)
==1408==    by 0xA5BEFF7: terminus_store::store::sync::SyncNamedGraph::head::ha3dbb3d9050b7690 (sync.rs:236)
==1408==    by 0xA281B08: named_graph_get_head (lib.rs:85)
==1408==    by 0x9EB194C: pl_head (terminus_store.c:76)
==1408== 
==1408== 4,341,320 bytes in 18,394 blocks are still reachable in loss record 4,104 of 4,104
==1408==    at 0x4C2BBAF: malloc (vg_replace_malloc.c:299)
==1408==    by 0x4E63358: PL_malloc_atomic_unmanaged (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E95587: compileClause (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E95BCE: assert_term (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E96136: record_clause (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E961C9: pl_record_clause3_va (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E79C62: PL_next_solution (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EB675A: query_loop (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EB6F8A: prologToplevel (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EF4ACF: PL_initialise (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x108718: main (in /usr/lib/swipl/bin/x86_64-linux/swipl)
==1408== 
==1408== LEAK SUMMARY:
==1408==    definitely lost: 160,800 bytes in 205 blocks
==1408==    indirectly lost: 8,192 bytes in 1 blocks
==1408==      possibly lost: 1,221,173 bytes in 14,903 blocks
==1408==    still reachable: 15,880,378 bytes in 149,101 blocks
==1408==                       of which reachable via heuristic:
==1408==                         length64           : 106,520 bytes in 3 blocks
==1408==                         newarray           : 55,160 bytes in 1,594 blocks
==1408==         suppressed: 0 bytes in 0 blocks
==1408== 
==1408== For counts of detected and suppressed errors, rerun with: -v
==1408== Use --track-origins=yes to see where uninitialised values come from
==1408== ERROR SUMMARY: 69934 errors from 461 contexts (suppressed: 0 from 0)",2019-11-06T13:23:13Z,rrooij,https://github.com/terminusdb/terminusdb/issues/36#issuecomment-550306581,"Useful bash one-liner:
while :; do curl 'http://localhost:6363/' -H 'Authorization: Basic OnJvb3Q='; done

Start up a system monitor and see the memory usage growing",useful bash one - liner while do curl ' http//localhost6363/ ' -h ' authorization basic onjvb3q= ' do start up a system monitor and see the memory usage grow,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,36,2019-11-05T11:03:16Z,rrooij,Memory leaks,https://github.com/terminusdb/terminusdb/issues/36,"Problem
When querying a database, the memory usage of terminus-server keeps growing. This occurs on the dev branch with the terminus-store backend.
How to reproduce

Start a system monitor (for instance gnome-system-monitor)
Play around in the dashboard (click on schema, query etc.) and see the memory size of terminus-server getting bigger.

Additional information
Some valgrind logs of querying and the like:
==1408== 39,312 bytes in 1 blocks are still reachable in loss record 4,049 of 4,104
==1408==    at 0x4C2BBAF: malloc (vg_replace_malloc.c:299)
==1408==    by 0x4E63358: PL_malloc_atomic_unmanaged (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E5E9AE: do_init_atoms (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E6515C: PL_new_atom (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E5CD68: PL_register_blob_type (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EFE9FF: initReservedSymbols (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E5EB38: do_init_atoms (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E6515C: PL_new_atom (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4F2C681: setPrologFlag (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E6DFAD: PL_set_prolog_flag (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EF42AD: PL_initialise (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x108718: main (in /usr/lib/swipl/bin/x86_64-linux/swipl)
==1408== 
==1408== 39,312 bytes in 1 blocks are possibly lost in loss record 4,050 of 4,104
==1408==    at 0x4C2BBAF: malloc (vg_replace_malloc.c:299)
==1408==    by 0x4E63358: PL_malloc_atomic_unmanaged (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E5E9AE: do_init_atoms (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E6515C: PL_new_atom (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4F2C681: setPrologFlag (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E6DFAD: PL_set_prolog_flag (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EF42AD: PL_initialise (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x108718: main (in /usr/lib/swipl/bin/x86_64-linux/swipl)
==1408== 
==1408== 39,936 bytes in 18 blocks are still reachable in loss record 4,051 of 4,104
==1408==    at 0x4C2BBAF: malloc (vg_replace_malloc.c:299)
==1408==    by 0x4E60CCD: allocHeapOrHalt (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4F1F10A: htable_alloc_kvs (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4F1F47F: htable_put (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4F1F891: addHTable (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EB923A: lookupProcedure (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EDB3F5: loadXRc (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EDBE20: loadPredicate (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EDC5CC: loadPart (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EDCA4D: loadStatement (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EDC5CC: loadPart (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EDCA4D: loadStatement (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408== 
==1408== 41,400 bytes in 1,035 blocks are still reachable in loss record 4,052 of 4,104
==1408==    at 0x4C2BBAF: malloc (vg_replace_malloc.c:299)
==1408==    by 0x4E60CCD: allocHeapOrHalt (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E9BB28: lookupFunctorDef (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EAAB96: pl_functor3_va (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E79C62: PL_next_solution (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EB6C2B: callProlog (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4F08067: pl_with_mutex (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E79F63: PL_next_solution (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EB6C2B: callProlog (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EB6E3C: pl_notrace1_va (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E79C62: PL_next_solution (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EBB0F1: trapUndefined (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408== 
==1408== 41,472 bytes in 432 blocks are still reachable in loss record 4,053 of 4,104
==1408==    at 0x4C2BBAF: malloc (vg_replace_malloc.c:299)
==1408==    by 0x4E60CCD: allocHeapOrHalt (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EB9186: lookupProcedure (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EBB445: pl_get_predicate_attribute (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E79FA5: PL_next_solution (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EB675A: query_loop (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EB6F8A: prologToplevel (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E6D8AC: PL_toplevel (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x108724: main (in /usr/lib/swipl/bin/x86_64-linux/swipl)
==1408== 
==1408== 41,760 bytes in 1,740 blocks are still reachable in loss record 4,054 of 4,104
==1408==    at 0x4C2BBAF: malloc (vg_replace_malloc.c:299)
==1408==    by 0x4E60CCD: allocHeapOrHalt (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EB8B75: newClauseRef (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EB8C11: assertProcedure (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EDBE90: loadPredicate (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EDC5CC: loadPart (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EDCA4D: loadStatement (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EDC5CC: loadPart (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EDCA4D: loadStatement (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EDC5CC: loadPart (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EDCA4D: loadStatement (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EDD25B: loadWicFromStream (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408== 
==1408== 41,984 bytes in 8 blocks are still reachable in loss record 4,055 of 4,104
==1408==    at 0x4C2BBAF: malloc (vg_replace_malloc.c:299)
==1408==    by 0x4E60CCD: allocHeapOrHalt (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4F1F10A: htable_alloc_kvs (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4F1F47F: htable_put (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4F1F891: addHTable (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EB923A: lookupProcedure (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E8DCC5: compileSubClause (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E9399F: compileBody (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E94FFC: compileClause (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E95BCE: assert_term (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E96136: record_clause (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E961C9: pl_record_clause3_va (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408== 
==1408== 42,744 bytes in 137 blocks are possibly lost in loss record 4,056 of 4,104
==1408==    at 0x4C2BBAF: malloc (vg_replace_malloc.c:299)
==1408==    by 0xA25005B: alloc::alloc::alloc::he71f6672ff69332a (alloc.rs:84)
==1408==    by 0xA24FFCB: alloc::alloc::exchange_malloc::h46527e14373cbbb6 (alloc.rs:206)
==1408==    by 0xA1844FF: new<terminus_store::layer::child::ChildSubjectLookup<terminus_store::storage::directory::SharedMmap>> (boxed.rs:121)
==1408==    by 0xA1844FF: _$LT$terminus_store..layer..child..ChildLayer$LT$M$GT$$u20$as$u20$terminus_store..layer..layer..Layer$GT$::lookup_subject::h0c968e5564fb6aac (child.rs:378)
==1408==    by 0xA34C2AA: _$LT$terminus_store..store..StoreLayer$u20$as$u20$terminus_store..layer..layer..Layer$GT$::lookup_subject::hc31873af51cae373 (mod.rs:205)
==1408==    by 0xA5BEC30: _$LT$terminus_store..store..sync..SyncStoreLayer$u20$as$u20$terminus_store..layer..layer..Layer$GT$::lookup_subject::hf2365060bef0b9b4 (sync.rs:163)
==1408==    by 0xA284D7D: layer_lookup_subject (lib.rs:351)
==1408==    by 0x9EB2A6B: pl_layer_lookup_subject (terminus_store.c:589)
==1408==    by 0x4E79FA5: PL_next_solution (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EB6C2B: callProlog (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EE5A3B: start_thread (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x60384A3: start_thread (pthread_create.c:456)
==1408== 
==1408== 44,032 bytes in 10 blocks are still reachable in loss record 4,057 of 4,104
==1408==    at 0x4C2BBAF: malloc (vg_replace_malloc.c:299)
==1408==    by 0x4E60CCD: allocHeapOrHalt (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4F1F10A: htable_alloc_kvs (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4F1F47F: htable_put (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4F1F891: addHTable (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EB923A: lookupProcedure (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E8DCC5: compileSubClause (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E9399F: compileBody (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E9396F: compileBody (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E94FFC: compileClause (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E95BCE: assert_term (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E96136: record_clause (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408== 
==1408== 44,032 bytes in 56 blocks are still reachable in loss record 4,058 of 4,104
==1408==    at 0x4C2BBAF: malloc (vg_replace_malloc.c:299)
==1408==    by 0x4E60CCD: allocHeapOrHalt (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4F1F10A: htable_alloc_kvs (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4F1F47F: htable_put (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4F1F901: updateHTable (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EA9135: exportProcedure (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EA92D8: export_pi (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EA92F0: pl_export1_va (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E79C62: PL_next_solution (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EB675A: query_loop (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EB6F8A: prologToplevel (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EF4ACF: PL_initialise (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408== 
==1408== 46,608 bytes in 1,942 blocks are still reachable in loss record 4,059 of 4,104
==1408==    at 0x4C2BBAF: malloc (vg_replace_malloc.c:299)
==1408==    by 0x4E60CCD: allocHeapOrHalt (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EB8B75: newClauseRef (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E616D2: addClauseBucket (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E618AD: addClauseToIndex (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E61BC0: hashDefinition (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E62368: first_clause_guarded (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E636FA: firstClause (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E7B4D8: PL_next_solution (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EB675A: query_loop (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EB6F8A: prologToplevel (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EF4ACF: PL_initialise (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408== 
==1408== 47,104 bytes in 7 blocks are still reachable in loss record 4,060 of 4,104
==1408==    at 0x4C2BBAF: malloc (vg_replace_malloc.c:299)
==1408==    by 0x4E60CCD: allocHeapOrHalt (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4F1F10A: htable_alloc_kvs (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4F1F47F: htable_put (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4F1F891: addHTable (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EB923A: lookupProcedure (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E95E06: assert_term (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E96136: record_clause (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E961C9: pl_record_clause3_va (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E79C62: PL_next_solution (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EB675A: query_loop (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EB6F8A: prologToplevel (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408== 
==1408== 49,152 bytes in 1 blocks are possibly lost in loss record 4,061 of 4,104
==1408==    at 0x4C2BBAF: malloc (vg_replace_malloc.c:299)
==1408==    by 0x4E63358: PL_malloc_atomic_unmanaged (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E5C628: allocateAtomBlock.part.6 (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E5D474: lookupBlob (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E5D6F4: lookupAtom (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E9A5DD: registerBuiltins (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E9A908: initBuildIns (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4ED0CCC: setupProlog (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EF42B2: PL_initialise (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x108718: main (in /usr/lib/swipl/bin/x86_64-linux/swipl)
==1408== 
==1408== 49,248 bytes in 2 blocks are still reachable in loss record 4,062 of 4,104
==1408==    at 0x4C2BBAF: malloc (vg_replace_malloc.c:299)
==1408==    by 0x4E60CCD: allocHeapOrHalt (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E61ACD: hashDefinition (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E62368: first_clause_guarded (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E636FA: firstClause (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E7B4D8: PL_next_solution (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EBB0F1: trapUndefined (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E72C10: PL_next_solution (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EB675A: query_loop (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EB6F8A: prologToplevel (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EF4ACF: PL_initialise (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x108718: main (in /usr/lib/swipl/bin/x86_64-linux/swipl)
==1408== 
==1408== 49,440 bytes in 56 blocks are still reachable in loss record 4,063 of 4,104
==1408==    at 0x4C2BBAF: malloc (vg_replace_malloc.c:299)
==1408==    by 0x4E60CCD: allocHeapOrHalt (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E61ACD: hashDefinition (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E62368: first_clause_guarded (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E636FA: firstClause (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E7B4D8: PL_next_solution (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EB675A: query_loop (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EB6F8A: prologToplevel (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EF4ACF: PL_initialise (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x108718: main (in /usr/lib/swipl/bin/x86_64-linux/swipl)
==1408== 
==1408== 54,643 bytes in 1 blocks are still reachable in loss record 4,064 of 4,104
==1408==    at 0x4C2BBAF: malloc (vg_replace_malloc.c:299)
==1408==    by 0x4E63358: PL_malloc_atomic_unmanaged (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E8D2C8: initWamTable (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4ED0B9E: setupProlog (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EF42B2: PL_initialise (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x108718: main (in /usr/lib/swipl/bin/x86_64-linux/swipl)
==1408== 
==1408== 56,952 bytes in 113 blocks are still reachable in loss record 4,065 of 4,104
==1408==    at 0x4C2BBAF: malloc (vg_replace_malloc.c:299)
==1408==    by 0xA66B22B: alloc::alloc::alloc::h5720529faf6a4289 (alloc.rs:84)
==1408==    by 0xA66B19B: alloc::alloc::exchange_malloc::h96e4bb19607190fc (alloc.rs:206)
==1408==    by 0xA677F65: new<crossbeam_queue::seg_queue::Block<alloc::sync::Arc<tokio_threadpool::task::Task>>> (boxed.rs:121)
==1408==    by 0xA677F65: crossbeam_queue::seg_queue::SegQueue$LT$T$GT$::push::h60912bed1aa34d4c (seg_queue.rs:201)
==1408==    by 0xA68D264: tokio_threadpool::worker::entry::WorkerEntry::remotely_complete_task::hb324e0f074062ccf (entry.rs:266)
==1408==    by 0xA69BA66: tokio_threadpool::worker::Worker::run_task::h1ecf2a8dd4e7574e (mod.rs:515)
==1408==    by 0xA69AEDF: tokio_threadpool::worker::Worker::try_steal_task::hbf0e24cb88f05cbd (mod.rs:416)
==1408==    by 0xA69A4B9: tokio_threadpool::worker::Worker::try_run_task::h54601ba776355230 (mod.rs:301)
==1408==    by 0xA69A2F5: tokio_threadpool::worker::Worker::run::ha620171d3f5df792 (mod.rs:241)
==1408==    by 0xA61BA9B: tokio::runtime::threadpool::builder::Builder::build::_$u7b$$u7b$closure$u7d$$u7d$::_$u7b$$u7b$closure$u7d$$u7d$::_$u7b$$u7b$closure$u7d$$u7d$::_$u7b$$u7b$closure$u7d$$u7d$::hf1a0f86368d24d90 (builder.rs:390)
==1408==    by 0xA629E3B: tokio_timer::timer::handle::with_default::_$u7b$$u7b$closure$u7d$$u7d$::h7f92321d998b87fc (handle.rs:101)
==1408==    by 0xA627C63: std::thread::local::LocalKey$LT$T$GT$::try_with::h0591efca88fdf8c2 (local.rs:262)
==1408== 
==1408== 58,544 bytes in 3,659 blocks are still reachable in loss record 4,066 of 4,104
==1408==    at 0x4C2BBAF: malloc (vg_replace_malloc.c:299)
==1408==    by 0x4E60CCD: allocHeapOrHalt (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EEE291: addProcedureSourceFile (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E95CB0: assert_term (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E96136: record_clause (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E961C9: pl_record_clause3_va (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E79C62: PL_next_solution (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EB675A: query_loop (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EB6F8A: prologToplevel (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EF4ACF: PL_initialise (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x108718: main (in /usr/lib/swipl/bin/x86_64-linux/swipl)
==1408== 
==1408== 59,920 bytes in 1,498 blocks are still reachable in loss record 4,067 of 4,104
==1408==    at 0x4C2BBAF: malloc (vg_replace_malloc.c:299)
==1408==    by 0x4E60CCD: allocHeapOrHalt (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E9BB28: lookupFunctorDef (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EC05F9: build_term (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EC5B14: simple_term (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EC6FFB: complex_term (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EC7A2A: read_term (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EC90E0: read_clause (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EC9374: pl_read_clause3_va (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E79C62: PL_next_solution (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EB675A: query_loop (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EB6F8A: prologToplevel (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408== 
==1408== 64,416 bytes in 671 blocks are still reachable in loss record 4,068 of 4,104
==1408==    at 0x4C2BBAF: malloc (vg_replace_malloc.c:299)
==1408==    by 0x4E60CCD: allocHeapOrHalt (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EB9186: lookupProcedure (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E8DCC5: compileSubClause (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E9399F: compileBody (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E94FFC: compileClause (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E95BCE: assert_term (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E96136: record_clause (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E961C9: pl_record_clause3_va (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E79C62: PL_next_solution (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EB675A: query_loop (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EB6F8A: prologToplevel (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408== 
==1408== 64,524 bytes in 6,007 blocks are still reachable in loss record 4,069 of 4,104
==1408==    at 0x4C2BBAF: malloc (vg_replace_malloc.c:299)
==1408==    by 0x4E63358: PL_malloc_atomic_unmanaged (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E5D4D8: lookupBlob (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E5D6F4: lookupAtom (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EC448C: get_token__LD.part.37 (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EC4EBB: simple_term (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EC6FFB: complex_term (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EC7A2A: read_term (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EC90E0: read_clause (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EC9374: pl_read_clause3_va (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E79C62: PL_next_solution (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EB675A: query_loop (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408== 
==1408== 64,800 bytes in 675 blocks are still reachable in loss record 4,070 of 4,104
==1408==    at 0x4C2BBAF: malloc (vg_replace_malloc.c:299)
==1408==    by 0x4E60CCD: allocHeapOrHalt (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EB9186: lookupProcedure (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E95E06: assert_term (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E96136: record_clause (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E961C9: pl_record_clause3_va (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E79C62: PL_next_solution (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EB675A: query_loop (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EB6F8A: prologToplevel (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EF4ACF: PL_initialise (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x108718: main (in /usr/lib/swipl/bin/x86_64-linux/swipl)
==1408== 
==1408== 65,536 bytes in 1 blocks are still reachable in loss record 4,071 of 4,104
==1408==    at 0x4C2BBAF: malloc (vg_replace_malloc.c:299)
==1408==    by 0x4E60CCD: allocHeapOrHalt (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4F1F10A: htable_alloc_kvs (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4F1F47F: htable_put (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4F1F891: addHTable (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4F1F931: addNewHTable (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EB7F06: importDefinitionModule (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EDC5CC: loadPart (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EDCA4D: loadStatement (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EDC5CC: loadPart (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EDCA4D: loadStatement (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EDD25B: loadWicFromStream (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408== 
==1408== 65,536 bytes in 1 blocks are still reachable in loss record 4,072 of 4,104
==1408==    at 0x4C2BBAF: malloc (vg_replace_malloc.c:299)
==1408==    by 0x4E60CCD: allocHeapOrHalt (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E9B755: rehashFunctors.part.0 (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E9BADE: lookupFunctorDef (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EB4E49: pl_univ2_va (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E79C62: PL_next_solution (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EB675A: query_loop (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EB6F8A: prologToplevel (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E6D8AC: PL_toplevel (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x108724: main (in /usr/lib/swipl/bin/x86_64-linux/swipl)
==1408== 
==1408== 65,536 bytes in 1 blocks are definitely lost in loss record 4,073 of 4,104
==1408==    at 0x4C2BBAF: malloc (vg_replace_malloc.c:299)
==1408==    by 0x4E63358: PL_malloc_atomic_unmanaged (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E9B94F: registerFunctor (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E9BC9A: lookupFunctorDef (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EC05F9: build_term (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EC5B14: simple_term (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EC6FFB: complex_term (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EC7A2A: read_term (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EC90E0: read_clause (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EC9374: pl_read_clause3_va (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E79C62: PL_next_solution (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EB675A: query_loop (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408== 
==1408== 65,544 bytes in 1 blocks are still reachable in loss record 4,074 of 4,104
==1408==    at 0x4C2DDCF: realloc (vg_replace_malloc.c:785)
==1408==    by 0x4ED0708: stack_realloc (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E9FCA3: growStacks (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4ECF053: pl_trim_stacks0_va (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E79C62: PL_next_solution (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EB675A: query_loop (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EB6F8A: prologToplevel (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E6D8AC: PL_toplevel (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x108724: main (in /usr/lib/swipl/bin/x86_64-linux/swipl)
==1408== 
==1408== 69,360 bytes in 68 blocks are still reachable in loss record 4,075 of 4,104
==1408==    at 0x4C2BBAF: malloc (vg_replace_malloc.c:299)
==1408==    by 0x111EA983: ??? (in /usr/lib/x86_64-linux-gnu/libedit.so.2.0.55)
==1408==    by 0x111EB281: ??? (in /usr/lib/x86_64-linux-gnu/libedit.so.2.0.55)
==1408==    by 0x111EB4D7: ??? (in /usr/lib/x86_64-linux-gnu/libedit.so.2.0.55)
==1408==    by 0x111E0CA9: el_init_fd (in /usr/lib/x86_64-linux-gnu/libedit.so.2.0.55)
==1408==    by 0x111E0DC4: el_init (in /usr/lib/x86_64-linux-gnu/libedit.so.2.0.55)
==1408==    by 0x10FD2378: pl_wrap (in /usr/lib/swipl/lib/x86_64-linux/libedit4pl.so)
==1408==    by 0x4E79FEC: PL_next_solution (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EB675A: query_loop (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EB6F8A: prologToplevel (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E6D8AC: PL_toplevel (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x108724: main (in /usr/lib/swipl/bin/x86_64-linux/swipl)
==1408== 
==1408== 69,360 bytes in 68 blocks are still reachable in loss record 4,076 of 4,104
==1408==    at 0x4C2BBAF: malloc (vg_replace_malloc.c:299)
==1408==    by 0x111EAA23: ??? (in /usr/lib/x86_64-linux-gnu/libedit.so.2.0.55)
==1408==    by 0x111EB281: ??? (in /usr/lib/x86_64-linux-gnu/libedit.so.2.0.55)
==1408==    by 0x111EB4D7: ??? (in /usr/lib/x86_64-linux-gnu/libedit.so.2.0.55)
==1408==    by 0x111E0CA9: el_init_fd (in /usr/lib/x86_64-linux-gnu/libedit.so.2.0.55)
==1408==    by 0x111E0DC4: el_init (in /usr/lib/x86_64-linux-gnu/libedit.so.2.0.55)
==1408==    by 0x10FD2378: pl_wrap (in /usr/lib/swipl/lib/x86_64-linux/libedit4pl.so)
==1408==    by 0x4E79FEC: PL_next_solution (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EB675A: query_loop (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EB6F8A: prologToplevel (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E6D8AC: PL_toplevel (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x108724: main (in /usr/lib/swipl/bin/x86_64-linux/swipl)
==1408== 
==1408== 75,376 bytes in 1,346 blocks are possibly lost in loss record 4,077 of 4,104
==1408==    at 0x4C2BBAF: malloc (vg_replace_malloc.c:299)
==1408==    by 0xA25005B: alloc::alloc::alloc::he71f6672ff69332a (alloc.rs:84)
==1408==    by 0xA24FFCB: alloc::alloc::exchange_malloc::h46527e14373cbbb6 (alloc.rs:206)
==1408==    by 0xA281C29: new<terminus_store::store::sync::SyncStoreLayer> (boxed.rs:121)
==1408==    by 0xA281C29: named_graph_get_head (lib.rs:92)
==1408==    by 0x9EB194C: pl_head (terminus_store.c:76)
==1408==    by 0x4E79F63: PL_next_solution (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EB6C2B: callProlog (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EE5A3B: start_thread (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x60384A3: start_thread (pthread_create.c:456)
==1408== 
==1408== 77,066 bytes in 1,562 blocks are possibly lost in loss record 4,078 of 4,104
==1408==    at 0x4C2BBAF: malloc (vg_replace_malloc.c:299)
==1408==    by 0xA6FF59B: alloc::alloc::alloc::hd4aea4183502f317 (alloc.rs:84)
==1408==    by 0xA6FF381: _$LT$alloc..alloc..Global$u20$as$u20$core..alloc..Alloc$GT$::alloc::h480641302563a5de (alloc.rs:172)
==1408==    by 0xA6FD4F1: alloc::raw_vec::RawVec$LT$T$C$A$GT$::allocate_in::hdf0c932391516013 (raw_vec.rs:98)
==1408==    by 0xA6FD315: alloc::raw_vec::RawVec$LT$T$GT$::with_capacity::he59c15e7bbc32409 (raw_vec.rs:142)
==1408==    by 0xA6FB7D3: alloc::vec::Vec$LT$T$GT$::with_capacity::h01034cb82ca16857 (vec.rs:356)
==1408==    by 0xA6FBEE0: alloc::slice::hack::to_vec::ha47e2e0d5411d604 (slice.rs:158)
==1408==    by 0xA6FF714: alloc::slice::_$LT$impl$u20$$u5b$T$u5d$$GT$::to_vec::h4998449c8f1d41db (slice.rs:379)
==1408==    by 0xA6D8684: alloc::slice::_$LT$impl$u20$alloc..borrow..ToOwned$u20$for$u20$$u5b$T$u5d$$GT$::to_owned::h46d730f63c69ab9c (slice.rs:716)
==1408==    by 0xA519399: alloc::str::_$LT$impl$u20$alloc..borrow..ToOwned$u20$for$u20$str$GT$::to_owned::h6c513b92cb4e973a (str.rs:207)
==1408==    by 0xA42AE95: terminus_store::storage::directory::get_label_from_file::hfe4de0f255ba6d3f (directory.rs:185)
==1408==    by 0xA42C7B0: _$LT$terminus_store..storage..directory..DirectoryLabelStore$u20$as$u20$terminus_store..storage..label..LabelStore$GT$::get_label::h3bc90d719ba2e3c2 (directory.rs:257)
==1408== 
==1408== 83,136 bytes in 866 blocks are still reachable in loss record 4,079 of 4,104
==1408==    at 0x4C2BBAF: malloc (vg_replace_malloc.c:299)
==1408==    by 0x4E60CCD: allocHeapOrHalt (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EB9186: lookupProcedure (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EDB3F5: loadXRc (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EDBE20: loadPredicate (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EDC5CC: loadPart (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EDCA4D: loadStatement (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EDC5CC: loadPart (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EDCA4D: loadStatement (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EDC5CC: loadPart (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EDCA4D: loadStatement (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EDD25B: loadWicFromStream (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408== 
==1408== 86,880 bytes in 905 blocks are still reachable in loss record 4,080 of 4,104
==1408==    at 0x4C2BBAF: malloc (vg_replace_malloc.c:299)
==1408==    by 0x4E60CCD: allocHeapOrHalt (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EB9186: lookupProcedure (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E8DCC5: compileSubClause (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E9399F: compileBody (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E9396F: compileBody (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E94FFC: compileClause (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E95BCE: assert_term (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E96136: record_clause (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E961C9: pl_record_clause3_va (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E79C62: PL_next_solution (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EB675A: query_loop (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408== 
==1408== 89,536 bytes in 293 blocks are still reachable in loss record 4,081 of 4,104
==1408==    at 0x4C2BBAF: malloc (vg_replace_malloc.c:299)
==1408==    by 0x4E63358: PL_malloc_atomic_unmanaged (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E95587: compileClause (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E95BCE: assert_term (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E96136: record_clause (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E961C9: pl_record_clause3_va (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E79C62: PL_next_solution (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EBB0F1: trapUndefined (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E72C10: PL_next_solution (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EB675A: query_loop (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EB6F8A: prologToplevel (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EF4ACF: PL_initialise (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408== 
==1408== 98,304 bytes in 1 blocks are still reachable in loss record 4,082 of 4,104
==1408==    at 0x4C2BBAF: malloc (vg_replace_malloc.c:299)
==1408==    by 0x4E63358: PL_malloc_atomic_unmanaged (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E5C628: allocateAtomBlock.part.6 (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E5D474: lookupBlob (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E5D6F4: lookupAtom (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4ED8292: getAtom (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EDB304: loadXRc (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EDB370: loadXRc (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EDC786: loadPart (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EDCA4D: loadStatement (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EDC5CC: loadPart (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EDCA4D: loadStatement (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408== 
==1408== 98,304 bytes in 8 blocks are still reachable in loss record 4,083 of 4,104
==1408==    at 0x4C2BBAF: malloc (vg_replace_malloc.c:299)
==1408==    by 0xA6EE06B: alloc::alloc::alloc::ha26ad29b2718f601 (alloc.rs:84)
==1408==    by 0xA6EDE51: _$LT$alloc..alloc..Global$u20$as$u20$core..alloc..Alloc$GT$::alloc::hd4fce9f3a1e51fd6 (alloc.rs:172)
==1408==    by 0xA6E9FF1: alloc::raw_vec::RawVec$LT$T$C$A$GT$::allocate_in::h4992af088d19145b (raw_vec.rs:98)
==1408==    by 0xA6E9E65: alloc::raw_vec::RawVec$LT$T$GT$::with_capacity::h40203dd55ac5499a (raw_vec.rs:142)
==1408==    by 0xA6F5033: alloc::vec::Vec$LT$T$GT$::with_capacity::ha16120202ccaaf0f (vec.rs:356)
==1408==    by 0xA6F0728: mio::sys::unix::epoll::Events::with_capacity::h7ee83912fc62963e (epoll.rs:184)
==1408==    by 0xA6E4C37: mio::poll::Events::with_capacity::h76433514da4913ed (poll.rs:1362)
==1408==    by 0xA63B1CD: tokio_reactor::Reactor::new::hded064c06aa038fb (lib.rs:256)
==1408==    by 0xA61AF56: tokio::runtime::threadpool::builder::Builder::build::h5c3ebe8c09353878 (builder.rs:356)
==1408==    by 0xA61A374: tokio::runtime::threadpool::Runtime::new::h1069aba4c726ff08 (mod.rs:145)
==1408==    by 0xA4BC54D: __static_ref_initialize (sync.rs:18)
==1408==    by 0xA4BC54D: core::ops::function::FnOnce::call_once::h343c611514ba4125 (function.rs:235)
==1408== 
==1408== 99,968 bytes in 1,562 blocks are possibly lost in loss record 4,084 of 4,104
==1408==    at 0x4C2BBAF: malloc (vg_replace_malloc.c:299)
==1408==    by 0xA25005B: alloc::alloc::alloc::he71f6672ff69332a (alloc.rs:84)
==1408==    by 0xA24FFCB: alloc::alloc::exchange_malloc::h46527e14373cbbb6 (alloc.rs:206)
==1408==    by 0xA281811: new<terminus_store::store::sync::SyncNamedGraph> (boxed.rs:121)
==1408==    by 0xA281811: open_named_graph (lib.rs:70)
==1408==    by 0x9EB18A6: pl_open_named_graph (terminus_store.c:56)
==1408==    by 0x4E79FA5: PL_next_solution (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EB6C2B: callProlog (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EE5A3B: start_thread (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x60384A3: start_thread (pthread_create.c:456)
==1408== 
==1408== 115,536 bytes in 574 blocks are still reachable in loss record 4,085 of 4,104
==1408==    at 0x4C2BBAF: malloc (vg_replace_malloc.c:299)
==1408==    by 0x4E63358: PL_malloc_atomic_unmanaged (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E95587: compileClause (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E95BCE: assert_term (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E95FE5: pl_assertz11_va (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E79C62: PL_next_solution (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EB675A: query_loop (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EB6F8A: prologToplevel (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EF4ACF: PL_initialise (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x108718: main (in /usr/lib/swipl/bin/x86_64-linux/swipl)
==1408== 
==1408== 118,848 bytes in 7,428 blocks are still reachable in loss record 4,086 of 4,104
==1408==    at 0x4C2BBAF: malloc (vg_replace_malloc.c:299)
==1408==    by 0x4E60CCD: allocHeapOrHalt (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EA8621: import (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EA8B70: pl_import2_va (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E79C62: PL_next_solution (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EB675A: query_loop (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EB6F8A: prologToplevel (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EF4ACF: PL_initialise (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x108718: main (in /usr/lib/swipl/bin/x86_64-linux/swipl)
==1408== 
==1408== 121,344 bytes in 1,264 blocks are still reachable in loss record 4,087 of 4,104
==1408==    at 0x4C2BBAF: malloc (vg_replace_malloc.c:299)
==1408==    by 0x4E60CCD: allocHeapOrHalt (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EB9186: lookupProcedure (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EA9293: export_pi (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EA92F0: pl_export1_va (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E79C62: PL_next_solution (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EB675A: query_loop (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EB6F8A: prologToplevel (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EF4ACF: PL_initialise (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x108718: main (in /usr/lib/swipl/bin/x86_64-linux/swipl)
==1408== 
==1408== 131,072 bytes in 1 blocks are still reachable in loss record 4,088 of 4,104
==1408==    at 0x4C2BBAF: malloc (vg_replace_malloc.c:299)
==1408==    by 0x4E60CCD: allocHeapOrHalt (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E5D154: lookupBlob (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E5D6F4: lookupAtom (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EC448C: get_token__LD.part.37 (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EC4EBB: simple_term (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EC6FFB: complex_term (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EC7A2A: read_term (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EC90E0: read_clause (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EC9374: pl_read_clause3_va (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E79C62: PL_next_solution (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EB675A: query_loop (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408== 
==1408== 135,264 bytes in 1,409 blocks are still reachable in loss record 4,089 of 4,104
==1408==    at 0x4C2BBAF: malloc (vg_replace_malloc.c:299)
==1408==    by 0x4E60CCD: allocHeapOrHalt (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EB9186: lookupProcedure (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EBB445: pl_get_predicate_attribute (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E79FA5: PL_next_solution (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EB675A: query_loop (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EB6F8A: prologToplevel (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EF4ACF: PL_initialise (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x108718: main (in /usr/lib/swipl/bin/x86_64-linux/swipl)
==1408== 
==1408== 151,216 bytes in 610 blocks are still reachable in loss record 4,090 of 4,104
==1408==    at 0x4C2BBAF: malloc (vg_replace_malloc.c:299)
==1408==    by 0x4E63358: PL_malloc_atomic_unmanaged (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EDBC36: loadPredicate (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EDC5CC: loadPart (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EDCA4D: loadStatement (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EDD25B: loadWicFromStream (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EF4983: PL_initialise (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x108718: main (in /usr/lib/swipl/bin/x86_64-linux/swipl)
==1408== 
==1408== 162,352 bytes in 1,274 blocks are still reachable in loss record 4,091 of 4,104
==1408==    at 0x4C2BBAF: malloc (vg_replace_malloc.c:299)
==1408==    by 0x4E63358: PL_malloc_atomic_unmanaged (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E95587: compileClause (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E95BCE: assert_term (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E95FE5: pl_assertz11_va (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E79C62: PL_next_solution (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EB6C2B: callProlog (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4F08067: pl_with_mutex (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E79F63: PL_next_solution (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EB6C2B: callProlog (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EB6E3C: pl_notrace1_va (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E79C62: PL_next_solution (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408== 
==1408== 163,840 bytes in 3 blocks are still reachable in loss record 4,092 of 4,104
==1408==    at 0x4C2DDCF: realloc (vg_replace_malloc.c:785)
==1408==    by 0xA66B2FC: alloc::alloc::realloc::h8a506f3cbb6b31d2 (alloc.rs:128)
==1408==    by 0xA66B085: _$LT$alloc..alloc..Global$u20$as$u20$core..alloc..Alloc$GT$::realloc::hb3182a071ab63438 (alloc.rs:187)
==1408==    by 0xA68B4FC: alloc::raw_vec::RawVec$LT$T$C$A$GT$::reserve_internal::h831f47b119d22fb7 (raw_vec.rs:670)
==1408==    by 0xA68C321: alloc::raw_vec::RawVec$LT$T$C$A$GT$::reserve::h0ab4df7f50cf5bac (raw_vec.rs:495)
==1408==    by 0xA67AB59: alloc::vec::Vec$LT$T$GT$::reserve::h52b1de304f27808a (vec.rs:458)
==1408==    by 0xA67A9D8: alloc::vec::Vec$LT$T$GT$::push::ha3ca665daa1a1f25 (vec.rs:1103)
==1408==    by 0xA6A52CA: slab::Slab$LT$T$GT$::insert_at::h368e1806d6c9a02c (lib.rs:632)
==1408==    by 0xA6A4F8D: slab::Slab$LT$T$GT$::insert::h00760eb0289b4efd (lib.rs:593)
==1408==    by 0xA68D0FD: tokio_threadpool::worker::entry::WorkerEntry::register_task::h2dc22e18db88036e (entry.rs:246)
==1408==    by 0xA69B3C2: tokio_threadpool::worker::Worker::run_task::h1ecf2a8dd4e7574e (mod.rs:456)
==1408==    by 0xA69AEDF: tokio_threadpool::worker::Worker::try_steal_task::hbf0e24cb88f05cbd (mod.rs:416)
==1408== 
==1408== 199,848 bytes in 711 blocks are still reachable in loss record 4,093 of 4,104
==1408==    at 0x4C2BBAF: malloc (vg_replace_malloc.c:299)
==1408==    by 0x4E63358: PL_malloc_atomic_unmanaged (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E95587: compileClause (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E95BCE: assert_term (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E96136: record_clause (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E961C9: pl_record_clause3_va (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E79C62: PL_next_solution (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EB675A: query_loop (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EB6F8A: prologToplevel (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E6D8AC: PL_toplevel (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x108724: main (in /usr/lib/swipl/bin/x86_64-linux/swipl)
==1408== 
==1408== 320,912 bytes in 160 blocks are still reachable in loss record 4,094 of 4,104
==1408==    at 0x4C2DDCF: realloc (vg_replace_malloc.c:785)
==1408==    by 0xA6D97FC: alloc::alloc::realloc::h44faf8366b2dd8f5 (alloc.rs:128)
==1408==    by 0xA6D95C5: _$LT$alloc..alloc..Global$u20$as$u20$core..alloc..Alloc$GT$::realloc::h1401f2a0cb8d81be (alloc.rs:187)
==1408==    by 0xA6E2A2C: alloc::raw_vec::RawVec$LT$T$C$A$GT$::reserve_internal::h4c6160995cb76387 (raw_vec.rs:670)
==1408==    by 0xA6E2DF1: alloc::raw_vec::RawVec$LT$T$C$A$GT$::reserve::h02e7114964f068bf (raw_vec.rs:495)
==1408==    by 0xA6DAD09: alloc::vec::Vec$LT$T$GT$::reserve::h3e715be6561b28cf (vec.rs:458)
==1408==    by 0xA439AD7: std::io::read_to_end_with_reservation::ha4f7121a3e92b640 (mod.rs:369)
==1408==    by 0xA4399D9: std::io::read_to_end::h26ee9a584052e32d (mod.rs:356)
==1408==    by 0xA439F54: std::io::Read::read_to_end::h2b0c8315939ac5b6 (mod.rs:648)
==1408==    by 0xA5248F2: _$LT$tokio_io..io..read_to_end..ReadToEnd$LT$A$GT$$u20$as$u20$futures..future..Future$GT$::poll::hb08d089e542610dd (read_to_end.rs:56)
==1408==    by 0xA364B9E: _$LT$futures..future..map..Map$LT$A$C$F$GT$$u20$as$u20$futures..future..Future$GT$::poll::h6f8cdb8e2e4ee012 (map.rs:30)
==1408==    by 0xA394429: _$LT$futures..future..either..Either$LT$A$C$B$GT$$u20$as$u20$futures..future..Future$GT$::poll::h5e4dca6ff650928d (either.rs:36)
==1408== 
==1408== 330,120 bytes in 655 blocks are still reachable in loss record 4,095 of 4,104
==1408==    at 0x4C2BBAF: malloc (vg_replace_malloc.c:299)
==1408==    by 0xA66B22B: alloc::alloc::alloc::h5720529faf6a4289 (alloc.rs:84)
==1408==    by 0xA66B19B: alloc::alloc::exchange_malloc::h96e4bb19607190fc (alloc.rs:206)
==1408==    by 0xA677F65: new<crossbeam_queue::seg_queue::Block<alloc::sync::Arc<tokio_threadpool::task::Task>>> (boxed.rs:121)
==1408==    by 0xA677F65: crossbeam_queue::seg_queue::SegQueue$LT$T$GT$::push::h60912bed1aa34d4c (seg_queue.rs:201)
==1408==    by 0xA68D264: tokio_threadpool::worker::entry::WorkerEntry::remotely_complete_task::hb324e0f074062ccf (entry.rs:266)
==1408==    by 0xA69BA66: tokio_threadpool::worker::Worker::run_task::h1ecf2a8dd4e7574e (mod.rs:515)
==1408==    by 0xA69AA56: tokio_threadpool::worker::Worker::try_run_owned_task::hd80973516daca83e (mod.rs:390)
==1408==    by 0xA69A49C: tokio_threadpool::worker::Worker::try_run_task::h54601ba776355230 (mod.rs:297)
==1408==    by 0xA69A2F5: tokio_threadpool::worker::Worker::run::ha620171d3f5df792 (mod.rs:241)
==1408==    by 0xA61BA9B: tokio::runtime::threadpool::builder::Builder::build::_$u7b$$u7b$closure$u7d$$u7d$::_$u7b$$u7b$closure$u7d$$u7d$::_$u7b$$u7b$closure$u7d$$u7d$::_$u7b$$u7b$closure$u7d$$u7d$::hf1a0f86368d24d90 (builder.rs:390)
==1408==    by 0xA629E3B: tokio_timer::timer::handle::with_default::_$u7b$$u7b$closure$u7d$$u7d$::h7f92321d998b87fc (handle.rs:101)
==1408==    by 0xA627C63: std::thread::local::LocalKey$LT$T$GT$::try_with::h0591efca88fdf8c2 (local.rs:262)
==1408== 
==1408== 330,752 bytes in 125 blocks are still reachable in loss record 4,096 of 4,104
==1408==    at 0x4C2BBAF: malloc (vg_replace_malloc.c:299)
==1408==    by 0x4E60CCD: allocHeapOrHalt (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4F1F10A: htable_alloc_kvs (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4F1F47F: htable_put (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4F1F891: addHTable (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EA867E: import (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EA8B70: pl_import2_va (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E79C62: PL_next_solution (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EB675A: query_loop (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EB6F8A: prologToplevel (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EF4ACF: PL_initialise (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x108718: main (in /usr/lib/swipl/bin/x86_64-linux/swipl)
==1408== 
==1408== 360,448 bytes in 5 blocks are still reachable in loss record 4,097 of 4,104
==1408==    at 0x4C2DDCF: realloc (vg_replace_malloc.c:785)
==1408==    by 0xA66B2FC: alloc::alloc::realloc::h8a506f3cbb6b31d2 (alloc.rs:128)
==1408==    by 0xA66B085: _$LT$alloc..alloc..Global$u20$as$u20$core..alloc..Alloc$GT$::realloc::hb3182a071ab63438 (alloc.rs:187)
==1408==    by 0xA68B4FC: alloc::raw_vec::RawVec$LT$T$C$A$GT$::reserve_internal::h831f47b119d22fb7 (raw_vec.rs:670)
==1408==    by 0xA68C321: alloc::raw_vec::RawVec$LT$T$C$A$GT$::reserve::h0ab4df7f50cf5bac (raw_vec.rs:495)
==1408==    by 0xA67AB59: alloc::vec::Vec$LT$T$GT$::reserve::h52b1de304f27808a (vec.rs:458)
==1408==    by 0xA67A9D8: alloc::vec::Vec$LT$T$GT$::push::ha3ca665daa1a1f25 (vec.rs:1103)
==1408==    by 0xA6A52CA: slab::Slab$LT$T$GT$::insert_at::h368e1806d6c9a02c (lib.rs:632)
==1408==    by 0xA6A4F8D: slab::Slab$LT$T$GT$::insert::h00760eb0289b4efd (lib.rs:593)
==1408==    by 0xA68D0FD: tokio_threadpool::worker::entry::WorkerEntry::register_task::h2dc22e18db88036e (entry.rs:246)
==1408==    by 0xA69B3C2: tokio_threadpool::worker::Worker::run_task::h1ecf2a8dd4e7574e (mod.rs:456)
==1408==    by 0xA69AA56: tokio_threadpool::worker::Worker::try_run_owned_task::hd80973516daca83e (mod.rs:390)
==1408== 
==1408== 441,456 bytes in 18,394 blocks are still reachable in loss record 4,098 of 4,104
==1408==    at 0x4C2BBAF: malloc (vg_replace_malloc.c:299)
==1408==    by 0x4E60CCD: allocHeapOrHalt (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EB8B75: newClauseRef (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EB8C11: assertProcedure (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E95CC7: assert_term (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E96136: record_clause (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E961C9: pl_record_clause3_va (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E79C62: PL_next_solution (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EB675A: query_loop (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EB6F8A: prologToplevel (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EF4ACF: PL_initialise (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x108718: main (in /usr/lib/swipl/bin/x86_64-linux/swipl)
==1408== 
==1408== 460,440 bytes in 1,740 blocks are still reachable in loss record 4,099 of 4,104
==1408==    at 0x4C2BBAF: malloc (vg_replace_malloc.c:299)
==1408==    by 0x4E63358: PL_malloc_atomic_unmanaged (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EDBC36: loadPredicate (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EDC5CC: loadPart (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EDCA4D: loadStatement (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EDC5CC: loadPart (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EDCA4D: loadStatement (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EDC5CC: loadPart (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EDCA4D: loadStatement (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EDD25B: loadWicFromStream (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EF4983: PL_initialise (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x108718: main (in /usr/lib/swipl/bin/x86_64-linux/swipl)
==1408== 
==1408== 589,824 bytes in 2 blocks are possibly lost in loss record 4,100 of 4,104
==1408==    at 0x4C2BBAF: malloc (vg_replace_malloc.c:299)
==1408==    by 0x4E63358: PL_malloc_atomic_unmanaged (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E5C628: allocateAtomBlock.part.6 (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E5D474: lookupBlob (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E5D6F4: lookupAtom (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4F2191C: PL_unify_text (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EB4603: atomic_list_concat (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E79C62: PL_next_solution (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EB675A: query_loop (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EB6F8A: prologToplevel (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EF4ACF: PL_initialise (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x108718: main (in /usr/lib/swipl/bin/x86_64-linux/swipl)
==1408== 
==1408== 786,432 bytes in 1 blocks are still reachable in loss record 4,101 of 4,104
==1408==    at 0x4C2BBAF: malloc (vg_replace_malloc.c:299)
==1408==    by 0x4E63358: PL_malloc_atomic_unmanaged (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E5C628: allocateAtomBlock.part.6 (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E5D474: lookupBlob (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E5D6F4: lookupAtom (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EC448C: get_token__LD.part.37 (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EC4EBB: simple_term (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EC6FFB: complex_term (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EC7A2A: read_term (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EC90E0: read_clause (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EC9374: pl_read_clause3_va (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E79C62: PL_next_solution (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408== 
==1408== 1,532,416 bytes in 11,972 blocks are still reachable in loss record 4,102 of 4,104
==1408==    at 0x4C2BBAF: malloc (vg_replace_malloc.c:299)
==1408==    by 0xA66B22B: alloc::alloc::alloc::h5720529faf6a4289 (alloc.rs:84)
==1408==    by 0xA66B19B: alloc::alloc::exchange_malloc::h96e4bb19607190fc (alloc.rs:206)
==1408==    by 0xA6A6DBD: alloc::sync::Arc$LT$T$GT$::new::h39278ecbb3715592 (sync.rs:306)
==1408==    by 0xA6992FF: _$LT$$RF$tokio_threadpool..sender..Sender$u20$as$u20$tokio_executor..executor..Executor$GT$::spawn::he41a8e947bc04f02 (sender.rs:168)
==1408==    by 0xA4B9EE5: tokio_threadpool::sender::Sender::spawn::hff27a00b7db63eee (sender.rs:84)
==1408==    by 0xA4E555D: _$LT$tokio_threadpool..sender..Sender$u20$as$u20$futures..future..Executor$LT$T$GT$$GT$::execute::heb0eb5a86d418f85 (sender.rs:207)
==1408==    by 0xA4B84B1: _$LT$tokio..runtime..threadpool..task_executor..TaskExecutor$u20$as$u20$futures..future..Executor$LT$T$GT$$GT$::execute::hba082e0ddfdb4670 (task_executor.rs:65)
==1408==    by 0xA38E4BB: futures::sync::oneshot::spawn::h1836a491ca64c44b (oneshot.rs:517)
==1408==    by 0xA5BF4BB: terminus_store::store::sync::SyncStore::open::hdc8943dc25014f5b (sync.rs:282)
==1408==    by 0xA28170E: open_named_graph (lib.rs:67)
==1408==    by 0x9EB18A6: pl_open_named_graph (terminus_store.c:56)
==1408== 
==1408== 1,532,416 bytes in 11,972 blocks are still reachable in loss record 4,103 of 4,104
==1408==    at 0x4C2BBAF: malloc (vg_replace_malloc.c:299)
==1408==    by 0xA66B22B: alloc::alloc::alloc::h5720529faf6a4289 (alloc.rs:84)
==1408==    by 0xA66B19B: alloc::alloc::exchange_malloc::h96e4bb19607190fc (alloc.rs:206)
==1408==    by 0xA6A6DBD: alloc::sync::Arc$LT$T$GT$::new::h39278ecbb3715592 (sync.rs:306)
==1408==    by 0xA6992FF: _$LT$$RF$tokio_threadpool..sender..Sender$u20$as$u20$tokio_executor..executor..Executor$GT$::spawn::he41a8e947bc04f02 (sender.rs:168)
==1408==    by 0xA4B9DC5: tokio_threadpool::sender::Sender::spawn::hedd6cb8ecb813ff8 (sender.rs:84)
==1408==    by 0xA4E538D: _$LT$tokio_threadpool..sender..Sender$u20$as$u20$futures..future..Executor$LT$T$GT$$GT$::execute::hc30fa031c999c532 (sender.rs:207)
==1408==    by 0xA4B838F: _$LT$tokio..runtime..threadpool..task_executor..TaskExecutor$u20$as$u20$futures..future..Executor$LT$T$GT$$GT$::execute::h78ab25535157aa3b (task_executor.rs:65)
==1408==    by 0xA38F6A5: futures::sync::oneshot::spawn::hbf18f4b4bf54e1ba (oneshot.rs:517)
==1408==    by 0xA5BEFF7: terminus_store::store::sync::SyncNamedGraph::head::ha3dbb3d9050b7690 (sync.rs:236)
==1408==    by 0xA281B08: named_graph_get_head (lib.rs:85)
==1408==    by 0x9EB194C: pl_head (terminus_store.c:76)
==1408== 
==1408== 4,341,320 bytes in 18,394 blocks are still reachable in loss record 4,104 of 4,104
==1408==    at 0x4C2BBAF: malloc (vg_replace_malloc.c:299)
==1408==    by 0x4E63358: PL_malloc_atomic_unmanaged (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E95587: compileClause (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E95BCE: assert_term (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E96136: record_clause (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E961C9: pl_record_clause3_va (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4E79C62: PL_next_solution (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EB675A: query_loop (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EB6F8A: prologToplevel (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x4EF4ACF: PL_initialise (in /usr/lib/swipl/lib/x86_64-linux/libswipl.so.8.0.3)
==1408==    by 0x108718: main (in /usr/lib/swipl/bin/x86_64-linux/swipl)
==1408== 
==1408== LEAK SUMMARY:
==1408==    definitely lost: 160,800 bytes in 205 blocks
==1408==    indirectly lost: 8,192 bytes in 1 blocks
==1408==      possibly lost: 1,221,173 bytes in 14,903 blocks
==1408==    still reachable: 15,880,378 bytes in 149,101 blocks
==1408==                       of which reachable via heuristic:
==1408==                         length64           : 106,520 bytes in 3 blocks
==1408==                         newarray           : 55,160 bytes in 1,594 blocks
==1408==         suppressed: 0 bytes in 0 blocks
==1408== 
==1408== For counts of detected and suppressed errors, rerun with: -v
==1408== Use --track-origins=yes to see where uninitialised values come from
==1408== ERROR SUMMARY: 69934 errors from 461 contexts (suppressed: 0 from 0)",2019-11-28T12:37:19Z,rrooij,https://github.com/terminusdb/terminusdb/issues/36#issuecomment-559478787,The leak seems to be solved on the newest terminus_store_prolog bindings.,the leak seem to be solve on the new terminus_store_prolog binding,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,37,2019-11-13T05:11:34Z,Immortalin,Query with Datalog,https://github.com/terminusdb/terminusdb/issues/37,Like datomic,2019-12-06T20:31:12Z,GavinMendelGleason,https://github.com/terminusdb/terminusdb/issues/37#issuecomment-562726843,WOQL is essentially a datalog.,woql be essentially a datalog,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,38,2019-11-14T11:15:20Z,AnnieAtDataChemist,server interferes with doc_server/1,https://github.com/terminusdb/terminusdb/issues/38,The terminus-server is interfering with doc_server/1,2020-04-28T12:01:17Z,rrooij,https://github.com/terminusdb/terminusdb/issues/38#issuecomment-620561933,"Can't reproduce, works for me",can not reproduce work for -PRON-,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,39,2019-11-14T14:24:37Z,Anniepoo,not recovering properly from invalid document in dashboard,https://github.com/terminusdb/terminusdb/issues/39,"Follow the 'my first TerminusDB' example until you have the schema


make a person.  Fill in the id doc:twice
and make a friend doc:nonexist


Click Save
EXPECTED: error message
OBSERVED: no reaction


click Save again
EXPECTED: ????
OBSERVED: NOW I get an error message - the message refers to the root of the document space, and generally is unhelpful



1 Violation Detected in Document
vio:UntypedInstanceThe subject 'http://localhost:6363/annie/document/foo' has no defined class.
vio:subjecthttp://localhost:6363/annie/document/foo",2020-10-05T21:42:53Z,kevinchekovfeeney,https://github.com/terminusdb/terminusdb/issues/39#issuecomment-703905299,This no longer applies - refers to version 1.0 frame code,this no long apply - refer to version 10 frame code,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,47,2019-11-22T17:41:50Z,pmoura,BUILD.md file in `dev` branch missing dependency?,https://github.com/terminusdb/terminusdb/issues/47,"In the dev branch, the BUILD.md file doesn't contain the following block compared with the master branch:
HDT Library

You will also need to install hdt-cpp. You can git clone the source tree from this repository:

git clone https://github.com/rdfhdt/hdt-cpp

But installing this dependency seems to still be required. In the master branch, the text above should ideally be moved before the instructions to install the SWI-Prolog hdt pack otherwise the installation may fail due to missing dependencies (that are listed in the hdt-cpp repo readme page). Back to the dev branch, if we try to set the admin user password using the command from the BUILD.md file:
$ utils/db_util -k ""my_password_here"" -s ""my_server_name_here""

we will get an error that db_util doesn't exist. The command seems to have been renamed to initialize_database. But if we try it without first installing hdt-cppfrom its clone, we get an error that the required rdf2hdt command is not installed.
Summarizing:

Installing hdt-cpp still seems to be required when using the dev branch.
There's a typo in the dev branch BUILD.md file: db_util  -> initialize_database .

I'm hesitant to submit a PR to fix the BUILD.md file in the dev branch as these issue seem to be caused by work-in-progress...",2019-11-23T19:59:37Z,rrooij,https://github.com/terminusdb/terminusdb/issues/47#issuecomment-557828370,"Hey @pmoura, thanks a lot for spotting this error.
initialize_database has been renamed to db_util because I want it to be a general configuration utility in the future instead of only a database initialization script. For instance, it must be possible in the future to change your port and the like without having to initialize a whole new database.
I'm not sure whether hdt is still required in dev. I thought it was obsolete since the terminus_store_prolog backend but it might still be used in a place I didn't see.
The dev branch works fine without hdt on my laptop.
db_util is still just db_util in the dev branch. I don't know how you got the initialize_database script there.",hey @pmoura thank a lot for spot this error initialize_database have be rename to db_util because i want -PRON- to be a general configuration utility in the future instead of only a database initialization script for instance -PRON- must be possible in the future to change -PRON- port and the like without have to initialize a whole new database -PRON- be not sure whether hdt be still require in dev i think -PRON- be obsolete since the terminus_store_prolog backend but -PRON- may still be use in a place i do not see the dev branch work fine without hdt on -PRON- laptop db_util be still just db_util in the dev branch i do not know how -PRON- get the initialize_database script there,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,47,2019-11-22T17:41:50Z,pmoura,BUILD.md file in `dev` branch missing dependency?,https://github.com/terminusdb/terminusdb/issues/47,"In the dev branch, the BUILD.md file doesn't contain the following block compared with the master branch:
HDT Library

You will also need to install hdt-cpp. You can git clone the source tree from this repository:

git clone https://github.com/rdfhdt/hdt-cpp

But installing this dependency seems to still be required. In the master branch, the text above should ideally be moved before the instructions to install the SWI-Prolog hdt pack otherwise the installation may fail due to missing dependencies (that are listed in the hdt-cpp repo readme page). Back to the dev branch, if we try to set the admin user password using the command from the BUILD.md file:
$ utils/db_util -k ""my_password_here"" -s ""my_server_name_here""

we will get an error that db_util doesn't exist. The command seems to have been renamed to initialize_database. But if we try it without first installing hdt-cppfrom its clone, we get an error that the required rdf2hdt command is not installed.
Summarizing:

Installing hdt-cpp still seems to be required when using the dev branch.
There's a typo in the dev branch BUILD.md file: db_util  -> initialize_database .

I'm hesitant to submit a PR to fix the BUILD.md file in the dev branch as these issue seem to be caused by work-in-progress...",2019-11-23T20:54:16Z,pmoura,https://github.com/terminusdb/terminusdb/issues/47#issuecomment-557831980,"I re-checked the dev branch and I think you're correct. I have been experimenting both master and dev branches on macOS and Ubuntu and likely I did mixed my findings. Sorry for the noise! So, other than my suggestion to improve the text ordering on the BUILD.md file in the master branch, the other issues are raised should be invalid. Closing this ticket.",i re - check the dev branch and i think -PRON- be correct i have be experiment both master and dev branch on macos and ubuntu and likely i do mix -PRON- finding sorry for the noise so other than -PRON- suggestion to improve the text ordering on the buildmd file in the master branch the other issue be raise should be invalid close this ticket,1,1
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,48,2019-12-02T16:04:33Z,rrooij,Export whole databases as turtle,https://github.com/terminusdb/terminusdb/issues/48,It would be nice to be able to export not only the schema as turtle but other parts of the database as well.,2020-10-01T14:26:56Z,GavinMendelGleason,https://github.com/terminusdb/terminusdb/issues/48#issuecomment-702173639,This is now possible for any graph.,this be now possible for any graph,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,49,2019-12-04T10:50:22Z,GavinMendelGleason,Frames should use dictionaries,https://github.com/terminusdb/terminusdb/issues/49,"Currently we're lists of terms in frames.pl and really this should use dictionaries as it would make the code more suscinct, faster and more readable. It would also reduce the need to convert back and forth between dictionaries and this representation.",2021-03-12T12:21:41Z,GavinMendelGleason,https://github.com/terminusdb/terminusdb/issues/49#issuecomment-797457153,No plans to do this at the minute.,no plan to do this at the minute,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,50,2019-12-04T19:10:33Z,pmoura,Spurious choice-points in frame.pl ?,https://github.com/terminusdb/terminusdb/issues/50,"Are the implicit choice-points created by the following member/2 and select/3 calls required?
https://github.com/terminusdb/terminus-server/blob/df1ba67477fd0d32f9f4062eb18e62e2982b2117/library/frame.pl#L818-L819
The call to the realise_triples/4 predicate from the object_edges/3 predicate suggests that those calls could be replaced with memberchk/2 and selectchk/3. As the realise_triples/4 predicate is recursive, these choice-points accumulate.
P.S. Given sort/2 semantics, the object_edges/3 predicate can also be rewritten as:
object_edges(URI,Database,Edges) :-
    (   most_specific_type(URI,Class,Database),
        class_frame(Class,Database,Frame),
        realise_triples(URI,Frame,Database,Unsorted)
    ->  sort(Unsorted,Edges)
    % There is no type in the database, so it doesn't exist...
    ;   Edges=[]).",2019-12-10T10:54:29Z,GavinMendelGleason,https://github.com/terminusdb/terminusdb/issues/50#issuecomment-563979230,The choice points are indeed spurious. This entire section should be rewriten and the object_edges version looks better as well.,the choice point be indeed spurious this entire section should be rewriten and the object_edges version look well as well,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,50,2019-12-04T19:10:33Z,pmoura,Spurious choice-points in frame.pl ?,https://github.com/terminusdb/terminusdb/issues/50,"Are the implicit choice-points created by the following member/2 and select/3 calls required?
https://github.com/terminusdb/terminus-server/blob/df1ba67477fd0d32f9f4062eb18e62e2982b2117/library/frame.pl#L818-L819
The call to the realise_triples/4 predicate from the object_edges/3 predicate suggests that those calls could be replaced with memberchk/2 and selectchk/3. As the realise_triples/4 predicate is recursive, these choice-points accumulate.
P.S. Given sort/2 semantics, the object_edges/3 predicate can also be rewritten as:
object_edges(URI,Database,Edges) :-
    (   most_specific_type(URI,Class,Database),
        class_frame(Class,Database,Frame),
        realise_triples(URI,Frame,Database,Unsorted)
    ->  sort(Unsorted,Edges)
    % There is no type in the database, so it doesn't exist...
    ;   Edges=[]).",2021-03-12T12:16:53Z,GavinMendelGleason,https://github.com/terminusdb/terminusdb/issues/50#issuecomment-797454815,Fixed in e28b1f4,fix in e28b1f4,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,51,2019-12-05T08:43:33Z,rrooij,Linting in the CI process,https://github.com/terminusdb/terminusdb/issues/51,"We already run tests, but are not running a linter. @pmoura 's logtalk seems to have a great linter that we could use.",2019-12-05T10:17:34Z,pmoura,https://github.com/terminusdb/terminusdb/issues/51#issuecomment-562064401,"Indeed most of the feedback I have been providing, both as issues and pull requests, result from applying the Logtalk tools to the Prolog files in this project (I routinely perform these checks on several public Prolog projects as part of the development of Logtalk tools). In a few cases, feedback result from manual code inspections.
Is possible with some effort to automate running the Logtalk linter. Not all files can be fully checked, however, due to compilation errors that result from trying to compile modules as objects as required to use the linter. These errors result from the use of proprietary stuff inside modules that cannot be sensibly supported (Logtalk is Prolog system agnostic). Still, the number of reported issues have been helpful in improving several projects.
Currently, in the dev branch, most reported lint issues are missing use_module/1-2 and multifile/1 directives. There are several module dependencies in several modules that are undeclared with the code only working due to the auto-loading mechanism. While I think this is bad practice, it's also widespread practice (among SWI-Prolog users) and thus I hesitate in submitting a pull request that adds all those missing directives. Your call. Note, however, that the linter is controlled by a set of flags and thus is simple to turn off these particular warnings.
I can look into the best way of automating running the linter if you want and report back. I will need to run some tests on my Ubuntu VM, however, as my main OS is macOS and the required packs by your project cannot currently all be installed on macOS.",indeed most of the feedback i have be provide both as issue and pull request result from apply the logtalk tool to the prolog file in this project ( i routinely perform these check on several public prolog project as part of the development of logtalk tool ) in a few case feedback result from manual code inspection be possible with some effort to automate run the logtalk linter not all file can be fully check however due to compilation error that result from try to compile module as object as require to use the linter these error result from the use of proprietary stuff inside module that can not be sensibly support ( logtalk be prolog system agnostic ) still the number of report issue have be helpful in improve several project currently in the dev branch most report lint issue be miss use_module/1 - 2 and multifile/1 directive there be several module dependency in several module that be undeclared with the code only work due to the auto - loading mechanism while i think this be bad practice -PRON- be also widespread practice ( among swi - prolog user ) and thus i hesitate in submit a pull request that add all those miss directive -PRON- call note however that the linter be control by a set of flag and thus be simple to turn off these particular warning i can look into the good way of automate run the linter if -PRON- want and report back i will need to run some test on -PRON- ubuntu vm however as -PRON- main os be macos and the require pack by -PRON- project can not currently all be instal on macos,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,51,2019-12-05T08:43:33Z,rrooij,Linting in the CI process,https://github.com/terminusdb/terminusdb/issues/51,"We already run tests, but are not running a linter. @pmoura 's logtalk seems to have a great linter that we could use.",2019-12-06T10:13:26Z,pmoura,https://github.com/terminusdb/terminusdb/issues/51#issuecomment-562514523,"Yesterday and this morning I moved from the linter to the checking targets provided by Logtalk's make tool (https://logtalk.org/manuals/refman/predicates/logtalk_make_1.html), which resulted in some new issues and pull requests. So, ideally, CI would run both.",yesterday and this morning i move from the linter to the check target provide by logtalk 's make tool ( https//logtalkorg / manual / refman / predicates / logtalk_make_1html ) which result in some new issue and pull request so ideally ci would run both,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,51,2019-12-05T08:43:33Z,rrooij,Linting in the CI process,https://github.com/terminusdb/terminusdb/issues/51,"We already run tests, but are not running a linter. @pmoura 's logtalk seems to have a great linter that we could use.",2021-03-08T15:22:01Z,matko,https://github.com/terminusdb/terminusdb/issues/51#issuecomment-792827443,@rrooij is this still something you want to look into?,@rrooij be this still something -PRON- want to look into,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,55,2019-12-05T18:27:10Z,pmoura,Circular module dependencies (just FYI),https://github.com/terminusdb/terminusdb/issues/55,"Maybe the following data on circular module dependencies could be useful if you're planning some refactoring:
?- {@}.
% Scanning for circular entity dependencies ...
*     Circular references:
*       database-schema
*         in file /Users/pmoura/terminus-server/library/database.pl at line 104
*         in file /Users/pmoura/terminus-server/library/schema.pl at line 32
*       database-sdk
*         in file /Users/pmoura/terminus-server/library/database.pl at line 59
*         in file /Users/pmoura/terminus-server/library/sdk.pl at line 31
*       database-triplestore
*         in file /Users/pmoura/terminus-server/library/database.pl at line 66
*         in file /Users/pmoura/terminus-server/library/triplestore.pl at line 25
*       schema-triplestore
*         in file /Users/pmoura/terminus-server/library/schema.pl at line 33
*         in file /Users/pmoura/terminus-server/library/triplestore.pl at line 21
*       validate_instance-validate_schema
*         in file /Users/pmoura/terminus-server/library/validate_instance.pl at line 32
*         in file /Users/pmoura/terminus-server/library/validate_schema.pl at line 106
*       database-schema-schema_util
*         in file /Users/pmoura/terminus-server/library/database.pl at line 104
*         in file /Users/pmoura/terminus-server/library/schema.pl at line 35
*         in file /Users/pmoura/terminus-server/library/schema_util.pl at line 35
*       database-schema-triplestore
*         in file /Users/pmoura/terminus-server/library/database.pl at line 104
*         in file /Users/pmoura/terminus-server/library/schema.pl at line 33
*         in file /Users/pmoura/terminus-server/library/triplestore.pl at line 25
*       database-sdk-woql_compile
*         in file /Users/pmoura/terminus-server/library/database.pl at line 59
*         in file /Users/pmoura/terminus-server/library/sdk.pl at line 32
*         in file /Users/pmoura/terminus-server/library/woql_compile.pl at line 34
*     
% ... completed scanning for circular entity dependencies
true.

Logtalk make tool only reports two-way and three-way circular dependencies, however.",2019-12-06T12:15:55Z,GavinMendelGleason,https://github.com/terminusdb/terminusdb/issues/55#issuecomment-562549561,"This is a lot more circularity than I'd like! I rooted out some of this ealier, but we need another go at it.",this be a lot more circularity than -PRON- 'd like i root out some of this eali but -PRON- need another go at -PRON-,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,55,2019-12-05T18:27:10Z,pmoura,Circular module dependencies (just FYI),https://github.com/terminusdb/terminusdb/issues/55,"Maybe the following data on circular module dependencies could be useful if you're planning some refactoring:
?- {@}.
% Scanning for circular entity dependencies ...
*     Circular references:
*       database-schema
*         in file /Users/pmoura/terminus-server/library/database.pl at line 104
*         in file /Users/pmoura/terminus-server/library/schema.pl at line 32
*       database-sdk
*         in file /Users/pmoura/terminus-server/library/database.pl at line 59
*         in file /Users/pmoura/terminus-server/library/sdk.pl at line 31
*       database-triplestore
*         in file /Users/pmoura/terminus-server/library/database.pl at line 66
*         in file /Users/pmoura/terminus-server/library/triplestore.pl at line 25
*       schema-triplestore
*         in file /Users/pmoura/terminus-server/library/schema.pl at line 33
*         in file /Users/pmoura/terminus-server/library/triplestore.pl at line 21
*       validate_instance-validate_schema
*         in file /Users/pmoura/terminus-server/library/validate_instance.pl at line 32
*         in file /Users/pmoura/terminus-server/library/validate_schema.pl at line 106
*       database-schema-schema_util
*         in file /Users/pmoura/terminus-server/library/database.pl at line 104
*         in file /Users/pmoura/terminus-server/library/schema.pl at line 35
*         in file /Users/pmoura/terminus-server/library/schema_util.pl at line 35
*       database-schema-triplestore
*         in file /Users/pmoura/terminus-server/library/database.pl at line 104
*         in file /Users/pmoura/terminus-server/library/schema.pl at line 33
*         in file /Users/pmoura/terminus-server/library/triplestore.pl at line 25
*       database-sdk-woql_compile
*         in file /Users/pmoura/terminus-server/library/database.pl at line 59
*         in file /Users/pmoura/terminus-server/library/sdk.pl at line 32
*         in file /Users/pmoura/terminus-server/library/woql_compile.pl at line 34
*     
% ... completed scanning for circular entity dependencies
true.

Logtalk make tool only reports two-way and three-way circular dependencies, however.",2020-10-01T13:36:04Z,GavinMendelGleason,https://github.com/terminusdb/terminusdb/issues/55#issuecomment-702139881,"Some circularity has been removed, the remainder will stick around for now.",some circularity have be remove the remainder will stick around for now,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,57,2019-12-05T21:48:16Z,pmoura,Missing predicate: write_cors_headers/1,https://github.com/terminusdb/terminusdb/issues/57,"The Logtalk make tool reports this missing ppredicate (on the dev branch):
*       write_cors_headers/1
*         referenced from object api
*         in file /Users/pmoura/terminus-server/library/api.pl at line 287

But the capabilities.pl module have the definition:
/*
 * write_cors_headers(Resource_URI) is det.
 *
 * Writes cors headers associated with Resource_URI
 */
write_cors_headers(Resource_URI, DB) :-
  ...
Work in progress?",2019-12-09T14:10:55Z,pmoura,https://github.com/terminusdb/terminusdb/issues/57#issuecomment-563255221,Fixed in df4c9bc.,fix in df4c9bc,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,58,2019-12-05T21:52:50Z,pmoura,Missing predicate: schema:subsumptionOf/3,https://github.com/terminusdb/terminusdb/issues/58,"The Logtalk make tool reports (on the dev branch):
*       schema::subsumptionOf/3
*         referenced from object frame
*         in file /Users/pmoura/terminus-server/library/frame.pl at line 149

I could not find any definition in the schema module for this predicate (or in the schema_util that it reexports). But the validate_schema module does define the subsumptionOf/3 predicate.",2019-12-06T12:11:58Z,GavinMendelGleason,https://github.com/terminusdb/terminusdb/issues/58#issuecomment-562548538,"I think this is correct, is it not @matko ?",i think this be correct be -PRON- not @matko,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,58,2019-12-05T21:52:50Z,pmoura,Missing predicate: schema:subsumptionOf/3,https://github.com/terminusdb/terminusdb/issues/58,"The Logtalk make tool reports (on the dev branch):
*       schema::subsumptionOf/3
*         referenced from object frame
*         in file /Users/pmoura/terminus-server/library/frame.pl at line 149

I could not find any definition in the schema module for this predicate (or in the schema_util that it reexports). But the validate_schema module does define the subsumptionOf/3 predicate.",2020-07-09T14:51:09Z,rrooij,https://github.com/terminusdb/terminusdb/issues/58#issuecomment-656174521,We got rid of subsumptionOf/3 :),-PRON- get rid of subsumptionof/3 ),0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,59,2019-12-05T22:01:04Z,pmoura,Missing predicate: woql_context/1,https://github.com/terminusdb/terminusdb/issues/59,"The Logtalk make tool reports (on the dev branch):
*       woql_context/1
*         referenced from object woql_compile
*         in file /Users/pmoura/terminus-server/library/woql_compile.pl at line 458

The only definition I find for the predicate is in the prefixes module where the predicate is an exported dynamic predicate. Not clear how the predicate becomes visible in the woql_compile module as there's not reference there (notably, via a use_module directive) to the prefixes module.",,,,,,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,60,2019-12-05T22:14:44Z,pmoura,Missing predicate: is_rdf_object/1,https://github.com/terminusdb/terminusdb/issues/60,"Reported by the Logtalk make tool (on the devbranch):
*       is_rdf_object/1
*         referenced from object types
*         in file /Users/pmoura/terminus-server/library/types.pl at line 210

That line does contain a likely relevant comment:
% Why is this alone?
error:has_type(rdf_object, Rdf_Object):-
    is_rdf_object(Rdf_Object).
Thus, I assume this is an already known issue?",,,,,,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,61,2019-12-05T22:18:44Z,pmoura,Missing predicate: initialise_prefix_db/1,https://github.com/terminusdb/terminusdb/issues/61,"Reported by the Logtalk make tool (on the dev branch):
*       initialise_prefix_db/1
*         referenced from object database_utils
*         in file /Users/pmoura/terminus-server/library/database_utils.pl at line 52

There's a definition for this predicate in the prefixes module. But there's no use_module(prefixes) directive in the database_utils module from where the predicate is called.",2019-12-05T22:30:37Z,pmoura,https://github.com/terminusdb/terminusdb/issues/61#issuecomment-562347889,"P.S. It's most likely that this and other missing predicates (reported in the other issues) are visible, despite the missing use_module directives, due to the default inheritance from the user module. E.g. this particular case, the prefixes module exports the predicate to user upon loading and then the database_utils module picks it yup from there. But I think you will agree this is not best practice although this likely explains the absence of predicate existence errors.",ps -PRON- be most likely that this and other miss predicate ( report in the other issue ) be visible despite the missing use_module directive due to the default inheritance from the user module eg this particular case the prefix module export the predicate to user upon loading and then the database_utils module pick -PRON- yup from there but i think -PRON- will agree this be not good practice although this likely explain the absence of predicate existence error,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,65,2019-12-06T10:08:48Z,pmoura,Non-existing database_record_schema_list/2 predicate or wrong arity?,https://github.com/terminusdb/terminusdb/issues/65,"The following line calls a non-existent database_record_schema_list/2 predicate predicate:
https://github.com/terminusdb/terminus-server/blob/5f7b20c67d2fd4ccbc5dffd721669796bc883a27/library/database.pl#L272
But the database module exports and defines a database_record_schema_list/3 predicate.",2019-12-06T11:30:20Z,GavinMendelGleason,https://github.com/terminusdb/terminusdb/issues/65#issuecomment-562538057,This is a bug in is_schema_graph/2. Really the only way to do this is to create a database. Possibly is_schema_graph/2 should be removed.,this be a bug in is_schema_graph/2 really the only way to do this be to create a database possibly is_schema_graph/2 should be remove,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,65,2019-12-06T10:08:48Z,pmoura,Non-existing database_record_schema_list/2 predicate or wrong arity?,https://github.com/terminusdb/terminusdb/issues/65,"The following line calls a non-existent database_record_schema_list/2 predicate predicate:
https://github.com/terminusdb/terminus-server/blob/5f7b20c67d2fd4ccbc5dffd721669796bc883a27/library/database.pl#L272
But the database module exports and defines a database_record_schema_list/3 predicate.",2020-10-01T13:35:15Z,GavinMendelGleason,https://github.com/terminusdb/terminusdb/issues/65#issuecomment-702139304,This is no longer relevant as we use a different mechanism for transaction management.,this be no long relevant as -PRON- use a different mechanism for transaction management,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,67,2019-12-06T14:31:14Z,GavinMendelGleason,"{""into"", [G,Q]} undocumented",https://github.com/terminusdb/terminusdb/issues/67,No documentation for graph target setting.,2020-10-01T13:34:30Z,GavinMendelGleason,https://github.com/terminusdb/terminusdb/issues/67#issuecomment-702138824,This is no longer relevant - we now use JSON-LD with the WOQL ontology.,this be no long relevant - -PRON- now use json - ld with the woql ontology,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,68,2019-12-06T14:33:30Z,GavinMendelGleason,WOQL needs to find all needed write targets,https://github.com/terminusdb/terminusdb/issues/68,"The WOQL compiler needs to know which write targets exist at the point of setting up a transaction. This requires meta-analysis of the WOQL in advanced of compilation. We can do this with the capability traversal, or we can do a specialised traversal.",2019-12-08T16:05:19Z,GavinMendelGleason,https://github.com/terminusdb/terminusdb/issues/68#issuecomment-562963850,Should be fixed in b4f22d2,should be fix in b4f22d2,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,69,2019-12-06T17:37:16Z,GavinMendelGleason,Unbound variables in insert/delete clauses in WOQL need to be reported,https://github.com/terminusdb/terminusdb/issues/69,"Currently we give a 500 when we try to do an insert/delete if the variable is unbound. Instead we should throw a nice JSON error that can be interpreted on the client.
e.g.
{""when"" : [ {""true"" : []}, 
                 {""insert"" : [""v:a"", ""v:b"", ""v:c""]}]}",,,,,,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,75,2019-12-12T10:58:50Z,GavinMendelGleason,Marshall literal types in WOQL compile_wf//2,https://github.com/terminusdb/terminusdb/issues/75,All string and number returning functions should wrap their args as appropriate types. This will ensure we have a uniform treatment and avoid using these as URIs.,2020-01-08T14:50:12Z,GavinMendelGleason,https://github.com/terminusdb/terminusdb/issues/75#issuecomment-572103407,Fixed in dev and master,fix in dev and master,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,76,2019-12-12T11:12:15Z,GavinMendelGleason,WOQL 'get' should return literal wrapped elements. ,https://github.com/terminusdb/terminusdb/issues/76,"Currently we are giving raw strings which is awkward and error prone. We don't want to be required to specify types when they are already strings or numbers, and we don't want to accidentally put random strings in to URI positions.",2020-01-08T14:49:42Z,GavinMendelGleason,https://github.com/terminusdb/terminusdb/issues/76#issuecomment-572103186,Fixed in dev and master,fix in dev and master,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,77,2019-12-12T11:58:12Z,GavinMendelGleason,WOQL instantiation errors should return the bound variable name,https://github.com/terminusdb/terminusdb/issues/77,Currently we get a rather uninformative error message referring to an internal unbound prolog variable. We need to do a backwards lookup in the environment and report this instead. This could be a bit tricky as we need to catch in a context in which the environment is exposed to us. Potentially we need to do a re-throw wrapper around the entire prolog term compiled so that we can ensure this.,,,,,,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,78,2019-12-12T15:07:31Z,GavinMendelGleason,Schema Migration needs to do a partial instance check (instead of global),https://github.com/terminusdb/terminusdb/issues/78,"Currently we do a complete check of the entire database on schema migration. This is massive overkill. We should at maximum be recalculating only the classes that could be impacted by the change, and cardinality changes.",2020-10-01T13:36:32Z,GavinMendelGleason,https://github.com/terminusdb/terminusdb/issues/78#issuecomment-702140166,"We now do a ""blast radius"" calculation.","-PRON- now do a "" blast radius "" calculation",0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,79,2019-12-13T11:07:17Z,GavinMendelGleason,Date Marshalling,https://github.com/terminusdb/terminusdb/issues/79,All date Marshalling should go through one canonical predicate. Currently we are special casing dates in far too many places. This needs to be cleaned up ASAP.,2021-03-12T12:15:06Z,GavinMendelGleason,https://github.com/terminusdb/terminusdb/issues/79#issuecomment-797453845,Fixed in e28b1f4,fix in e28b1f4,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,81,2019-12-18T12:01:23Z,rrooij,Dashboard shouldn't be using the user's password,https://github.com/terminusdb/terminusdb/issues/81,People deploying terminus-server will always be exposed by the dashboard URL and have to secure it themselves. It is better to leave the default dashboard password at root rather than using the user supplied password.,,,,,,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,82,2020-01-08T00:39:55Z,kevinchekovfeeney,Init script should replace existing console index,https://github.com/terminusdb/terminusdb/issues/82,"The init script currently does not update the index html file produced by the index.tpl file template. This means that on updates, any updates to the html template will not be updated
This caused a bad problem with release of 1.1.0 when the docker would not work when installed in the same location as a previous install and had myself and gavin up till all hours trying to fix it in production",2020-02-24T14:13:49Z,rrooij,https://github.com/terminusdb/terminusdb/issues/82#issuecomment-590338514,Was just looking through the issues and this has been solved now. The index.html is generated each time when the db_util script is run.,be just look through the issue and this have be solve now the indexhtml be generate each time when the db_util script be run,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,83,2020-01-08T00:42:52Z,kevinchekovfeeney,URL encoding - %20 - are not interpreted correctly in ID URLs,https://github.com/terminusdb/terminusdb/issues/83,"The ids of documents, especially those generated by idgen can contain URL encodings of spaces ""%20"" these are automatically interpolated into their url decoding as spaces and they return an error. The problem shows up in the document api - there is no problem in query. The fix should reverse the encoding used in the Idgen",2020-01-08T14:48:57Z,GavinMendelGleason,https://github.com/terminusdb/terminusdb/issues/83#issuecomment-572102926,Fixed in dev,fix in dev,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,84,2020-01-10T10:55:33Z,GavinMendelGleason,WOQL VIOs should stream rather than wait,https://github.com/terminusdb/terminusdb/issues/84,"The system knows early when things have gone wrong. There is no reason to make the client wait indefinitely before alerting them to this fact. This would be usefully augmented by a ""stop query"" button.",2020-10-01T14:24:52Z,GavinMendelGleason,https://github.com/terminusdb/terminusdb/issues/84#issuecomment-702172256,We are not streamed yet but have an optional 'one vio only'.,-PRON- be not stream yet but have an optional ' one vio only ',0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,84,2020-01-10T10:55:33Z,GavinMendelGleason,WOQL VIOs should stream rather than wait,https://github.com/terminusdb/terminusdb/issues/84,"The system knows early when things have gone wrong. There is no reason to make the client wait indefinitely before alerting them to this fact. This would be usefully augmented by a ""stop query"" button.",2021-03-12T12:18:44Z,GavinMendelGleason,https://github.com/terminusdb/terminusdb/issues/84#issuecomment-797455811,Choice of one or many now in the API - no plans to add this at the moment.,choice of one or many now in the api - no plan to add this at the moment,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,87,2020-01-14T14:37:04Z,GavinMendelGleason,Terminus Server prefix management incorrect for schemata,https://github.com/terminusdb/terminusdb/issues/87,Prefix management is not being done properly in terminus-server for schemata. This needs to be maintained somewhere - presumably the metadata graph.,2020-10-01T14:24:20Z,GavinMendelGleason,https://github.com/terminusdb/terminusdb/issues/87#issuecomment-702171738,This is fixed.,this be fix,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,88,2020-01-15T12:50:57Z,GavinMendelGleason,More than one representation of literals,https://github.com/terminusdb/terminusdb/issues/88,"We need to normalise all of terminus-server to change literal(type(Type,X)) to X^^T and literal(lang(Lang,X)) to X@Lang.",2020-10-01T14:04:53Z,GavinMendelGleason,https://github.com/terminusdb/terminusdb/issues/88#issuecomment-702159281,"This has been almost completely regularised, aside from where external libraries are used.",this have be almost completely regularise aside from where external library be use,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,89,2020-01-20T10:03:44Z,GavinMendelGleason,Memory only read graph from rdf,https://github.com/terminusdb/terminusdb/issues/89,Create a memory only read graph from rdf in WOQL.,2021-03-12T12:14:34Z,GavinMendelGleason,https://github.com/terminusdb/terminusdb/issues/89#issuecomment-797453575,No plans to do this now.,no plan to do this now,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,90,2020-01-21T11:52:21Z,Anniepoo,move to library(optparse),https://github.com/terminusdb/terminusdb/issues/90,"Terminus Server uses system:main/0 calls user hook main/1 to be prologscript compliant, which is good.  But it hand parses (and not well) the argv list.  let's move this to a proper optparse parsing before it becomes a nuisance.",2021-01-15T08:34:20Z,GavinMendelGleason,https://github.com/terminusdb/terminusdb/issues/90#issuecomment-760752840,This is no longer true now that the we have the cli parser.,this be no longer true now that the -PRON- have the cli parser,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,91,2020-01-21T12:04:01Z,Anniepoo,busy_handler at startup not defined,https://github.com/terminusdb/terminusdb/issues/91,"during startup in server.pl we define a handler busy_loading which we retract when sync_backing_store finishes.
busy_loading wasn't actually there, I inserted a minimal page, but it needs properly fixed.",2021-01-15T08:32:53Z,GavinMendelGleason,https://github.com/terminusdb/terminusdb/issues/91#issuecomment-760752204,The current banner is sufficient.,the current banner be sufficient,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,92,2020-01-22T13:28:10Z,Anniepoo,Move cors_catch to http_request_expansion,https://github.com/terminusdb/terminusdb/issues/92,"all of the cors_catch wrapper stuff should be moved to http_request_expansion
Example
https://github.com/Anniepoo/identity/blob/master/prolog/identity/identity.pl",2020-10-01T14:47:27Z,GavinMendelGleason,https://github.com/terminusdb/terminusdb/issues/92#issuecomment-702186959,We should probably still do this...,-PRON- should probably still do this,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,92,2020-01-22T13:28:10Z,Anniepoo,Move cors_catch to http_request_expansion,https://github.com/terminusdb/terminusdb/issues/92,"all of the cors_catch wrapper stuff should be moved to http_request_expansion
Example
https://github.com/Anniepoo/identity/blob/master/prolog/identity/identity.pl",2021-03-08T15:36:24Z,matko,https://github.com/terminusdb/terminusdb/issues/92#issuecomment-792837143,Closing due to age of issue and subsequent refactors having made this obsolete.,close due to age of issue and subsequent refactor have make this obsolete,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,94,2020-01-29T08:06:09Z,spl,Potential inconsistency with hosted dashboard CSS,https://github.com/terminusdb/terminusdb/issues/94,"In the process of looking at TerminusDB that led me to report terminusdb/terminus-dashboard#15, I came across a potential problem. (I didn't find an actual instance of the problem, but I thought you might want to think about it before it becomes a problem.)
This issue is exemplified by this line in index.tpl:
<link rel=""stylesheet""
      href=""https://terminusdb.github.io/terminus-dashboard/dist/css/theme.css"">
I believe this means every local TerminusDB installation will pull down the currently released theme.css as found on the gh-pages branch in the terminus-dashboard repository. If I understand it correctly, this means you will need to be sure the CSS selectors in your HTML and JavaScript will need to be consistent with the ones found at the above link. Since there is no versioning of that link, this may lead to problems with different local installations.
Some possible solutions to this include:

releasing versions of theme.css and tying a server to a particular hosted version or
bundling the theme.css with the server itself.",2020-01-29T09:51:03Z,rrooij,https://github.com/terminusdb/terminusdb/issues/94#issuecomment-579677635,"I completely agree. This was meant as a temporary measure until we have a decent process of releasing the two together.
Several options are better, I think:

Have the terminus-dashboard as a normal subdirectory in terminus-server, tying it terminus-server (your option 2). The problem with this is that you want to use the console independently from terminus-server as well. Might be a bit of a hassle to clone terminus-server if you're only using it for a remote server.
Have specific versions that are well tested with versions of terminus-server and linked against. To use the console offline, we could download it on the first start or let people install it using npm
Fetch the console in a sort of installation script that will download and extract the right version for the terminus-server version that the user is installing",i completely agree this be mean as a temporary measure until -PRON- have a decent process of release the two together several option be well i think have the terminus - dashboard as a normal subdirectory in terminus - server tie -PRON- terminus - server ( -PRON- option 2 ) the problem with this be that -PRON- want to use the console independently from terminus - server as well may be a bit of a hassle to clone terminus - server if -PRON- be only use -PRON- for a remote server have specific version that be well test with version of terminus - server and link against to use the console offline -PRON- could download -PRON- on the first start or let people install -PRON- use npm fetch the console in a sort of installation script that will download and extract the right version for the terminus - server version that the user be instal,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,94,2020-01-29T08:06:09Z,spl,Potential inconsistency with hosted dashboard CSS,https://github.com/terminusdb/terminusdb/issues/94,"In the process of looking at TerminusDB that led me to report terminusdb/terminus-dashboard#15, I came across a potential problem. (I didn't find an actual instance of the problem, but I thought you might want to think about it before it becomes a problem.)
This issue is exemplified by this line in index.tpl:
<link rel=""stylesheet""
      href=""https://terminusdb.github.io/terminus-dashboard/dist/css/theme.css"">
I believe this means every local TerminusDB installation will pull down the currently released theme.css as found on the gh-pages branch in the terminus-dashboard repository. If I understand it correctly, this means you will need to be sure the CSS selectors in your HTML and JavaScript will need to be consistent with the ones found at the above link. Since there is no versioning of that link, this may lead to problems with different local installations.
Some possible solutions to this include:

releasing versions of theme.css and tying a server to a particular hosted version or
bundling the theme.css with the server itself.",2020-01-29T10:48:33Z,spl,https://github.com/terminusdb/terminusdb/issues/94#issuecomment-579699612,"Have the terminus-dashboard as a normal subdirectory in terminus-server, tying it terminus-server (your option 2). The problem with this is that you want to use the console independently from terminus-server as well. Might be a bit of a hassle to clone terminus-server if you're only using it for a remote server.

@rrooij As a modified version of this alternative, instead of a normal subdirectory, you could use a git submodule, in which you specify which commit or tag of terminus-dashboard you require. This would allow you to continue developing the terminus-dashboard repository independently.",have the terminus - dashboard as a normal subdirectory in terminus - server tie -PRON- terminus - server ( -PRON- option 2 ) the problem with this be that -PRON- want to use the console independently from terminus - server as well may be a bit of a hassle to clone terminus - server if -PRON- be only use -PRON- for a remote server @rrooij as a modify version of this alternative instead of a normal subdirectory -PRON- could use a git submodule in which -PRON- specify which commit or tag of terminus - dashboard -PRON- require this would allow -PRON- to continue develop the terminus - dashboard repository independently,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,94,2020-01-29T08:06:09Z,spl,Potential inconsistency with hosted dashboard CSS,https://github.com/terminusdb/terminusdb/issues/94,"In the process of looking at TerminusDB that led me to report terminusdb/terminus-dashboard#15, I came across a potential problem. (I didn't find an actual instance of the problem, but I thought you might want to think about it before it becomes a problem.)
This issue is exemplified by this line in index.tpl:
<link rel=""stylesheet""
      href=""https://terminusdb.github.io/terminus-dashboard/dist/css/theme.css"">
I believe this means every local TerminusDB installation will pull down the currently released theme.css as found on the gh-pages branch in the terminus-dashboard repository. If I understand it correctly, this means you will need to be sure the CSS selectors in your HTML and JavaScript will need to be consistent with the ones found at the above link. Since there is no versioning of that link, this may lead to problems with different local installations.
Some possible solutions to this include:

releasing versions of theme.css and tying a server to a particular hosted version or
bundling the theme.css with the server itself.",2020-07-09T14:43:25Z,rrooij,https://github.com/terminusdb/terminusdb/issues/94#issuecomment-656170027,We now use versioned CSS and JS for terminusdb-console (previously called terminusdb-dashboard).,-PRON- now use versioned css and js for terminusdb - console ( previously call terminusdb - dashboard ),0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,97,2020-02-05T10:39:34Z,Anniepoo,config.pl configuration should come from settings file,https://github.com/terminusdb/terminusdb/issues/97,"Currently configuration comes from constants in facts in config.pl
Recommend these facts become rules that call settings/2 so this stuff can migrate to a settings file
(or ditch this and use settings directly?).",2020-10-01T14:03:50Z,GavinMendelGleason,https://github.com/terminusdb/terminusdb/issues/97#issuecomment-702158571,We now set from environment variables.,-PRON- now set from environment variable,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,98,2020-02-07T11:15:09Z,GavinMendelGleason,"""as"" in WOQL should take an optional third argument specifying casting type. ",https://github.com/terminusdb/terminusdb/issues/98,This would be a huge improvement of having to manually typcast all over the place in csv files.,2020-10-01T14:03:26Z,GavinMendelGleason,https://github.com/terminusdb/terminusdb/issues/98#issuecomment-702158291,This is now the case.,this be now the case,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,99,2020-02-07T12:19:34Z,GavinMendelGleason,"""when"" should return number of insertions / deletions",https://github.com/terminusdb/terminusdb/issues/99,Currently returns ubound binding set which is meaningless. Better to return the number of inserted and deleted triples as it allows you to know whether the transaction actually worked.,2020-10-01T14:03:07Z,GavinMendelGleason,https://github.com/terminusdb/terminusdb/issues/99#issuecomment-702158068,"""when"" is now deprecated",""" when "" be now deprecated",0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,100,2020-02-12T22:52:22Z,pmoura,Unification creates a cyclic term,https://github.com/terminusdb/terminusdb/issues/100,"Using a new linter check in the current Logtalk git version, I get (in the dev branch):
*     Suspicious call: A=literal(type(A,B)) as unification will succeed creating a cyclic term
*       while compiling object woql_compile
*       in file /Users/pmoura/terminus-server/library/woql_compile.pl between lines 612-619

Typo in the code?",2020-02-12T23:19:10Z,GavinMendelGleason,https://github.com/terminusdb/terminusdb/issues/100#issuecomment-585467336,"Yeah, looks like a typo alright!",yeah look like a typo alright,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,100,2020-02-12T22:52:22Z,pmoura,Unification creates a cyclic term,https://github.com/terminusdb/terminusdb/issues/100,"Using a new linter check in the current Logtalk git version, I get (in the dev branch):
*     Suspicious call: A=literal(type(A,B)) as unification will succeed creating a cyclic term
*       while compiling object woql_compile
*       in file /Users/pmoura/terminus-server/library/woql_compile.pl between lines 612-619

Typo in the code?",2020-03-02T17:44:01Z,pmoura,https://github.com/terminusdb/terminusdb/issues/100#issuecomment-593526467,Fixed in 7c2421f,fix in 7c2421f,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,102,2020-02-24T13:00:45Z,rrooij,Test for schema validation,https://github.com/terminusdb/terminusdb/issues/102,"My incomplete changes for improving schema checking speed passed tests, but was still broken. We need a proper test that does the following:

Create a schema
Add some triples
Create a new schema with added restrictions which should reject the triples created in 2",2020-10-01T14:00:27Z,GavinMendelGleason,https://github.com/terminusdb/terminusdb/issues/102#issuecomment-702156224,"We now have some schema tests, but these should be radically extended.",-PRON- now have some schema test but these should be radically extend,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,103,2020-02-25T19:02:26Z,Cheukting,Query that goes in an endless loop,https://github.com/terminusdb/terminusdb/issues/103,"terminus-server: v1.1.6
environment: docker (quickstart) on MacOS 10.14
simple example (WOQL.js on console):
WOQL.when(
    WOQL.and(
    WOQL.get(
            WOQL.as(""id"", ""v:Property"")
                .as(""label"", ""v:Label"")
                .as(""comment"", ""v:Comment"")
        ).file(""/app/local_files/all-layers-properties.csv""),
            WOQL.cast(""v:Property"", ""owl:Thing"", ""v:PropertyID""),
        ),
      WOQL.and(
        WOQL.add_quad(""v:PropertyID"", ""type"", ""owl:ObjectProperty"", ""schema""),
        WOQL.add_quad(""v:PropertyID"", ""label"", ""v:Label"", ""schema""),
        WOQL.add_quad(""v:PropertyID"", ""comment"", ""v:Comment"", ""schema"")
      )
    )

When click summit, the console does not return any result or warning, it just keep running.
The csv used can be found here: https://github.com/terminusdb/terminus-tutorials/tree/dev/schema.org",2020-02-25T19:12:44Z,rrooij,https://github.com/terminusdb/terminusdb/issues/103#issuecomment-591019083,"I can't reproduce this. What I did was the following:

Go to the terminus-tutorials dev branch
Run the schema_org.py script
Try your query in terminus-console, except I use .remote instead of .file. Also tested with .file locally and it works as well.

Environment: Debian Testing
terminus-server: dev and master
It reports lots of violations though.",i can not reproduce this what i do be the follow go to the terminus - tutorial dev branch run the schema_orgpy script try -PRON- query in terminus - console except i use remote instead of file also test with file locally and -PRON- work as well environment debian testing terminus - server dev and master -PRON- report lot of violation though,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,103,2020-02-25T19:02:26Z,Cheukting,Query that goes in an endless loop,https://github.com/terminusdb/terminusdb/issues/103,"terminus-server: v1.1.6
environment: docker (quickstart) on MacOS 10.14
simple example (WOQL.js on console):
WOQL.when(
    WOQL.and(
    WOQL.get(
            WOQL.as(""id"", ""v:Property"")
                .as(""label"", ""v:Label"")
                .as(""comment"", ""v:Comment"")
        ).file(""/app/local_files/all-layers-properties.csv""),
            WOQL.cast(""v:Property"", ""owl:Thing"", ""v:PropertyID""),
        ),
      WOQL.and(
        WOQL.add_quad(""v:PropertyID"", ""type"", ""owl:ObjectProperty"", ""schema""),
        WOQL.add_quad(""v:PropertyID"", ""label"", ""v:Label"", ""schema""),
        WOQL.add_quad(""v:PropertyID"", ""comment"", ""v:Comment"", ""schema"")
      )
    )

When click summit, the console does not return any result or warning, it just keep running.
The csv used can be found here: https://github.com/terminusdb/terminus-tutorials/tree/dev/schema.org",2020-02-25T19:25:19Z,Cheukting,https://github.com/terminusdb/terminusdb/issues/103#issuecomment-591024951,"You may have to set up the type schema first, run the following one by one:
WOQL.when(
  WOQL.and(
	WOQL.get(
	  WOQL.as(""id"", ""v:CID"")
		  .as(""label"", ""v:Label"")
		  .as(""comment"", ""v:Comment"")
          .as(""subTypes"", ""v:Kids"")
		  .as(""subTypeOf"", ""v:Parent"")
	  ).file(""/app/local_files/all-layers-types.csv""),
	WOQL.split(""v:Parent"", "", "", ""v:Parents""),
	WOQL.member(""v:Poppy"", ""v:Parents""),
	WOQL.split(""v:Kids"", "", "", ""v:PKID""),
	WOQL.member(""v:Kid"", ""v:PKID""),
	WOQL.cast(""v:CID"", ""owl:Thing"", ""v:ClassID""),
	WOQL.cast(""v:Kid"", ""owl:Thing"", ""v:ChildID""),
	WOQL.cast(""v:Poppy"", ""owl:Thing"", ""v:PID""),
    WOQL.cast(""v:Kid"", ""owl:Thing"", ""v:KidID"")
  ),
  WOQL.and(
   WOQL.doctype(""v:ClassID"").label(""v:Label"").description(""v:Comment""),
)
)

WOQL.when(
  WOQL.and(
	WOQL.get(
	  WOQL.as(""id"", ""v:CID"")
		  .as(""label"", ""v:Label"")
		  .as(""comment"", ""v:Comment"")
          .as(""subTypes"", ""v:Kids"")
		  .as(""subTypeOf"", ""v:Parent"")
	  ).file(""/app/local_files/all-layers-types.csv""),
	WOQL.split(""v:Parent"", "", "", ""v:Parents""),
	WOQL.member(""v:Poppy"", ""v:Parents""),
	WOQL.split(""v:Kids"", "", "", ""v:PKID""),
	WOQL.member(""v:Kid"", ""v:PKID""),
	WOQL.cast(""v:CID"", ""owl:Thing"", ""v:ClassID""),
	WOQL.cast(""v:Kid"", ""owl:Thing"", ""v:ChildID""),
	WOQL.cast(""v:Poppy"", ""owl:Thing"", ""v:PID""),
    WOQL.cast(""v:Kid"", ""owl:Thing"", ""v:KidID"")
  ),
  WOQL.and(
   WOQL.add_quad(""v:KidID"", ""subClassOf"", ""v:ClassID"", ""schema""),
)
)

WOQL.when(
  WOQL.and(
	WOQL.get(
	  WOQL.as(""id"", ""v:CID"")
		  .as(""label"", ""v:Label"")
		  .as(""comment"", ""v:Comment"")
          .as(""subTypes"", ""v:Kids"")
		  .as(""subTypeOf"", ""v:Parent"")
	  ).file(""/app/local_files/all-layers-types.csv""),
	WOQL.split(""v:Parent"", "", "", ""v:Parents""),
	WOQL.member(""v:Poppy"", ""v:Parents""),
	WOQL.split(""v:Kids"", "", "", ""v:PKID""),
	WOQL.member(""v:Kid"", ""v:PKID""),
	WOQL.cast(""v:CID"", ""owl:Thing"", ""v:ClassID""),
	WOQL.cast(""v:Kid"", ""owl:Thing"", ""v:ChildID""),
	WOQL.cast(""v:Poppy"", ""owl:Thing"", ""v:PID""),
    WOQL.cast(""v:Kid"", ""owl:Thing"", ""v:KidID"")
  ),
  WOQL.and(
   WOQL.add_quad(""v:ClassID"", ""subClassOf"", ""v:PID"", ""schema"")
)
)","-PRON- may have to set up the type schema first run the follow one by one woqlwhen ( woqland ( woqlget ( woqlas(""id "" "" vcid "" ) as(""label "" "" vlabel "" ) as(""comment "" "" vcomment "" ) as(""subtypes "" "" vkid "" ) as(""subtypeof "" "" vparent "" ) ) file(""/app / local_file / all - layer - typescsv "" ) woqlsplit(""vparent "" "" "" "" vparent "" ) woqlmember(""vpoppy "" "" vparent "" ) woqlsplit(""vkids "" "" "" "" vpkid "" ) woqlmember(""vkid "" "" vpkid "" ) woqlcast(""vcid "" "" owlthe "" "" vclassid "" ) woqlcast(""vkid "" "" owlthe "" "" vchildid "" ) woqlcast(""vpoppy "" "" owlthe "" "" vpid "" ) woqlcast(""vkid "" "" owlthe "" "" vkidid "" ) ) woqland ( woqldoctype(""vclassid"")label(""vlabel"")description(""vcomment "" ) ) ) woqlwhen ( woqland ( woqlget ( woqlas(""id "" "" vcid "" ) as(""label "" "" vlabel "" ) as(""comment "" "" vcomment "" ) as(""subtypes "" "" vkid "" ) as(""subtypeof "" "" vparent "" ) ) file(""/app / local_file / all - layer - typescsv "" ) woqlsplit(""vparent "" "" "" "" vparent "" ) woqlmember(""vpoppy "" "" vparent "" ) woqlsplit(""vkids "" "" "" "" vpkid "" ) woqlmember(""vkid "" "" vpkid "" ) woqlcast(""vcid "" "" owlthe "" "" vclassid "" ) woqlcast(""vkid "" "" owlthe "" "" vchildid "" ) woqlcast(""vpoppy "" "" owlthe "" "" vpid "" ) woqlcast(""vkid "" "" owlthe "" "" vkidid "" ) ) woqland ( woqladd_quad(""vkidid "" "" subclassof "" "" vclassid "" "" schema "" ) ) ) woqlwhen ( woqland ( woqlget ( woqlas(""id "" "" vcid "" ) as(""label "" "" vlabel "" ) as(""comment "" "" vcomment "" ) as(""subtypes "" "" vkid "" ) as(""subtypeof "" "" vparent "" ) ) file(""/app / local_file / all - layer - typescsv "" ) woqlsplit(""vparent "" "" "" "" vparent "" ) woqlmember(""vpoppy "" "" vparent "" ) woqlsplit(""vkids "" "" "" "" vpkid "" ) woqlmember(""vkid "" "" vpkid "" ) woqlcast(""vcid "" "" owlthe "" "" vclassid "" ) woqlcast(""vkid "" "" owlthe "" "" vchildid "" ) woqlcast(""vpoppy "" "" owlthe "" "" vpid "" ) woqlcast(""vkid "" "" owlthe "" "" vkidid "" ) ) woqland ( woqladd_quad(""vclassid "" "" subclassof "" "" vpid "" "" schema "" ) ) )",0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,103,2020-02-25T19:02:26Z,Cheukting,Query that goes in an endless loop,https://github.com/terminusdb/terminusdb/issues/103,"terminus-server: v1.1.6
environment: docker (quickstart) on MacOS 10.14
simple example (WOQL.js on console):
WOQL.when(
    WOQL.and(
    WOQL.get(
            WOQL.as(""id"", ""v:Property"")
                .as(""label"", ""v:Label"")
                .as(""comment"", ""v:Comment"")
        ).file(""/app/local_files/all-layers-properties.csv""),
            WOQL.cast(""v:Property"", ""owl:Thing"", ""v:PropertyID""),
        ),
      WOQL.and(
        WOQL.add_quad(""v:PropertyID"", ""type"", ""owl:ObjectProperty"", ""schema""),
        WOQL.add_quad(""v:PropertyID"", ""label"", ""v:Label"", ""schema""),
        WOQL.add_quad(""v:PropertyID"", ""comment"", ""v:Comment"", ""schema"")
      )
    )

When click summit, the console does not return any result or warning, it just keep running.
The csv used can be found here: https://github.com/terminusdb/terminus-tutorials/tree/dev/schema.org",2020-02-25T19:27:02Z,Cheukting,https://github.com/terminusdb/terminusdb/issues/103#issuecomment-591025762,"Don't run schema_org.py in testing, it is not related to this issue",do not run schema_orgpy in test -PRON- be not relate to this issue,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,104,2020-03-04T16:07:37Z,pmoura,Redundant imports in `database` module ?,https://github.com/terminusdb/terminusdb/issues/104,"In the database module we have:
https://github.com/terminusdb/terminus-server/blob/c9e40f72af6c629de4f671f8b5d60e78abab5d04/library/database.pl#L35-L39
But module triplestore already reexports library(terminus_store) (with the exception of two predicates that are not called by the database module).
P.S. found while checking a Logtalk linter warning about the open_memory_store/1 being imported from two different modules.",2020-10-01T13:59:47Z,GavinMendelGleason,https://github.com/terminusdb/terminusdb/issues/104#issuecomment-702155795,This is no longer true.,this be no long true,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,105,2020-03-04T16:34:27Z,pmoura,Missing comma results in syntax error,https://github.com/terminusdb/terminusdb/issues/105,"https://github.com/terminusdb/terminus-server/blob/c9e40f72af6c629de4f671f8b5d60e78abab5d04/library/validate.pl#L110
It should be:
descriptor: Descriptor,",2020-03-06T09:10:18Z,rrooij,https://github.com/terminusdb/terminusdb/issues/105#issuecomment-595673598,"Thanks, you're absolutely right!
The hub branch has a lot of work in progress that is sometimes not finished. That's why there may be syntax errors left in commits and the like.",thank -PRON- be absolutely right the hub branch have a lot of work in progress that be sometimes not finish that be why there may be syntax error leave in commit and the like,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,106,2020-03-09T18:51:53Z,pmoura,Unification creates cyclic term,https://github.com/terminusdb/terminusdb/issues/106,"In the following goal, the unification creates cyclic term:
https://github.com/terminusdb/terminus-server/blob/4c2dbdd4c3b46d7a0f1d5c470c7f6132ef99238a/library/validate.pl#L170-L173
Reported by the Logtalk linter as:
*     Suspicious call: repository_descriptor{database_descriptor:A,repository_name:B}=A as unification will succeed creating a cyclic term
*       while compiling object validate
*       in file /Users/pmoura/terminus-server/library/validate.pl between lines 164-187",2020-03-09T19:16:10Z,GavinMendelGleason,https://github.com/terminusdb/terminusdb/issues/106#issuecomment-596730760,Woops! We'll fix that. Cheers.,woop -PRON- will fix that cheer,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,106,2020-03-09T18:51:53Z,pmoura,Unification creates cyclic term,https://github.com/terminusdb/terminusdb/issues/106,"In the following goal, the unification creates cyclic term:
https://github.com/terminusdb/terminus-server/blob/4c2dbdd4c3b46d7a0f1d5c470c7f6132ef99238a/library/validate.pl#L170-L173
Reported by the Logtalk linter as:
*     Suspicious call: repository_descriptor{database_descriptor:A,repository_name:B}=A as unification will succeed creating a cyclic term
*       while compiling object validate
*       in file /Users/pmoura/terminus-server/library/validate.pl between lines 164-187",2020-03-17T15:43:04Z,pmoura,https://github.com/terminusdb/terminusdb/issues/106#issuecomment-600142005,Fixed in cdb23f0.,fix in cdb23f0,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,115,2020-03-17T18:03:38Z,pmoura,Typo in predicate name,https://github.com/terminusdb/terminusdb/issues/115,"The following is an obvious typo:
https://github.com/terminusdb/terminus-server/blob/969aa22d92c7018bc27f09fce6b3d01a2614c886/server/routes.pl#L780
Should it be terminus_database or terminus_database_name?
Reported by the Logtalk linter as:
*     Unknown predicate called but not defined: terminus_databasoe/1
*       while compiling object routes
*       in file /Users/pmoura/terminus-server/server/routes.pl between lines 770-792",2020-03-17T18:13:35Z,GavinMendelGleason,https://github.com/terminusdb/terminusdb/issues/115#issuecomment-600222063,"Indeed that's wrong. The logic is also somewhat wrong though. We need to open Terminus_DB once so we have a shared view in all of those predicates.
Descriptor = terminus_descriptor{},
open_descriptor(Descriptor, Terminus_DB),
...

Should probably go at the top of the predicate.",indeed that be wrong the logic be also somewhat wrong though -PRON- need to open terminus_db once so -PRON- have a shared view in all of those predicate descriptor = terminus_descriptor { } open_descriptor(descriptor terminus_db ) should probably go at the top of the predicate,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,115,2020-03-17T18:03:38Z,pmoura,Typo in predicate name,https://github.com/terminusdb/terminusdb/issues/115,"The following is an obvious typo:
https://github.com/terminusdb/terminus-server/blob/969aa22d92c7018bc27f09fce6b3d01a2614c886/server/routes.pl#L780
Should it be terminus_database or terminus_database_name?
Reported by the Logtalk linter as:
*     Unknown predicate called but not defined: terminus_databasoe/1
*       while compiling object routes
*       in file /Users/pmoura/terminus-server/server/routes.pl between lines 770-792",2020-04-20T10:27:15Z,rrooij,https://github.com/terminusdb/terminusdb/issues/115#issuecomment-616457650,Typo has been fixed! Thanks.,typo have be fix thank,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,116,2020-03-18T09:31:18Z,pmoura,Typo in predicate name: descriptor_type_order_List,https://github.com/terminusdb/terminusdb/issues/116,"https://github.com/terminusdb/terminus-server/blob/186693ba81850411e26619eacef8c34c026df8fc/core/transaction/validate.pl#L275
It should be descriptor_type_order_list.",2020-04-28T08:36:57Z,rrooij,https://github.com/terminusdb/terminusdb/issues/116#issuecomment-620464502,"Fixed, thanks!",fix thank,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,118,2020-03-22T16:48:57Z,pmoura,Duplicated export of empty_context/1 predicate,https://github.com/terminusdb/terminusdb/issues/118,"Already declared at line 9:
https://github.com/terminusdb/terminus-server/blob/d0408756479b7a38b817b6aafa3bf7170f678c50/core/query.pl#L82
Reported by the Logtalk linter as:
*     Duplicated directive: public empty_context/1
*       first found between lines 1-85
*       while compiling object query
*       in file /Users/pmoura/terminus-server/core/query.pl between lines 1-85",2020-10-01T13:59:10Z,GavinMendelGleason,https://github.com/terminusdb/terminusdb/issues/118#issuecomment-702155394,This is now fixed.,this be now fix,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,119,2020-03-22T18:37:23Z,pmoura,"Export of a non-defined predicate, empty_context/1, in module ""ask""",https://github.com/terminusdb/terminusdb/issues/119,"https://github.com/terminusdb/terminus-server/blob/ccf9556f144ab2cc4f4adfd4e0dd600607cc6480/core/query/ask.pl#L8
Found while looking into a Logtalk compiler error:
!     Permission error: modify uses_object_predicate empty_context/1
!       in directive use_module/2
!       in file /Users/pmoura/terminus-server/core/query.pl at or above line 94

The error occurs due to the query module using:
:- use_module(query/ask).
:- use_module(query/woql_compile).
and both ask and woql_compile modules exporting an empty_context/1 predicate (only defined in the second module).",2020-09-29T13:30:19Z,GavinMendelGleason,https://github.com/terminusdb/terminusdb/issues/119#issuecomment-700704301,This should be fixed now.,this should be fix now,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,120,2020-04-19T20:47:26Z,GavinMendelGleason,Casting to xsd:long is failing,https://github.com/terminusdb/terminusdb/issues/120,"Casting to xsd:long is failing on the following query.
const csv = WOQL.get(
    WOQL.as(""first_name"",""v:first_name"")
        .as(""last_name"", ""v:last_name"")
        .as(""phone_number"", ""v:phone_number"")
        .as(""city"", ""v:city"")
        .as(""age"", ""v:age"")
).remote(""https://raw.githubusercontent.com/graknlabs/examples/master/datasets/phone-calls/people.csv"") but then the  Typecast fails WOQL.typecast(""v:age"", ""xsd:long"" , ""v:age_Cast""),",2020-08-12T15:50:09Z,rrooij,https://github.com/terminusdb/terminusdb/issues/120#issuecomment-672956744,"It gives back an empty result, strangely:
WOQL.and(WOQL.get(
    WOQL.as(""first_name"",""v:First_name"")
        .as(""last_name"", ""v:Last_name"")
        .as(""phone_number"", ""v:Phone_number"")
        .as(""city"", ""v:city"")
        .as(""age"", ""v:age"")
).remote(""https://raw.githubusercontent.com/graknlabs/examples/master/datasets/phone-calls/people.csv""),
WOQL.typecast(""v:age"", ""xsd:long"", ""v:age_long""))","-PRON- give back an empty result strangely woqland(woqlget ( woqlas(""first_name""""vfirst_name "" ) as(""last_name "" "" vlast_name "" ) as(""phone_number "" "" vphone_number "" ) as(""city "" "" vcity "" ) as(""age "" "" vage "" ) ) remote(""https//rawgithubusercontentcom / graknlabs / example / master / dataset / phone - call / peoplecsv "" ) woqltypecast(""vage "" "" xsdlong "" "" vage_long "" ) )",0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,120,2020-04-19T20:47:26Z,GavinMendelGleason,Casting to xsd:long is failing,https://github.com/terminusdb/terminusdb/issues/120,"Casting to xsd:long is failing on the following query.
const csv = WOQL.get(
    WOQL.as(""first_name"",""v:first_name"")
        .as(""last_name"", ""v:last_name"")
        .as(""phone_number"", ""v:phone_number"")
        .as(""city"", ""v:city"")
        .as(""age"", ""v:age"")
).remote(""https://raw.githubusercontent.com/graknlabs/examples/master/datasets/phone-calls/people.csv"") but then the  Typecast fails WOQL.typecast(""v:age"", ""xsd:long"" , ""v:age_Cast""),",2020-11-11T10:39:42Z,k-tipp,https://github.com/terminusdb/terminusdb/issues/120#issuecomment-725347113,"I'm experiencing the same with xsd:date and xsd:boolean. Strangely, it works with xsd:dateTime. (all tested with the python client)",-PRON- be experience the same with xsddate and xsdboolean strangely -PRON- work with xsddatetime ( all test with the python client ),0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,120,2020-04-19T20:47:26Z,GavinMendelGleason,Casting to xsd:long is failing,https://github.com/terminusdb/terminusdb/issues/120,"Casting to xsd:long is failing on the following query.
const csv = WOQL.get(
    WOQL.as(""first_name"",""v:first_name"")
        .as(""last_name"", ""v:last_name"")
        .as(""phone_number"", ""v:phone_number"")
        .as(""city"", ""v:city"")
        .as(""age"", ""v:age"")
).remote(""https://raw.githubusercontent.com/graknlabs/examples/master/datasets/phone-calls/people.csv"") but then the  Typecast fails WOQL.typecast(""v:age"", ""xsd:long"" , ""v:age_Cast""),",2020-11-11T10:50:36Z,GavinMendelGleason,https://github.com/terminusdb/terminusdb/issues/120#issuecomment-725352421,I'll take a look at this today - it's definitely a bug.,-PRON- will take a look at this today - -PRON- be definitely a bug,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,120,2020-04-19T20:47:26Z,GavinMendelGleason,Casting to xsd:long is failing,https://github.com/terminusdb/terminusdb/issues/120,"Casting to xsd:long is failing on the following query.
const csv = WOQL.get(
    WOQL.as(""first_name"",""v:first_name"")
        .as(""last_name"", ""v:last_name"")
        .as(""phone_number"", ""v:phone_number"")
        .as(""city"", ""v:city"")
        .as(""age"", ""v:age"")
).remote(""https://raw.githubusercontent.com/graknlabs/examples/master/datasets/phone-calls/people.csv"") but then the  Typecast fails WOQL.typecast(""v:age"", ""xsd:long"" , ""v:age_Cast""),",2021-01-15T08:30:11Z,GavinMendelGleason,https://github.com/terminusdb/terminusdb/issues/120#issuecomment-760750814,Fixed in a2f3ce8,fix in a2f3ce8,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,123,2020-07-08T12:39:06Z,joepio,Consider using SHACL instead of OWL / RDFS for schema constraints,https://github.com/terminusdb/terminusdb/issues/123,"Let me start by saying: very impressive work! You guys have realized many of the ideas that I was pondering about in a nicely built, open source package. Awesome. Ok, to the issue.
Currently, TerminusDB uses the OWL (Web ontology language) + RDFS for storing the schema. I think OWL is might not be the right ontology, since it was designed not to validate graphs, but to provide reasoning abilities. For example, consider the ontology from the quickstart guide, and an example instance using the property wrongly:
scm:journey_bicycle
  a owl:ObjectProperty ;
  rdfs:domain scm:Journey ;
  rdfs:label ""Bicycle used"";
  rdfs:range scm:Bicycle ;

example:suzanne
   a example:Person ;
   scm:journey_bicycle ""I'm just trying to break things"" ;
I'd assume we don't want this to happen. However, it's perfectly valid RDF, and a reasoner would conclude that example:suzanne is a Journey and the string ""I'm just trying to break things"" is a Bicycle. That's now what you'd probably have in mind - you probably wanted to say ""If you use scm:journey_bicycle, make sure to put a scm:Bicycle on the right side!"".
This has to do with how rdfs:domain and rdfs:range are defined.
I've made the same mistake some time ago when modelling some OWL files using the Protege software... OWL is simply not built for constraining data - it's built for reasoning, for adding new triples, for drawing new conclusions. It's built with the Open World Assumption in mind, which assumes that all data that you put in the system is always correct. However, in practice, you need to make sure that data conforms to some model before accepting it, because otherwise things will break.
However, the more recent SHACL specification does do what you're probably looking for: it provides a schema language for defining data shapes. These shapes are important when creating instances and rendering tables, because you need valid shape constrained data.
Of course, your implementation could interpret these rdfs / owl properties as type constraints, but keep in mind that their formal definitions are different, and they might make things awkward later on.",2020-07-08T12:53:36Z,kevinchekovfeeney,https://github.com/terminusdb/terminusdb/issues/123#issuecomment-655500236,"Hi Joepio,
Yes - in TerminusDB you have two different types of OWL graphs - one is schema and it applies a closed world interpretation to the logical structure, the other is an inference graph which uses traditional OWL open world inference semantics.
We were involved tangentially in the SHACL specification - I was the coordinator of the ALIGNED project at the time - Dimitri Konstakostas was one of the main people behind SHACL and part of ALIGNED at the time. Gavin wrote a variety of proofs of inconsisitencies in SHACL and for a variety of reasons, we came to the conclusion that it was far inferior to just using OWL in a closed world regime - OWL has the benefit of having a really well thought out logical underpinning, well defined complexities and so on - SHACL not so much.",hi joepio yes - in terminusdb -PRON- have two different type of owl graphs - one be schema and -PRON- apply a closed world interpretation to the logical structure the other be an inference graph which use traditional owl open world inference semantic -PRON- be involve tangentially in the shacl specification - i be the coordinator of the align project at the time - dimitri konstakosta be one of the main people behind shacl and part of align at the time gavin write a variety of proof of inconsisitencie in shacl and for a variety of reason -PRON- come to the conclusion that -PRON- be far inferior to just use owl in a closed world regime - owl have the benefit of have a really well think out logical underpinning well define complexity and so on - shacl not so much,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,123,2020-07-08T12:39:06Z,joepio,Consider using SHACL instead of OWL / RDFS for schema constraints,https://github.com/terminusdb/terminusdb/issues/123,"Let me start by saying: very impressive work! You guys have realized many of the ideas that I was pondering about in a nicely built, open source package. Awesome. Ok, to the issue.
Currently, TerminusDB uses the OWL (Web ontology language) + RDFS for storing the schema. I think OWL is might not be the right ontology, since it was designed not to validate graphs, but to provide reasoning abilities. For example, consider the ontology from the quickstart guide, and an example instance using the property wrongly:
scm:journey_bicycle
  a owl:ObjectProperty ;
  rdfs:domain scm:Journey ;
  rdfs:label ""Bicycle used"";
  rdfs:range scm:Bicycle ;

example:suzanne
   a example:Person ;
   scm:journey_bicycle ""I'm just trying to break things"" ;
I'd assume we don't want this to happen. However, it's perfectly valid RDF, and a reasoner would conclude that example:suzanne is a Journey and the string ""I'm just trying to break things"" is a Bicycle. That's now what you'd probably have in mind - you probably wanted to say ""If you use scm:journey_bicycle, make sure to put a scm:Bicycle on the right side!"".
This has to do with how rdfs:domain and rdfs:range are defined.
I've made the same mistake some time ago when modelling some OWL files using the Protege software... OWL is simply not built for constraining data - it's built for reasoning, for adding new triples, for drawing new conclusions. It's built with the Open World Assumption in mind, which assumes that all data that you put in the system is always correct. However, in practice, you need to make sure that data conforms to some model before accepting it, because otherwise things will break.
However, the more recent SHACL specification does do what you're probably looking for: it provides a schema language for defining data shapes. These shapes are important when creating instances and rendering tables, because you need valid shape constrained data.
Of course, your implementation could interpret these rdfs / owl properties as type constraints, but keep in mind that their formal definitions are different, and they might make things awkward later on.",2020-07-08T13:39:31Z,GavinMendelGleason,https://github.com/terminusdb/terminusdb/issues/123#issuecomment-655526534,"Reiterating some of what Kevin said above for clarity: We treat OWL in two distinct reasoning regimes. In both reasoning regimes we take the unique names assumption (also a divergence from the OWL standard).


The first is a closed-world reasoning constraint regime. We do not make any inferences about what logical rules should hold aside from building the subsumption relation for classes and properties from what is asserted. Instead we check to see if the ones that are asserted are self-consistent and consistent with the information in the corresponding instance graph.


The second is a more traditional inference regime. We expand what can be inferred about the logical statements in the contraint ontology based on information asserted as inferential. These expanded logical statements can then take part in the constraining of information in the instance graph.


The advantage of this approach is that we can specify constraints and inference in a single uniform language. In practice we spend almost all of our energy on the constraint aspects as this is what is most useful for a database, although we do make use of inferencing for various things in the core system graph (transitivity etc).
We evaluated SHACL but ultimately felt that it was not sufficiently well defined (negation with cardinalities led to non-monotonic functors which can not be the basis of an inductive definition) and did not add much extra over the constraint interpretation of OWL and had the downside of requiring two languages rather than one.
We do lack the ability to specify inductive relationships clearly in OWL and I'd like to see this rectified. But given that the SHACL definition was inconsistent and they did not distinguish between greatest and least fixed-points, I think there is more work to be done to make this viable anyhow.",reiterate some of what kevin say above for clarity -PRON- treat owl in two distinct reasoning regime in both reasoning regime -PRON- take the unique name assumption ( also a divergence from the owl standard ) the first be a closed - world reasoning constraint regime -PRON- do not make any inference about what logical rule should hold aside from build the subsumption relation for class and property from what be assert instead -PRON- check to see if the one that be assert be self - consistent and consistent with the information in the corresponding instance graph the second be a more traditional inference regime -PRON- expand what can be infer about the logical statement in the contraint ontology base on information assert as inferential these expand logical statement can then take part in the constraining of information in the instance graph the advantage of this approach be that -PRON- can specify constraint and inference in a single uniform language in practice -PRON- spend almost all of -PRON- energy on the constraint aspect as this be what be most useful for a database although -PRON- do make use of inference for various thing in the core system graph ( transitivity etc ) -PRON- evaluate shacl but ultimately feel that -PRON- be not sufficiently well define ( negation with cardinality lead to non - monotonic functor which can not be the basis of an inductive definition ) and do not add much extra over the constraint interpretation of owl and have the downside of require two language rather than one -PRON- do lack the ability to specify inductive relationship clearly in owl and -PRON- 'd like to see this rectified but give that the shacl definition be inconsistent and -PRON- do not distinguish between great and least fix - point i think there be more work to be do to make this viable anyhow,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,123,2020-07-08T12:39:06Z,joepio,Consider using SHACL instead of OWL / RDFS for schema constraints,https://github.com/terminusdb/terminusdb/issues/123,"Let me start by saying: very impressive work! You guys have realized many of the ideas that I was pondering about in a nicely built, open source package. Awesome. Ok, to the issue.
Currently, TerminusDB uses the OWL (Web ontology language) + RDFS for storing the schema. I think OWL is might not be the right ontology, since it was designed not to validate graphs, but to provide reasoning abilities. For example, consider the ontology from the quickstart guide, and an example instance using the property wrongly:
scm:journey_bicycle
  a owl:ObjectProperty ;
  rdfs:domain scm:Journey ;
  rdfs:label ""Bicycle used"";
  rdfs:range scm:Bicycle ;

example:suzanne
   a example:Person ;
   scm:journey_bicycle ""I'm just trying to break things"" ;
I'd assume we don't want this to happen. However, it's perfectly valid RDF, and a reasoner would conclude that example:suzanne is a Journey and the string ""I'm just trying to break things"" is a Bicycle. That's now what you'd probably have in mind - you probably wanted to say ""If you use scm:journey_bicycle, make sure to put a scm:Bicycle on the right side!"".
This has to do with how rdfs:domain and rdfs:range are defined.
I've made the same mistake some time ago when modelling some OWL files using the Protege software... OWL is simply not built for constraining data - it's built for reasoning, for adding new triples, for drawing new conclusions. It's built with the Open World Assumption in mind, which assumes that all data that you put in the system is always correct. However, in practice, you need to make sure that data conforms to some model before accepting it, because otherwise things will break.
However, the more recent SHACL specification does do what you're probably looking for: it provides a schema language for defining data shapes. These shapes are important when creating instances and rendering tables, because you need valid shape constrained data.
Of course, your implementation could interpret these rdfs / owl properties as type constraints, but keep in mind that their formal definitions are different, and they might make things awkward later on.",2020-07-08T15:25:25Z,joepio,https://github.com/terminusdb/terminusdb/issues/123#issuecomment-655588944,"Thanks for the clarification and the quick reply! You've obviously put way more thought into this than I anticipated.
I understand the value of using a mathematically / logically consistent ontology, and combining inferencing and shape validation into a single model seems reasonable, too.
I'll take a better look at your implementation to understand how you actually interpreted RDF / OWL. I'll close this for now.",thank for the clarification and the quick reply -PRON- have obviously put way more think into this than i anticipate i understand the value of use a mathematically / logically consistent ontology and combine inferencing and shape validation into a single model seem reasonable too -PRON- will take a well look at -PRON- implementation to understand how -PRON- actually interpret rdf / owl -PRON- will close this for now,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,124,2020-07-08T15:45:47Z,rahuldeepattri,README.md points to wrong latest version of project release,https://github.com/terminusdb/terminusdb/issues/124,"Describe the bug
The README.md has a section here where latest version is shown which is v2.0.3 as of now.
The latest release points to Hotfix release which is v1.1.3

To Reproduce
Steps to reproduce the behavior:

Go to here
See error

Expected behavior
Both of them should point to latest stable release which should be v2.0.4 as of raising this issue.
Additional context
Shoutout to @eddiejaoude for finding this.",2020-07-08T15:50:49Z,eddiejaoude,https://github.com/terminusdb/terminusdb/issues/124#issuecomment-655603537,"We were looking to put a link on the latest release section in the README, however, we noticed that the version stated does not match the release section on GitHub.",-PRON- be look to put a link on the late release section in the readme however -PRON- notice that the version state do not match the release section on github,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,124,2020-07-08T15:45:47Z,rahuldeepattri,README.md points to wrong latest version of project release,https://github.com/terminusdb/terminusdb/issues/124,"Describe the bug
The README.md has a section here where latest version is shown which is v2.0.3 as of now.
The latest release points to Hotfix release which is v1.1.3

To Reproduce
Steps to reproduce the behavior:

Go to here
See error

Expected behavior
Both of them should point to latest stable release which should be v2.0.4 as of raising this issue.
Additional context
Shoutout to @eddiejaoude for finding this.",2020-07-08T15:54:50Z,rrooij,https://github.com/terminusdb/terminusdb/issues/124#issuecomment-655605890,"Thanks a lot for spotting this. We used GitHub releases for some time and then stopped using it, we are now using normal git tags instead. I will remove the GitHub release as it creates confusion.
I assumed that version tags were automatically seen as a ""new release"" by GitHub.","thank a lot for spot this -PRON- use github release for some time and then stop use -PRON- -PRON- be now use normal git tag instead i will remove the github release as -PRON- create confusion i assume that version tag be automatically see as a "" new release "" by github",0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,124,2020-07-08T15:45:47Z,rahuldeepattri,README.md points to wrong latest version of project release,https://github.com/terminusdb/terminusdb/issues/124,"Describe the bug
The README.md has a section here where latest version is shown which is v2.0.3 as of now.
The latest release points to Hotfix release which is v1.1.3

To Reproduce
Steps to reproduce the behavior:

Go to here
See error

Expected behavior
Both of them should point to latest stable release which should be v2.0.4 as of raising this issue.
Additional context
Shoutout to @eddiejaoude for finding this.",2020-07-08T16:27:17Z,eddiejaoude,https://github.com/terminusdb/terminusdb/issues/124#issuecomment-655623350,Sounds good 👍,sound good 👍,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,126,2020-07-16T12:01:58Z,GavinMendelGleason,"""Using"" should default the write graph as well.",https://github.com/terminusdb/terminusdb/issues/126,"The ""using"" woql word should fix the default write_graph to be whatever the relative graph ""instance/main"" resolves to. This avoids violating the principle of least surprise.
For instance:
let q = WOQL.query()
q.using(""admin/BicycleDataTest/_meta"").into(""instance/main"")
 .and(WOQL.idgen(""terminusdb:///repository/data/Remote"", [""origin""], ""v:Remote""),
      WOQL.add_triple(""v:Remote"", ""type"", ""repo:Remote"")
          .add_triple(""v:Remote"", ""repo:repository_name"", ""origin"")
          .add_triple(""v:Remote"", ""repo:remote_url"", ""https://hub-dev-server.dcm.ist/gavin/LastBikeTest"")
     )

Should work, while currently we must do:
let q = WOQL.query()
q.using(""admin/BicycleDataTest/_meta"").into(""instance/main"")
 .and(WOQL.idgen(""terminusdb:///repository/data/Remote"", [""origin""], ""v:Remote""),
      WOQL.add_triple(""v:Remote"", ""type"", ""repo:Remote"")
          .add_triple(""v:Remote"", ""repo:repository_name"", ""origin"")
          .add_triple(""v:Remote"", ""repo:remote_url"", ""https://hub-dev-server.dcm.ist/gavin/LastBikeTest"")
     )",2020-09-29T13:29:39Z,GavinMendelGleason,https://github.com/terminusdb/terminusdb/issues/126#issuecomment-700703846,Default graph should now be set appropriately in using.,default graph should now be set appropriately in use,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,127,2020-08-18T10:07:37Z,rrooij,Negatives can't be casted to xsd:decimal,https://github.com/terminusdb/terminusdb/issues/127,"Describe the bug
Casting negatives to a xsd:decimal does not work.
To Reproduce
Steps to reproduce the behavior:

Create an empty DB
Go to the query tab
Execute the following query

WOQL.typecast(""-58.08"", ""xsd:decimal"", ""v:Casted"")

Expected behavior
Negatives are valid decimals so should cast successfully
Info (please complete the following information):

OS: Debian Bullseye (testing)
Run from source
SWI-Prolog version 8.3.5 for x86_64-linux

Additional context
The following error appears:
  ""@type"":""vio:ViolationWithDatatypeObject"",
  ""vio:literal"":""-58.08"",
  ""vio:message"":""Unable to cast as (xsd:decimal): \""-58.08\""\n""
}```",2020-08-18T12:03:08Z,rrooij,https://github.com/terminusdb/terminusdb/issues/127#issuecomment-675436944,Fixed in commit 9adfac2,fix in commit 9adfac2,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,128,2020-08-18T12:05:00Z,rrooij,Casting to decimal loses precision,https://github.com/terminusdb/terminusdb/issues/128,"Describe the bug
Casting to a xsd:decimal loses precision
To Reproduce
Steps to reproduce the behavior:

Create an empty DB
Go to the query tab
Execute the following query

WOQL.typecast(""58.082342356456934053939"", ""xsd:decimal"", ""v:Casted"")

The result it returns is:
58.082342356456934

Expected behavior
Better precision
Info (please complete the following information):

OS: Debian Bullseye (testing)
Run from source
SWI-Prolog version 8.3.5 for x86_64-linux",,,,,,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,129,2020-08-25T12:33:37Z,rrooij,"Rebase/merge is slow, move it to Rust",https://github.com/terminusdb/terminusdb/issues/129,"On the current terminusdb-server on #dev, a merge re-adds each triple and re-adds each deletion as well. This makes it pretty slow. It should probably done in Rust instead of Prolog.",2020-09-25T15:46:18Z,GavinMendelGleason,https://github.com/terminusdb/terminusdb/issues/129#issuecomment-699004797,"Application of a commit on top of another commit has been pressed into rust, so closing this issue.",application of a commit on top of another commit have be press into rust so close this issue,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,130,2020-08-28T13:31:18Z,undefdev,Download center not available,https://github.com/terminusdb/terminusdb/issues/130,"Your download center link goes 404 on me.
(sorry, this is probably not the right place to post this issue, but I didn't know where else)",2020-08-28T14:11:26Z,rrooij,https://github.com/terminusdb/terminusdb/issues/130#issuecomment-682597910,"Good catch and solved! We were working on a better download page, but updated the link by accident already!",good catch and solve -PRON- be work on a well download page but update the link by accident already,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,131,2020-08-31T08:15:24Z,rrooij,HTTP should redirect to HTTPS when HTTPS is enabled,https://github.com/terminusdb/terminusdb/issues/131,Enabling HTTPS totally disables the HTTP server. A redirection to HTTPS would be more appropaite.,,,,,,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,132,2020-09-23T12:09:32Z,Cheukting,"Prefix not working properly when using ""using""",https://github.com/terminusdb/terminusdb/issues/132,"Describe the bug
Prefix not working properly when using ""using""
To Reproduce
Steps to reproduce the behavior:
Go to console and use ""using"" with prefix on the seshat dataset
Expected behavior
A clear and concise description of what you expected to happen.
NA
Screenshots
If applicable, add screenshots to help explain your problem.
NA
Info (please complete the following information):

OS: Linus
How did you run terminus-server: docker?
Gavin's set up

Additional context
Gavin found this bug",2020-09-29T13:29:01Z,GavinMendelGleason,https://github.com/terminusdb/terminusdb/issues/132#issuecomment-700703465,"Prefixes should now be set in the context for the given loaded ""using"".","prefix should now be set in the context for the give load "" use """,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,133,2020-09-25T15:40:54Z,luke-feeney,Time out when cloning large databases,https://github.com/terminusdb/terminusdb/issues/133,"Describe the bug
When I try to clone a large database, the action times out and does not complete. This does not happen when I have a better internet connection. I understand it is to do with the size and the Cloudflare DNS (the 'Client Max Upload Size' limits to be precise) . As it is not economically possible to solve the problem, would be good to have a chunking approach so that I can still get the clone even with a weaker connection. As DBs get even larger and users more scattered, this will become more of an issue.
To Reproduce
Steps to reproduce the behavior:

Go to an area with bad internet
Click on click on clone
Wait while it tries to clone
See error

Expected behavior
I expected the clone to happen,  even if it was a bit slower due to the connection - which I already know isn't great.
Screenshots
N/A
Info (please complete the following information):

OS: windows

Additional context
Known problem - keep delivering the dream, Terminators",2020-10-05T20:35:58Z,kevinchekovfeeney,https://github.com/terminusdb/terminusdb/issues/133#issuecomment-703874181,"This is an issue with the hub infrastrcture (cloudflare time outs for POSTs) not the server itself - I think the solution is to upgrade the hub infra account with cloudflare, not chunking as such.",this be an issue with the hub infrastrcture ( cloudflare time out for post ) not the server -PRON- - i think the solution be to upgrade the hub infra account with cloudflare not chunk as such,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,133,2020-09-25T15:40:54Z,luke-feeney,Time out when cloning large databases,https://github.com/terminusdb/terminusdb/issues/133,"Describe the bug
When I try to clone a large database, the action times out and does not complete. This does not happen when I have a better internet connection. I understand it is to do with the size and the Cloudflare DNS (the 'Client Max Upload Size' limits to be precise) . As it is not economically possible to solve the problem, would be good to have a chunking approach so that I can still get the clone even with a weaker connection. As DBs get even larger and users more scattered, this will become more of an issue.
To Reproduce
Steps to reproduce the behavior:

Go to an area with bad internet
Click on click on clone
Wait while it tries to clone
See error

Expected behavior
I expected the clone to happen,  even if it was a bit slower due to the connection - which I already know isn't great.
Screenshots
N/A
Info (please complete the following information):

OS: windows

Additional context
Known problem - keep delivering the dream, Terminators",2020-10-05T20:46:25Z,luke-feeney,https://github.com/terminusdb/terminusdb/issues/133#issuecomment-703879233,"that is the easiest solution, but the price is high, so it will likely be a while before we could implement - is chunking not going to be useful for larger database in any case?",that be the easy solution but the price be high so -PRON- will likely be a while before -PRON- could implement - be chunk not go to be useful for large database in any case,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,133,2020-09-25T15:40:54Z,luke-feeney,Time out when cloning large databases,https://github.com/terminusdb/terminusdb/issues/133,"Describe the bug
When I try to clone a large database, the action times out and does not complete. This does not happen when I have a better internet connection. I understand it is to do with the size and the Cloudflare DNS (the 'Client Max Upload Size' limits to be precise) . As it is not economically possible to solve the problem, would be good to have a chunking approach so that I can still get the clone even with a weaker connection. As DBs get even larger and users more scattered, this will become more of an issue.
To Reproduce
Steps to reproduce the behavior:

Go to an area with bad internet
Click on click on clone
Wait while it tries to clone
See error

Expected behavior
I expected the clone to happen,  even if it was a bit slower due to the connection - which I already know isn't great.
Screenshots
N/A
Info (please complete the following information):

OS: windows

Additional context
Known problem - keep delivering the dream, Terminators",2020-10-05T21:35:22Z,kevinchekovfeeney,https://github.com/terminusdb/terminusdb/issues/133#issuecomment-703902076,Chunking is going to be needed / useful for pushing / uploads - not so much for downloads / clone - actually we could make clone and pull use GET to get around the download limitations in the short term...,chunk be go to be need / useful for push / upload - not so much for download / clone - actually -PRON- could make clone and pull use get to get around the download limitation in the short term,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,133,2020-09-25T15:40:54Z,luke-feeney,Time out when cloning large databases,https://github.com/terminusdb/terminusdb/issues/133,"Describe the bug
When I try to clone a large database, the action times out and does not complete. This does not happen when I have a better internet connection. I understand it is to do with the size and the Cloudflare DNS (the 'Client Max Upload Size' limits to be precise) . As it is not economically possible to solve the problem, would be good to have a chunking approach so that I can still get the clone even with a weaker connection. As DBs get even larger and users more scattered, this will become more of an issue.
To Reproduce
Steps to reproduce the behavior:

Go to an area with bad internet
Click on click on clone
Wait while it tries to clone
See error

Expected behavior
I expected the clone to happen,  even if it was a bit slower due to the connection - which I already know isn't great.
Screenshots
N/A
Info (please complete the following information):

OS: windows

Additional context
Known problem - keep delivering the dream, Terminators",2021-01-15T08:38:13Z,GavinMendelGleason,https://github.com/terminusdb/terminusdb/issues/133#issuecomment-760754531,Looks like we're going to have to implement TUS as chunking did not work with cloudflare.,look like -PRON- be go to have to implement tus as chunking do not work with cloudflare,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,134,2020-10-17T21:58:15Z,bollwyvl,Allow serving HTTP(S) under a URL prefix other than /,https://github.com/terminusdb/terminusdb/issues/134,"Desired Feature
TERMINUSDB_URL_PREFIX (and CLI switches, I guess) existed, could be set to something other than /, and the console worked when hosted on a different URL base (e.g. inside another application).
Motivation
Over on terminusdb/terminusdb-tutorials#31 I've got a brutal, but working, build which starts up an interactive jupyter server with the ability to start the terminusdb server. It mostly works for kernel usage.
However, on Binder,  only one port is exposed per session, but we can use a proxy to get other ports exposed, the final URL ends up looking like:
https://hub.gke2.mybinder.org/user/bollwyvl-terminusdb-tutorials-8j8xs8nm/proxy/6363

This results in:



The console doesn't show any warnings... but is grabbing things from all over the internet... a separate issue, to be sure...
Alternatives
There aren't a lot: I've been through this with a bunch of different tools, and it's just generally hard to compose things if every app expects to be installed on /. Trying to rewrite URLs on the fly in the proxy is basically insane, the brave new SPA era.
Thanks!
Thanks for terminusdb! Looking forward to exploring it more!",2020-10-19T14:46:07Z,Cheukting,https://github.com/terminusdb/terminusdb/issues/134#issuecomment-712216581,Any comment on this @kevinchekovfeeney ?,any comment on this @kevinchekovfeeney,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,134,2020-10-17T21:58:15Z,bollwyvl,Allow serving HTTP(S) under a URL prefix other than /,https://github.com/terminusdb/terminusdb/issues/134,"Desired Feature
TERMINUSDB_URL_PREFIX (and CLI switches, I guess) existed, could be set to something other than /, and the console worked when hosted on a different URL base (e.g. inside another application).
Motivation
Over on terminusdb/terminusdb-tutorials#31 I've got a brutal, but working, build which starts up an interactive jupyter server with the ability to start the terminusdb server. It mostly works for kernel usage.
However, on Binder,  only one port is exposed per session, but we can use a proxy to get other ports exposed, the final URL ends up looking like:
https://hub.gke2.mybinder.org/user/bollwyvl-terminusdb-tutorials-8j8xs8nm/proxy/6363

This results in:



The console doesn't show any warnings... but is grabbing things from all over the internet... a separate issue, to be sure...
Alternatives
There aren't a lot: I've been through this with a bunch of different tools, and it's just generally hard to compose things if every app expects to be installed on /. Trying to rewrite URLs on the fly in the proxy is basically insane, the brave new SPA era.
Thanks!
Thanks for terminusdb! Looking forward to exploring it more!",2021-03-15T16:30:49Z,Cheukting,https://github.com/terminusdb/terminusdb/issues/134#issuecomment-799560340,see terminusdb/terminusdb-tutorials#31,see terminusdb / terminusdb - tutorials#31,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,134,2020-10-17T21:58:15Z,bollwyvl,Allow serving HTTP(S) under a URL prefix other than /,https://github.com/terminusdb/terminusdb/issues/134,"Desired Feature
TERMINUSDB_URL_PREFIX (and CLI switches, I guess) existed, could be set to something other than /, and the console worked when hosted on a different URL base (e.g. inside another application).
Motivation
Over on terminusdb/terminusdb-tutorials#31 I've got a brutal, but working, build which starts up an interactive jupyter server with the ability to start the terminusdb server. It mostly works for kernel usage.
However, on Binder,  only one port is exposed per session, but we can use a proxy to get other ports exposed, the final URL ends up looking like:
https://hub.gke2.mybinder.org/user/bollwyvl-terminusdb-tutorials-8j8xs8nm/proxy/6363

This results in:



The console doesn't show any warnings... but is grabbing things from all over the internet... a separate issue, to be sure...
Alternatives
There aren't a lot: I've been through this with a bunch of different tools, and it's just generally hard to compose things if every app expects to be installed on /. Trying to rewrite URLs on the fly in the proxy is basically insane, the brave new SPA era.
Thanks!
Thanks for terminusdb! Looking forward to exploring it more!",2021-03-16T13:03:04Z,rrooij,https://github.com/terminusdb/terminusdb/issues/134#issuecomment-800240139,"Are you able to use a reverse proxy for this and put a rewrite rule there? That would be the proper way to ""subfolder"" this.","be -PRON- able to use a reverse proxy for this and put a rewrite rule there that would be the proper way to "" subfolder "" this",0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,135,2020-10-17T22:05:55Z,bollwyvl,Self-contained HTML/JS/CSS assets,https://github.com/terminusdb/terminusdb/issues/135,"Desired Feature
All HTML/JS/CSS assets should be available for use offline, by default. If this is possible today, it's not super clear from the docs.
Motivation
On terminusdb/terminusdb-tutorials#31, I kinda have a build working. Using the dashboard is blocked by #134, but it's a bit worrying to see so many different hosts contacted during a page load of potentially sensitive information.


I'd prefer to only see requests terminate on hub.gke2.mybinder.org, which I nominally ""control"", though this is a public demo

I might just not understand the configuration system properly, so this could be a docs-only issue!
Alternatives
Yeah... them's kinda the breaks, it's gotta be in-distribution. It's hard to use tools that are grabbing things from CDN, etc. that might disappear tomorrow, for sensitive work (e.g health, finance, company proprietary).
Links

would like to see solved before attempting conda-forge/staged-recipes#12919 in anger",2021-03-15T16:37:16Z,Cheukting,https://github.com/terminusdb/terminusdb/issues/135#issuecomment-799564897,There will be a new release and it will be redesign so if you have further suggestions feel free to let us know in the future.,there will be a new release and -PRON- will be redesign so if -PRON- have further suggestion feel free to let -PRON- know in the future,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,136,2020-11-07T18:30:53Z,bascott,Can't add prefix,https://github.com/terminusdb/terminusdb/issues/136,"Describe the bug
Trying to add prefix foaf for http://xmlns.com/foaf/0.1/ to a database fails in the console, both through the UI and through a WOQL query (see screenshot below for error).
To Reproduce
Steps to reproduce the behavior:

Open the console.
Select a database.
Navigate to Schema tab.
Click New Prefix.
Enter foaf in the first box and http://xmlns.com/foaf/0.1/ in the second.
Click Create.

Expected behavior
The prefix foaf for IRI http://xmlns.com/foaf/0.1/ is listed in available prefixes.
Screenshots

Info

OS: Windows 10
terminus-server run using terminusdb/terminusdb-bootstrap@6d5617e",2020-11-10T13:05:06Z,rrooij,https://github.com/terminusdb/terminusdb/issues/136#issuecomment-724689024,"It works for me on the dev branch, but I can reproduce this on the latest master. Thanks a lot for reporting this!",-PRON- work for -PRON- on the dev branch but i can reproduce this on the late master thank a lot for report this,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,136,2020-11-07T18:30:53Z,bascott,Can't add prefix,https://github.com/terminusdb/terminusdb/issues/136,"Describe the bug
Trying to add prefix foaf for http://xmlns.com/foaf/0.1/ to a database fails in the console, both through the UI and through a WOQL query (see screenshot below for error).
To Reproduce
Steps to reproduce the behavior:

Open the console.
Select a database.
Navigate to Schema tab.
Click New Prefix.
Enter foaf in the first box and http://xmlns.com/foaf/0.1/ in the second.
Click Create.

Expected behavior
The prefix foaf for IRI http://xmlns.com/foaf/0.1/ is listed in available prefixes.
Screenshots

Info

OS: Windows 10
terminus-server run using terminusdb/terminusdb-bootstrap@6d5617e",2020-12-09T01:17:33Z,bascott,https://github.com/terminusdb/terminusdb/issues/136#issuecomment-741382031,Error no longer occurs as of terminusdb/terminusdb-bootstrap@3511051 and I am able to add documents using the prefixed term.,error no long occur as of terminusdb / terminusdb - bootstrap@3511051 and i be able to add document use the prefix term,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,137,2020-11-11T11:00:17Z,k-tipp,Wrong prefix for property names,https://github.com/terminusdb/terminusdb/issues/137,"Describe the bug
Instead of the ""scm"" prefix, ""xsd"" is selected, if the property id is equal to a datatype.
To Reproduce
client.create_database(dbid=""testdb"", accountid=""admin"", label=""test database"", description="""", include_schema=True)

client.db(""testdb"")

WOQLQuery().doctype(""RosterRecord"", label=""Roster Record"", description=""Roster Record"", graph=""schema"")
    .property(""name"", ""string"")
    .property(""date"", ""date"", label=""Registration Date"")
    .property(""paid"", ""string"", label=""Paid"")
    .property(""boolean"", ""boolean"")
    .property(""string"", ""string"").execute(client)

Expected behavior
scm:string, scm:boolean, scm:date instead of xsd:xxx
Screenshots
If applicable, add screenshots to help explain your problem.

Info (please complete the following information):
Using latest terminusdb-bootstrap and python client",2020-11-18T16:21:34Z,kevinchekovfeeney,https://github.com/terminusdb/terminusdb/issues/137#issuecomment-729790189,"That's a definite bug - should always go to scm when there is no prefix supplied for the property id - as a workaround, you can explicitly specify ""scm:string"" / or whatever. It's also something we should pick up in schema validation - a datatype and a property with the same ID doesn't seem to me like something that you ever actually should have.","that be a definite bug - should always go to scm when there be no prefix supply for the property -PRON- d - as a workaround -PRON- can explicitly specify "" scmstring "" / or whatever -PRON- be also something -PRON- should pick up in schema validation - a datatype and a property with the same -PRON- d do not seem to -PRON- like something that -PRON- ever actually should have",0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,137,2020-11-11T11:00:17Z,k-tipp,Wrong prefix for property names,https://github.com/terminusdb/terminusdb/issues/137,"Describe the bug
Instead of the ""scm"" prefix, ""xsd"" is selected, if the property id is equal to a datatype.
To Reproduce
client.create_database(dbid=""testdb"", accountid=""admin"", label=""test database"", description="""", include_schema=True)

client.db(""testdb"")

WOQLQuery().doctype(""RosterRecord"", label=""Roster Record"", description=""Roster Record"", graph=""schema"")
    .property(""name"", ""string"")
    .property(""date"", ""date"", label=""Registration Date"")
    .property(""paid"", ""string"", label=""Paid"")
    .property(""boolean"", ""boolean"")
    .property(""string"", ""string"").execute(client)

Expected behavior
scm:string, scm:boolean, scm:date instead of xsd:xxx
Screenshots
If applicable, add screenshots to help explain your problem.

Info (please complete the following information):
Using latest terminusdb-bootstrap and python client",2020-11-19T13:07:08Z,k-tipp,https://github.com/terminusdb/terminusdb/issues/137#issuecomment-730362971,"Thx for your feedback, I agree, registration_date would have been a better name, but if you just play around and dont think too much about your naming, funny things can happen ;-)",thx for -PRON- feedback i agree registration_date would have be a well name but if -PRON- just play around and do not think too much about -PRON- name funny thing can happen - ),0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,137,2020-11-11T11:00:17Z,k-tipp,Wrong prefix for property names,https://github.com/terminusdb/terminusdb/issues/137,"Describe the bug
Instead of the ""scm"" prefix, ""xsd"" is selected, if the property id is equal to a datatype.
To Reproduce
client.create_database(dbid=""testdb"", accountid=""admin"", label=""test database"", description="""", include_schema=True)

client.db(""testdb"")

WOQLQuery().doctype(""RosterRecord"", label=""Roster Record"", description=""Roster Record"", graph=""schema"")
    .property(""name"", ""string"")
    .property(""date"", ""date"", label=""Registration Date"")
    .property(""paid"", ""string"", label=""Paid"")
    .property(""boolean"", ""boolean"")
    .property(""string"", ""string"").execute(client)

Expected behavior
scm:string, scm:boolean, scm:date instead of xsd:xxx
Screenshots
If applicable, add screenshots to help explain your problem.

Info (please complete the following information):
Using latest terminusdb-bootstrap and python client",2020-11-27T10:44:33Z,luke-feeney,https://github.com/terminusdb/terminusdb/issues/137#issuecomment-734771167,"Thanks for this @k-tipp - we are going to discuss at the next issues and docs triage session in the Discord server
What: Issues and Docs Triage
Where: Discord Server (https://discord.gg/CzE5yZdjpr)
When: Every Thursday from this week until the end of time @ 2pm GMT; 3pm CET; 9am ET; 6am PT; 7.30pm IST
Will update here after",thank for this @k - tipp - -PRON- be go to discuss at the next issue and doc triage session in the discord server what issue and doc triage where discord server ( https//discordgg / cze5yzdjpr ) when every thursday from this week until the end of time @ 2 p.m. gmt 3 p.m. cet 9 a.m. et 6 a.m. pt 730pm ist will update here after,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,137,2020-11-11T11:00:17Z,k-tipp,Wrong prefix for property names,https://github.com/terminusdb/terminusdb/issues/137,"Describe the bug
Instead of the ""scm"" prefix, ""xsd"" is selected, if the property id is equal to a datatype.
To Reproduce
client.create_database(dbid=""testdb"", accountid=""admin"", label=""test database"", description="""", include_schema=True)

client.db(""testdb"")

WOQLQuery().doctype(""RosterRecord"", label=""Roster Record"", description=""Roster Record"", graph=""schema"")
    .property(""name"", ""string"")
    .property(""date"", ""date"", label=""Registration Date"")
    .property(""paid"", ""string"", label=""Paid"")
    .property(""boolean"", ""boolean"")
    .property(""string"", ""string"").execute(client)

Expected behavior
scm:string, scm:boolean, scm:date instead of xsd:xxx
Screenshots
If applicable, add screenshots to help explain your problem.

Info (please complete the following information):
Using latest terminusdb-bootstrap and python client",2021-01-14T14:15:04Z,kevinchekovfeeney,https://github.com/terminusdb/terminusdb/issues/137#issuecomment-760222847,Fixed in latest release,fix in late release,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,138,2020-11-18T10:29:08Z,EmmanuelOga,Provide a way to Reload/Refresh the current page of the desktop app,https://github.com/terminusdb/terminusdb/issues/138,"While using the JS API, after performing certain CRUD operation, sometimes I want to verify the result of the operation on the desktop app.
The desktop app doesn't provide a way to soft-refresh, so currently I either click a diff tab and come back, reload through the tray icon menu or CTRL-R. These last two ways takes 1 or 2 seconds to reload or sometimes more.
Would be nice to have a reload button on the UI able to reload just the current page: for instance, on the list of documents, just refresh the list of documents.
I wonder if the UI could receive notifications of the server when something updates and refresh on its own... although this sounds like a more complicated ask 😄 .",2020-11-18T10:30:31Z,EmmanuelOga,https://github.com/terminusdb/terminusdb/issues/138#issuecomment-729587410,cc: @dmytri @Francesca-Bit,cc @dmytri @francesca - bit,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,138,2020-11-18T10:29:08Z,EmmanuelOga,Provide a way to Reload/Refresh the current page of the desktop app,https://github.com/terminusdb/terminusdb/issues/138,"While using the JS API, after performing certain CRUD operation, sometimes I want to verify the result of the operation on the desktop app.
The desktop app doesn't provide a way to soft-refresh, so currently I either click a diff tab and come back, reload through the tray icon menu or CTRL-R. These last two ways takes 1 or 2 seconds to reload or sometimes more.
Would be nice to have a reload button on the UI able to reload just the current page: for instance, on the list of documents, just refresh the list of documents.
I wonder if the UI could receive notifications of the server when something updates and refresh on its own... although this sounds like a more complicated ask 😄 .",2020-11-27T10:40:38Z,luke-feeney,https://github.com/terminusdb/terminusdb/issues/138#issuecomment-734769421,"Thanks for this @EmmanuelOga - we are going to discuss at the next issues and docs triage session in the Discord server
What: Issues and Docs Triage
Where: Discord Server (https://discord.gg/CzE5yZdjpr)
When: Every Thursday from this week until the end of time @ 2pm GMT; 3pm CET; 9am ET; 6am PT; 7.30pm IST
Will report back after (and please come along - thou I know timings is v awkward)",thank for this @emmanueloga - -PRON- be go to discuss at the next issue and doc triage session in the discord server what issue and doc triage where discord server ( https//discordgg / cze5yzdjpr ) when every thursday from this week until the end of time @ 2 p.m. gmt 3 p.m. cet 9 a.m. et 6 a.m. pt 730pm ist will report back after ( and please come along - thou i know timing be v awkward ),0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,138,2020-11-18T10:29:08Z,EmmanuelOga,Provide a way to Reload/Refresh the current page of the desktop app,https://github.com/terminusdb/terminusdb/issues/138,"While using the JS API, after performing certain CRUD operation, sometimes I want to verify the result of the operation on the desktop app.
The desktop app doesn't provide a way to soft-refresh, so currently I either click a diff tab and come back, reload through the tray icon menu or CTRL-R. These last two ways takes 1 or 2 seconds to reload or sometimes more.
Would be nice to have a reload button on the UI able to reload just the current page: for instance, on the list of documents, just refresh the list of documents.
I wonder if the UI could receive notifications of the server when something updates and refresh on its own... although this sounds like a more complicated ask 😄 .",2021-01-07T15:04:03Z,luke-feeney,https://github.com/terminusdb/terminusdb/issues/138#issuecomment-756170770,"Looking at adding a reload button to the document page so you can reload without having to refresh everything. Ctl-R works, but agree that it isn't the best when in the document viewer. Thanks.",look at add a reload button to the document page so -PRON- can reload without have to refresh everything ctl - r work but agree that -PRON- be not the good when in the document viewer thank,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,138,2020-11-18T10:29:08Z,EmmanuelOga,Provide a way to Reload/Refresh the current page of the desktop app,https://github.com/terminusdb/terminusdb/issues/138,"While using the JS API, after performing certain CRUD operation, sometimes I want to verify the result of the operation on the desktop app.
The desktop app doesn't provide a way to soft-refresh, so currently I either click a diff tab and come back, reload through the tray icon menu or CTRL-R. These last two ways takes 1 or 2 seconds to reload or sometimes more.
Would be nice to have a reload button on the UI able to reload just the current page: for instance, on the list of documents, just refresh the list of documents.
I wonder if the UI could receive notifications of the server when something updates and refresh on its own... although this sounds like a more complicated ask 😄 .",2021-02-19T17:05:00Z,Francesca-Bit,https://github.com/terminusdb/terminusdb/issues/138#issuecomment-782207288,we added a refresh button under the tables,-PRON- add a refresh button under the table,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,139,2020-11-18T10:36:07Z,EmmanuelOga,Display URL of the server and location of configuration files on Desktop App,https://github.com/terminusdb/terminusdb/issues/139,"As a first time user of the desktop app, which I downloaded from https://terminusdb.com/hub/download, I couldn't tell which port the server was running at and how to change it if I wanted to.
Eventually I realized most tutorials were talking to localhost:6363, so I picked that and it worked!
Would be nice to display on the desktop app this information. Also, showing the location of the configuration file would be helpful.
cc: @dmytri @Francesca-Bit",2020-11-27T10:43:07Z,luke-feeney,https://github.com/terminusdb/terminusdb/issues/139#issuecomment-734770496,"Thanks for this @EmmanuelOga - we are going to discuss this issue at the next issues and docs triage session in the Discord server. Plan is to discuss each issue that comes up weekly (once we have got 4.0 out the door)
What: Issues and Docs Triage
Where: Discord Server (https://discord.gg/CzE5yZdjpr)
When: Every Thursday from this week until the end of time @ 2pm GMT; 3pm CET; 9am ET; 6am PT; 7.30pm IST
Open to all to come and talk about issues.",thank for this @emmanueloga - -PRON- be go to discuss this issue at the next issue and doc triage session in the discord server plan be to discuss each issue that come up weekly ( once -PRON- have get 40 out the door ) what issue and doc triage where discord server ( https//discordgg / cze5yzdjpr ) when every thursday from this week until the end of time @ 2 p.m. gmt 3 p.m. cet 9 a.m. et 6 a.m. pt 730pm ist open to all to come and talk about issue,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,139,2020-11-18T10:36:07Z,EmmanuelOga,Display URL of the server and location of configuration files on Desktop App,https://github.com/terminusdb/terminusdb/issues/139,"As a first time user of the desktop app, which I downloaded from https://terminusdb.com/hub/download, I couldn't tell which port the server was running at and how to change it if I wanted to.
Eventually I realized most tutorials were talking to localhost:6363, so I picked that and it worked!
Would be nice to display on the desktop app this information. Also, showing the location of the configuration file would be helpful.
cc: @dmytri @Francesca-Bit",2020-12-17T14:17:49Z,kevinchekovfeeney,https://github.com/terminusdb/terminusdb/issues/139#issuecomment-747465246,This is in console from 4.0.1 - accessible via a (i) button in the top navigation bar,this be in console from 401 - accessible via a ( i ) button in the top navigation bar,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,140,2020-11-30T09:52:49Z,EmmanuelOga,Apparent encoding issue attempting to insert a fishy Unicode character (🐠),https://github.com/terminusdb/terminusdb/issues/140,"Hello,
I'm executing this query which contains this character: 🐠.
If I remove the character, the query executes just fine.
Note that this is using node v14.10.0 with terminusdb client 4.0.0. The JavaScript source file is properly encoded in utf-8 as far as I can tell.
TDB version is 3.0.1 on windows 10 64.
The server response is:
Couldn't create doc:public/: Error: API Error <!DOCTYPE html>
<html>
<head>
<title>500 Internal server error</title>

<meta http-equiv=""content-type"" content=""text/html; charset=UTF-8"">

</head>
<body>

<h1>Internal server error</h1>

<p>
string_codes/2: Cannot represent due to `character_code'</p>

<address>
<a href=""http://www.swi-prolog.org"">SWI-Prolog</a> httpd at DESKTOP-SVB6D03
</address>

</body>
</html>
Code: 500 url: https://127.0.0.1:6363/api/woql/admin/emmanuelogacom/local/branch/main
Done in 1.52s.",2020-11-30T12:30:52Z,GavinMendelGleason,https://github.com/terminusdb/terminusdb/issues/140#issuecomment-735758147,"This is actually a known bug in SWIPL's handling of multibyte codes on windows. It works correctly on linux.
We have a strategy for fixing this but it's relatively complicated as it involves fixing the SWIPL handling of strings on windows.",this be actually a know bug in swipl 's handling of multibyte code on window -PRON- work correctly on linux -PRON- have a strategy for fix this but -PRON- be relatively complicated as -PRON- involve fix the swipl handling of string on window,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,140,2020-11-30T09:52:49Z,EmmanuelOga,Apparent encoding issue attempting to insert a fishy Unicode character (🐠),https://github.com/terminusdb/terminusdb/issues/140,"Hello,
I'm executing this query which contains this character: 🐠.
If I remove the character, the query executes just fine.
Note that this is using node v14.10.0 with terminusdb client 4.0.0. The JavaScript source file is properly encoded in utf-8 as far as I can tell.
TDB version is 3.0.1 on windows 10 64.
The server response is:
Couldn't create doc:public/: Error: API Error <!DOCTYPE html>
<html>
<head>
<title>500 Internal server error</title>

<meta http-equiv=""content-type"" content=""text/html; charset=UTF-8"">

</head>
<body>

<h1>Internal server error</h1>

<p>
string_codes/2: Cannot represent due to `character_code'</p>

<address>
<a href=""http://www.swi-prolog.org"">SWI-Prolog</a> httpd at DESKTOP-SVB6D03
</address>

</body>
</html>
Code: 500 url: https://127.0.0.1:6363/api/woql/admin/emmanuelogacom/local/branch/main
Done in 1.52s.",2020-11-30T12:41:38Z,EmmanuelOga,https://github.com/terminusdb/terminusdb/issues/140#issuecomment-735762840,Got it! It seems like I should finally give https://github.com/terminusdb/terminusdb-bootstrap a try!,get -PRON- -PRON- seem like i should finally give https//githubcom / terminusdb / terminusdb - bootstrap a try,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,140,2020-11-30T09:52:49Z,EmmanuelOga,Apparent encoding issue attempting to insert a fishy Unicode character (🐠),https://github.com/terminusdb/terminusdb/issues/140,"Hello,
I'm executing this query which contains this character: 🐠.
If I remove the character, the query executes just fine.
Note that this is using node v14.10.0 with terminusdb client 4.0.0. The JavaScript source file is properly encoded in utf-8 as far as I can tell.
TDB version is 3.0.1 on windows 10 64.
The server response is:
Couldn't create doc:public/: Error: API Error <!DOCTYPE html>
<html>
<head>
<title>500 Internal server error</title>

<meta http-equiv=""content-type"" content=""text/html; charset=UTF-8"">

</head>
<body>

<h1>Internal server error</h1>

<p>
string_codes/2: Cannot represent due to `character_code'</p>

<address>
<a href=""http://www.swi-prolog.org"">SWI-Prolog</a> httpd at DESKTOP-SVB6D03
</address>

</body>
</html>
Code: 500 url: https://127.0.0.1:6363/api/woql/admin/emmanuelogacom/local/branch/main
Done in 1.52s.",2020-11-30T13:00:31Z,luke-feeney,https://github.com/terminusdb/terminusdb/issues/140#issuecomment-735771351,Closing as seems to be clear! Will reopen with any updates on SWIPL fix.,closing as seem to be clear will reopen with any update on swipl fix,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,141,2020-12-09T02:35:08Z,kokizzu,What happened when datasets more than RAM?,https://github.com/terminusdb/terminusdb/issues/141,"In RediGraph it's cannot add more record (unless using enterprise version of redis that could store in SSD), TigerGraph and DGraph and Neo4j is fine, because it's not in-memory database, how about terminusDB?",2020-12-09T10:41:03Z,GavinMendelGleason,https://github.com/terminusdb/terminusdb/issues/141#issuecomment-741687677,You can't have more records than available memory in TerminusDB. However we do store information in a succinct data structure which improves the total amount of information you can fit in memory substantially.,-PRON- can not have more record than available memory in terminusdb however -PRON- do store information in a succinct data structure which improve the total amount of information -PRON- can fit in memory substantially,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,143,2020-12-20T00:23:32Z,panasenco,Add commit message to Schema Builder,https://github.com/terminusdb/terminusdb/issues/143,"Is your feature request related to a problem? Please describe.
I love Schema Builder, but really dislike that I can't set the commit message when saving my schema.
Describe the solution you'd like
Add a commit message text field when saving a schema with Schema Builder - exactly the same as the Query tab has (""Enter reason for update here"").
Describe alternatives you've considered
Using WOQL.js directly is a possibility, but Schema Builder is so much easier to work with...",2020-12-21T12:57:24Z,luke-feeney,https://github.com/terminusdb/terminusdb/issues/143#issuecomment-748960224,Thanks for this @panasenco - really helpful feedback and great that you like the new model builder. We'll review over the break and discuss in our first issue triage of 2021. That will be Thursday 7 Jan I think. I'll report back!,thank for this @panasenco - really helpful feedback and great that -PRON- like the new model builder -PRON- will review over the break and discuss in -PRON- first issue triage of 2021 that will be thursday 7 jan i think -PRON- will report back,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,143,2020-12-20T00:23:32Z,panasenco,Add commit message to Schema Builder,https://github.com/terminusdb/terminusdb/issues/143,"Is your feature request related to a problem? Please describe.
I love Schema Builder, but really dislike that I can't set the commit message when saving my schema.
Describe the solution you'd like
Add a commit message text field when saving a schema with Schema Builder - exactly the same as the Query tab has (""Enter reason for update here"").
Describe alternatives you've considered
Using WOQL.js directly is a possibility, but Schema Builder is so much easier to work with...",2021-01-07T15:06:50Z,luke-feeney,https://github.com/terminusdb/terminusdb/issues/143#issuecomment-756172648,@Francesca-Bit is working on a solution - should have something to report in the not too distant future.,@francesca - bit be work on a solution - should have something to report in the not too distant future,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,143,2020-12-20T00:23:32Z,panasenco,Add commit message to Schema Builder,https://github.com/terminusdb/terminusdb/issues/143,"Is your feature request related to a problem? Please describe.
I love Schema Builder, but really dislike that I can't set the commit message when saving my schema.
Describe the solution you'd like
Add a commit message text field when saving a schema with Schema Builder - exactly the same as the Query tab has (""Enter reason for update here"").
Describe alternatives you've considered
Using WOQL.js directly is a possibility, but Schema Builder is so much easier to work with...",2021-01-08T10:12:12Z,Francesca-Bit,https://github.com/terminusdb/terminusdb/issues/143#issuecomment-756671122,Fixed in dev/canary branch,fix in dev / canary branch,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,144,2020-12-21T12:24:57Z,MichaelSavin,Loading CSV files with semicolon separators is not supported,https://github.com/terminusdb/terminusdb/issues/144,"Describe the bug
Loading CSV files with semicolon separators is not supported
To Reproduce

Create a new database
Load this CSV o3.txt

Error
terminus_store_rust_error('CSV error: record 1 (line: 1, byte: 26): found record with 4 fields, but the previous record has 1 fields')",2020-12-21T14:19:10Z,luke-feeney,https://github.com/terminusdb/terminusdb/issues/144#issuecomment-748996791,Thanks @MichaelSavin - we are reviewing this and will include in our next issue triage (unless we can fix in advance). That will be Thursday 7 January I think. Will report back.,thanks @michaelsavin - -PRON- be review this and will include in -PRON- next issue triage ( unless -PRON- can fix in advance ) that will be thursday 7 january i think will report back,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,144,2020-12-21T12:24:57Z,MichaelSavin,Loading CSV files with semicolon separators is not supported,https://github.com/terminusdb/terminusdb/issues/144,"Describe the bug
Loading CSV files with semicolon separators is not supported
To Reproduce

Create a new database
Load this CSV o3.txt

Error
terminus_store_rust_error('CSV error: record 1 (line: 1, byte: 26): found record with 4 fields, but the previous record has 1 fields')",2021-01-07T15:02:42Z,luke-feeney,https://github.com/terminusdb/terminusdb/issues/144#issuecomment-756169936,"This is in the backlog to solve - we currently just support commas, but there is underlying support for other separators we just have to expose. Will keep the issue open so we can double back when resolved",this be in the backlog to solve - -PRON- currently just support comma but there be underlie support for other separator -PRON- just have to expose will keep the issue open so -PRON- can double back when resolve,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,144,2020-12-21T12:24:57Z,MichaelSavin,Loading CSV files with semicolon separators is not supported,https://github.com/terminusdb/terminusdb/issues/144,"Describe the bug
Loading CSV files with semicolon separators is not supported
To Reproduce

Create a new database
Load this CSV o3.txt

Error
terminus_store_rust_error('CSV error: record 1 (line: 1, byte: 26): found record with 4 fields, but the previous record has 1 fields')",2021-03-15T16:39:57Z,Cheukting,https://github.com/terminusdb/terminusdb/issues/144#issuecomment-799566773,@GavinMendelGleason will give you a more relevant explanation,@gavinmendelgleason will give -PRON- a more relevant explanation,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,144,2020-12-21T12:24:57Z,MichaelSavin,Loading CSV files with semicolon separators is not supported,https://github.com/terminusdb/terminusdb/issues/144,"Describe the bug
Loading CSV files with semicolon separators is not supported
To Reproduce

Create a new database
Load this CSV o3.txt

Error
terminus_store_rust_error('CSV error: record 1 (line: 1, byte: 26): found record with 4 fields, but the previous record has 1 fields')",2021-03-15T16:41:04Z,GavinMendelGleason,https://github.com/terminusdb/terminusdb/issues/144#issuecomment-799567595,We will not be extending CSV to other separators on near term road map.,-PRON- will not be extend csv to other separator on near term road map,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,145,2020-12-21T12:52:47Z,MichaelSavin,CSV import - UTF-8 cyrillyc symbols is not supported,https://github.com/terminusdb/terminusdb/issues/145,"Describe the bug
Loading CSV UTF-8 files with cyryllic is not supported
To Reproduce

Create a new database
Load this CSV
o01.txt

Error
io_error(write,(0x55b461b58f00)) context(system:write/2,'Encoding cannot represent character')",2020-12-21T14:20:56Z,luke-feeney,https://github.com/terminusdb/terminusdb/issues/145#issuecomment-748997663,This is a interesting one! Will have to check it out. Will also include for discussion in our next issue triage. Please come along to discuss. Will be Thursday 7 Jan at 2pm GMT in the TerminusDB Discord server.,this be a interesting one will have to check -PRON- out will also include for discussion in -PRON- next issue triage please come along to discuss will be thursday 7 jan at 2 p.m. gmt in the terminusdb discord server,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,145,2020-12-21T12:52:47Z,MichaelSavin,CSV import - UTF-8 cyrillyc symbols is not supported,https://github.com/terminusdb/terminusdb/issues/145,"Describe the bug
Loading CSV UTF-8 files with cyryllic is not supported
To Reproduce

Create a new database
Load this CSV
o01.txt

Error
io_error(write,(0x55b461b58f00)) context(system:write/2,'Encoding cannot represent character')",2021-01-05T09:13:08Z,rrooij,https://github.com/terminusdb/terminusdb/issues/145#issuecomment-754509811,On which platform / OS are you running TerminusDB?,on which platform / os be -PRON- run terminusdb,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,145,2020-12-21T12:52:47Z,MichaelSavin,CSV import - UTF-8 cyrillyc symbols is not supported,https://github.com/terminusdb/terminusdb/issues/145,"Describe the bug
Loading CSV UTF-8 files with cyryllic is not supported
To Reproduce

Create a new database
Load this CSV
o01.txt

Error
io_error(write,(0x55b461b58f00)) context(system:write/2,'Encoding cannot represent character')",2021-01-05T09:45:04Z,MichaelSavin,https://github.com/terminusdb/terminusdb/issues/145#issuecomment-754526711,Windows 10,windows 10,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,145,2020-12-21T12:52:47Z,MichaelSavin,CSV import - UTF-8 cyrillyc symbols is not supported,https://github.com/terminusdb/terminusdb/issues/145,"Describe the bug
Loading CSV UTF-8 files with cyryllic is not supported
To Reproduce

Create a new database
Load this CSV
o01.txt

Error
io_error(write,(0x55b461b58f00)) context(system:write/2,'Encoding cannot represent character')",2021-01-28T14:40:11Z,GavinMendelGleason,https://github.com/terminusdb/terminusdb/issues/145#issuecomment-769114921,This is a bug in swipl on windows 10 and its inability to properly process utf8. The work around is to use the docker at the minute. We're currently working out a permanent solution to fix the bug in swipl but it will be a few months yet before we can solve it.,this be a bug in swipl on windows 10 and -PRON- inability to properly process utf8 the work around be to use the docker at the minute -PRON- be currently work out a permanent solution to fix the bug in swipl but -PRON- will be a few month yet before -PRON- can solve -PRON-,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,146,2021-01-04T04:24:59Z,adhollander,Provide labels for entities linked through object properties in document views,https://github.com/terminusdb/terminusdb/issues/146,"I love the direction TerminusDB is going, especially in its 4.0 release. I've been looking for years now for an interface allowing non-programmers to be able to create and maintain RDF linked datasets, and TerminusDB is definitely on the right track. But here are a couple of related enhancements that I think are key if people are to work with it successfully as a CMS for RDF datasets.
When one uses the desktop client to edit a document in a database, usually there will be other entities linked to the document via object properties. (E.g. Organization produces Dataset). As far as I can tell, in the UI under the documents tab, if one wants to link another document to a given document via an object property, one has to enter in the full id for the related document (e.g. doc:Dataset_iqr9w1607571537493). This is a burdensome step for users to follow.
What the interface should do at this point is to prompt the user with the rdfs:label strings for the entities that are possibilities as the objects of the object property. (e.g. a listing of all the Datasets already in the system). This might be done via a dropdown menu or a typeahead form. The user would select one or multiple entries, and when the data is saved it would store the correct id for the referent.
Similarly, in the page view for a document, the record should display the rdfs:label for the referent for an object property in addition to or instead of the id string.
Thanks, Allan",2021-01-04T09:37:24Z,kevinchekovfeeney,https://github.com/terminusdb/terminusdb/issues/146#issuecomment-753868160,Thanks Allan - definitely agree with you - in fact these are coming in version 4.2 - nearly ready!,thanks allan - definitely agree with -PRON- - in fact these be come in version 42 - nearly ready,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,146,2021-01-04T04:24:59Z,adhollander,Provide labels for entities linked through object properties in document views,https://github.com/terminusdb/terminusdb/issues/146,"I love the direction TerminusDB is going, especially in its 4.0 release. I've been looking for years now for an interface allowing non-programmers to be able to create and maintain RDF linked datasets, and TerminusDB is definitely on the right track. But here are a couple of related enhancements that I think are key if people are to work with it successfully as a CMS for RDF datasets.
When one uses the desktop client to edit a document in a database, usually there will be other entities linked to the document via object properties. (E.g. Organization produces Dataset). As far as I can tell, in the UI under the documents tab, if one wants to link another document to a given document via an object property, one has to enter in the full id for the related document (e.g. doc:Dataset_iqr9w1607571537493). This is a burdensome step for users to follow.
What the interface should do at this point is to prompt the user with the rdfs:label strings for the entities that are possibilities as the objects of the object property. (e.g. a listing of all the Datasets already in the system). This might be done via a dropdown menu or a typeahead form. The user would select one or multiple entries, and when the data is saved it would store the correct id for the referent.
Similarly, in the page view for a document, the record should display the rdfs:label for the referent for an object property in addition to or instead of the id string.
Thanks, Allan",2021-03-15T16:41:05Z,Cheukting,https://github.com/terminusdb/terminusdb/issues/146#issuecomment-799567608,Hey it's already out @adhollander I hope you have already check it out? Let us know if you have further suggestions.,hey -PRON- be already out @adhollander i hope -PRON- have already check -PRON- out let -PRON- know if -PRON- have further suggestion,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,147,2021-01-08T09:26:20Z,rrooij,woql tests currently need a store for tests to succeed,https://github.com/terminusdb/terminusdb/issues/147,"Describe the bug
When running the TerminusDB test suite without an initialized store, almost all of the WOQL tests that use system_descriptors fail. This is because they depend on an existing store being initialized.
To Reproduce
Steps to reproduce the behavior:

Remove your storage folder
Run the tests ./terminusdb test --test woql

Expected behavior
Tests should be able to run without an initialized store.",2021-01-13T15:25:26Z,rrooij,https://github.com/terminusdb/terminusdb/issues/147#issuecomment-759520924,"Current tests that need refactoring in woql_compile

subsumption
path
select
isa_node",current test that need refactore in woql_compile subsumption path select isa_node,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,147,2021-01-08T09:26:20Z,rrooij,woql tests currently need a store for tests to succeed,https://github.com/terminusdb/terminusdb/issues/147,"Describe the bug
When running the TerminusDB test suite without an initialized store, almost all of the WOQL tests that use system_descriptors fail. This is because they depend on an existing store being initialized.
To Reproduce
Steps to reproduce the behavior:

Remove your storage folder
Run the tests ./terminusdb test --test woql

Expected behavior
Tests should be able to run without an initialized store.",2021-01-14T10:10:17Z,rrooij,https://github.com/terminusdb/terminusdb/issues/147#issuecomment-760096016,Fixed in 9770259,fix in 9770259,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,148,2021-01-12T14:06:07Z,KittyJose,bug in datatype xdd:url,https://github.com/terminusdb/terminusdb/issues/148,"Describe the bug
Unable to add a datatype property of type xdd:url
To Reproduce
create a schema with a property of datatype xdd:url
typescast value to xddurl and attempt to add triple with casted variable
Expected behavior
Screenshots
If applicable, add screenshots to help explain your problem.
Info (please complete the following information):
 ""api:witnesses"": [
        {
          ""@type"": ""vio:UntypedInstance"",
          ""vio:message"": {
            ""@type"": ""xsd:string"",
            ""@value"": ""The subject 'https://github.com/KittyJose' has no defined class.""
          },
          ""vio:property"": {
            ""@type"": ""xsd:anyURI"",
            ""@value"": ""terminusdb:///schema#gitHub_user_html_url""
          },
          ""vio:subject"": {
            ""@type"": ""xsd:anyURI"",
            ""@value"": ""https://github.com/KittyJose""
          }
        }

Additional context
Add any other context about the problem here.",2021-01-15T08:28:03Z,GavinMendelGleason,https://github.com/terminusdb/terminusdb/issues/148#issuecomment-760749805,Fixed in a2f3ce8,fix in a2f3ce8,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,150,2021-01-21T16:46:09Z,kghenderson,cloning ssl error,https://github.com/terminusdb/terminusdb/issues/150,"Describe the bug
Attempting to clone the Bikes Tutorial DB from TerminusHub using the Terminus desktop app on Fedora 33 Linux.
Repository fails to clone with error:
Error: ssl_error('1416F086','SSL routines',tls_process_server_certificate,'certificate verify failed') [46] throw(error(ssl_error('1416F086','SSL routines',tls_process_server_certificate,'certificate verify failed'),_12004)) [44] catch(http:http_protocol_hook(https,...,<stream>(0x7f8110010150,0x7f81100106a0),_12064,...),error(ssl_error('1416F086','SSL routines',tls_process_server_certificate,'certificate verify failed'),_12072),http_open:(...,...)) at /tmp/.mount_.org.cdCakCk/usr/lib/swi-prolog/boot/init.pl:531 [43] http_open:try_http_proxy(direct,[uri('https://hub.terminusdb.com/api/pack/kevin/bikes'),...|...],_12134,'<garbage_collected>') at /tmp/.mount_.org.cdCakCk/usr/lib/swi-prolog/library/http/http_open.pl:448 [41] http_client:http_get('https://hub.terminusdb.com/api/pack/kevin/bikes',_12190,[post(...),...|...]) at /tmp/.mount_.org.cdCakCk/usr/lib/swi-prolog/library/http/http_client.pl:139 [39] db_fetch:authorized_fetch('Bearer eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCIsImtpZCI6InhrMEZTOTdVWVFvbHB4ODREeFlpeCJ9.eyJodHRwOi8vdGVybWludXNkYi5jb20vc2NoZW1hL3N5c3RlbSNhZ2VudF9uYW1lIjoia2doZW5kZXJzb24iLCJodHRwOi8vdGVybWludXNkYi5jb20vc2NoZW1hL3N5c3RlbSN1c2VyX2lkZW50aWZpZXIiOiJrZXZpbkBrZ2hlbmRlcnNvbi5jb20iLCJpc3MiOiJodHRwczovL3Rlcm1pbnVzZGIuZXUuYXV0aDAuY29tLyIsInN1YiI6ImF1dGgwfDVmZWRmZTIwMzkxOWJjMDA3NmE2Y2U2OSIsImF1ZCI6WyJodHRwczovL3Rlcm1pbnVzZGIuY29tL2h1YnNlcnZpY2VzIiwiaHR0cHM6Ly90ZXJtaW51c2RiLmV1LmF1dGgwLmNvbS91c2VyaW5mbyJdLCJpYXQiOjE2MTEyNDcxMDksImV4cCI6MTYxMTI4MzEwOSwiYXpwIjoiNEVZSUhRYVZMZ3dpNVY1bTJnUk5ZZHRwWlpmYnd0RHoiLCJzY29wZSI6Im9wZW5pZCBwcm9maWxlIGVtYWlsIn0.hD6DoOAMj790oN3yzcXWatGR7mbn3oto_vd7kmWy_mobmXwNTQVmOUwNr2Ha-Yd1r6ysqfwipl7aNR-FG3GbmwNyVOleZL8THoiF8jp1p6jKsanzhGUw4Q_EDPNWv7brcdkoWtcKZqEg1ZshE7AZhaOZUoLYMX64Cq_uKe6Y7tcKmGf34CoD-pBu6I56s9OGMdk8gS0CWm4FwJYVfYppUjvdbnrW0mY_tW3644OWwOI_N_V_X3-_wqrxj7TfHKhlvzXdcKUId3Pa9RldKF7Mn-pqJAJVkNDybvEl423kxJ73lT01i46ovoBq60-picfQLJCC0_RfqNctONg8siFpkw','<garbage_collected>',none,_12250) at /tmp/.mount_.org.cdCakCk/usr/share/terminusdb/core/api/db_fetch.pl:75 [38] '<meta-call>'('<garbage_collected>') <foreign> [37] database:with_transaction_(query_context{all_witnesses:false,authorization:'terminusdb:///system/data/admin',bindings:[],default_collection:database_descriptor{database_name:""bikes"",organization_name:""admin""},files:[],filter:type_filter{types: ...},prefixes:_12392{api:'http://terminusdb.com/schema/api#',doc:'terminusdb:///repository/data/',layer:'http://terminusdb.com/schema/layer#',owl:'http://www.w3.org/2002/07/owl#',rdf:'http://www.w3.org/1999/02/22-rdf-syntax-ns#',rdfs:'http://www.w3.org/2000/01/rdf-schema#',ref:'http://terminusdb.com/schema/ref#',repo:'http://terminusdb.com/schema/repository#',system:'http://terminusdb.com/schema/system#',terminus:'http://terminusdb.com/schema/system#',vio:'http://terminusdb.com/schema/vio#',woql:'http://terminusdb.com/schema/woql#',xdd:'http://terminusdb.com/schema/xdd#',xsd:'http://www.w3.org/2001/XMLSchema#'},selected:[],system:system_descriptor{},transaction_objects:[...],update_guard:_12366,write_graph:repo_graph{database_name:""bikes"",name:""main"",organization_name:""admin"",type:instance}},db_fetch:(...,...),_12324) at /tmp/.mount_.org.cdCakCk/usr/share/terminusdb/core/transaction/database.pl:213 [36] setup_call_catcher_cleanup(database:true,database:with_transaction_(...,...,_12554),_12532,database:(...,...)) at /tmp/.mount_.org.cdCakCk/usr/lib/swi-prolog/boot/init.pl:612 [32] db_clone:clone_('<garbage_collected>','terminusdb:///system/data/admin',admin,bikes,'<garbage_collected>','<garbage_collected>',false,'<garbage_collected>','<garbage_collected>',_12620) at /tmp/.mount_.org.cdCakCk/usr/share/terminusdb/core/api/db_clone.pl:55 [31] setup_call_catcher_cleanup(db_clone:true,db_clone:clone_(...,'terminusdb:///system/data/admin',admin,bikes,""TerminusDB Bikes Tutorial"",""This is the knowledge graph produced by the TerminusDB Bikes Tutorial which is commonly used as an introduction to TerminusDB. This database contains the final results in a format that you can clone complete with all of the data and structure intact so that you don't have to do it yourself. "",false,""https://hub.terminusdb.com/kevin/bikes"",...,_12690),exception(_12694),db_clone:(...;true)) at /tmp/.mount_.org.cdCakCk/usr/lib/swi-prolog/boot/init.pl:612 [30] db_clone:clone('<garbage_collected>','terminusdb:///system/data/admin',admin,bikes,'<garbage_collected>','<garbage_collected>',false,'<garbage_collected>','<garbage_collected>','<garbage_collected>') at /tmp/.mount_.org.cdCakCk/usr/share/terminusdb/core/api/db_clone.pl:17 [29] '<meta-call>'('<garbage_collected>') <foreign> [28] catch(routes:(...,...),error(ssl_error('1416F086','SSL routines',tls_process_server_certificate,'certificate verify failed'),context(_12860,_12862)),routes:do_or_die(...,...)) at /tmp/.mount_.org.cdCakCk/usr/lib/swi-prolog/boot/init.pl:530 [27] catch_with_backtrace('<garbage_collected>','<garbage_collected>','<garbage_collected>') at /tmp/.mount_.org.cdCakCk/usr/lib/swi-prolog/boot/init.pl:580 Note: some frames are missing due to last-call optimization. Re-run your program in debug mode (:- debug.) to get more detail.less
{
  ""data"": {
    ""api:message"": ""Error: ssl_error('1416F086','SSL routines',tls_process_server_certificate,'certificate verify failed')\n  [46] throw(error(ssl_error('1416F086','SSL routines',tls_process_server_certificate,'certificate verify failed'),_12004))\n  [44] catch(http:http_protocol_hook(https,...,<stream>(0x7f8110010150,0x7f81100106a0),_12064,...),error(ssl_error('1416F086','SSL routines',tls_process_server_certificate,'certificate verify failed'),_12072),http_open:(...,...)) at /tmp/.mount_.org.cdCakCk/usr/lib/swi-prolog/boot/init.pl:531\n  [43] http_open:try_http_proxy(direct,[uri('https://hub.terminusdb.com/api/pack/kevin/bikes'),...|...],_12134,'<garbage_collected>') at /tmp/.mount_.org.cdCakCk/usr/lib/swi-prolog/library/http/http_open.pl:448\n  [41] http_client:http_get('https://hub.terminusdb.com/api/pack/kevin/bikes',_12190,[post(...),...|...]) at /tmp/.mount_.org.cdCakCk/usr/lib/swi-prolog/library/http/http_client.pl:139\n  [39] db_fetch:authorized_fetch('Bearer eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCIsImtpZCI6InhrMEZTOTdVWVFvbHB4ODREeFlpeCJ9.eyJodHRwOi8vdGVybWludXNkYi5jb20vc2NoZW1hL3N5c3RlbSNhZ2VudF9uYW1lIjoia2doZW5kZXJzb24iLCJodHRwOi8vdGVybWludXNkYi5jb20vc2NoZW1hL3N5c3RlbSN1c2VyX2lkZW50aWZpZXIiOiJrZXZpbkBrZ2hlbmRlcnNvbi5jb20iLCJpc3MiOiJodHRwczovL3Rlcm1pbnVzZGIuZXUuYXV0aDAuY29tLyIsInN1YiI6ImF1dGgwfDVmZWRmZTIwMzkxOWJjMDA3NmE2Y2U2OSIsImF1ZCI6WyJodHRwczovL3Rlcm1pbnVzZGIuY29tL2h1YnNlcnZpY2VzIiwiaHR0cHM6Ly90ZXJtaW51c2RiLmV1LmF1dGgwLmNvbS91c2VyaW5mbyJdLCJpYXQiOjE2MTEyNDcxMDksImV4cCI6MTYxMTI4MzEwOSwiYXpwIjoiNEVZSUhRYVZMZ3dpNVY1bTJnUk5ZZHRwWlpmYnd0RHoiLCJzY29wZSI6Im9wZW5pZCBwcm9maWxlIGVtYWlsIn0.hD6DoOAMj790oN3yzcXWatGR7mbn3oto_vd7kmWy_mobmXwNTQVmOUwNr2Ha-Yd1r6ysqfwipl7aNR-FG3GbmwNyVOleZL8THoiF8jp1p6jKsanzhGUw4Q_EDPNWv7brcdkoWtcKZqEg1ZshE7AZhaOZUoLYMX64Cq_uKe6Y7tcKmGf34CoD-pBu6I56s9OGMdk8gS0CWm4FwJYVfYppUjvdbnrW0mY_tW3644OWwOI_N_V_X3-_wqrxj7TfHKhlvzXdcKUId3Pa9RldKF7Mn-pqJAJVkNDybvEl423kxJ73lT01i46ovoBq60-picfQLJCC0_RfqNctONg8siFpkw','<garbage_collected>',none,_12250) at /tmp/.mount_.org.cdCakCk/usr/share/terminusdb/core/api/db_fetch.pl:75\n  [38] '<meta-call>'('<garbage_collected>') <foreign>\n  [37] database:with_transaction_(query_context{all_witnesses:false,authorization:'terminusdb:///system/data/admin',bindings:[],default_collection:database_descriptor{database_name:\""bikes\"",organization_name:\""admin\""},files:[],filter:type_filter{types: ...},prefixes:_12392{api:'http://terminusdb.com/schema/api#',doc:'terminusdb:///repository/data/',layer:'http://terminusdb.com/schema/layer#',owl:'http://www.w3.org/2002/07/owl#',rdf:'http://www.w3.org/1999/02/22-rdf-syntax-ns#',rdfs:'http://www.w3.org/2000/01/rdf-schema#',ref:'http://terminusdb.com/schema/ref#',repo:'http://terminusdb.com/schema/repository#',system:'http://terminusdb.com/schema/system#',terminus:'http://terminusdb.com/schema/system#',vio:'http://terminusdb.com/schema/vio#',woql:'http://terminusdb.com/schema/woql#',xdd:'http://terminusdb.com/schema/xdd#',xsd:'http://www.w3.org/2001/XMLSchema#'},selected:[],system:system_descriptor{},transaction_objects:[...],update_guard:_12366,write_graph:repo_graph{database_name:\""bikes\"",name:\""main\"",organization_name:\""admin\"",type:instance}},db_fetch:(...,...),_12324) at /tmp/.mount_.org.cdCakCk/usr/share/terminusdb/core/transaction/database.pl:213\n  [36] setup_call_catcher_cleanup(database:true,database:with_transaction_(...,...,_12554),_12532,database:(...,...)) at /tmp/.mount_.org.cdCakCk/usr/lib/swi-prolog/boot/init.pl:612\n  [32] db_clone:clone_('<garbage_collected>','terminusdb:///system/data/admin',admin,bikes,'<garbage_collected>','<garbage_collected>',false,'<garbage_collected>','<garbage_collected>',_12620) at /tmp/.mount_.org.cdCakCk/usr/share/terminusdb/core/api/db_clone.pl:55\n  [31] setup_call_catcher_cleanup(db_clone:true,db_clone:clone_(...,'terminusdb:///system/data/admin',admin,bikes,\""TerminusDB Bikes Tutorial\"",\""This is the knowledge graph produced by the TerminusDB Bikes Tutorial which is commonly used as an introduction to TerminusDB. This database contains the final results in a format that you can clone complete with all of the data and structure intact so that you don't have to do it yourself. \"",false,\""https://hub.terminusdb.com/kevin/bikes\"",...,_12690),exception(_12694),db_clone:(...;true)) at /tmp/.mount_.org.cdCakCk/usr/lib/swi-prolog/boot/init.pl:612\n  [30] db_clone:clone('<garbage_collected>','terminusdb:///system/data/admin',admin,bikes,'<garbage_collected>','<garbage_collected>',false,'<garbage_collected>','<garbage_collected>','<garbage_collected>') at /tmp/.mount_.org.cdCakCk/usr/share/terminusdb/core/api/db_clone.pl:17\n  [29] '<meta-call>'('<garbage_collected>') <foreign>\n  [28] catch(routes:(...,...),error(ssl_error('1416F086','SSL routines',tls_process_server_certificate,'certificate verify failed'),context(_12860,_12862)),routes:do_or_die(...,...)) at /tmp/.mount_.org.cdCakCk/usr/lib/swi-prolog/boot/init.pl:530\n  [27] catch_with_backtrace('<garbage_collected>','<garbage_collected>','<garbage_collected>') at /tmp/.mount_.org.cdCakCk/usr/lib/swi-prolog/boot/init.pl:580\n\nNote: some frames are missing due to last-call optimization.\nRe-run your program in debug mode (:- debug.) to get more detail.\n\n"",
    ""api:status"": ""api:server_error"",
    ""end"": ""2021-01-21T16:40:32.348Z"",
    ""start"": ""2021-01-21T16:40:32.348Z"",
    ""duration"": ""2021-01-21T16:40:32.348Z""
  }
}",2021-01-21T20:44:04Z,rrooij,https://github.com/terminusdb/terminusdb/issues/150#issuecomment-764925140,Thanks for your bug report! Which version of terminusdb-desktop are you running? The latest 4.1.0 release?,thank for -PRON- bug report which version of terminusdb - desktop be -PRON- run the late 410 release,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,150,2021-01-21T16:46:09Z,kghenderson,cloning ssl error,https://github.com/terminusdb/terminusdb/issues/150,"Describe the bug
Attempting to clone the Bikes Tutorial DB from TerminusHub using the Terminus desktop app on Fedora 33 Linux.
Repository fails to clone with error:
Error: ssl_error('1416F086','SSL routines',tls_process_server_certificate,'certificate verify failed') [46] throw(error(ssl_error('1416F086','SSL routines',tls_process_server_certificate,'certificate verify failed'),_12004)) [44] catch(http:http_protocol_hook(https,...,<stream>(0x7f8110010150,0x7f81100106a0),_12064,...),error(ssl_error('1416F086','SSL routines',tls_process_server_certificate,'certificate verify failed'),_12072),http_open:(...,...)) at /tmp/.mount_.org.cdCakCk/usr/lib/swi-prolog/boot/init.pl:531 [43] http_open:try_http_proxy(direct,[uri('https://hub.terminusdb.com/api/pack/kevin/bikes'),...|...],_12134,'<garbage_collected>') at /tmp/.mount_.org.cdCakCk/usr/lib/swi-prolog/library/http/http_open.pl:448 [41] http_client:http_get('https://hub.terminusdb.com/api/pack/kevin/bikes',_12190,[post(...),...|...]) at /tmp/.mount_.org.cdCakCk/usr/lib/swi-prolog/library/http/http_client.pl:139 [39] db_fetch:authorized_fetch('Bearer eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCIsImtpZCI6InhrMEZTOTdVWVFvbHB4ODREeFlpeCJ9.eyJodHRwOi8vdGVybWludXNkYi5jb20vc2NoZW1hL3N5c3RlbSNhZ2VudF9uYW1lIjoia2doZW5kZXJzb24iLCJodHRwOi8vdGVybWludXNkYi5jb20vc2NoZW1hL3N5c3RlbSN1c2VyX2lkZW50aWZpZXIiOiJrZXZpbkBrZ2hlbmRlcnNvbi5jb20iLCJpc3MiOiJodHRwczovL3Rlcm1pbnVzZGIuZXUuYXV0aDAuY29tLyIsInN1YiI6ImF1dGgwfDVmZWRmZTIwMzkxOWJjMDA3NmE2Y2U2OSIsImF1ZCI6WyJodHRwczovL3Rlcm1pbnVzZGIuY29tL2h1YnNlcnZpY2VzIiwiaHR0cHM6Ly90ZXJtaW51c2RiLmV1LmF1dGgwLmNvbS91c2VyaW5mbyJdLCJpYXQiOjE2MTEyNDcxMDksImV4cCI6MTYxMTI4MzEwOSwiYXpwIjoiNEVZSUhRYVZMZ3dpNVY1bTJnUk5ZZHRwWlpmYnd0RHoiLCJzY29wZSI6Im9wZW5pZCBwcm9maWxlIGVtYWlsIn0.hD6DoOAMj790oN3yzcXWatGR7mbn3oto_vd7kmWy_mobmXwNTQVmOUwNr2Ha-Yd1r6ysqfwipl7aNR-FG3GbmwNyVOleZL8THoiF8jp1p6jKsanzhGUw4Q_EDPNWv7brcdkoWtcKZqEg1ZshE7AZhaOZUoLYMX64Cq_uKe6Y7tcKmGf34CoD-pBu6I56s9OGMdk8gS0CWm4FwJYVfYppUjvdbnrW0mY_tW3644OWwOI_N_V_X3-_wqrxj7TfHKhlvzXdcKUId3Pa9RldKF7Mn-pqJAJVkNDybvEl423kxJ73lT01i46ovoBq60-picfQLJCC0_RfqNctONg8siFpkw','<garbage_collected>',none,_12250) at /tmp/.mount_.org.cdCakCk/usr/share/terminusdb/core/api/db_fetch.pl:75 [38] '<meta-call>'('<garbage_collected>') <foreign> [37] database:with_transaction_(query_context{all_witnesses:false,authorization:'terminusdb:///system/data/admin',bindings:[],default_collection:database_descriptor{database_name:""bikes"",organization_name:""admin""},files:[],filter:type_filter{types: ...},prefixes:_12392{api:'http://terminusdb.com/schema/api#',doc:'terminusdb:///repository/data/',layer:'http://terminusdb.com/schema/layer#',owl:'http://www.w3.org/2002/07/owl#',rdf:'http://www.w3.org/1999/02/22-rdf-syntax-ns#',rdfs:'http://www.w3.org/2000/01/rdf-schema#',ref:'http://terminusdb.com/schema/ref#',repo:'http://terminusdb.com/schema/repository#',system:'http://terminusdb.com/schema/system#',terminus:'http://terminusdb.com/schema/system#',vio:'http://terminusdb.com/schema/vio#',woql:'http://terminusdb.com/schema/woql#',xdd:'http://terminusdb.com/schema/xdd#',xsd:'http://www.w3.org/2001/XMLSchema#'},selected:[],system:system_descriptor{},transaction_objects:[...],update_guard:_12366,write_graph:repo_graph{database_name:""bikes"",name:""main"",organization_name:""admin"",type:instance}},db_fetch:(...,...),_12324) at /tmp/.mount_.org.cdCakCk/usr/share/terminusdb/core/transaction/database.pl:213 [36] setup_call_catcher_cleanup(database:true,database:with_transaction_(...,...,_12554),_12532,database:(...,...)) at /tmp/.mount_.org.cdCakCk/usr/lib/swi-prolog/boot/init.pl:612 [32] db_clone:clone_('<garbage_collected>','terminusdb:///system/data/admin',admin,bikes,'<garbage_collected>','<garbage_collected>',false,'<garbage_collected>','<garbage_collected>',_12620) at /tmp/.mount_.org.cdCakCk/usr/share/terminusdb/core/api/db_clone.pl:55 [31] setup_call_catcher_cleanup(db_clone:true,db_clone:clone_(...,'terminusdb:///system/data/admin',admin,bikes,""TerminusDB Bikes Tutorial"",""This is the knowledge graph produced by the TerminusDB Bikes Tutorial which is commonly used as an introduction to TerminusDB. This database contains the final results in a format that you can clone complete with all of the data and structure intact so that you don't have to do it yourself. "",false,""https://hub.terminusdb.com/kevin/bikes"",...,_12690),exception(_12694),db_clone:(...;true)) at /tmp/.mount_.org.cdCakCk/usr/lib/swi-prolog/boot/init.pl:612 [30] db_clone:clone('<garbage_collected>','terminusdb:///system/data/admin',admin,bikes,'<garbage_collected>','<garbage_collected>',false,'<garbage_collected>','<garbage_collected>','<garbage_collected>') at /tmp/.mount_.org.cdCakCk/usr/share/terminusdb/core/api/db_clone.pl:17 [29] '<meta-call>'('<garbage_collected>') <foreign> [28] catch(routes:(...,...),error(ssl_error('1416F086','SSL routines',tls_process_server_certificate,'certificate verify failed'),context(_12860,_12862)),routes:do_or_die(...,...)) at /tmp/.mount_.org.cdCakCk/usr/lib/swi-prolog/boot/init.pl:530 [27] catch_with_backtrace('<garbage_collected>','<garbage_collected>','<garbage_collected>') at /tmp/.mount_.org.cdCakCk/usr/lib/swi-prolog/boot/init.pl:580 Note: some frames are missing due to last-call optimization. Re-run your program in debug mode (:- debug.) to get more detail.less
{
  ""data"": {
    ""api:message"": ""Error: ssl_error('1416F086','SSL routines',tls_process_server_certificate,'certificate verify failed')\n  [46] throw(error(ssl_error('1416F086','SSL routines',tls_process_server_certificate,'certificate verify failed'),_12004))\n  [44] catch(http:http_protocol_hook(https,...,<stream>(0x7f8110010150,0x7f81100106a0),_12064,...),error(ssl_error('1416F086','SSL routines',tls_process_server_certificate,'certificate verify failed'),_12072),http_open:(...,...)) at /tmp/.mount_.org.cdCakCk/usr/lib/swi-prolog/boot/init.pl:531\n  [43] http_open:try_http_proxy(direct,[uri('https://hub.terminusdb.com/api/pack/kevin/bikes'),...|...],_12134,'<garbage_collected>') at /tmp/.mount_.org.cdCakCk/usr/lib/swi-prolog/library/http/http_open.pl:448\n  [41] http_client:http_get('https://hub.terminusdb.com/api/pack/kevin/bikes',_12190,[post(...),...|...]) at /tmp/.mount_.org.cdCakCk/usr/lib/swi-prolog/library/http/http_client.pl:139\n  [39] db_fetch:authorized_fetch('Bearer eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCIsImtpZCI6InhrMEZTOTdVWVFvbHB4ODREeFlpeCJ9.eyJodHRwOi8vdGVybWludXNkYi5jb20vc2NoZW1hL3N5c3RlbSNhZ2VudF9uYW1lIjoia2doZW5kZXJzb24iLCJodHRwOi8vdGVybWludXNkYi5jb20vc2NoZW1hL3N5c3RlbSN1c2VyX2lkZW50aWZpZXIiOiJrZXZpbkBrZ2hlbmRlcnNvbi5jb20iLCJpc3MiOiJodHRwczovL3Rlcm1pbnVzZGIuZXUuYXV0aDAuY29tLyIsInN1YiI6ImF1dGgwfDVmZWRmZTIwMzkxOWJjMDA3NmE2Y2U2OSIsImF1ZCI6WyJodHRwczovL3Rlcm1pbnVzZGIuY29tL2h1YnNlcnZpY2VzIiwiaHR0cHM6Ly90ZXJtaW51c2RiLmV1LmF1dGgwLmNvbS91c2VyaW5mbyJdLCJpYXQiOjE2MTEyNDcxMDksImV4cCI6MTYxMTI4MzEwOSwiYXpwIjoiNEVZSUhRYVZMZ3dpNVY1bTJnUk5ZZHRwWlpmYnd0RHoiLCJzY29wZSI6Im9wZW5pZCBwcm9maWxlIGVtYWlsIn0.hD6DoOAMj790oN3yzcXWatGR7mbn3oto_vd7kmWy_mobmXwNTQVmOUwNr2Ha-Yd1r6ysqfwipl7aNR-FG3GbmwNyVOleZL8THoiF8jp1p6jKsanzhGUw4Q_EDPNWv7brcdkoWtcKZqEg1ZshE7AZhaOZUoLYMX64Cq_uKe6Y7tcKmGf34CoD-pBu6I56s9OGMdk8gS0CWm4FwJYVfYppUjvdbnrW0mY_tW3644OWwOI_N_V_X3-_wqrxj7TfHKhlvzXdcKUId3Pa9RldKF7Mn-pqJAJVkNDybvEl423kxJ73lT01i46ovoBq60-picfQLJCC0_RfqNctONg8siFpkw','<garbage_collected>',none,_12250) at /tmp/.mount_.org.cdCakCk/usr/share/terminusdb/core/api/db_fetch.pl:75\n  [38] '<meta-call>'('<garbage_collected>') <foreign>\n  [37] database:with_transaction_(query_context{all_witnesses:false,authorization:'terminusdb:///system/data/admin',bindings:[],default_collection:database_descriptor{database_name:\""bikes\"",organization_name:\""admin\""},files:[],filter:type_filter{types: ...},prefixes:_12392{api:'http://terminusdb.com/schema/api#',doc:'terminusdb:///repository/data/',layer:'http://terminusdb.com/schema/layer#',owl:'http://www.w3.org/2002/07/owl#',rdf:'http://www.w3.org/1999/02/22-rdf-syntax-ns#',rdfs:'http://www.w3.org/2000/01/rdf-schema#',ref:'http://terminusdb.com/schema/ref#',repo:'http://terminusdb.com/schema/repository#',system:'http://terminusdb.com/schema/system#',terminus:'http://terminusdb.com/schema/system#',vio:'http://terminusdb.com/schema/vio#',woql:'http://terminusdb.com/schema/woql#',xdd:'http://terminusdb.com/schema/xdd#',xsd:'http://www.w3.org/2001/XMLSchema#'},selected:[],system:system_descriptor{},transaction_objects:[...],update_guard:_12366,write_graph:repo_graph{database_name:\""bikes\"",name:\""main\"",organization_name:\""admin\"",type:instance}},db_fetch:(...,...),_12324) at /tmp/.mount_.org.cdCakCk/usr/share/terminusdb/core/transaction/database.pl:213\n  [36] setup_call_catcher_cleanup(database:true,database:with_transaction_(...,...,_12554),_12532,database:(...,...)) at /tmp/.mount_.org.cdCakCk/usr/lib/swi-prolog/boot/init.pl:612\n  [32] db_clone:clone_('<garbage_collected>','terminusdb:///system/data/admin',admin,bikes,'<garbage_collected>','<garbage_collected>',false,'<garbage_collected>','<garbage_collected>',_12620) at /tmp/.mount_.org.cdCakCk/usr/share/terminusdb/core/api/db_clone.pl:55\n  [31] setup_call_catcher_cleanup(db_clone:true,db_clone:clone_(...,'terminusdb:///system/data/admin',admin,bikes,\""TerminusDB Bikes Tutorial\"",\""This is the knowledge graph produced by the TerminusDB Bikes Tutorial which is commonly used as an introduction to TerminusDB. This database contains the final results in a format that you can clone complete with all of the data and structure intact so that you don't have to do it yourself. \"",false,\""https://hub.terminusdb.com/kevin/bikes\"",...,_12690),exception(_12694),db_clone:(...;true)) at /tmp/.mount_.org.cdCakCk/usr/lib/swi-prolog/boot/init.pl:612\n  [30] db_clone:clone('<garbage_collected>','terminusdb:///system/data/admin',admin,bikes,'<garbage_collected>','<garbage_collected>',false,'<garbage_collected>','<garbage_collected>','<garbage_collected>') at /tmp/.mount_.org.cdCakCk/usr/share/terminusdb/core/api/db_clone.pl:17\n  [29] '<meta-call>'('<garbage_collected>') <foreign>\n  [28] catch(routes:(...,...),error(ssl_error('1416F086','SSL routines',tls_process_server_certificate,'certificate verify failed'),context(_12860,_12862)),routes:do_or_die(...,...)) at /tmp/.mount_.org.cdCakCk/usr/lib/swi-prolog/boot/init.pl:530\n  [27] catch_with_backtrace('<garbage_collected>','<garbage_collected>','<garbage_collected>') at /tmp/.mount_.org.cdCakCk/usr/lib/swi-prolog/boot/init.pl:580\n\nNote: some frames are missing due to last-call optimization.\nRe-run your program in debug mode (:- debug.) to get more detail.\n\n"",
    ""api:status"": ""api:server_error"",
    ""end"": ""2021-01-21T16:40:32.348Z"",
    ""start"": ""2021-01-21T16:40:32.348Z"",
    ""duration"": ""2021-01-21T16:40:32.348Z""
  }
}",2021-01-21T21:36:24Z,kghenderson,https://github.com/terminusdb/terminusdb/issues/150#issuecomment-764956551,"I downloaded it today, so yes, console version 4.1.1 & server version 4.1.0
I repeated the test with the bank balance example db but same error.
When I restart the application I see both of those db's on the dashboard but they are empty,
When I attempt to do the pull from the 'synchronize' tab, I also get the same error (below):
Not sure if I can run in debug mode seeing as it's an AppImage
Failed to pull updates from origin main to local main (0.065 seconds)
Error: instantiation_error [34] query_context{}:<_21174 [33] ask:create_context(_21214,_21216) at /tmp/.mount_.org.cojBlTI/usr/share/terminusdb/core/query/ask.pl:96 [32] ask:ask(_21252,t(_21260,ref:branch_name,""main""^^ ...),[compress_prefixes(true)]) at /tmp/.mount_.org.cojBlTI/usr/share/terminusdb/core/query/ask.pl:223 [30] descriptor:open_descriptor(branch_descriptor{branch_name:""main"",repository_descriptor:repository_descriptor{database_descriptor: ...,repository_name:""origin""}},commit_info{},_21326,[],'<garbage_collected>') at /tmp/.mount_.org.cojBlTI/usr/share/terminusdb/core/transaction/descriptor.pl:508 [27] utils:do_or_die('<garbage_collected>',error(not_a_valid_remote_branch(...),_21402)) at /tmp/.mount_.org.cojBlTI/usr/share/terminusdb/core/util/utils.pl:98 [26] db_pull:pull(transaction_object{descriptor:system_descriptor{},inference_objects:[...],instance_objects:[...],schema_objects:[...]},'terminusdb:///system/data/admin','admin/bikes/local/branch/main','<garbage_collected>','<garbage_collected>',routes:authorized_fetch('Bearer eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCIsImtpZCI6InhrMEZTOTdVWVFvbHB4ODREeFlpeCJ9.eyJodHRwOi8vdGVybWludXNkYi5jb20vc2NoZW1hL3N5c3RlbSNhZ2VudF9uYW1lIjoia2doZW5kZXJzb24iLCJodHRwOi8vdGVybWludXNkYi5jb20vc2NoZW1hL3N5c3RlbSN1c2VyX2lkZW50aWZpZXIiOiJrZXZpbkBrZ2hlbmRlcnNvbi5jb20iLCJpc3MiOiJodHRwczovL3Rlcm1pbnVzZGIuZXUuYXV0aDAuY29tLyIsInN1YiI6ImF1dGgwfDVmZWRmZTIwMzkxOWJjMDA3NmE2Y2U2OSIsImF1ZCI6WyJodHRwczovL3Rlcm1pbnVzZGIuY29tL2h1YnNlcnZpY2VzIiwiaHR0cHM6Ly90ZXJtaW51c2RiLmV1LmF1dGgwLmNvbS91c2VyaW5mbyJdLCJpYXQiOjE2MTEyNjQ4MDYsImV4cCI6MTYxMTMwMDgwNiwiYXpwIjoiNEVZSUhRYVZMZ3dpNVY1bTJnUk5ZZHRwWlpmYnd0RHoiLCJzY29wZSI6Im9wZW5pZCBwcm9maWxlIGVtYWlsIn0.xRz6xbmrh_Wku-gSsn22tRJKvtHxKaUAXaVYzNd5xLrAhoI5oCUg8JV8aspJlGP0NQ9LRvPN78ESh9Bce556o9CNiyiFPdj88YFRwhIzgjh6JFUIL8GL1InrqxAxCDiP9UmeieebIV48otAmkk9W0gGM5ko0vNFQyjitGBl9MFcm7i4z_ASqhAQQpQsAqlX3KIJltlzVGP9UTHMG8XkgPBIvdi62j87gXSXpMVWh_B5y_j0S0LTEQgMB2zv5rgMLLBatpzXTrHd65hUpRAOWV-FHoKYSb7QJ-69kiRh6NBXR0Rpn0bv9DsKKIU15wnUQX2p8l0SocSPcrk_EhfsIuA'),_21454) at /tmp/.mount_.org.cojBlTI/usr/share/terminusdb/core/api/db_pull.pl:35 [25] '<meta-call>'('<garbage_collected>') <foreign> [24] catch(routes:(...,...),error(instantiation_error,context(...,_21598)),routes:do_or_die(...,...)) at /tmp/.mount_.org.cojBlTI/usr/lib/swi-prolog/boot/init.pl:530 [23] catch_with_backtrace('<garbage_collected>','<garbage_collected>','<garbage_collected>') at /tmp/.mount_.org.cojBlTI/usr/lib/swi-prolog/boot/init.pl:580 Note: some frames are missing due to last-call optimization. Re-run your program in debug mode (:- debug.) to get more detail.","i download -PRON- today so yes console version 411 & server version 410 i repeat the test with the bank balance example db but same error when i restart the application i see both of those db be on the dashboard but -PRON- be empty when i attempt to do the pull from the ' synchronize ' tab i also get the same error ( below ) not sure if i can run in debug mode see as -PRON- be an appimage fail to pull update from origin main to local main ( 0065 second ) error instantiation_error [ 34 ] query_context{}<_21174 [ 33 ] askcreate_context(_21214_21216 ) at /tmp / mount_orgcojblti / usr / share / terminusdb / core / query / askpl96 [ 32 ] askask(_21252t(_21260refbranch_name""main""^^ ) [ compress_prefixes(true ) ] ) at /tmp / mount_orgcojblti / usr / share / terminusdb / core / query / askpl223 [ 30 ] descriptoropen_descriptor(branch_descriptor{branch_name""main""repository_descriptorrepository_descriptor{database_descriptor repository_name""origin""}}commit_info{}_21326[]'<garbage_collecte > ' ) at /tmp / mount_orgcojblti / usr / share / terminusdb / core / transaction / descriptorpl508 [ 27 ] utilsdo_or_die('<garbage_collected>'error(not_a_valid_remote_branch()_21402 ) ) at /tmp / mount_orgcojblti / usr / share / terminusdb / core / util / utilspl98 [ 26 ] db_pullpull(transaction_object{descriptorsystem_descriptor{}inference_objects[]instance_objects[]schema_objects[]}'terminusdb///system / datum / admin''admin / bike / local / branch / main''<garbage_collected>''<garbage_collected>'routesauthorized_fetch('bearer eyjhbgcioijsuzi1niisinr5cci6ikpxvcisimtpzci6inhrmeztotdvwvfvbhb4odreeflpecj9eyjodhrwoi8vdgvybwludxnkyi5jb20vc2nozw1hl3n5c3rlbsnhz2vudf9uyw1lijoia2dozw5kzxjzb24ilcjodhrwoi8vdgvybwludxnkyi5jb20vc2nozw1hl3n5c3rlbsn1c2vyx2lkzw50awzpzxiioijrzxzpbkbrz2hlbmrlcnnvbi5jb20ilcjpc3mioijodhrwczovl3rlcm1pbnvzzgiuzxuuyxv0adauy29tlyisinn1yii6imf1dggwfdvmzwrmztiwmzkxowjjmda3nme2y2u2osisimf1zci6wyjodhrwczovl3rlcm1pbnvzzgiuy29tl2h1ynnlcnzpy2vziiwiahr0chm6ly90zxjtaw51c2rilmv1lmf1dggwlmnvbs91c2vyaw5mbyjdlcjpyxqioje2mteynjq4mdysimv4cci6mtyxmtmwmdgwniwiyxpwijoinevzsuhryvzmz3dpnvy1btjnuk5zzhrwwlpmynd0rhoilcjzy29wzsi6im9wzw5pzcbwcm9mawxligvtywlsin0xrz6xbmrh_wku - gssn22trjkvthxkauaxavyznd5xlrahoi5ocug8jv8aspjlgp0nq9lrvpn78esh9bce556o9cniyifpdj88yfrwhizgjh6jfuil8gl1inrqxaxcdip9umeieebiv48otamkk9w0ggm5ko0vnfqyjitgbl9mfcm7i4z_asqhaqqpqsaqlx3kijltlzvgp9uthmg8xkgpbivdi62j87gxsxpmvwh_b5y_j0s0lteqgmb2zv5rgmllbatpzxtrhd65hupraowv - fhokysb7qj-69kirh6nbxr0rpn0bv9dskkiu15wnuqx2p8l0socspcrk_ehfsiua')_21454 ) at /tmp / mount_orgcojblti / usr / share / terminusdb / core / api / db_pullpl35 [ 25 ] ' < meta - call>'('<garbage_collecte > ' ) < foreign > [ 24 ] catch(routes()error(instantiation_errorcontext(_21598))routesdo_or_die ( ) ) at /tmp / mount_orgcojblti / usr / lib / swi - prolog / boot / initpl530 [ 23 ] catch_with_backtrace('<garbage_collected>''<garbage_collected>''<garbage_collecte > ' ) at /tmp / mount_orgcojblti / usr / lib / swi - prolog / boot / initpl580 note some frame be miss due to last - call optimization re - run -PRON- program in debug mode ( - debug ) to get more detail",0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,150,2021-01-21T16:46:09Z,kghenderson,cloning ssl error,https://github.com/terminusdb/terminusdb/issues/150,"Describe the bug
Attempting to clone the Bikes Tutorial DB from TerminusHub using the Terminus desktop app on Fedora 33 Linux.
Repository fails to clone with error:
Error: ssl_error('1416F086','SSL routines',tls_process_server_certificate,'certificate verify failed') [46] throw(error(ssl_error('1416F086','SSL routines',tls_process_server_certificate,'certificate verify failed'),_12004)) [44] catch(http:http_protocol_hook(https,...,<stream>(0x7f8110010150,0x7f81100106a0),_12064,...),error(ssl_error('1416F086','SSL routines',tls_process_server_certificate,'certificate verify failed'),_12072),http_open:(...,...)) at /tmp/.mount_.org.cdCakCk/usr/lib/swi-prolog/boot/init.pl:531 [43] http_open:try_http_proxy(direct,[uri('https://hub.terminusdb.com/api/pack/kevin/bikes'),...|...],_12134,'<garbage_collected>') at /tmp/.mount_.org.cdCakCk/usr/lib/swi-prolog/library/http/http_open.pl:448 [41] http_client:http_get('https://hub.terminusdb.com/api/pack/kevin/bikes',_12190,[post(...),...|...]) at /tmp/.mount_.org.cdCakCk/usr/lib/swi-prolog/library/http/http_client.pl:139 [39] db_fetch:authorized_fetch('Bearer eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCIsImtpZCI6InhrMEZTOTdVWVFvbHB4ODREeFlpeCJ9.eyJodHRwOi8vdGVybWludXNkYi5jb20vc2NoZW1hL3N5c3RlbSNhZ2VudF9uYW1lIjoia2doZW5kZXJzb24iLCJodHRwOi8vdGVybWludXNkYi5jb20vc2NoZW1hL3N5c3RlbSN1c2VyX2lkZW50aWZpZXIiOiJrZXZpbkBrZ2hlbmRlcnNvbi5jb20iLCJpc3MiOiJodHRwczovL3Rlcm1pbnVzZGIuZXUuYXV0aDAuY29tLyIsInN1YiI6ImF1dGgwfDVmZWRmZTIwMzkxOWJjMDA3NmE2Y2U2OSIsImF1ZCI6WyJodHRwczovL3Rlcm1pbnVzZGIuY29tL2h1YnNlcnZpY2VzIiwiaHR0cHM6Ly90ZXJtaW51c2RiLmV1LmF1dGgwLmNvbS91c2VyaW5mbyJdLCJpYXQiOjE2MTEyNDcxMDksImV4cCI6MTYxMTI4MzEwOSwiYXpwIjoiNEVZSUhRYVZMZ3dpNVY1bTJnUk5ZZHRwWlpmYnd0RHoiLCJzY29wZSI6Im9wZW5pZCBwcm9maWxlIGVtYWlsIn0.hD6DoOAMj790oN3yzcXWatGR7mbn3oto_vd7kmWy_mobmXwNTQVmOUwNr2Ha-Yd1r6ysqfwipl7aNR-FG3GbmwNyVOleZL8THoiF8jp1p6jKsanzhGUw4Q_EDPNWv7brcdkoWtcKZqEg1ZshE7AZhaOZUoLYMX64Cq_uKe6Y7tcKmGf34CoD-pBu6I56s9OGMdk8gS0CWm4FwJYVfYppUjvdbnrW0mY_tW3644OWwOI_N_V_X3-_wqrxj7TfHKhlvzXdcKUId3Pa9RldKF7Mn-pqJAJVkNDybvEl423kxJ73lT01i46ovoBq60-picfQLJCC0_RfqNctONg8siFpkw','<garbage_collected>',none,_12250) at /tmp/.mount_.org.cdCakCk/usr/share/terminusdb/core/api/db_fetch.pl:75 [38] '<meta-call>'('<garbage_collected>') <foreign> [37] database:with_transaction_(query_context{all_witnesses:false,authorization:'terminusdb:///system/data/admin',bindings:[],default_collection:database_descriptor{database_name:""bikes"",organization_name:""admin""},files:[],filter:type_filter{types: ...},prefixes:_12392{api:'http://terminusdb.com/schema/api#',doc:'terminusdb:///repository/data/',layer:'http://terminusdb.com/schema/layer#',owl:'http://www.w3.org/2002/07/owl#',rdf:'http://www.w3.org/1999/02/22-rdf-syntax-ns#',rdfs:'http://www.w3.org/2000/01/rdf-schema#',ref:'http://terminusdb.com/schema/ref#',repo:'http://terminusdb.com/schema/repository#',system:'http://terminusdb.com/schema/system#',terminus:'http://terminusdb.com/schema/system#',vio:'http://terminusdb.com/schema/vio#',woql:'http://terminusdb.com/schema/woql#',xdd:'http://terminusdb.com/schema/xdd#',xsd:'http://www.w3.org/2001/XMLSchema#'},selected:[],system:system_descriptor{},transaction_objects:[...],update_guard:_12366,write_graph:repo_graph{database_name:""bikes"",name:""main"",organization_name:""admin"",type:instance}},db_fetch:(...,...),_12324) at /tmp/.mount_.org.cdCakCk/usr/share/terminusdb/core/transaction/database.pl:213 [36] setup_call_catcher_cleanup(database:true,database:with_transaction_(...,...,_12554),_12532,database:(...,...)) at /tmp/.mount_.org.cdCakCk/usr/lib/swi-prolog/boot/init.pl:612 [32] db_clone:clone_('<garbage_collected>','terminusdb:///system/data/admin',admin,bikes,'<garbage_collected>','<garbage_collected>',false,'<garbage_collected>','<garbage_collected>',_12620) at /tmp/.mount_.org.cdCakCk/usr/share/terminusdb/core/api/db_clone.pl:55 [31] setup_call_catcher_cleanup(db_clone:true,db_clone:clone_(...,'terminusdb:///system/data/admin',admin,bikes,""TerminusDB Bikes Tutorial"",""This is the knowledge graph produced by the TerminusDB Bikes Tutorial which is commonly used as an introduction to TerminusDB. This database contains the final results in a format that you can clone complete with all of the data and structure intact so that you don't have to do it yourself. "",false,""https://hub.terminusdb.com/kevin/bikes"",...,_12690),exception(_12694),db_clone:(...;true)) at /tmp/.mount_.org.cdCakCk/usr/lib/swi-prolog/boot/init.pl:612 [30] db_clone:clone('<garbage_collected>','terminusdb:///system/data/admin',admin,bikes,'<garbage_collected>','<garbage_collected>',false,'<garbage_collected>','<garbage_collected>','<garbage_collected>') at /tmp/.mount_.org.cdCakCk/usr/share/terminusdb/core/api/db_clone.pl:17 [29] '<meta-call>'('<garbage_collected>') <foreign> [28] catch(routes:(...,...),error(ssl_error('1416F086','SSL routines',tls_process_server_certificate,'certificate verify failed'),context(_12860,_12862)),routes:do_or_die(...,...)) at /tmp/.mount_.org.cdCakCk/usr/lib/swi-prolog/boot/init.pl:530 [27] catch_with_backtrace('<garbage_collected>','<garbage_collected>','<garbage_collected>') at /tmp/.mount_.org.cdCakCk/usr/lib/swi-prolog/boot/init.pl:580 Note: some frames are missing due to last-call optimization. Re-run your program in debug mode (:- debug.) to get more detail.less
{
  ""data"": {
    ""api:message"": ""Error: ssl_error('1416F086','SSL routines',tls_process_server_certificate,'certificate verify failed')\n  [46] throw(error(ssl_error('1416F086','SSL routines',tls_process_server_certificate,'certificate verify failed'),_12004))\n  [44] catch(http:http_protocol_hook(https,...,<stream>(0x7f8110010150,0x7f81100106a0),_12064,...),error(ssl_error('1416F086','SSL routines',tls_process_server_certificate,'certificate verify failed'),_12072),http_open:(...,...)) at /tmp/.mount_.org.cdCakCk/usr/lib/swi-prolog/boot/init.pl:531\n  [43] http_open:try_http_proxy(direct,[uri('https://hub.terminusdb.com/api/pack/kevin/bikes'),...|...],_12134,'<garbage_collected>') at /tmp/.mount_.org.cdCakCk/usr/lib/swi-prolog/library/http/http_open.pl:448\n  [41] http_client:http_get('https://hub.terminusdb.com/api/pack/kevin/bikes',_12190,[post(...),...|...]) at /tmp/.mount_.org.cdCakCk/usr/lib/swi-prolog/library/http/http_client.pl:139\n  [39] db_fetch:authorized_fetch('Bearer eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCIsImtpZCI6InhrMEZTOTdVWVFvbHB4ODREeFlpeCJ9.eyJodHRwOi8vdGVybWludXNkYi5jb20vc2NoZW1hL3N5c3RlbSNhZ2VudF9uYW1lIjoia2doZW5kZXJzb24iLCJodHRwOi8vdGVybWludXNkYi5jb20vc2NoZW1hL3N5c3RlbSN1c2VyX2lkZW50aWZpZXIiOiJrZXZpbkBrZ2hlbmRlcnNvbi5jb20iLCJpc3MiOiJodHRwczovL3Rlcm1pbnVzZGIuZXUuYXV0aDAuY29tLyIsInN1YiI6ImF1dGgwfDVmZWRmZTIwMzkxOWJjMDA3NmE2Y2U2OSIsImF1ZCI6WyJodHRwczovL3Rlcm1pbnVzZGIuY29tL2h1YnNlcnZpY2VzIiwiaHR0cHM6Ly90ZXJtaW51c2RiLmV1LmF1dGgwLmNvbS91c2VyaW5mbyJdLCJpYXQiOjE2MTEyNDcxMDksImV4cCI6MTYxMTI4MzEwOSwiYXpwIjoiNEVZSUhRYVZMZ3dpNVY1bTJnUk5ZZHRwWlpmYnd0RHoiLCJzY29wZSI6Im9wZW5pZCBwcm9maWxlIGVtYWlsIn0.hD6DoOAMj790oN3yzcXWatGR7mbn3oto_vd7kmWy_mobmXwNTQVmOUwNr2Ha-Yd1r6ysqfwipl7aNR-FG3GbmwNyVOleZL8THoiF8jp1p6jKsanzhGUw4Q_EDPNWv7brcdkoWtcKZqEg1ZshE7AZhaOZUoLYMX64Cq_uKe6Y7tcKmGf34CoD-pBu6I56s9OGMdk8gS0CWm4FwJYVfYppUjvdbnrW0mY_tW3644OWwOI_N_V_X3-_wqrxj7TfHKhlvzXdcKUId3Pa9RldKF7Mn-pqJAJVkNDybvEl423kxJ73lT01i46ovoBq60-picfQLJCC0_RfqNctONg8siFpkw','<garbage_collected>',none,_12250) at /tmp/.mount_.org.cdCakCk/usr/share/terminusdb/core/api/db_fetch.pl:75\n  [38] '<meta-call>'('<garbage_collected>') <foreign>\n  [37] database:with_transaction_(query_context{all_witnesses:false,authorization:'terminusdb:///system/data/admin',bindings:[],default_collection:database_descriptor{database_name:\""bikes\"",organization_name:\""admin\""},files:[],filter:type_filter{types: ...},prefixes:_12392{api:'http://terminusdb.com/schema/api#',doc:'terminusdb:///repository/data/',layer:'http://terminusdb.com/schema/layer#',owl:'http://www.w3.org/2002/07/owl#',rdf:'http://www.w3.org/1999/02/22-rdf-syntax-ns#',rdfs:'http://www.w3.org/2000/01/rdf-schema#',ref:'http://terminusdb.com/schema/ref#',repo:'http://terminusdb.com/schema/repository#',system:'http://terminusdb.com/schema/system#',terminus:'http://terminusdb.com/schema/system#',vio:'http://terminusdb.com/schema/vio#',woql:'http://terminusdb.com/schema/woql#',xdd:'http://terminusdb.com/schema/xdd#',xsd:'http://www.w3.org/2001/XMLSchema#'},selected:[],system:system_descriptor{},transaction_objects:[...],update_guard:_12366,write_graph:repo_graph{database_name:\""bikes\"",name:\""main\"",organization_name:\""admin\"",type:instance}},db_fetch:(...,...),_12324) at /tmp/.mount_.org.cdCakCk/usr/share/terminusdb/core/transaction/database.pl:213\n  [36] setup_call_catcher_cleanup(database:true,database:with_transaction_(...,...,_12554),_12532,database:(...,...)) at /tmp/.mount_.org.cdCakCk/usr/lib/swi-prolog/boot/init.pl:612\n  [32] db_clone:clone_('<garbage_collected>','terminusdb:///system/data/admin',admin,bikes,'<garbage_collected>','<garbage_collected>',false,'<garbage_collected>','<garbage_collected>',_12620) at /tmp/.mount_.org.cdCakCk/usr/share/terminusdb/core/api/db_clone.pl:55\n  [31] setup_call_catcher_cleanup(db_clone:true,db_clone:clone_(...,'terminusdb:///system/data/admin',admin,bikes,\""TerminusDB Bikes Tutorial\"",\""This is the knowledge graph produced by the TerminusDB Bikes Tutorial which is commonly used as an introduction to TerminusDB. This database contains the final results in a format that you can clone complete with all of the data and structure intact so that you don't have to do it yourself. \"",false,\""https://hub.terminusdb.com/kevin/bikes\"",...,_12690),exception(_12694),db_clone:(...;true)) at /tmp/.mount_.org.cdCakCk/usr/lib/swi-prolog/boot/init.pl:612\n  [30] db_clone:clone('<garbage_collected>','terminusdb:///system/data/admin',admin,bikes,'<garbage_collected>','<garbage_collected>',false,'<garbage_collected>','<garbage_collected>','<garbage_collected>') at /tmp/.mount_.org.cdCakCk/usr/share/terminusdb/core/api/db_clone.pl:17\n  [29] '<meta-call>'('<garbage_collected>') <foreign>\n  [28] catch(routes:(...,...),error(ssl_error('1416F086','SSL routines',tls_process_server_certificate,'certificate verify failed'),context(_12860,_12862)),routes:do_or_die(...,...)) at /tmp/.mount_.org.cdCakCk/usr/lib/swi-prolog/boot/init.pl:530\n  [27] catch_with_backtrace('<garbage_collected>','<garbage_collected>','<garbage_collected>') at /tmp/.mount_.org.cdCakCk/usr/lib/swi-prolog/boot/init.pl:580\n\nNote: some frames are missing due to last-call optimization.\nRe-run your program in debug mode (:- debug.) to get more detail.\n\n"",
    ""api:status"": ""api:server_error"",
    ""end"": ""2021-01-21T16:40:32.348Z"",
    ""start"": ""2021-01-21T16:40:32.348Z"",
    ""duration"": ""2021-01-21T16:40:32.348Z""
  }
}",2021-01-21T21:53:59Z,kevinchekovfeeney,https://github.com/terminusdb/terminusdb/issues/150#issuecomment-764965804,Can you try logging out and in again - there was a problem with a certificate on the hub server earlier on and it may have cached a bad jwt configuration if you logged in during the issue,can -PRON- try log out and in again - there be a problem with a certificate on the hub server early on and -PRON- may have cache a bad jwt configuration if -PRON- log in during the issue,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,150,2021-01-21T16:46:09Z,kghenderson,cloning ssl error,https://github.com/terminusdb/terminusdb/issues/150,"Describe the bug
Attempting to clone the Bikes Tutorial DB from TerminusHub using the Terminus desktop app on Fedora 33 Linux.
Repository fails to clone with error:
Error: ssl_error('1416F086','SSL routines',tls_process_server_certificate,'certificate verify failed') [46] throw(error(ssl_error('1416F086','SSL routines',tls_process_server_certificate,'certificate verify failed'),_12004)) [44] catch(http:http_protocol_hook(https,...,<stream>(0x7f8110010150,0x7f81100106a0),_12064,...),error(ssl_error('1416F086','SSL routines',tls_process_server_certificate,'certificate verify failed'),_12072),http_open:(...,...)) at /tmp/.mount_.org.cdCakCk/usr/lib/swi-prolog/boot/init.pl:531 [43] http_open:try_http_proxy(direct,[uri('https://hub.terminusdb.com/api/pack/kevin/bikes'),...|...],_12134,'<garbage_collected>') at /tmp/.mount_.org.cdCakCk/usr/lib/swi-prolog/library/http/http_open.pl:448 [41] http_client:http_get('https://hub.terminusdb.com/api/pack/kevin/bikes',_12190,[post(...),...|...]) at /tmp/.mount_.org.cdCakCk/usr/lib/swi-prolog/library/http/http_client.pl:139 [39] db_fetch:authorized_fetch('Bearer eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCIsImtpZCI6InhrMEZTOTdVWVFvbHB4ODREeFlpeCJ9.eyJodHRwOi8vdGVybWludXNkYi5jb20vc2NoZW1hL3N5c3RlbSNhZ2VudF9uYW1lIjoia2doZW5kZXJzb24iLCJodHRwOi8vdGVybWludXNkYi5jb20vc2NoZW1hL3N5c3RlbSN1c2VyX2lkZW50aWZpZXIiOiJrZXZpbkBrZ2hlbmRlcnNvbi5jb20iLCJpc3MiOiJodHRwczovL3Rlcm1pbnVzZGIuZXUuYXV0aDAuY29tLyIsInN1YiI6ImF1dGgwfDVmZWRmZTIwMzkxOWJjMDA3NmE2Y2U2OSIsImF1ZCI6WyJodHRwczovL3Rlcm1pbnVzZGIuY29tL2h1YnNlcnZpY2VzIiwiaHR0cHM6Ly90ZXJtaW51c2RiLmV1LmF1dGgwLmNvbS91c2VyaW5mbyJdLCJpYXQiOjE2MTEyNDcxMDksImV4cCI6MTYxMTI4MzEwOSwiYXpwIjoiNEVZSUhRYVZMZ3dpNVY1bTJnUk5ZZHRwWlpmYnd0RHoiLCJzY29wZSI6Im9wZW5pZCBwcm9maWxlIGVtYWlsIn0.hD6DoOAMj790oN3yzcXWatGR7mbn3oto_vd7kmWy_mobmXwNTQVmOUwNr2Ha-Yd1r6ysqfwipl7aNR-FG3GbmwNyVOleZL8THoiF8jp1p6jKsanzhGUw4Q_EDPNWv7brcdkoWtcKZqEg1ZshE7AZhaOZUoLYMX64Cq_uKe6Y7tcKmGf34CoD-pBu6I56s9OGMdk8gS0CWm4FwJYVfYppUjvdbnrW0mY_tW3644OWwOI_N_V_X3-_wqrxj7TfHKhlvzXdcKUId3Pa9RldKF7Mn-pqJAJVkNDybvEl423kxJ73lT01i46ovoBq60-picfQLJCC0_RfqNctONg8siFpkw','<garbage_collected>',none,_12250) at /tmp/.mount_.org.cdCakCk/usr/share/terminusdb/core/api/db_fetch.pl:75 [38] '<meta-call>'('<garbage_collected>') <foreign> [37] database:with_transaction_(query_context{all_witnesses:false,authorization:'terminusdb:///system/data/admin',bindings:[],default_collection:database_descriptor{database_name:""bikes"",organization_name:""admin""},files:[],filter:type_filter{types: ...},prefixes:_12392{api:'http://terminusdb.com/schema/api#',doc:'terminusdb:///repository/data/',layer:'http://terminusdb.com/schema/layer#',owl:'http://www.w3.org/2002/07/owl#',rdf:'http://www.w3.org/1999/02/22-rdf-syntax-ns#',rdfs:'http://www.w3.org/2000/01/rdf-schema#',ref:'http://terminusdb.com/schema/ref#',repo:'http://terminusdb.com/schema/repository#',system:'http://terminusdb.com/schema/system#',terminus:'http://terminusdb.com/schema/system#',vio:'http://terminusdb.com/schema/vio#',woql:'http://terminusdb.com/schema/woql#',xdd:'http://terminusdb.com/schema/xdd#',xsd:'http://www.w3.org/2001/XMLSchema#'},selected:[],system:system_descriptor{},transaction_objects:[...],update_guard:_12366,write_graph:repo_graph{database_name:""bikes"",name:""main"",organization_name:""admin"",type:instance}},db_fetch:(...,...),_12324) at /tmp/.mount_.org.cdCakCk/usr/share/terminusdb/core/transaction/database.pl:213 [36] setup_call_catcher_cleanup(database:true,database:with_transaction_(...,...,_12554),_12532,database:(...,...)) at /tmp/.mount_.org.cdCakCk/usr/lib/swi-prolog/boot/init.pl:612 [32] db_clone:clone_('<garbage_collected>','terminusdb:///system/data/admin',admin,bikes,'<garbage_collected>','<garbage_collected>',false,'<garbage_collected>','<garbage_collected>',_12620) at /tmp/.mount_.org.cdCakCk/usr/share/terminusdb/core/api/db_clone.pl:55 [31] setup_call_catcher_cleanup(db_clone:true,db_clone:clone_(...,'terminusdb:///system/data/admin',admin,bikes,""TerminusDB Bikes Tutorial"",""This is the knowledge graph produced by the TerminusDB Bikes Tutorial which is commonly used as an introduction to TerminusDB. This database contains the final results in a format that you can clone complete with all of the data and structure intact so that you don't have to do it yourself. "",false,""https://hub.terminusdb.com/kevin/bikes"",...,_12690),exception(_12694),db_clone:(...;true)) at /tmp/.mount_.org.cdCakCk/usr/lib/swi-prolog/boot/init.pl:612 [30] db_clone:clone('<garbage_collected>','terminusdb:///system/data/admin',admin,bikes,'<garbage_collected>','<garbage_collected>',false,'<garbage_collected>','<garbage_collected>','<garbage_collected>') at /tmp/.mount_.org.cdCakCk/usr/share/terminusdb/core/api/db_clone.pl:17 [29] '<meta-call>'('<garbage_collected>') <foreign> [28] catch(routes:(...,...),error(ssl_error('1416F086','SSL routines',tls_process_server_certificate,'certificate verify failed'),context(_12860,_12862)),routes:do_or_die(...,...)) at /tmp/.mount_.org.cdCakCk/usr/lib/swi-prolog/boot/init.pl:530 [27] catch_with_backtrace('<garbage_collected>','<garbage_collected>','<garbage_collected>') at /tmp/.mount_.org.cdCakCk/usr/lib/swi-prolog/boot/init.pl:580 Note: some frames are missing due to last-call optimization. Re-run your program in debug mode (:- debug.) to get more detail.less
{
  ""data"": {
    ""api:message"": ""Error: ssl_error('1416F086','SSL routines',tls_process_server_certificate,'certificate verify failed')\n  [46] throw(error(ssl_error('1416F086','SSL routines',tls_process_server_certificate,'certificate verify failed'),_12004))\n  [44] catch(http:http_protocol_hook(https,...,<stream>(0x7f8110010150,0x7f81100106a0),_12064,...),error(ssl_error('1416F086','SSL routines',tls_process_server_certificate,'certificate verify failed'),_12072),http_open:(...,...)) at /tmp/.mount_.org.cdCakCk/usr/lib/swi-prolog/boot/init.pl:531\n  [43] http_open:try_http_proxy(direct,[uri('https://hub.terminusdb.com/api/pack/kevin/bikes'),...|...],_12134,'<garbage_collected>') at /tmp/.mount_.org.cdCakCk/usr/lib/swi-prolog/library/http/http_open.pl:448\n  [41] http_client:http_get('https://hub.terminusdb.com/api/pack/kevin/bikes',_12190,[post(...),...|...]) at /tmp/.mount_.org.cdCakCk/usr/lib/swi-prolog/library/http/http_client.pl:139\n  [39] db_fetch:authorized_fetch('Bearer eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCIsImtpZCI6InhrMEZTOTdVWVFvbHB4ODREeFlpeCJ9.eyJodHRwOi8vdGVybWludXNkYi5jb20vc2NoZW1hL3N5c3RlbSNhZ2VudF9uYW1lIjoia2doZW5kZXJzb24iLCJodHRwOi8vdGVybWludXNkYi5jb20vc2NoZW1hL3N5c3RlbSN1c2VyX2lkZW50aWZpZXIiOiJrZXZpbkBrZ2hlbmRlcnNvbi5jb20iLCJpc3MiOiJodHRwczovL3Rlcm1pbnVzZGIuZXUuYXV0aDAuY29tLyIsInN1YiI6ImF1dGgwfDVmZWRmZTIwMzkxOWJjMDA3NmE2Y2U2OSIsImF1ZCI6WyJodHRwczovL3Rlcm1pbnVzZGIuY29tL2h1YnNlcnZpY2VzIiwiaHR0cHM6Ly90ZXJtaW51c2RiLmV1LmF1dGgwLmNvbS91c2VyaW5mbyJdLCJpYXQiOjE2MTEyNDcxMDksImV4cCI6MTYxMTI4MzEwOSwiYXpwIjoiNEVZSUhRYVZMZ3dpNVY1bTJnUk5ZZHRwWlpmYnd0RHoiLCJzY29wZSI6Im9wZW5pZCBwcm9maWxlIGVtYWlsIn0.hD6DoOAMj790oN3yzcXWatGR7mbn3oto_vd7kmWy_mobmXwNTQVmOUwNr2Ha-Yd1r6ysqfwipl7aNR-FG3GbmwNyVOleZL8THoiF8jp1p6jKsanzhGUw4Q_EDPNWv7brcdkoWtcKZqEg1ZshE7AZhaOZUoLYMX64Cq_uKe6Y7tcKmGf34CoD-pBu6I56s9OGMdk8gS0CWm4FwJYVfYppUjvdbnrW0mY_tW3644OWwOI_N_V_X3-_wqrxj7TfHKhlvzXdcKUId3Pa9RldKF7Mn-pqJAJVkNDybvEl423kxJ73lT01i46ovoBq60-picfQLJCC0_RfqNctONg8siFpkw','<garbage_collected>',none,_12250) at /tmp/.mount_.org.cdCakCk/usr/share/terminusdb/core/api/db_fetch.pl:75\n  [38] '<meta-call>'('<garbage_collected>') <foreign>\n  [37] database:with_transaction_(query_context{all_witnesses:false,authorization:'terminusdb:///system/data/admin',bindings:[],default_collection:database_descriptor{database_name:\""bikes\"",organization_name:\""admin\""},files:[],filter:type_filter{types: ...},prefixes:_12392{api:'http://terminusdb.com/schema/api#',doc:'terminusdb:///repository/data/',layer:'http://terminusdb.com/schema/layer#',owl:'http://www.w3.org/2002/07/owl#',rdf:'http://www.w3.org/1999/02/22-rdf-syntax-ns#',rdfs:'http://www.w3.org/2000/01/rdf-schema#',ref:'http://terminusdb.com/schema/ref#',repo:'http://terminusdb.com/schema/repository#',system:'http://terminusdb.com/schema/system#',terminus:'http://terminusdb.com/schema/system#',vio:'http://terminusdb.com/schema/vio#',woql:'http://terminusdb.com/schema/woql#',xdd:'http://terminusdb.com/schema/xdd#',xsd:'http://www.w3.org/2001/XMLSchema#'},selected:[],system:system_descriptor{},transaction_objects:[...],update_guard:_12366,write_graph:repo_graph{database_name:\""bikes\"",name:\""main\"",organization_name:\""admin\"",type:instance}},db_fetch:(...,...),_12324) at /tmp/.mount_.org.cdCakCk/usr/share/terminusdb/core/transaction/database.pl:213\n  [36] setup_call_catcher_cleanup(database:true,database:with_transaction_(...,...,_12554),_12532,database:(...,...)) at /tmp/.mount_.org.cdCakCk/usr/lib/swi-prolog/boot/init.pl:612\n  [32] db_clone:clone_('<garbage_collected>','terminusdb:///system/data/admin',admin,bikes,'<garbage_collected>','<garbage_collected>',false,'<garbage_collected>','<garbage_collected>',_12620) at /tmp/.mount_.org.cdCakCk/usr/share/terminusdb/core/api/db_clone.pl:55\n  [31] setup_call_catcher_cleanup(db_clone:true,db_clone:clone_(...,'terminusdb:///system/data/admin',admin,bikes,\""TerminusDB Bikes Tutorial\"",\""This is the knowledge graph produced by the TerminusDB Bikes Tutorial which is commonly used as an introduction to TerminusDB. This database contains the final results in a format that you can clone complete with all of the data and structure intact so that you don't have to do it yourself. \"",false,\""https://hub.terminusdb.com/kevin/bikes\"",...,_12690),exception(_12694),db_clone:(...;true)) at /tmp/.mount_.org.cdCakCk/usr/lib/swi-prolog/boot/init.pl:612\n  [30] db_clone:clone('<garbage_collected>','terminusdb:///system/data/admin',admin,bikes,'<garbage_collected>','<garbage_collected>',false,'<garbage_collected>','<garbage_collected>','<garbage_collected>') at /tmp/.mount_.org.cdCakCk/usr/share/terminusdb/core/api/db_clone.pl:17\n  [29] '<meta-call>'('<garbage_collected>') <foreign>\n  [28] catch(routes:(...,...),error(ssl_error('1416F086','SSL routines',tls_process_server_certificate,'certificate verify failed'),context(_12860,_12862)),routes:do_or_die(...,...)) at /tmp/.mount_.org.cdCakCk/usr/lib/swi-prolog/boot/init.pl:530\n  [27] catch_with_backtrace('<garbage_collected>','<garbage_collected>','<garbage_collected>') at /tmp/.mount_.org.cdCakCk/usr/lib/swi-prolog/boot/init.pl:580\n\nNote: some frames are missing due to last-call optimization.\nRe-run your program in debug mode (:- debug.) to get more detail.\n\n"",
    ""api:status"": ""api:server_error"",
    ""end"": ""2021-01-21T16:40:32.348Z"",
    ""start"": ""2021-01-21T16:40:32.348Z"",
    ""duration"": ""2021-01-21T16:40:32.348Z""
  }
}",2021-01-21T23:06:20Z,kghenderson,https://github.com/terminusdb/terminusdb/issues/150#issuecomment-764998270,"Through the desktop app I signed out, closed & reopened the app (for good measure), and signed back in - but still the same error. Just for fun I tried to clone another db and that's still giving me the error as well.",through the desktop app i sign out close & reopen the app ( for good measure ) and sign back in - but still the same error just for fun i try to clone another db and that be still give -PRON- the error as well,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,150,2021-01-21T16:46:09Z,kghenderson,cloning ssl error,https://github.com/terminusdb/terminusdb/issues/150,"Describe the bug
Attempting to clone the Bikes Tutorial DB from TerminusHub using the Terminus desktop app on Fedora 33 Linux.
Repository fails to clone with error:
Error: ssl_error('1416F086','SSL routines',tls_process_server_certificate,'certificate verify failed') [46] throw(error(ssl_error('1416F086','SSL routines',tls_process_server_certificate,'certificate verify failed'),_12004)) [44] catch(http:http_protocol_hook(https,...,<stream>(0x7f8110010150,0x7f81100106a0),_12064,...),error(ssl_error('1416F086','SSL routines',tls_process_server_certificate,'certificate verify failed'),_12072),http_open:(...,...)) at /tmp/.mount_.org.cdCakCk/usr/lib/swi-prolog/boot/init.pl:531 [43] http_open:try_http_proxy(direct,[uri('https://hub.terminusdb.com/api/pack/kevin/bikes'),...|...],_12134,'<garbage_collected>') at /tmp/.mount_.org.cdCakCk/usr/lib/swi-prolog/library/http/http_open.pl:448 [41] http_client:http_get('https://hub.terminusdb.com/api/pack/kevin/bikes',_12190,[post(...),...|...]) at /tmp/.mount_.org.cdCakCk/usr/lib/swi-prolog/library/http/http_client.pl:139 [39] db_fetch:authorized_fetch('Bearer eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCIsImtpZCI6InhrMEZTOTdVWVFvbHB4ODREeFlpeCJ9.eyJodHRwOi8vdGVybWludXNkYi5jb20vc2NoZW1hL3N5c3RlbSNhZ2VudF9uYW1lIjoia2doZW5kZXJzb24iLCJodHRwOi8vdGVybWludXNkYi5jb20vc2NoZW1hL3N5c3RlbSN1c2VyX2lkZW50aWZpZXIiOiJrZXZpbkBrZ2hlbmRlcnNvbi5jb20iLCJpc3MiOiJodHRwczovL3Rlcm1pbnVzZGIuZXUuYXV0aDAuY29tLyIsInN1YiI6ImF1dGgwfDVmZWRmZTIwMzkxOWJjMDA3NmE2Y2U2OSIsImF1ZCI6WyJodHRwczovL3Rlcm1pbnVzZGIuY29tL2h1YnNlcnZpY2VzIiwiaHR0cHM6Ly90ZXJtaW51c2RiLmV1LmF1dGgwLmNvbS91c2VyaW5mbyJdLCJpYXQiOjE2MTEyNDcxMDksImV4cCI6MTYxMTI4MzEwOSwiYXpwIjoiNEVZSUhRYVZMZ3dpNVY1bTJnUk5ZZHRwWlpmYnd0RHoiLCJzY29wZSI6Im9wZW5pZCBwcm9maWxlIGVtYWlsIn0.hD6DoOAMj790oN3yzcXWatGR7mbn3oto_vd7kmWy_mobmXwNTQVmOUwNr2Ha-Yd1r6ysqfwipl7aNR-FG3GbmwNyVOleZL8THoiF8jp1p6jKsanzhGUw4Q_EDPNWv7brcdkoWtcKZqEg1ZshE7AZhaOZUoLYMX64Cq_uKe6Y7tcKmGf34CoD-pBu6I56s9OGMdk8gS0CWm4FwJYVfYppUjvdbnrW0mY_tW3644OWwOI_N_V_X3-_wqrxj7TfHKhlvzXdcKUId3Pa9RldKF7Mn-pqJAJVkNDybvEl423kxJ73lT01i46ovoBq60-picfQLJCC0_RfqNctONg8siFpkw','<garbage_collected>',none,_12250) at /tmp/.mount_.org.cdCakCk/usr/share/terminusdb/core/api/db_fetch.pl:75 [38] '<meta-call>'('<garbage_collected>') <foreign> [37] database:with_transaction_(query_context{all_witnesses:false,authorization:'terminusdb:///system/data/admin',bindings:[],default_collection:database_descriptor{database_name:""bikes"",organization_name:""admin""},files:[],filter:type_filter{types: ...},prefixes:_12392{api:'http://terminusdb.com/schema/api#',doc:'terminusdb:///repository/data/',layer:'http://terminusdb.com/schema/layer#',owl:'http://www.w3.org/2002/07/owl#',rdf:'http://www.w3.org/1999/02/22-rdf-syntax-ns#',rdfs:'http://www.w3.org/2000/01/rdf-schema#',ref:'http://terminusdb.com/schema/ref#',repo:'http://terminusdb.com/schema/repository#',system:'http://terminusdb.com/schema/system#',terminus:'http://terminusdb.com/schema/system#',vio:'http://terminusdb.com/schema/vio#',woql:'http://terminusdb.com/schema/woql#',xdd:'http://terminusdb.com/schema/xdd#',xsd:'http://www.w3.org/2001/XMLSchema#'},selected:[],system:system_descriptor{},transaction_objects:[...],update_guard:_12366,write_graph:repo_graph{database_name:""bikes"",name:""main"",organization_name:""admin"",type:instance}},db_fetch:(...,...),_12324) at /tmp/.mount_.org.cdCakCk/usr/share/terminusdb/core/transaction/database.pl:213 [36] setup_call_catcher_cleanup(database:true,database:with_transaction_(...,...,_12554),_12532,database:(...,...)) at /tmp/.mount_.org.cdCakCk/usr/lib/swi-prolog/boot/init.pl:612 [32] db_clone:clone_('<garbage_collected>','terminusdb:///system/data/admin',admin,bikes,'<garbage_collected>','<garbage_collected>',false,'<garbage_collected>','<garbage_collected>',_12620) at /tmp/.mount_.org.cdCakCk/usr/share/terminusdb/core/api/db_clone.pl:55 [31] setup_call_catcher_cleanup(db_clone:true,db_clone:clone_(...,'terminusdb:///system/data/admin',admin,bikes,""TerminusDB Bikes Tutorial"",""This is the knowledge graph produced by the TerminusDB Bikes Tutorial which is commonly used as an introduction to TerminusDB. This database contains the final results in a format that you can clone complete with all of the data and structure intact so that you don't have to do it yourself. "",false,""https://hub.terminusdb.com/kevin/bikes"",...,_12690),exception(_12694),db_clone:(...;true)) at /tmp/.mount_.org.cdCakCk/usr/lib/swi-prolog/boot/init.pl:612 [30] db_clone:clone('<garbage_collected>','terminusdb:///system/data/admin',admin,bikes,'<garbage_collected>','<garbage_collected>',false,'<garbage_collected>','<garbage_collected>','<garbage_collected>') at /tmp/.mount_.org.cdCakCk/usr/share/terminusdb/core/api/db_clone.pl:17 [29] '<meta-call>'('<garbage_collected>') <foreign> [28] catch(routes:(...,...),error(ssl_error('1416F086','SSL routines',tls_process_server_certificate,'certificate verify failed'),context(_12860,_12862)),routes:do_or_die(...,...)) at /tmp/.mount_.org.cdCakCk/usr/lib/swi-prolog/boot/init.pl:530 [27] catch_with_backtrace('<garbage_collected>','<garbage_collected>','<garbage_collected>') at /tmp/.mount_.org.cdCakCk/usr/lib/swi-prolog/boot/init.pl:580 Note: some frames are missing due to last-call optimization. Re-run your program in debug mode (:- debug.) to get more detail.less
{
  ""data"": {
    ""api:message"": ""Error: ssl_error('1416F086','SSL routines',tls_process_server_certificate,'certificate verify failed')\n  [46] throw(error(ssl_error('1416F086','SSL routines',tls_process_server_certificate,'certificate verify failed'),_12004))\n  [44] catch(http:http_protocol_hook(https,...,<stream>(0x7f8110010150,0x7f81100106a0),_12064,...),error(ssl_error('1416F086','SSL routines',tls_process_server_certificate,'certificate verify failed'),_12072),http_open:(...,...)) at /tmp/.mount_.org.cdCakCk/usr/lib/swi-prolog/boot/init.pl:531\n  [43] http_open:try_http_proxy(direct,[uri('https://hub.terminusdb.com/api/pack/kevin/bikes'),...|...],_12134,'<garbage_collected>') at /tmp/.mount_.org.cdCakCk/usr/lib/swi-prolog/library/http/http_open.pl:448\n  [41] http_client:http_get('https://hub.terminusdb.com/api/pack/kevin/bikes',_12190,[post(...),...|...]) at /tmp/.mount_.org.cdCakCk/usr/lib/swi-prolog/library/http/http_client.pl:139\n  [39] db_fetch:authorized_fetch('Bearer eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCIsImtpZCI6InhrMEZTOTdVWVFvbHB4ODREeFlpeCJ9.eyJodHRwOi8vdGVybWludXNkYi5jb20vc2NoZW1hL3N5c3RlbSNhZ2VudF9uYW1lIjoia2doZW5kZXJzb24iLCJodHRwOi8vdGVybWludXNkYi5jb20vc2NoZW1hL3N5c3RlbSN1c2VyX2lkZW50aWZpZXIiOiJrZXZpbkBrZ2hlbmRlcnNvbi5jb20iLCJpc3MiOiJodHRwczovL3Rlcm1pbnVzZGIuZXUuYXV0aDAuY29tLyIsInN1YiI6ImF1dGgwfDVmZWRmZTIwMzkxOWJjMDA3NmE2Y2U2OSIsImF1ZCI6WyJodHRwczovL3Rlcm1pbnVzZGIuY29tL2h1YnNlcnZpY2VzIiwiaHR0cHM6Ly90ZXJtaW51c2RiLmV1LmF1dGgwLmNvbS91c2VyaW5mbyJdLCJpYXQiOjE2MTEyNDcxMDksImV4cCI6MTYxMTI4MzEwOSwiYXpwIjoiNEVZSUhRYVZMZ3dpNVY1bTJnUk5ZZHRwWlpmYnd0RHoiLCJzY29wZSI6Im9wZW5pZCBwcm9maWxlIGVtYWlsIn0.hD6DoOAMj790oN3yzcXWatGR7mbn3oto_vd7kmWy_mobmXwNTQVmOUwNr2Ha-Yd1r6ysqfwipl7aNR-FG3GbmwNyVOleZL8THoiF8jp1p6jKsanzhGUw4Q_EDPNWv7brcdkoWtcKZqEg1ZshE7AZhaOZUoLYMX64Cq_uKe6Y7tcKmGf34CoD-pBu6I56s9OGMdk8gS0CWm4FwJYVfYppUjvdbnrW0mY_tW3644OWwOI_N_V_X3-_wqrxj7TfHKhlvzXdcKUId3Pa9RldKF7Mn-pqJAJVkNDybvEl423kxJ73lT01i46ovoBq60-picfQLJCC0_RfqNctONg8siFpkw','<garbage_collected>',none,_12250) at /tmp/.mount_.org.cdCakCk/usr/share/terminusdb/core/api/db_fetch.pl:75\n  [38] '<meta-call>'('<garbage_collected>') <foreign>\n  [37] database:with_transaction_(query_context{all_witnesses:false,authorization:'terminusdb:///system/data/admin',bindings:[],default_collection:database_descriptor{database_name:\""bikes\"",organization_name:\""admin\""},files:[],filter:type_filter{types: ...},prefixes:_12392{api:'http://terminusdb.com/schema/api#',doc:'terminusdb:///repository/data/',layer:'http://terminusdb.com/schema/layer#',owl:'http://www.w3.org/2002/07/owl#',rdf:'http://www.w3.org/1999/02/22-rdf-syntax-ns#',rdfs:'http://www.w3.org/2000/01/rdf-schema#',ref:'http://terminusdb.com/schema/ref#',repo:'http://terminusdb.com/schema/repository#',system:'http://terminusdb.com/schema/system#',terminus:'http://terminusdb.com/schema/system#',vio:'http://terminusdb.com/schema/vio#',woql:'http://terminusdb.com/schema/woql#',xdd:'http://terminusdb.com/schema/xdd#',xsd:'http://www.w3.org/2001/XMLSchema#'},selected:[],system:system_descriptor{},transaction_objects:[...],update_guard:_12366,write_graph:repo_graph{database_name:\""bikes\"",name:\""main\"",organization_name:\""admin\"",type:instance}},db_fetch:(...,...),_12324) at /tmp/.mount_.org.cdCakCk/usr/share/terminusdb/core/transaction/database.pl:213\n  [36] setup_call_catcher_cleanup(database:true,database:with_transaction_(...,...,_12554),_12532,database:(...,...)) at /tmp/.mount_.org.cdCakCk/usr/lib/swi-prolog/boot/init.pl:612\n  [32] db_clone:clone_('<garbage_collected>','terminusdb:///system/data/admin',admin,bikes,'<garbage_collected>','<garbage_collected>',false,'<garbage_collected>','<garbage_collected>',_12620) at /tmp/.mount_.org.cdCakCk/usr/share/terminusdb/core/api/db_clone.pl:55\n  [31] setup_call_catcher_cleanup(db_clone:true,db_clone:clone_(...,'terminusdb:///system/data/admin',admin,bikes,\""TerminusDB Bikes Tutorial\"",\""This is the knowledge graph produced by the TerminusDB Bikes Tutorial which is commonly used as an introduction to TerminusDB. This database contains the final results in a format that you can clone complete with all of the data and structure intact so that you don't have to do it yourself. \"",false,\""https://hub.terminusdb.com/kevin/bikes\"",...,_12690),exception(_12694),db_clone:(...;true)) at /tmp/.mount_.org.cdCakCk/usr/lib/swi-prolog/boot/init.pl:612\n  [30] db_clone:clone('<garbage_collected>','terminusdb:///system/data/admin',admin,bikes,'<garbage_collected>','<garbage_collected>',false,'<garbage_collected>','<garbage_collected>','<garbage_collected>') at /tmp/.mount_.org.cdCakCk/usr/share/terminusdb/core/api/db_clone.pl:17\n  [29] '<meta-call>'('<garbage_collected>') <foreign>\n  [28] catch(routes:(...,...),error(ssl_error('1416F086','SSL routines',tls_process_server_certificate,'certificate verify failed'),context(_12860,_12862)),routes:do_or_die(...,...)) at /tmp/.mount_.org.cdCakCk/usr/lib/swi-prolog/boot/init.pl:530\n  [27] catch_with_backtrace('<garbage_collected>','<garbage_collected>','<garbage_collected>') at /tmp/.mount_.org.cdCakCk/usr/lib/swi-prolog/boot/init.pl:580\n\nNote: some frames are missing due to last-call optimization.\nRe-run your program in debug mode (:- debug.) to get more detail.\n\n"",
    ""api:status"": ""api:server_error"",
    ""end"": ""2021-01-21T16:40:32.348Z"",
    ""start"": ""2021-01-21T16:40:32.348Z"",
    ""duration"": ""2021-01-21T16:40:32.348Z""
  }
}",2021-01-21T23:10:57Z,kghenderson,https://github.com/terminusdb/terminusdb/issues/150#issuecomment-765000121,"Not sure if it matters, but just for fun I logged in to my profile settings on terminusdb.com from both firefox & chromium but didn't get any certificate errors - is there perhaps anything baked into the appimage?",not sure if -PRON- matter but just for fun i log in to -PRON- profile setting on terminusdbcom from both firefox & chromium but do not get any certificate error - be there perhaps anything bake into the appimage,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,150,2021-01-21T16:46:09Z,kghenderson,cloning ssl error,https://github.com/terminusdb/terminusdb/issues/150,"Describe the bug
Attempting to clone the Bikes Tutorial DB from TerminusHub using the Terminus desktop app on Fedora 33 Linux.
Repository fails to clone with error:
Error: ssl_error('1416F086','SSL routines',tls_process_server_certificate,'certificate verify failed') [46] throw(error(ssl_error('1416F086','SSL routines',tls_process_server_certificate,'certificate verify failed'),_12004)) [44] catch(http:http_protocol_hook(https,...,<stream>(0x7f8110010150,0x7f81100106a0),_12064,...),error(ssl_error('1416F086','SSL routines',tls_process_server_certificate,'certificate verify failed'),_12072),http_open:(...,...)) at /tmp/.mount_.org.cdCakCk/usr/lib/swi-prolog/boot/init.pl:531 [43] http_open:try_http_proxy(direct,[uri('https://hub.terminusdb.com/api/pack/kevin/bikes'),...|...],_12134,'<garbage_collected>') at /tmp/.mount_.org.cdCakCk/usr/lib/swi-prolog/library/http/http_open.pl:448 [41] http_client:http_get('https://hub.terminusdb.com/api/pack/kevin/bikes',_12190,[post(...),...|...]) at /tmp/.mount_.org.cdCakCk/usr/lib/swi-prolog/library/http/http_client.pl:139 [39] db_fetch:authorized_fetch('Bearer eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCIsImtpZCI6InhrMEZTOTdVWVFvbHB4ODREeFlpeCJ9.eyJodHRwOi8vdGVybWludXNkYi5jb20vc2NoZW1hL3N5c3RlbSNhZ2VudF9uYW1lIjoia2doZW5kZXJzb24iLCJodHRwOi8vdGVybWludXNkYi5jb20vc2NoZW1hL3N5c3RlbSN1c2VyX2lkZW50aWZpZXIiOiJrZXZpbkBrZ2hlbmRlcnNvbi5jb20iLCJpc3MiOiJodHRwczovL3Rlcm1pbnVzZGIuZXUuYXV0aDAuY29tLyIsInN1YiI6ImF1dGgwfDVmZWRmZTIwMzkxOWJjMDA3NmE2Y2U2OSIsImF1ZCI6WyJodHRwczovL3Rlcm1pbnVzZGIuY29tL2h1YnNlcnZpY2VzIiwiaHR0cHM6Ly90ZXJtaW51c2RiLmV1LmF1dGgwLmNvbS91c2VyaW5mbyJdLCJpYXQiOjE2MTEyNDcxMDksImV4cCI6MTYxMTI4MzEwOSwiYXpwIjoiNEVZSUhRYVZMZ3dpNVY1bTJnUk5ZZHRwWlpmYnd0RHoiLCJzY29wZSI6Im9wZW5pZCBwcm9maWxlIGVtYWlsIn0.hD6DoOAMj790oN3yzcXWatGR7mbn3oto_vd7kmWy_mobmXwNTQVmOUwNr2Ha-Yd1r6ysqfwipl7aNR-FG3GbmwNyVOleZL8THoiF8jp1p6jKsanzhGUw4Q_EDPNWv7brcdkoWtcKZqEg1ZshE7AZhaOZUoLYMX64Cq_uKe6Y7tcKmGf34CoD-pBu6I56s9OGMdk8gS0CWm4FwJYVfYppUjvdbnrW0mY_tW3644OWwOI_N_V_X3-_wqrxj7TfHKhlvzXdcKUId3Pa9RldKF7Mn-pqJAJVkNDybvEl423kxJ73lT01i46ovoBq60-picfQLJCC0_RfqNctONg8siFpkw','<garbage_collected>',none,_12250) at /tmp/.mount_.org.cdCakCk/usr/share/terminusdb/core/api/db_fetch.pl:75 [38] '<meta-call>'('<garbage_collected>') <foreign> [37] database:with_transaction_(query_context{all_witnesses:false,authorization:'terminusdb:///system/data/admin',bindings:[],default_collection:database_descriptor{database_name:""bikes"",organization_name:""admin""},files:[],filter:type_filter{types: ...},prefixes:_12392{api:'http://terminusdb.com/schema/api#',doc:'terminusdb:///repository/data/',layer:'http://terminusdb.com/schema/layer#',owl:'http://www.w3.org/2002/07/owl#',rdf:'http://www.w3.org/1999/02/22-rdf-syntax-ns#',rdfs:'http://www.w3.org/2000/01/rdf-schema#',ref:'http://terminusdb.com/schema/ref#',repo:'http://terminusdb.com/schema/repository#',system:'http://terminusdb.com/schema/system#',terminus:'http://terminusdb.com/schema/system#',vio:'http://terminusdb.com/schema/vio#',woql:'http://terminusdb.com/schema/woql#',xdd:'http://terminusdb.com/schema/xdd#',xsd:'http://www.w3.org/2001/XMLSchema#'},selected:[],system:system_descriptor{},transaction_objects:[...],update_guard:_12366,write_graph:repo_graph{database_name:""bikes"",name:""main"",organization_name:""admin"",type:instance}},db_fetch:(...,...),_12324) at /tmp/.mount_.org.cdCakCk/usr/share/terminusdb/core/transaction/database.pl:213 [36] setup_call_catcher_cleanup(database:true,database:with_transaction_(...,...,_12554),_12532,database:(...,...)) at /tmp/.mount_.org.cdCakCk/usr/lib/swi-prolog/boot/init.pl:612 [32] db_clone:clone_('<garbage_collected>','terminusdb:///system/data/admin',admin,bikes,'<garbage_collected>','<garbage_collected>',false,'<garbage_collected>','<garbage_collected>',_12620) at /tmp/.mount_.org.cdCakCk/usr/share/terminusdb/core/api/db_clone.pl:55 [31] setup_call_catcher_cleanup(db_clone:true,db_clone:clone_(...,'terminusdb:///system/data/admin',admin,bikes,""TerminusDB Bikes Tutorial"",""This is the knowledge graph produced by the TerminusDB Bikes Tutorial which is commonly used as an introduction to TerminusDB. This database contains the final results in a format that you can clone complete with all of the data and structure intact so that you don't have to do it yourself. "",false,""https://hub.terminusdb.com/kevin/bikes"",...,_12690),exception(_12694),db_clone:(...;true)) at /tmp/.mount_.org.cdCakCk/usr/lib/swi-prolog/boot/init.pl:612 [30] db_clone:clone('<garbage_collected>','terminusdb:///system/data/admin',admin,bikes,'<garbage_collected>','<garbage_collected>',false,'<garbage_collected>','<garbage_collected>','<garbage_collected>') at /tmp/.mount_.org.cdCakCk/usr/share/terminusdb/core/api/db_clone.pl:17 [29] '<meta-call>'('<garbage_collected>') <foreign> [28] catch(routes:(...,...),error(ssl_error('1416F086','SSL routines',tls_process_server_certificate,'certificate verify failed'),context(_12860,_12862)),routes:do_or_die(...,...)) at /tmp/.mount_.org.cdCakCk/usr/lib/swi-prolog/boot/init.pl:530 [27] catch_with_backtrace('<garbage_collected>','<garbage_collected>','<garbage_collected>') at /tmp/.mount_.org.cdCakCk/usr/lib/swi-prolog/boot/init.pl:580 Note: some frames are missing due to last-call optimization. Re-run your program in debug mode (:- debug.) to get more detail.less
{
  ""data"": {
    ""api:message"": ""Error: ssl_error('1416F086','SSL routines',tls_process_server_certificate,'certificate verify failed')\n  [46] throw(error(ssl_error('1416F086','SSL routines',tls_process_server_certificate,'certificate verify failed'),_12004))\n  [44] catch(http:http_protocol_hook(https,...,<stream>(0x7f8110010150,0x7f81100106a0),_12064,...),error(ssl_error('1416F086','SSL routines',tls_process_server_certificate,'certificate verify failed'),_12072),http_open:(...,...)) at /tmp/.mount_.org.cdCakCk/usr/lib/swi-prolog/boot/init.pl:531\n  [43] http_open:try_http_proxy(direct,[uri('https://hub.terminusdb.com/api/pack/kevin/bikes'),...|...],_12134,'<garbage_collected>') at /tmp/.mount_.org.cdCakCk/usr/lib/swi-prolog/library/http/http_open.pl:448\n  [41] http_client:http_get('https://hub.terminusdb.com/api/pack/kevin/bikes',_12190,[post(...),...|...]) at /tmp/.mount_.org.cdCakCk/usr/lib/swi-prolog/library/http/http_client.pl:139\n  [39] db_fetch:authorized_fetch('Bearer eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCIsImtpZCI6InhrMEZTOTdVWVFvbHB4ODREeFlpeCJ9.eyJodHRwOi8vdGVybWludXNkYi5jb20vc2NoZW1hL3N5c3RlbSNhZ2VudF9uYW1lIjoia2doZW5kZXJzb24iLCJodHRwOi8vdGVybWludXNkYi5jb20vc2NoZW1hL3N5c3RlbSN1c2VyX2lkZW50aWZpZXIiOiJrZXZpbkBrZ2hlbmRlcnNvbi5jb20iLCJpc3MiOiJodHRwczovL3Rlcm1pbnVzZGIuZXUuYXV0aDAuY29tLyIsInN1YiI6ImF1dGgwfDVmZWRmZTIwMzkxOWJjMDA3NmE2Y2U2OSIsImF1ZCI6WyJodHRwczovL3Rlcm1pbnVzZGIuY29tL2h1YnNlcnZpY2VzIiwiaHR0cHM6Ly90ZXJtaW51c2RiLmV1LmF1dGgwLmNvbS91c2VyaW5mbyJdLCJpYXQiOjE2MTEyNDcxMDksImV4cCI6MTYxMTI4MzEwOSwiYXpwIjoiNEVZSUhRYVZMZ3dpNVY1bTJnUk5ZZHRwWlpmYnd0RHoiLCJzY29wZSI6Im9wZW5pZCBwcm9maWxlIGVtYWlsIn0.hD6DoOAMj790oN3yzcXWatGR7mbn3oto_vd7kmWy_mobmXwNTQVmOUwNr2Ha-Yd1r6ysqfwipl7aNR-FG3GbmwNyVOleZL8THoiF8jp1p6jKsanzhGUw4Q_EDPNWv7brcdkoWtcKZqEg1ZshE7AZhaOZUoLYMX64Cq_uKe6Y7tcKmGf34CoD-pBu6I56s9OGMdk8gS0CWm4FwJYVfYppUjvdbnrW0mY_tW3644OWwOI_N_V_X3-_wqrxj7TfHKhlvzXdcKUId3Pa9RldKF7Mn-pqJAJVkNDybvEl423kxJ73lT01i46ovoBq60-picfQLJCC0_RfqNctONg8siFpkw','<garbage_collected>',none,_12250) at /tmp/.mount_.org.cdCakCk/usr/share/terminusdb/core/api/db_fetch.pl:75\n  [38] '<meta-call>'('<garbage_collected>') <foreign>\n  [37] database:with_transaction_(query_context{all_witnesses:false,authorization:'terminusdb:///system/data/admin',bindings:[],default_collection:database_descriptor{database_name:\""bikes\"",organization_name:\""admin\""},files:[],filter:type_filter{types: ...},prefixes:_12392{api:'http://terminusdb.com/schema/api#',doc:'terminusdb:///repository/data/',layer:'http://terminusdb.com/schema/layer#',owl:'http://www.w3.org/2002/07/owl#',rdf:'http://www.w3.org/1999/02/22-rdf-syntax-ns#',rdfs:'http://www.w3.org/2000/01/rdf-schema#',ref:'http://terminusdb.com/schema/ref#',repo:'http://terminusdb.com/schema/repository#',system:'http://terminusdb.com/schema/system#',terminus:'http://terminusdb.com/schema/system#',vio:'http://terminusdb.com/schema/vio#',woql:'http://terminusdb.com/schema/woql#',xdd:'http://terminusdb.com/schema/xdd#',xsd:'http://www.w3.org/2001/XMLSchema#'},selected:[],system:system_descriptor{},transaction_objects:[...],update_guard:_12366,write_graph:repo_graph{database_name:\""bikes\"",name:\""main\"",organization_name:\""admin\"",type:instance}},db_fetch:(...,...),_12324) at /tmp/.mount_.org.cdCakCk/usr/share/terminusdb/core/transaction/database.pl:213\n  [36] setup_call_catcher_cleanup(database:true,database:with_transaction_(...,...,_12554),_12532,database:(...,...)) at /tmp/.mount_.org.cdCakCk/usr/lib/swi-prolog/boot/init.pl:612\n  [32] db_clone:clone_('<garbage_collected>','terminusdb:///system/data/admin',admin,bikes,'<garbage_collected>','<garbage_collected>',false,'<garbage_collected>','<garbage_collected>',_12620) at /tmp/.mount_.org.cdCakCk/usr/share/terminusdb/core/api/db_clone.pl:55\n  [31] setup_call_catcher_cleanup(db_clone:true,db_clone:clone_(...,'terminusdb:///system/data/admin',admin,bikes,\""TerminusDB Bikes Tutorial\"",\""This is the knowledge graph produced by the TerminusDB Bikes Tutorial which is commonly used as an introduction to TerminusDB. This database contains the final results in a format that you can clone complete with all of the data and structure intact so that you don't have to do it yourself. \"",false,\""https://hub.terminusdb.com/kevin/bikes\"",...,_12690),exception(_12694),db_clone:(...;true)) at /tmp/.mount_.org.cdCakCk/usr/lib/swi-prolog/boot/init.pl:612\n  [30] db_clone:clone('<garbage_collected>','terminusdb:///system/data/admin',admin,bikes,'<garbage_collected>','<garbage_collected>',false,'<garbage_collected>','<garbage_collected>','<garbage_collected>') at /tmp/.mount_.org.cdCakCk/usr/share/terminusdb/core/api/db_clone.pl:17\n  [29] '<meta-call>'('<garbage_collected>') <foreign>\n  [28] catch(routes:(...,...),error(ssl_error('1416F086','SSL routines',tls_process_server_certificate,'certificate verify failed'),context(_12860,_12862)),routes:do_or_die(...,...)) at /tmp/.mount_.org.cdCakCk/usr/lib/swi-prolog/boot/init.pl:530\n  [27] catch_with_backtrace('<garbage_collected>','<garbage_collected>','<garbage_collected>') at /tmp/.mount_.org.cdCakCk/usr/lib/swi-prolog/boot/init.pl:580\n\nNote: some frames are missing due to last-call optimization.\nRe-run your program in debug mode (:- debug.) to get more detail.\n\n"",
    ""api:status"": ""api:server_error"",
    ""end"": ""2021-01-21T16:40:32.348Z"",
    ""start"": ""2021-01-21T16:40:32.348Z"",
    ""duration"": ""2021-01-21T16:40:32.348Z""
  }
}",2021-01-21T23:16:55Z,kghenderson,https://github.com/terminusdb/terminusdb/issues/150#issuecomment-765002510,"I opened localhost connection from an external browser, i.e. not using the app, and am receiving an error - not sure if this is expected, but it might be the cause and/or be related.",i open localhost connection from an external browser ie not use the app and be receive an error - not sure if this be expect but -PRON- may be the cause and/or be related,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,150,2021-01-21T16:46:09Z,kghenderson,cloning ssl error,https://github.com/terminusdb/terminusdb/issues/150,"Describe the bug
Attempting to clone the Bikes Tutorial DB from TerminusHub using the Terminus desktop app on Fedora 33 Linux.
Repository fails to clone with error:
Error: ssl_error('1416F086','SSL routines',tls_process_server_certificate,'certificate verify failed') [46] throw(error(ssl_error('1416F086','SSL routines',tls_process_server_certificate,'certificate verify failed'),_12004)) [44] catch(http:http_protocol_hook(https,...,<stream>(0x7f8110010150,0x7f81100106a0),_12064,...),error(ssl_error('1416F086','SSL routines',tls_process_server_certificate,'certificate verify failed'),_12072),http_open:(...,...)) at /tmp/.mount_.org.cdCakCk/usr/lib/swi-prolog/boot/init.pl:531 [43] http_open:try_http_proxy(direct,[uri('https://hub.terminusdb.com/api/pack/kevin/bikes'),...|...],_12134,'<garbage_collected>') at /tmp/.mount_.org.cdCakCk/usr/lib/swi-prolog/library/http/http_open.pl:448 [41] http_client:http_get('https://hub.terminusdb.com/api/pack/kevin/bikes',_12190,[post(...),...|...]) at /tmp/.mount_.org.cdCakCk/usr/lib/swi-prolog/library/http/http_client.pl:139 [39] db_fetch:authorized_fetch('Bearer eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCIsImtpZCI6InhrMEZTOTdVWVFvbHB4ODREeFlpeCJ9.eyJodHRwOi8vdGVybWludXNkYi5jb20vc2NoZW1hL3N5c3RlbSNhZ2VudF9uYW1lIjoia2doZW5kZXJzb24iLCJodHRwOi8vdGVybWludXNkYi5jb20vc2NoZW1hL3N5c3RlbSN1c2VyX2lkZW50aWZpZXIiOiJrZXZpbkBrZ2hlbmRlcnNvbi5jb20iLCJpc3MiOiJodHRwczovL3Rlcm1pbnVzZGIuZXUuYXV0aDAuY29tLyIsInN1YiI6ImF1dGgwfDVmZWRmZTIwMzkxOWJjMDA3NmE2Y2U2OSIsImF1ZCI6WyJodHRwczovL3Rlcm1pbnVzZGIuY29tL2h1YnNlcnZpY2VzIiwiaHR0cHM6Ly90ZXJtaW51c2RiLmV1LmF1dGgwLmNvbS91c2VyaW5mbyJdLCJpYXQiOjE2MTEyNDcxMDksImV4cCI6MTYxMTI4MzEwOSwiYXpwIjoiNEVZSUhRYVZMZ3dpNVY1bTJnUk5ZZHRwWlpmYnd0RHoiLCJzY29wZSI6Im9wZW5pZCBwcm9maWxlIGVtYWlsIn0.hD6DoOAMj790oN3yzcXWatGR7mbn3oto_vd7kmWy_mobmXwNTQVmOUwNr2Ha-Yd1r6ysqfwipl7aNR-FG3GbmwNyVOleZL8THoiF8jp1p6jKsanzhGUw4Q_EDPNWv7brcdkoWtcKZqEg1ZshE7AZhaOZUoLYMX64Cq_uKe6Y7tcKmGf34CoD-pBu6I56s9OGMdk8gS0CWm4FwJYVfYppUjvdbnrW0mY_tW3644OWwOI_N_V_X3-_wqrxj7TfHKhlvzXdcKUId3Pa9RldKF7Mn-pqJAJVkNDybvEl423kxJ73lT01i46ovoBq60-picfQLJCC0_RfqNctONg8siFpkw','<garbage_collected>',none,_12250) at /tmp/.mount_.org.cdCakCk/usr/share/terminusdb/core/api/db_fetch.pl:75 [38] '<meta-call>'('<garbage_collected>') <foreign> [37] database:with_transaction_(query_context{all_witnesses:false,authorization:'terminusdb:///system/data/admin',bindings:[],default_collection:database_descriptor{database_name:""bikes"",organization_name:""admin""},files:[],filter:type_filter{types: ...},prefixes:_12392{api:'http://terminusdb.com/schema/api#',doc:'terminusdb:///repository/data/',layer:'http://terminusdb.com/schema/layer#',owl:'http://www.w3.org/2002/07/owl#',rdf:'http://www.w3.org/1999/02/22-rdf-syntax-ns#',rdfs:'http://www.w3.org/2000/01/rdf-schema#',ref:'http://terminusdb.com/schema/ref#',repo:'http://terminusdb.com/schema/repository#',system:'http://terminusdb.com/schema/system#',terminus:'http://terminusdb.com/schema/system#',vio:'http://terminusdb.com/schema/vio#',woql:'http://terminusdb.com/schema/woql#',xdd:'http://terminusdb.com/schema/xdd#',xsd:'http://www.w3.org/2001/XMLSchema#'},selected:[],system:system_descriptor{},transaction_objects:[...],update_guard:_12366,write_graph:repo_graph{database_name:""bikes"",name:""main"",organization_name:""admin"",type:instance}},db_fetch:(...,...),_12324) at /tmp/.mount_.org.cdCakCk/usr/share/terminusdb/core/transaction/database.pl:213 [36] setup_call_catcher_cleanup(database:true,database:with_transaction_(...,...,_12554),_12532,database:(...,...)) at /tmp/.mount_.org.cdCakCk/usr/lib/swi-prolog/boot/init.pl:612 [32] db_clone:clone_('<garbage_collected>','terminusdb:///system/data/admin',admin,bikes,'<garbage_collected>','<garbage_collected>',false,'<garbage_collected>','<garbage_collected>',_12620) at /tmp/.mount_.org.cdCakCk/usr/share/terminusdb/core/api/db_clone.pl:55 [31] setup_call_catcher_cleanup(db_clone:true,db_clone:clone_(...,'terminusdb:///system/data/admin',admin,bikes,""TerminusDB Bikes Tutorial"",""This is the knowledge graph produced by the TerminusDB Bikes Tutorial which is commonly used as an introduction to TerminusDB. This database contains the final results in a format that you can clone complete with all of the data and structure intact so that you don't have to do it yourself. "",false,""https://hub.terminusdb.com/kevin/bikes"",...,_12690),exception(_12694),db_clone:(...;true)) at /tmp/.mount_.org.cdCakCk/usr/lib/swi-prolog/boot/init.pl:612 [30] db_clone:clone('<garbage_collected>','terminusdb:///system/data/admin',admin,bikes,'<garbage_collected>','<garbage_collected>',false,'<garbage_collected>','<garbage_collected>','<garbage_collected>') at /tmp/.mount_.org.cdCakCk/usr/share/terminusdb/core/api/db_clone.pl:17 [29] '<meta-call>'('<garbage_collected>') <foreign> [28] catch(routes:(...,...),error(ssl_error('1416F086','SSL routines',tls_process_server_certificate,'certificate verify failed'),context(_12860,_12862)),routes:do_or_die(...,...)) at /tmp/.mount_.org.cdCakCk/usr/lib/swi-prolog/boot/init.pl:530 [27] catch_with_backtrace('<garbage_collected>','<garbage_collected>','<garbage_collected>') at /tmp/.mount_.org.cdCakCk/usr/lib/swi-prolog/boot/init.pl:580 Note: some frames are missing due to last-call optimization. Re-run your program in debug mode (:- debug.) to get more detail.less
{
  ""data"": {
    ""api:message"": ""Error: ssl_error('1416F086','SSL routines',tls_process_server_certificate,'certificate verify failed')\n  [46] throw(error(ssl_error('1416F086','SSL routines',tls_process_server_certificate,'certificate verify failed'),_12004))\n  [44] catch(http:http_protocol_hook(https,...,<stream>(0x7f8110010150,0x7f81100106a0),_12064,...),error(ssl_error('1416F086','SSL routines',tls_process_server_certificate,'certificate verify failed'),_12072),http_open:(...,...)) at /tmp/.mount_.org.cdCakCk/usr/lib/swi-prolog/boot/init.pl:531\n  [43] http_open:try_http_proxy(direct,[uri('https://hub.terminusdb.com/api/pack/kevin/bikes'),...|...],_12134,'<garbage_collected>') at /tmp/.mount_.org.cdCakCk/usr/lib/swi-prolog/library/http/http_open.pl:448\n  [41] http_client:http_get('https://hub.terminusdb.com/api/pack/kevin/bikes',_12190,[post(...),...|...]) at /tmp/.mount_.org.cdCakCk/usr/lib/swi-prolog/library/http/http_client.pl:139\n  [39] db_fetch:authorized_fetch('Bearer eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCIsImtpZCI6InhrMEZTOTdVWVFvbHB4ODREeFlpeCJ9.eyJodHRwOi8vdGVybWludXNkYi5jb20vc2NoZW1hL3N5c3RlbSNhZ2VudF9uYW1lIjoia2doZW5kZXJzb24iLCJodHRwOi8vdGVybWludXNkYi5jb20vc2NoZW1hL3N5c3RlbSN1c2VyX2lkZW50aWZpZXIiOiJrZXZpbkBrZ2hlbmRlcnNvbi5jb20iLCJpc3MiOiJodHRwczovL3Rlcm1pbnVzZGIuZXUuYXV0aDAuY29tLyIsInN1YiI6ImF1dGgwfDVmZWRmZTIwMzkxOWJjMDA3NmE2Y2U2OSIsImF1ZCI6WyJodHRwczovL3Rlcm1pbnVzZGIuY29tL2h1YnNlcnZpY2VzIiwiaHR0cHM6Ly90ZXJtaW51c2RiLmV1LmF1dGgwLmNvbS91c2VyaW5mbyJdLCJpYXQiOjE2MTEyNDcxMDksImV4cCI6MTYxMTI4MzEwOSwiYXpwIjoiNEVZSUhRYVZMZ3dpNVY1bTJnUk5ZZHRwWlpmYnd0RHoiLCJzY29wZSI6Im9wZW5pZCBwcm9maWxlIGVtYWlsIn0.hD6DoOAMj790oN3yzcXWatGR7mbn3oto_vd7kmWy_mobmXwNTQVmOUwNr2Ha-Yd1r6ysqfwipl7aNR-FG3GbmwNyVOleZL8THoiF8jp1p6jKsanzhGUw4Q_EDPNWv7brcdkoWtcKZqEg1ZshE7AZhaOZUoLYMX64Cq_uKe6Y7tcKmGf34CoD-pBu6I56s9OGMdk8gS0CWm4FwJYVfYppUjvdbnrW0mY_tW3644OWwOI_N_V_X3-_wqrxj7TfHKhlvzXdcKUId3Pa9RldKF7Mn-pqJAJVkNDybvEl423kxJ73lT01i46ovoBq60-picfQLJCC0_RfqNctONg8siFpkw','<garbage_collected>',none,_12250) at /tmp/.mount_.org.cdCakCk/usr/share/terminusdb/core/api/db_fetch.pl:75\n  [38] '<meta-call>'('<garbage_collected>') <foreign>\n  [37] database:with_transaction_(query_context{all_witnesses:false,authorization:'terminusdb:///system/data/admin',bindings:[],default_collection:database_descriptor{database_name:\""bikes\"",organization_name:\""admin\""},files:[],filter:type_filter{types: ...},prefixes:_12392{api:'http://terminusdb.com/schema/api#',doc:'terminusdb:///repository/data/',layer:'http://terminusdb.com/schema/layer#',owl:'http://www.w3.org/2002/07/owl#',rdf:'http://www.w3.org/1999/02/22-rdf-syntax-ns#',rdfs:'http://www.w3.org/2000/01/rdf-schema#',ref:'http://terminusdb.com/schema/ref#',repo:'http://terminusdb.com/schema/repository#',system:'http://terminusdb.com/schema/system#',terminus:'http://terminusdb.com/schema/system#',vio:'http://terminusdb.com/schema/vio#',woql:'http://terminusdb.com/schema/woql#',xdd:'http://terminusdb.com/schema/xdd#',xsd:'http://www.w3.org/2001/XMLSchema#'},selected:[],system:system_descriptor{},transaction_objects:[...],update_guard:_12366,write_graph:repo_graph{database_name:\""bikes\"",name:\""main\"",organization_name:\""admin\"",type:instance}},db_fetch:(...,...),_12324) at /tmp/.mount_.org.cdCakCk/usr/share/terminusdb/core/transaction/database.pl:213\n  [36] setup_call_catcher_cleanup(database:true,database:with_transaction_(...,...,_12554),_12532,database:(...,...)) at /tmp/.mount_.org.cdCakCk/usr/lib/swi-prolog/boot/init.pl:612\n  [32] db_clone:clone_('<garbage_collected>','terminusdb:///system/data/admin',admin,bikes,'<garbage_collected>','<garbage_collected>',false,'<garbage_collected>','<garbage_collected>',_12620) at /tmp/.mount_.org.cdCakCk/usr/share/terminusdb/core/api/db_clone.pl:55\n  [31] setup_call_catcher_cleanup(db_clone:true,db_clone:clone_(...,'terminusdb:///system/data/admin',admin,bikes,\""TerminusDB Bikes Tutorial\"",\""This is the knowledge graph produced by the TerminusDB Bikes Tutorial which is commonly used as an introduction to TerminusDB. This database contains the final results in a format that you can clone complete with all of the data and structure intact so that you don't have to do it yourself. \"",false,\""https://hub.terminusdb.com/kevin/bikes\"",...,_12690),exception(_12694),db_clone:(...;true)) at /tmp/.mount_.org.cdCakCk/usr/lib/swi-prolog/boot/init.pl:612\n  [30] db_clone:clone('<garbage_collected>','terminusdb:///system/data/admin',admin,bikes,'<garbage_collected>','<garbage_collected>',false,'<garbage_collected>','<garbage_collected>','<garbage_collected>') at /tmp/.mount_.org.cdCakCk/usr/share/terminusdb/core/api/db_clone.pl:17\n  [29] '<meta-call>'('<garbage_collected>') <foreign>\n  [28] catch(routes:(...,...),error(ssl_error('1416F086','SSL routines',tls_process_server_certificate,'certificate verify failed'),context(_12860,_12862)),routes:do_or_die(...,...)) at /tmp/.mount_.org.cdCakCk/usr/lib/swi-prolog/boot/init.pl:530\n  [27] catch_with_backtrace('<garbage_collected>','<garbage_collected>','<garbage_collected>') at /tmp/.mount_.org.cdCakCk/usr/lib/swi-prolog/boot/init.pl:580\n\nNote: some frames are missing due to last-call optimization.\nRe-run your program in debug mode (:- debug.) to get more detail.\n\n"",
    ""api:status"": ""api:server_error"",
    ""end"": ""2021-01-21T16:40:32.348Z"",
    ""start"": ""2021-01-21T16:40:32.348Z"",
    ""duration"": ""2021-01-21T16:40:32.348Z""
  }
}",2021-01-22T10:19:07Z,kevinchekovfeeney,https://github.com/terminusdb/terminusdb/issues/150#issuecomment-765302337,This is a chrome issue - it refuses to accept https connection to 127.0.0.1 - you have to accept the risks.,this be a chrome issue - -PRON- refuse to accept https connection to 127001 - -PRON- have to accept the risk,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,150,2021-01-21T16:46:09Z,kghenderson,cloning ssl error,https://github.com/terminusdb/terminusdb/issues/150,"Describe the bug
Attempting to clone the Bikes Tutorial DB from TerminusHub using the Terminus desktop app on Fedora 33 Linux.
Repository fails to clone with error:
Error: ssl_error('1416F086','SSL routines',tls_process_server_certificate,'certificate verify failed') [46] throw(error(ssl_error('1416F086','SSL routines',tls_process_server_certificate,'certificate verify failed'),_12004)) [44] catch(http:http_protocol_hook(https,...,<stream>(0x7f8110010150,0x7f81100106a0),_12064,...),error(ssl_error('1416F086','SSL routines',tls_process_server_certificate,'certificate verify failed'),_12072),http_open:(...,...)) at /tmp/.mount_.org.cdCakCk/usr/lib/swi-prolog/boot/init.pl:531 [43] http_open:try_http_proxy(direct,[uri('https://hub.terminusdb.com/api/pack/kevin/bikes'),...|...],_12134,'<garbage_collected>') at /tmp/.mount_.org.cdCakCk/usr/lib/swi-prolog/library/http/http_open.pl:448 [41] http_client:http_get('https://hub.terminusdb.com/api/pack/kevin/bikes',_12190,[post(...),...|...]) at /tmp/.mount_.org.cdCakCk/usr/lib/swi-prolog/library/http/http_client.pl:139 [39] db_fetch:authorized_fetch('Bearer eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCIsImtpZCI6InhrMEZTOTdVWVFvbHB4ODREeFlpeCJ9.eyJodHRwOi8vdGVybWludXNkYi5jb20vc2NoZW1hL3N5c3RlbSNhZ2VudF9uYW1lIjoia2doZW5kZXJzb24iLCJodHRwOi8vdGVybWludXNkYi5jb20vc2NoZW1hL3N5c3RlbSN1c2VyX2lkZW50aWZpZXIiOiJrZXZpbkBrZ2hlbmRlcnNvbi5jb20iLCJpc3MiOiJodHRwczovL3Rlcm1pbnVzZGIuZXUuYXV0aDAuY29tLyIsInN1YiI6ImF1dGgwfDVmZWRmZTIwMzkxOWJjMDA3NmE2Y2U2OSIsImF1ZCI6WyJodHRwczovL3Rlcm1pbnVzZGIuY29tL2h1YnNlcnZpY2VzIiwiaHR0cHM6Ly90ZXJtaW51c2RiLmV1LmF1dGgwLmNvbS91c2VyaW5mbyJdLCJpYXQiOjE2MTEyNDcxMDksImV4cCI6MTYxMTI4MzEwOSwiYXpwIjoiNEVZSUhRYVZMZ3dpNVY1bTJnUk5ZZHRwWlpmYnd0RHoiLCJzY29wZSI6Im9wZW5pZCBwcm9maWxlIGVtYWlsIn0.hD6DoOAMj790oN3yzcXWatGR7mbn3oto_vd7kmWy_mobmXwNTQVmOUwNr2Ha-Yd1r6ysqfwipl7aNR-FG3GbmwNyVOleZL8THoiF8jp1p6jKsanzhGUw4Q_EDPNWv7brcdkoWtcKZqEg1ZshE7AZhaOZUoLYMX64Cq_uKe6Y7tcKmGf34CoD-pBu6I56s9OGMdk8gS0CWm4FwJYVfYppUjvdbnrW0mY_tW3644OWwOI_N_V_X3-_wqrxj7TfHKhlvzXdcKUId3Pa9RldKF7Mn-pqJAJVkNDybvEl423kxJ73lT01i46ovoBq60-picfQLJCC0_RfqNctONg8siFpkw','<garbage_collected>',none,_12250) at /tmp/.mount_.org.cdCakCk/usr/share/terminusdb/core/api/db_fetch.pl:75 [38] '<meta-call>'('<garbage_collected>') <foreign> [37] database:with_transaction_(query_context{all_witnesses:false,authorization:'terminusdb:///system/data/admin',bindings:[],default_collection:database_descriptor{database_name:""bikes"",organization_name:""admin""},files:[],filter:type_filter{types: ...},prefixes:_12392{api:'http://terminusdb.com/schema/api#',doc:'terminusdb:///repository/data/',layer:'http://terminusdb.com/schema/layer#',owl:'http://www.w3.org/2002/07/owl#',rdf:'http://www.w3.org/1999/02/22-rdf-syntax-ns#',rdfs:'http://www.w3.org/2000/01/rdf-schema#',ref:'http://terminusdb.com/schema/ref#',repo:'http://terminusdb.com/schema/repository#',system:'http://terminusdb.com/schema/system#',terminus:'http://terminusdb.com/schema/system#',vio:'http://terminusdb.com/schema/vio#',woql:'http://terminusdb.com/schema/woql#',xdd:'http://terminusdb.com/schema/xdd#',xsd:'http://www.w3.org/2001/XMLSchema#'},selected:[],system:system_descriptor{},transaction_objects:[...],update_guard:_12366,write_graph:repo_graph{database_name:""bikes"",name:""main"",organization_name:""admin"",type:instance}},db_fetch:(...,...),_12324) at /tmp/.mount_.org.cdCakCk/usr/share/terminusdb/core/transaction/database.pl:213 [36] setup_call_catcher_cleanup(database:true,database:with_transaction_(...,...,_12554),_12532,database:(...,...)) at /tmp/.mount_.org.cdCakCk/usr/lib/swi-prolog/boot/init.pl:612 [32] db_clone:clone_('<garbage_collected>','terminusdb:///system/data/admin',admin,bikes,'<garbage_collected>','<garbage_collected>',false,'<garbage_collected>','<garbage_collected>',_12620) at /tmp/.mount_.org.cdCakCk/usr/share/terminusdb/core/api/db_clone.pl:55 [31] setup_call_catcher_cleanup(db_clone:true,db_clone:clone_(...,'terminusdb:///system/data/admin',admin,bikes,""TerminusDB Bikes Tutorial"",""This is the knowledge graph produced by the TerminusDB Bikes Tutorial which is commonly used as an introduction to TerminusDB. This database contains the final results in a format that you can clone complete with all of the data and structure intact so that you don't have to do it yourself. "",false,""https://hub.terminusdb.com/kevin/bikes"",...,_12690),exception(_12694),db_clone:(...;true)) at /tmp/.mount_.org.cdCakCk/usr/lib/swi-prolog/boot/init.pl:612 [30] db_clone:clone('<garbage_collected>','terminusdb:///system/data/admin',admin,bikes,'<garbage_collected>','<garbage_collected>',false,'<garbage_collected>','<garbage_collected>','<garbage_collected>') at /tmp/.mount_.org.cdCakCk/usr/share/terminusdb/core/api/db_clone.pl:17 [29] '<meta-call>'('<garbage_collected>') <foreign> [28] catch(routes:(...,...),error(ssl_error('1416F086','SSL routines',tls_process_server_certificate,'certificate verify failed'),context(_12860,_12862)),routes:do_or_die(...,...)) at /tmp/.mount_.org.cdCakCk/usr/lib/swi-prolog/boot/init.pl:530 [27] catch_with_backtrace('<garbage_collected>','<garbage_collected>','<garbage_collected>') at /tmp/.mount_.org.cdCakCk/usr/lib/swi-prolog/boot/init.pl:580 Note: some frames are missing due to last-call optimization. Re-run your program in debug mode (:- debug.) to get more detail.less
{
  ""data"": {
    ""api:message"": ""Error: ssl_error('1416F086','SSL routines',tls_process_server_certificate,'certificate verify failed')\n  [46] throw(error(ssl_error('1416F086','SSL routines',tls_process_server_certificate,'certificate verify failed'),_12004))\n  [44] catch(http:http_protocol_hook(https,...,<stream>(0x7f8110010150,0x7f81100106a0),_12064,...),error(ssl_error('1416F086','SSL routines',tls_process_server_certificate,'certificate verify failed'),_12072),http_open:(...,...)) at /tmp/.mount_.org.cdCakCk/usr/lib/swi-prolog/boot/init.pl:531\n  [43] http_open:try_http_proxy(direct,[uri('https://hub.terminusdb.com/api/pack/kevin/bikes'),...|...],_12134,'<garbage_collected>') at /tmp/.mount_.org.cdCakCk/usr/lib/swi-prolog/library/http/http_open.pl:448\n  [41] http_client:http_get('https://hub.terminusdb.com/api/pack/kevin/bikes',_12190,[post(...),...|...]) at /tmp/.mount_.org.cdCakCk/usr/lib/swi-prolog/library/http/http_client.pl:139\n  [39] db_fetch:authorized_fetch('Bearer eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCIsImtpZCI6InhrMEZTOTdVWVFvbHB4ODREeFlpeCJ9.eyJodHRwOi8vdGVybWludXNkYi5jb20vc2NoZW1hL3N5c3RlbSNhZ2VudF9uYW1lIjoia2doZW5kZXJzb24iLCJodHRwOi8vdGVybWludXNkYi5jb20vc2NoZW1hL3N5c3RlbSN1c2VyX2lkZW50aWZpZXIiOiJrZXZpbkBrZ2hlbmRlcnNvbi5jb20iLCJpc3MiOiJodHRwczovL3Rlcm1pbnVzZGIuZXUuYXV0aDAuY29tLyIsInN1YiI6ImF1dGgwfDVmZWRmZTIwMzkxOWJjMDA3NmE2Y2U2OSIsImF1ZCI6WyJodHRwczovL3Rlcm1pbnVzZGIuY29tL2h1YnNlcnZpY2VzIiwiaHR0cHM6Ly90ZXJtaW51c2RiLmV1LmF1dGgwLmNvbS91c2VyaW5mbyJdLCJpYXQiOjE2MTEyNDcxMDksImV4cCI6MTYxMTI4MzEwOSwiYXpwIjoiNEVZSUhRYVZMZ3dpNVY1bTJnUk5ZZHRwWlpmYnd0RHoiLCJzY29wZSI6Im9wZW5pZCBwcm9maWxlIGVtYWlsIn0.hD6DoOAMj790oN3yzcXWatGR7mbn3oto_vd7kmWy_mobmXwNTQVmOUwNr2Ha-Yd1r6ysqfwipl7aNR-FG3GbmwNyVOleZL8THoiF8jp1p6jKsanzhGUw4Q_EDPNWv7brcdkoWtcKZqEg1ZshE7AZhaOZUoLYMX64Cq_uKe6Y7tcKmGf34CoD-pBu6I56s9OGMdk8gS0CWm4FwJYVfYppUjvdbnrW0mY_tW3644OWwOI_N_V_X3-_wqrxj7TfHKhlvzXdcKUId3Pa9RldKF7Mn-pqJAJVkNDybvEl423kxJ73lT01i46ovoBq60-picfQLJCC0_RfqNctONg8siFpkw','<garbage_collected>',none,_12250) at /tmp/.mount_.org.cdCakCk/usr/share/terminusdb/core/api/db_fetch.pl:75\n  [38] '<meta-call>'('<garbage_collected>') <foreign>\n  [37] database:with_transaction_(query_context{all_witnesses:false,authorization:'terminusdb:///system/data/admin',bindings:[],default_collection:database_descriptor{database_name:\""bikes\"",organization_name:\""admin\""},files:[],filter:type_filter{types: ...},prefixes:_12392{api:'http://terminusdb.com/schema/api#',doc:'terminusdb:///repository/data/',layer:'http://terminusdb.com/schema/layer#',owl:'http://www.w3.org/2002/07/owl#',rdf:'http://www.w3.org/1999/02/22-rdf-syntax-ns#',rdfs:'http://www.w3.org/2000/01/rdf-schema#',ref:'http://terminusdb.com/schema/ref#',repo:'http://terminusdb.com/schema/repository#',system:'http://terminusdb.com/schema/system#',terminus:'http://terminusdb.com/schema/system#',vio:'http://terminusdb.com/schema/vio#',woql:'http://terminusdb.com/schema/woql#',xdd:'http://terminusdb.com/schema/xdd#',xsd:'http://www.w3.org/2001/XMLSchema#'},selected:[],system:system_descriptor{},transaction_objects:[...],update_guard:_12366,write_graph:repo_graph{database_name:\""bikes\"",name:\""main\"",organization_name:\""admin\"",type:instance}},db_fetch:(...,...),_12324) at /tmp/.mount_.org.cdCakCk/usr/share/terminusdb/core/transaction/database.pl:213\n  [36] setup_call_catcher_cleanup(database:true,database:with_transaction_(...,...,_12554),_12532,database:(...,...)) at /tmp/.mount_.org.cdCakCk/usr/lib/swi-prolog/boot/init.pl:612\n  [32] db_clone:clone_('<garbage_collected>','terminusdb:///system/data/admin',admin,bikes,'<garbage_collected>','<garbage_collected>',false,'<garbage_collected>','<garbage_collected>',_12620) at /tmp/.mount_.org.cdCakCk/usr/share/terminusdb/core/api/db_clone.pl:55\n  [31] setup_call_catcher_cleanup(db_clone:true,db_clone:clone_(...,'terminusdb:///system/data/admin',admin,bikes,\""TerminusDB Bikes Tutorial\"",\""This is the knowledge graph produced by the TerminusDB Bikes Tutorial which is commonly used as an introduction to TerminusDB. This database contains the final results in a format that you can clone complete with all of the data and structure intact so that you don't have to do it yourself. \"",false,\""https://hub.terminusdb.com/kevin/bikes\"",...,_12690),exception(_12694),db_clone:(...;true)) at /tmp/.mount_.org.cdCakCk/usr/lib/swi-prolog/boot/init.pl:612\n  [30] db_clone:clone('<garbage_collected>','terminusdb:///system/data/admin',admin,bikes,'<garbage_collected>','<garbage_collected>',false,'<garbage_collected>','<garbage_collected>','<garbage_collected>') at /tmp/.mount_.org.cdCakCk/usr/share/terminusdb/core/api/db_clone.pl:17\n  [29] '<meta-call>'('<garbage_collected>') <foreign>\n  [28] catch(routes:(...,...),error(ssl_error('1416F086','SSL routines',tls_process_server_certificate,'certificate verify failed'),context(_12860,_12862)),routes:do_or_die(...,...)) at /tmp/.mount_.org.cdCakCk/usr/lib/swi-prolog/boot/init.pl:530\n  [27] catch_with_backtrace('<garbage_collected>','<garbage_collected>','<garbage_collected>') at /tmp/.mount_.org.cdCakCk/usr/lib/swi-prolog/boot/init.pl:580\n\nNote: some frames are missing due to last-call optimization.\nRe-run your program in debug mode (:- debug.) to get more detail.\n\n"",
    ""api:status"": ""api:server_error"",
    ""end"": ""2021-01-21T16:40:32.348Z"",
    ""start"": ""2021-01-21T16:40:32.348Z"",
    ""duration"": ""2021-01-21T16:40:32.348Z""
  }
}",2021-01-22T15:04:37Z,kghenderson,https://github.com/terminusdb/terminusdb/issues/150#issuecomment-765466758,"OK, thanks - but to be clear, my issue that that the TerminusDB AppImage won't clone or sync with TerminusHub - I was just trying to explore it a bit further :-)",ok thank - but to be clear -PRON- issue that that the terminusdb appimage will not clone or sync with terminushub - i be just try to explore -PRON- a bit far - ),0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,150,2021-01-21T16:46:09Z,kghenderson,cloning ssl error,https://github.com/terminusdb/terminusdb/issues/150,"Describe the bug
Attempting to clone the Bikes Tutorial DB from TerminusHub using the Terminus desktop app on Fedora 33 Linux.
Repository fails to clone with error:
Error: ssl_error('1416F086','SSL routines',tls_process_server_certificate,'certificate verify failed') [46] throw(error(ssl_error('1416F086','SSL routines',tls_process_server_certificate,'certificate verify failed'),_12004)) [44] catch(http:http_protocol_hook(https,...,<stream>(0x7f8110010150,0x7f81100106a0),_12064,...),error(ssl_error('1416F086','SSL routines',tls_process_server_certificate,'certificate verify failed'),_12072),http_open:(...,...)) at /tmp/.mount_.org.cdCakCk/usr/lib/swi-prolog/boot/init.pl:531 [43] http_open:try_http_proxy(direct,[uri('https://hub.terminusdb.com/api/pack/kevin/bikes'),...|...],_12134,'<garbage_collected>') at /tmp/.mount_.org.cdCakCk/usr/lib/swi-prolog/library/http/http_open.pl:448 [41] http_client:http_get('https://hub.terminusdb.com/api/pack/kevin/bikes',_12190,[post(...),...|...]) at /tmp/.mount_.org.cdCakCk/usr/lib/swi-prolog/library/http/http_client.pl:139 [39] db_fetch:authorized_fetch('Bearer eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCIsImtpZCI6InhrMEZTOTdVWVFvbHB4ODREeFlpeCJ9.eyJodHRwOi8vdGVybWludXNkYi5jb20vc2NoZW1hL3N5c3RlbSNhZ2VudF9uYW1lIjoia2doZW5kZXJzb24iLCJodHRwOi8vdGVybWludXNkYi5jb20vc2NoZW1hL3N5c3RlbSN1c2VyX2lkZW50aWZpZXIiOiJrZXZpbkBrZ2hlbmRlcnNvbi5jb20iLCJpc3MiOiJodHRwczovL3Rlcm1pbnVzZGIuZXUuYXV0aDAuY29tLyIsInN1YiI6ImF1dGgwfDVmZWRmZTIwMzkxOWJjMDA3NmE2Y2U2OSIsImF1ZCI6WyJodHRwczovL3Rlcm1pbnVzZGIuY29tL2h1YnNlcnZpY2VzIiwiaHR0cHM6Ly90ZXJtaW51c2RiLmV1LmF1dGgwLmNvbS91c2VyaW5mbyJdLCJpYXQiOjE2MTEyNDcxMDksImV4cCI6MTYxMTI4MzEwOSwiYXpwIjoiNEVZSUhRYVZMZ3dpNVY1bTJnUk5ZZHRwWlpmYnd0RHoiLCJzY29wZSI6Im9wZW5pZCBwcm9maWxlIGVtYWlsIn0.hD6DoOAMj790oN3yzcXWatGR7mbn3oto_vd7kmWy_mobmXwNTQVmOUwNr2Ha-Yd1r6ysqfwipl7aNR-FG3GbmwNyVOleZL8THoiF8jp1p6jKsanzhGUw4Q_EDPNWv7brcdkoWtcKZqEg1ZshE7AZhaOZUoLYMX64Cq_uKe6Y7tcKmGf34CoD-pBu6I56s9OGMdk8gS0CWm4FwJYVfYppUjvdbnrW0mY_tW3644OWwOI_N_V_X3-_wqrxj7TfHKhlvzXdcKUId3Pa9RldKF7Mn-pqJAJVkNDybvEl423kxJ73lT01i46ovoBq60-picfQLJCC0_RfqNctONg8siFpkw','<garbage_collected>',none,_12250) at /tmp/.mount_.org.cdCakCk/usr/share/terminusdb/core/api/db_fetch.pl:75 [38] '<meta-call>'('<garbage_collected>') <foreign> [37] database:with_transaction_(query_context{all_witnesses:false,authorization:'terminusdb:///system/data/admin',bindings:[],default_collection:database_descriptor{database_name:""bikes"",organization_name:""admin""},files:[],filter:type_filter{types: ...},prefixes:_12392{api:'http://terminusdb.com/schema/api#',doc:'terminusdb:///repository/data/',layer:'http://terminusdb.com/schema/layer#',owl:'http://www.w3.org/2002/07/owl#',rdf:'http://www.w3.org/1999/02/22-rdf-syntax-ns#',rdfs:'http://www.w3.org/2000/01/rdf-schema#',ref:'http://terminusdb.com/schema/ref#',repo:'http://terminusdb.com/schema/repository#',system:'http://terminusdb.com/schema/system#',terminus:'http://terminusdb.com/schema/system#',vio:'http://terminusdb.com/schema/vio#',woql:'http://terminusdb.com/schema/woql#',xdd:'http://terminusdb.com/schema/xdd#',xsd:'http://www.w3.org/2001/XMLSchema#'},selected:[],system:system_descriptor{},transaction_objects:[...],update_guard:_12366,write_graph:repo_graph{database_name:""bikes"",name:""main"",organization_name:""admin"",type:instance}},db_fetch:(...,...),_12324) at /tmp/.mount_.org.cdCakCk/usr/share/terminusdb/core/transaction/database.pl:213 [36] setup_call_catcher_cleanup(database:true,database:with_transaction_(...,...,_12554),_12532,database:(...,...)) at /tmp/.mount_.org.cdCakCk/usr/lib/swi-prolog/boot/init.pl:612 [32] db_clone:clone_('<garbage_collected>','terminusdb:///system/data/admin',admin,bikes,'<garbage_collected>','<garbage_collected>',false,'<garbage_collected>','<garbage_collected>',_12620) at /tmp/.mount_.org.cdCakCk/usr/share/terminusdb/core/api/db_clone.pl:55 [31] setup_call_catcher_cleanup(db_clone:true,db_clone:clone_(...,'terminusdb:///system/data/admin',admin,bikes,""TerminusDB Bikes Tutorial"",""This is the knowledge graph produced by the TerminusDB Bikes Tutorial which is commonly used as an introduction to TerminusDB. This database contains the final results in a format that you can clone complete with all of the data and structure intact so that you don't have to do it yourself. "",false,""https://hub.terminusdb.com/kevin/bikes"",...,_12690),exception(_12694),db_clone:(...;true)) at /tmp/.mount_.org.cdCakCk/usr/lib/swi-prolog/boot/init.pl:612 [30] db_clone:clone('<garbage_collected>','terminusdb:///system/data/admin',admin,bikes,'<garbage_collected>','<garbage_collected>',false,'<garbage_collected>','<garbage_collected>','<garbage_collected>') at /tmp/.mount_.org.cdCakCk/usr/share/terminusdb/core/api/db_clone.pl:17 [29] '<meta-call>'('<garbage_collected>') <foreign> [28] catch(routes:(...,...),error(ssl_error('1416F086','SSL routines',tls_process_server_certificate,'certificate verify failed'),context(_12860,_12862)),routes:do_or_die(...,...)) at /tmp/.mount_.org.cdCakCk/usr/lib/swi-prolog/boot/init.pl:530 [27] catch_with_backtrace('<garbage_collected>','<garbage_collected>','<garbage_collected>') at /tmp/.mount_.org.cdCakCk/usr/lib/swi-prolog/boot/init.pl:580 Note: some frames are missing due to last-call optimization. Re-run your program in debug mode (:- debug.) to get more detail.less
{
  ""data"": {
    ""api:message"": ""Error: ssl_error('1416F086','SSL routines',tls_process_server_certificate,'certificate verify failed')\n  [46] throw(error(ssl_error('1416F086','SSL routines',tls_process_server_certificate,'certificate verify failed'),_12004))\n  [44] catch(http:http_protocol_hook(https,...,<stream>(0x7f8110010150,0x7f81100106a0),_12064,...),error(ssl_error('1416F086','SSL routines',tls_process_server_certificate,'certificate verify failed'),_12072),http_open:(...,...)) at /tmp/.mount_.org.cdCakCk/usr/lib/swi-prolog/boot/init.pl:531\n  [43] http_open:try_http_proxy(direct,[uri('https://hub.terminusdb.com/api/pack/kevin/bikes'),...|...],_12134,'<garbage_collected>') at /tmp/.mount_.org.cdCakCk/usr/lib/swi-prolog/library/http/http_open.pl:448\n  [41] http_client:http_get('https://hub.terminusdb.com/api/pack/kevin/bikes',_12190,[post(...),...|...]) at /tmp/.mount_.org.cdCakCk/usr/lib/swi-prolog/library/http/http_client.pl:139\n  [39] db_fetch:authorized_fetch('Bearer eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCIsImtpZCI6InhrMEZTOTdVWVFvbHB4ODREeFlpeCJ9.eyJodHRwOi8vdGVybWludXNkYi5jb20vc2NoZW1hL3N5c3RlbSNhZ2VudF9uYW1lIjoia2doZW5kZXJzb24iLCJodHRwOi8vdGVybWludXNkYi5jb20vc2NoZW1hL3N5c3RlbSN1c2VyX2lkZW50aWZpZXIiOiJrZXZpbkBrZ2hlbmRlcnNvbi5jb20iLCJpc3MiOiJodHRwczovL3Rlcm1pbnVzZGIuZXUuYXV0aDAuY29tLyIsInN1YiI6ImF1dGgwfDVmZWRmZTIwMzkxOWJjMDA3NmE2Y2U2OSIsImF1ZCI6WyJodHRwczovL3Rlcm1pbnVzZGIuY29tL2h1YnNlcnZpY2VzIiwiaHR0cHM6Ly90ZXJtaW51c2RiLmV1LmF1dGgwLmNvbS91c2VyaW5mbyJdLCJpYXQiOjE2MTEyNDcxMDksImV4cCI6MTYxMTI4MzEwOSwiYXpwIjoiNEVZSUhRYVZMZ3dpNVY1bTJnUk5ZZHRwWlpmYnd0RHoiLCJzY29wZSI6Im9wZW5pZCBwcm9maWxlIGVtYWlsIn0.hD6DoOAMj790oN3yzcXWatGR7mbn3oto_vd7kmWy_mobmXwNTQVmOUwNr2Ha-Yd1r6ysqfwipl7aNR-FG3GbmwNyVOleZL8THoiF8jp1p6jKsanzhGUw4Q_EDPNWv7brcdkoWtcKZqEg1ZshE7AZhaOZUoLYMX64Cq_uKe6Y7tcKmGf34CoD-pBu6I56s9OGMdk8gS0CWm4FwJYVfYppUjvdbnrW0mY_tW3644OWwOI_N_V_X3-_wqrxj7TfHKhlvzXdcKUId3Pa9RldKF7Mn-pqJAJVkNDybvEl423kxJ73lT01i46ovoBq60-picfQLJCC0_RfqNctONg8siFpkw','<garbage_collected>',none,_12250) at /tmp/.mount_.org.cdCakCk/usr/share/terminusdb/core/api/db_fetch.pl:75\n  [38] '<meta-call>'('<garbage_collected>') <foreign>\n  [37] database:with_transaction_(query_context{all_witnesses:false,authorization:'terminusdb:///system/data/admin',bindings:[],default_collection:database_descriptor{database_name:\""bikes\"",organization_name:\""admin\""},files:[],filter:type_filter{types: ...},prefixes:_12392{api:'http://terminusdb.com/schema/api#',doc:'terminusdb:///repository/data/',layer:'http://terminusdb.com/schema/layer#',owl:'http://www.w3.org/2002/07/owl#',rdf:'http://www.w3.org/1999/02/22-rdf-syntax-ns#',rdfs:'http://www.w3.org/2000/01/rdf-schema#',ref:'http://terminusdb.com/schema/ref#',repo:'http://terminusdb.com/schema/repository#',system:'http://terminusdb.com/schema/system#',terminus:'http://terminusdb.com/schema/system#',vio:'http://terminusdb.com/schema/vio#',woql:'http://terminusdb.com/schema/woql#',xdd:'http://terminusdb.com/schema/xdd#',xsd:'http://www.w3.org/2001/XMLSchema#'},selected:[],system:system_descriptor{},transaction_objects:[...],update_guard:_12366,write_graph:repo_graph{database_name:\""bikes\"",name:\""main\"",organization_name:\""admin\"",type:instance}},db_fetch:(...,...),_12324) at /tmp/.mount_.org.cdCakCk/usr/share/terminusdb/core/transaction/database.pl:213\n  [36] setup_call_catcher_cleanup(database:true,database:with_transaction_(...,...,_12554),_12532,database:(...,...)) at /tmp/.mount_.org.cdCakCk/usr/lib/swi-prolog/boot/init.pl:612\n  [32] db_clone:clone_('<garbage_collected>','terminusdb:///system/data/admin',admin,bikes,'<garbage_collected>','<garbage_collected>',false,'<garbage_collected>','<garbage_collected>',_12620) at /tmp/.mount_.org.cdCakCk/usr/share/terminusdb/core/api/db_clone.pl:55\n  [31] setup_call_catcher_cleanup(db_clone:true,db_clone:clone_(...,'terminusdb:///system/data/admin',admin,bikes,\""TerminusDB Bikes Tutorial\"",\""This is the knowledge graph produced by the TerminusDB Bikes Tutorial which is commonly used as an introduction to TerminusDB. This database contains the final results in a format that you can clone complete with all of the data and structure intact so that you don't have to do it yourself. \"",false,\""https://hub.terminusdb.com/kevin/bikes\"",...,_12690),exception(_12694),db_clone:(...;true)) at /tmp/.mount_.org.cdCakCk/usr/lib/swi-prolog/boot/init.pl:612\n  [30] db_clone:clone('<garbage_collected>','terminusdb:///system/data/admin',admin,bikes,'<garbage_collected>','<garbage_collected>',false,'<garbage_collected>','<garbage_collected>','<garbage_collected>') at /tmp/.mount_.org.cdCakCk/usr/share/terminusdb/core/api/db_clone.pl:17\n  [29] '<meta-call>'('<garbage_collected>') <foreign>\n  [28] catch(routes:(...,...),error(ssl_error('1416F086','SSL routines',tls_process_server_certificate,'certificate verify failed'),context(_12860,_12862)),routes:do_or_die(...,...)) at /tmp/.mount_.org.cdCakCk/usr/lib/swi-prolog/boot/init.pl:530\n  [27] catch_with_backtrace('<garbage_collected>','<garbage_collected>','<garbage_collected>') at /tmp/.mount_.org.cdCakCk/usr/lib/swi-prolog/boot/init.pl:580\n\nNote: some frames are missing due to last-call optimization.\nRe-run your program in debug mode (:- debug.) to get more detail.\n\n"",
    ""api:status"": ""api:server_error"",
    ""end"": ""2021-01-21T16:40:32.348Z"",
    ""start"": ""2021-01-21T16:40:32.348Z"",
    ""duration"": ""2021-01-21T16:40:32.348Z""
  }
}",2021-01-22T15:06:08Z,luke-feeney,https://github.com/terminusdb/terminusdb/issues/150#issuecomment-765467872,"Are you still having that issue? We saw some problems over the last while, but seem to be resolved (at least on windows and bootstrap!)",be -PRON- still have that issue -PRON- see some problem over the last while but seem to be resolve ( at least on window and bootstrap ),0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,150,2021-01-21T16:46:09Z,kghenderson,cloning ssl error,https://github.com/terminusdb/terminusdb/issues/150,"Describe the bug
Attempting to clone the Bikes Tutorial DB from TerminusHub using the Terminus desktop app on Fedora 33 Linux.
Repository fails to clone with error:
Error: ssl_error('1416F086','SSL routines',tls_process_server_certificate,'certificate verify failed') [46] throw(error(ssl_error('1416F086','SSL routines',tls_process_server_certificate,'certificate verify failed'),_12004)) [44] catch(http:http_protocol_hook(https,...,<stream>(0x7f8110010150,0x7f81100106a0),_12064,...),error(ssl_error('1416F086','SSL routines',tls_process_server_certificate,'certificate verify failed'),_12072),http_open:(...,...)) at /tmp/.mount_.org.cdCakCk/usr/lib/swi-prolog/boot/init.pl:531 [43] http_open:try_http_proxy(direct,[uri('https://hub.terminusdb.com/api/pack/kevin/bikes'),...|...],_12134,'<garbage_collected>') at /tmp/.mount_.org.cdCakCk/usr/lib/swi-prolog/library/http/http_open.pl:448 [41] http_client:http_get('https://hub.terminusdb.com/api/pack/kevin/bikes',_12190,[post(...),...|...]) at /tmp/.mount_.org.cdCakCk/usr/lib/swi-prolog/library/http/http_client.pl:139 [39] db_fetch:authorized_fetch('Bearer eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCIsImtpZCI6InhrMEZTOTdVWVFvbHB4ODREeFlpeCJ9.eyJodHRwOi8vdGVybWludXNkYi5jb20vc2NoZW1hL3N5c3RlbSNhZ2VudF9uYW1lIjoia2doZW5kZXJzb24iLCJodHRwOi8vdGVybWludXNkYi5jb20vc2NoZW1hL3N5c3RlbSN1c2VyX2lkZW50aWZpZXIiOiJrZXZpbkBrZ2hlbmRlcnNvbi5jb20iLCJpc3MiOiJodHRwczovL3Rlcm1pbnVzZGIuZXUuYXV0aDAuY29tLyIsInN1YiI6ImF1dGgwfDVmZWRmZTIwMzkxOWJjMDA3NmE2Y2U2OSIsImF1ZCI6WyJodHRwczovL3Rlcm1pbnVzZGIuY29tL2h1YnNlcnZpY2VzIiwiaHR0cHM6Ly90ZXJtaW51c2RiLmV1LmF1dGgwLmNvbS91c2VyaW5mbyJdLCJpYXQiOjE2MTEyNDcxMDksImV4cCI6MTYxMTI4MzEwOSwiYXpwIjoiNEVZSUhRYVZMZ3dpNVY1bTJnUk5ZZHRwWlpmYnd0RHoiLCJzY29wZSI6Im9wZW5pZCBwcm9maWxlIGVtYWlsIn0.hD6DoOAMj790oN3yzcXWatGR7mbn3oto_vd7kmWy_mobmXwNTQVmOUwNr2Ha-Yd1r6ysqfwipl7aNR-FG3GbmwNyVOleZL8THoiF8jp1p6jKsanzhGUw4Q_EDPNWv7brcdkoWtcKZqEg1ZshE7AZhaOZUoLYMX64Cq_uKe6Y7tcKmGf34CoD-pBu6I56s9OGMdk8gS0CWm4FwJYVfYppUjvdbnrW0mY_tW3644OWwOI_N_V_X3-_wqrxj7TfHKhlvzXdcKUId3Pa9RldKF7Mn-pqJAJVkNDybvEl423kxJ73lT01i46ovoBq60-picfQLJCC0_RfqNctONg8siFpkw','<garbage_collected>',none,_12250) at /tmp/.mount_.org.cdCakCk/usr/share/terminusdb/core/api/db_fetch.pl:75 [38] '<meta-call>'('<garbage_collected>') <foreign> [37] database:with_transaction_(query_context{all_witnesses:false,authorization:'terminusdb:///system/data/admin',bindings:[],default_collection:database_descriptor{database_name:""bikes"",organization_name:""admin""},files:[],filter:type_filter{types: ...},prefixes:_12392{api:'http://terminusdb.com/schema/api#',doc:'terminusdb:///repository/data/',layer:'http://terminusdb.com/schema/layer#',owl:'http://www.w3.org/2002/07/owl#',rdf:'http://www.w3.org/1999/02/22-rdf-syntax-ns#',rdfs:'http://www.w3.org/2000/01/rdf-schema#',ref:'http://terminusdb.com/schema/ref#',repo:'http://terminusdb.com/schema/repository#',system:'http://terminusdb.com/schema/system#',terminus:'http://terminusdb.com/schema/system#',vio:'http://terminusdb.com/schema/vio#',woql:'http://terminusdb.com/schema/woql#',xdd:'http://terminusdb.com/schema/xdd#',xsd:'http://www.w3.org/2001/XMLSchema#'},selected:[],system:system_descriptor{},transaction_objects:[...],update_guard:_12366,write_graph:repo_graph{database_name:""bikes"",name:""main"",organization_name:""admin"",type:instance}},db_fetch:(...,...),_12324) at /tmp/.mount_.org.cdCakCk/usr/share/terminusdb/core/transaction/database.pl:213 [36] setup_call_catcher_cleanup(database:true,database:with_transaction_(...,...,_12554),_12532,database:(...,...)) at /tmp/.mount_.org.cdCakCk/usr/lib/swi-prolog/boot/init.pl:612 [32] db_clone:clone_('<garbage_collected>','terminusdb:///system/data/admin',admin,bikes,'<garbage_collected>','<garbage_collected>',false,'<garbage_collected>','<garbage_collected>',_12620) at /tmp/.mount_.org.cdCakCk/usr/share/terminusdb/core/api/db_clone.pl:55 [31] setup_call_catcher_cleanup(db_clone:true,db_clone:clone_(...,'terminusdb:///system/data/admin',admin,bikes,""TerminusDB Bikes Tutorial"",""This is the knowledge graph produced by the TerminusDB Bikes Tutorial which is commonly used as an introduction to TerminusDB. This database contains the final results in a format that you can clone complete with all of the data and structure intact so that you don't have to do it yourself. "",false,""https://hub.terminusdb.com/kevin/bikes"",...,_12690),exception(_12694),db_clone:(...;true)) at /tmp/.mount_.org.cdCakCk/usr/lib/swi-prolog/boot/init.pl:612 [30] db_clone:clone('<garbage_collected>','terminusdb:///system/data/admin',admin,bikes,'<garbage_collected>','<garbage_collected>',false,'<garbage_collected>','<garbage_collected>','<garbage_collected>') at /tmp/.mount_.org.cdCakCk/usr/share/terminusdb/core/api/db_clone.pl:17 [29] '<meta-call>'('<garbage_collected>') <foreign> [28] catch(routes:(...,...),error(ssl_error('1416F086','SSL routines',tls_process_server_certificate,'certificate verify failed'),context(_12860,_12862)),routes:do_or_die(...,...)) at /tmp/.mount_.org.cdCakCk/usr/lib/swi-prolog/boot/init.pl:530 [27] catch_with_backtrace('<garbage_collected>','<garbage_collected>','<garbage_collected>') at /tmp/.mount_.org.cdCakCk/usr/lib/swi-prolog/boot/init.pl:580 Note: some frames are missing due to last-call optimization. Re-run your program in debug mode (:- debug.) to get more detail.less
{
  ""data"": {
    ""api:message"": ""Error: ssl_error('1416F086','SSL routines',tls_process_server_certificate,'certificate verify failed')\n  [46] throw(error(ssl_error('1416F086','SSL routines',tls_process_server_certificate,'certificate verify failed'),_12004))\n  [44] catch(http:http_protocol_hook(https,...,<stream>(0x7f8110010150,0x7f81100106a0),_12064,...),error(ssl_error('1416F086','SSL routines',tls_process_server_certificate,'certificate verify failed'),_12072),http_open:(...,...)) at /tmp/.mount_.org.cdCakCk/usr/lib/swi-prolog/boot/init.pl:531\n  [43] http_open:try_http_proxy(direct,[uri('https://hub.terminusdb.com/api/pack/kevin/bikes'),...|...],_12134,'<garbage_collected>') at /tmp/.mount_.org.cdCakCk/usr/lib/swi-prolog/library/http/http_open.pl:448\n  [41] http_client:http_get('https://hub.terminusdb.com/api/pack/kevin/bikes',_12190,[post(...),...|...]) at /tmp/.mount_.org.cdCakCk/usr/lib/swi-prolog/library/http/http_client.pl:139\n  [39] db_fetch:authorized_fetch('Bearer eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCIsImtpZCI6InhrMEZTOTdVWVFvbHB4ODREeFlpeCJ9.eyJodHRwOi8vdGVybWludXNkYi5jb20vc2NoZW1hL3N5c3RlbSNhZ2VudF9uYW1lIjoia2doZW5kZXJzb24iLCJodHRwOi8vdGVybWludXNkYi5jb20vc2NoZW1hL3N5c3RlbSN1c2VyX2lkZW50aWZpZXIiOiJrZXZpbkBrZ2hlbmRlcnNvbi5jb20iLCJpc3MiOiJodHRwczovL3Rlcm1pbnVzZGIuZXUuYXV0aDAuY29tLyIsInN1YiI6ImF1dGgwfDVmZWRmZTIwMzkxOWJjMDA3NmE2Y2U2OSIsImF1ZCI6WyJodHRwczovL3Rlcm1pbnVzZGIuY29tL2h1YnNlcnZpY2VzIiwiaHR0cHM6Ly90ZXJtaW51c2RiLmV1LmF1dGgwLmNvbS91c2VyaW5mbyJdLCJpYXQiOjE2MTEyNDcxMDksImV4cCI6MTYxMTI4MzEwOSwiYXpwIjoiNEVZSUhRYVZMZ3dpNVY1bTJnUk5ZZHRwWlpmYnd0RHoiLCJzY29wZSI6Im9wZW5pZCBwcm9maWxlIGVtYWlsIn0.hD6DoOAMj790oN3yzcXWatGR7mbn3oto_vd7kmWy_mobmXwNTQVmOUwNr2Ha-Yd1r6ysqfwipl7aNR-FG3GbmwNyVOleZL8THoiF8jp1p6jKsanzhGUw4Q_EDPNWv7brcdkoWtcKZqEg1ZshE7AZhaOZUoLYMX64Cq_uKe6Y7tcKmGf34CoD-pBu6I56s9OGMdk8gS0CWm4FwJYVfYppUjvdbnrW0mY_tW3644OWwOI_N_V_X3-_wqrxj7TfHKhlvzXdcKUId3Pa9RldKF7Mn-pqJAJVkNDybvEl423kxJ73lT01i46ovoBq60-picfQLJCC0_RfqNctONg8siFpkw','<garbage_collected>',none,_12250) at /tmp/.mount_.org.cdCakCk/usr/share/terminusdb/core/api/db_fetch.pl:75\n  [38] '<meta-call>'('<garbage_collected>') <foreign>\n  [37] database:with_transaction_(query_context{all_witnesses:false,authorization:'terminusdb:///system/data/admin',bindings:[],default_collection:database_descriptor{database_name:\""bikes\"",organization_name:\""admin\""},files:[],filter:type_filter{types: ...},prefixes:_12392{api:'http://terminusdb.com/schema/api#',doc:'terminusdb:///repository/data/',layer:'http://terminusdb.com/schema/layer#',owl:'http://www.w3.org/2002/07/owl#',rdf:'http://www.w3.org/1999/02/22-rdf-syntax-ns#',rdfs:'http://www.w3.org/2000/01/rdf-schema#',ref:'http://terminusdb.com/schema/ref#',repo:'http://terminusdb.com/schema/repository#',system:'http://terminusdb.com/schema/system#',terminus:'http://terminusdb.com/schema/system#',vio:'http://terminusdb.com/schema/vio#',woql:'http://terminusdb.com/schema/woql#',xdd:'http://terminusdb.com/schema/xdd#',xsd:'http://www.w3.org/2001/XMLSchema#'},selected:[],system:system_descriptor{},transaction_objects:[...],update_guard:_12366,write_graph:repo_graph{database_name:\""bikes\"",name:\""main\"",organization_name:\""admin\"",type:instance}},db_fetch:(...,...),_12324) at /tmp/.mount_.org.cdCakCk/usr/share/terminusdb/core/transaction/database.pl:213\n  [36] setup_call_catcher_cleanup(database:true,database:with_transaction_(...,...,_12554),_12532,database:(...,...)) at /tmp/.mount_.org.cdCakCk/usr/lib/swi-prolog/boot/init.pl:612\n  [32] db_clone:clone_('<garbage_collected>','terminusdb:///system/data/admin',admin,bikes,'<garbage_collected>','<garbage_collected>',false,'<garbage_collected>','<garbage_collected>',_12620) at /tmp/.mount_.org.cdCakCk/usr/share/terminusdb/core/api/db_clone.pl:55\n  [31] setup_call_catcher_cleanup(db_clone:true,db_clone:clone_(...,'terminusdb:///system/data/admin',admin,bikes,\""TerminusDB Bikes Tutorial\"",\""This is the knowledge graph produced by the TerminusDB Bikes Tutorial which is commonly used as an introduction to TerminusDB. This database contains the final results in a format that you can clone complete with all of the data and structure intact so that you don't have to do it yourself. \"",false,\""https://hub.terminusdb.com/kevin/bikes\"",...,_12690),exception(_12694),db_clone:(...;true)) at /tmp/.mount_.org.cdCakCk/usr/lib/swi-prolog/boot/init.pl:612\n  [30] db_clone:clone('<garbage_collected>','terminusdb:///system/data/admin',admin,bikes,'<garbage_collected>','<garbage_collected>',false,'<garbage_collected>','<garbage_collected>','<garbage_collected>') at /tmp/.mount_.org.cdCakCk/usr/share/terminusdb/core/api/db_clone.pl:17\n  [29] '<meta-call>'('<garbage_collected>') <foreign>\n  [28] catch(routes:(...,...),error(ssl_error('1416F086','SSL routines',tls_process_server_certificate,'certificate verify failed'),context(_12860,_12862)),routes:do_or_die(...,...)) at /tmp/.mount_.org.cdCakCk/usr/lib/swi-prolog/boot/init.pl:530\n  [27] catch_with_backtrace('<garbage_collected>','<garbage_collected>','<garbage_collected>') at /tmp/.mount_.org.cdCakCk/usr/lib/swi-prolog/boot/init.pl:580\n\nNote: some frames are missing due to last-call optimization.\nRe-run your program in debug mode (:- debug.) to get more detail.\n\n"",
    ""api:status"": ""api:server_error"",
    ""end"": ""2021-01-21T16:40:32.348Z"",
    ""start"": ""2021-01-21T16:40:32.348Z"",
    ""duration"": ""2021-01-21T16:40:32.348Z""
  }
}",2021-01-22T15:07:49Z,kghenderson,https://github.com/terminusdb/terminusdb/issues/150#issuecomment-765469137,I tried it again this morning and yes I am.,i try -PRON- again this morning and yes i be,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,150,2021-01-21T16:46:09Z,kghenderson,cloning ssl error,https://github.com/terminusdb/terminusdb/issues/150,"Describe the bug
Attempting to clone the Bikes Tutorial DB from TerminusHub using the Terminus desktop app on Fedora 33 Linux.
Repository fails to clone with error:
Error: ssl_error('1416F086','SSL routines',tls_process_server_certificate,'certificate verify failed') [46] throw(error(ssl_error('1416F086','SSL routines',tls_process_server_certificate,'certificate verify failed'),_12004)) [44] catch(http:http_protocol_hook(https,...,<stream>(0x7f8110010150,0x7f81100106a0),_12064,...),error(ssl_error('1416F086','SSL routines',tls_process_server_certificate,'certificate verify failed'),_12072),http_open:(...,...)) at /tmp/.mount_.org.cdCakCk/usr/lib/swi-prolog/boot/init.pl:531 [43] http_open:try_http_proxy(direct,[uri('https://hub.terminusdb.com/api/pack/kevin/bikes'),...|...],_12134,'<garbage_collected>') at /tmp/.mount_.org.cdCakCk/usr/lib/swi-prolog/library/http/http_open.pl:448 [41] http_client:http_get('https://hub.terminusdb.com/api/pack/kevin/bikes',_12190,[post(...),...|...]) at /tmp/.mount_.org.cdCakCk/usr/lib/swi-prolog/library/http/http_client.pl:139 [39] db_fetch:authorized_fetch('Bearer eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCIsImtpZCI6InhrMEZTOTdVWVFvbHB4ODREeFlpeCJ9.eyJodHRwOi8vdGVybWludXNkYi5jb20vc2NoZW1hL3N5c3RlbSNhZ2VudF9uYW1lIjoia2doZW5kZXJzb24iLCJodHRwOi8vdGVybWludXNkYi5jb20vc2NoZW1hL3N5c3RlbSN1c2VyX2lkZW50aWZpZXIiOiJrZXZpbkBrZ2hlbmRlcnNvbi5jb20iLCJpc3MiOiJodHRwczovL3Rlcm1pbnVzZGIuZXUuYXV0aDAuY29tLyIsInN1YiI6ImF1dGgwfDVmZWRmZTIwMzkxOWJjMDA3NmE2Y2U2OSIsImF1ZCI6WyJodHRwczovL3Rlcm1pbnVzZGIuY29tL2h1YnNlcnZpY2VzIiwiaHR0cHM6Ly90ZXJtaW51c2RiLmV1LmF1dGgwLmNvbS91c2VyaW5mbyJdLCJpYXQiOjE2MTEyNDcxMDksImV4cCI6MTYxMTI4MzEwOSwiYXpwIjoiNEVZSUhRYVZMZ3dpNVY1bTJnUk5ZZHRwWlpmYnd0RHoiLCJzY29wZSI6Im9wZW5pZCBwcm9maWxlIGVtYWlsIn0.hD6DoOAMj790oN3yzcXWatGR7mbn3oto_vd7kmWy_mobmXwNTQVmOUwNr2Ha-Yd1r6ysqfwipl7aNR-FG3GbmwNyVOleZL8THoiF8jp1p6jKsanzhGUw4Q_EDPNWv7brcdkoWtcKZqEg1ZshE7AZhaOZUoLYMX64Cq_uKe6Y7tcKmGf34CoD-pBu6I56s9OGMdk8gS0CWm4FwJYVfYppUjvdbnrW0mY_tW3644OWwOI_N_V_X3-_wqrxj7TfHKhlvzXdcKUId3Pa9RldKF7Mn-pqJAJVkNDybvEl423kxJ73lT01i46ovoBq60-picfQLJCC0_RfqNctONg8siFpkw','<garbage_collected>',none,_12250) at /tmp/.mount_.org.cdCakCk/usr/share/terminusdb/core/api/db_fetch.pl:75 [38] '<meta-call>'('<garbage_collected>') <foreign> [37] database:with_transaction_(query_context{all_witnesses:false,authorization:'terminusdb:///system/data/admin',bindings:[],default_collection:database_descriptor{database_name:""bikes"",organization_name:""admin""},files:[],filter:type_filter{types: ...},prefixes:_12392{api:'http://terminusdb.com/schema/api#',doc:'terminusdb:///repository/data/',layer:'http://terminusdb.com/schema/layer#',owl:'http://www.w3.org/2002/07/owl#',rdf:'http://www.w3.org/1999/02/22-rdf-syntax-ns#',rdfs:'http://www.w3.org/2000/01/rdf-schema#',ref:'http://terminusdb.com/schema/ref#',repo:'http://terminusdb.com/schema/repository#',system:'http://terminusdb.com/schema/system#',terminus:'http://terminusdb.com/schema/system#',vio:'http://terminusdb.com/schema/vio#',woql:'http://terminusdb.com/schema/woql#',xdd:'http://terminusdb.com/schema/xdd#',xsd:'http://www.w3.org/2001/XMLSchema#'},selected:[],system:system_descriptor{},transaction_objects:[...],update_guard:_12366,write_graph:repo_graph{database_name:""bikes"",name:""main"",organization_name:""admin"",type:instance}},db_fetch:(...,...),_12324) at /tmp/.mount_.org.cdCakCk/usr/share/terminusdb/core/transaction/database.pl:213 [36] setup_call_catcher_cleanup(database:true,database:with_transaction_(...,...,_12554),_12532,database:(...,...)) at /tmp/.mount_.org.cdCakCk/usr/lib/swi-prolog/boot/init.pl:612 [32] db_clone:clone_('<garbage_collected>','terminusdb:///system/data/admin',admin,bikes,'<garbage_collected>','<garbage_collected>',false,'<garbage_collected>','<garbage_collected>',_12620) at /tmp/.mount_.org.cdCakCk/usr/share/terminusdb/core/api/db_clone.pl:55 [31] setup_call_catcher_cleanup(db_clone:true,db_clone:clone_(...,'terminusdb:///system/data/admin',admin,bikes,""TerminusDB Bikes Tutorial"",""This is the knowledge graph produced by the TerminusDB Bikes Tutorial which is commonly used as an introduction to TerminusDB. This database contains the final results in a format that you can clone complete with all of the data and structure intact so that you don't have to do it yourself. "",false,""https://hub.terminusdb.com/kevin/bikes"",...,_12690),exception(_12694),db_clone:(...;true)) at /tmp/.mount_.org.cdCakCk/usr/lib/swi-prolog/boot/init.pl:612 [30] db_clone:clone('<garbage_collected>','terminusdb:///system/data/admin',admin,bikes,'<garbage_collected>','<garbage_collected>',false,'<garbage_collected>','<garbage_collected>','<garbage_collected>') at /tmp/.mount_.org.cdCakCk/usr/share/terminusdb/core/api/db_clone.pl:17 [29] '<meta-call>'('<garbage_collected>') <foreign> [28] catch(routes:(...,...),error(ssl_error('1416F086','SSL routines',tls_process_server_certificate,'certificate verify failed'),context(_12860,_12862)),routes:do_or_die(...,...)) at /tmp/.mount_.org.cdCakCk/usr/lib/swi-prolog/boot/init.pl:530 [27] catch_with_backtrace('<garbage_collected>','<garbage_collected>','<garbage_collected>') at /tmp/.mount_.org.cdCakCk/usr/lib/swi-prolog/boot/init.pl:580 Note: some frames are missing due to last-call optimization. Re-run your program in debug mode (:- debug.) to get more detail.less
{
  ""data"": {
    ""api:message"": ""Error: ssl_error('1416F086','SSL routines',tls_process_server_certificate,'certificate verify failed')\n  [46] throw(error(ssl_error('1416F086','SSL routines',tls_process_server_certificate,'certificate verify failed'),_12004))\n  [44] catch(http:http_protocol_hook(https,...,<stream>(0x7f8110010150,0x7f81100106a0),_12064,...),error(ssl_error('1416F086','SSL routines',tls_process_server_certificate,'certificate verify failed'),_12072),http_open:(...,...)) at /tmp/.mount_.org.cdCakCk/usr/lib/swi-prolog/boot/init.pl:531\n  [43] http_open:try_http_proxy(direct,[uri('https://hub.terminusdb.com/api/pack/kevin/bikes'),...|...],_12134,'<garbage_collected>') at /tmp/.mount_.org.cdCakCk/usr/lib/swi-prolog/library/http/http_open.pl:448\n  [41] http_client:http_get('https://hub.terminusdb.com/api/pack/kevin/bikes',_12190,[post(...),...|...]) at /tmp/.mount_.org.cdCakCk/usr/lib/swi-prolog/library/http/http_client.pl:139\n  [39] db_fetch:authorized_fetch('Bearer eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCIsImtpZCI6InhrMEZTOTdVWVFvbHB4ODREeFlpeCJ9.eyJodHRwOi8vdGVybWludXNkYi5jb20vc2NoZW1hL3N5c3RlbSNhZ2VudF9uYW1lIjoia2doZW5kZXJzb24iLCJodHRwOi8vdGVybWludXNkYi5jb20vc2NoZW1hL3N5c3RlbSN1c2VyX2lkZW50aWZpZXIiOiJrZXZpbkBrZ2hlbmRlcnNvbi5jb20iLCJpc3MiOiJodHRwczovL3Rlcm1pbnVzZGIuZXUuYXV0aDAuY29tLyIsInN1YiI6ImF1dGgwfDVmZWRmZTIwMzkxOWJjMDA3NmE2Y2U2OSIsImF1ZCI6WyJodHRwczovL3Rlcm1pbnVzZGIuY29tL2h1YnNlcnZpY2VzIiwiaHR0cHM6Ly90ZXJtaW51c2RiLmV1LmF1dGgwLmNvbS91c2VyaW5mbyJdLCJpYXQiOjE2MTEyNDcxMDksImV4cCI6MTYxMTI4MzEwOSwiYXpwIjoiNEVZSUhRYVZMZ3dpNVY1bTJnUk5ZZHRwWlpmYnd0RHoiLCJzY29wZSI6Im9wZW5pZCBwcm9maWxlIGVtYWlsIn0.hD6DoOAMj790oN3yzcXWatGR7mbn3oto_vd7kmWy_mobmXwNTQVmOUwNr2Ha-Yd1r6ysqfwipl7aNR-FG3GbmwNyVOleZL8THoiF8jp1p6jKsanzhGUw4Q_EDPNWv7brcdkoWtcKZqEg1ZshE7AZhaOZUoLYMX64Cq_uKe6Y7tcKmGf34CoD-pBu6I56s9OGMdk8gS0CWm4FwJYVfYppUjvdbnrW0mY_tW3644OWwOI_N_V_X3-_wqrxj7TfHKhlvzXdcKUId3Pa9RldKF7Mn-pqJAJVkNDybvEl423kxJ73lT01i46ovoBq60-picfQLJCC0_RfqNctONg8siFpkw','<garbage_collected>',none,_12250) at /tmp/.mount_.org.cdCakCk/usr/share/terminusdb/core/api/db_fetch.pl:75\n  [38] '<meta-call>'('<garbage_collected>') <foreign>\n  [37] database:with_transaction_(query_context{all_witnesses:false,authorization:'terminusdb:///system/data/admin',bindings:[],default_collection:database_descriptor{database_name:\""bikes\"",organization_name:\""admin\""},files:[],filter:type_filter{types: ...},prefixes:_12392{api:'http://terminusdb.com/schema/api#',doc:'terminusdb:///repository/data/',layer:'http://terminusdb.com/schema/layer#',owl:'http://www.w3.org/2002/07/owl#',rdf:'http://www.w3.org/1999/02/22-rdf-syntax-ns#',rdfs:'http://www.w3.org/2000/01/rdf-schema#',ref:'http://terminusdb.com/schema/ref#',repo:'http://terminusdb.com/schema/repository#',system:'http://terminusdb.com/schema/system#',terminus:'http://terminusdb.com/schema/system#',vio:'http://terminusdb.com/schema/vio#',woql:'http://terminusdb.com/schema/woql#',xdd:'http://terminusdb.com/schema/xdd#',xsd:'http://www.w3.org/2001/XMLSchema#'},selected:[],system:system_descriptor{},transaction_objects:[...],update_guard:_12366,write_graph:repo_graph{database_name:\""bikes\"",name:\""main\"",organization_name:\""admin\"",type:instance}},db_fetch:(...,...),_12324) at /tmp/.mount_.org.cdCakCk/usr/share/terminusdb/core/transaction/database.pl:213\n  [36] setup_call_catcher_cleanup(database:true,database:with_transaction_(...,...,_12554),_12532,database:(...,...)) at /tmp/.mount_.org.cdCakCk/usr/lib/swi-prolog/boot/init.pl:612\n  [32] db_clone:clone_('<garbage_collected>','terminusdb:///system/data/admin',admin,bikes,'<garbage_collected>','<garbage_collected>',false,'<garbage_collected>','<garbage_collected>',_12620) at /tmp/.mount_.org.cdCakCk/usr/share/terminusdb/core/api/db_clone.pl:55\n  [31] setup_call_catcher_cleanup(db_clone:true,db_clone:clone_(...,'terminusdb:///system/data/admin',admin,bikes,\""TerminusDB Bikes Tutorial\"",\""This is the knowledge graph produced by the TerminusDB Bikes Tutorial which is commonly used as an introduction to TerminusDB. This database contains the final results in a format that you can clone complete with all of the data and structure intact so that you don't have to do it yourself. \"",false,\""https://hub.terminusdb.com/kevin/bikes\"",...,_12690),exception(_12694),db_clone:(...;true)) at /tmp/.mount_.org.cdCakCk/usr/lib/swi-prolog/boot/init.pl:612\n  [30] db_clone:clone('<garbage_collected>','terminusdb:///system/data/admin',admin,bikes,'<garbage_collected>','<garbage_collected>',false,'<garbage_collected>','<garbage_collected>','<garbage_collected>') at /tmp/.mount_.org.cdCakCk/usr/share/terminusdb/core/api/db_clone.pl:17\n  [29] '<meta-call>'('<garbage_collected>') <foreign>\n  [28] catch(routes:(...,...),error(ssl_error('1416F086','SSL routines',tls_process_server_certificate,'certificate verify failed'),context(_12860,_12862)),routes:do_or_die(...,...)) at /tmp/.mount_.org.cdCakCk/usr/lib/swi-prolog/boot/init.pl:530\n  [27] catch_with_backtrace('<garbage_collected>','<garbage_collected>','<garbage_collected>') at /tmp/.mount_.org.cdCakCk/usr/lib/swi-prolog/boot/init.pl:580\n\nNote: some frames are missing due to last-call optimization.\nRe-run your program in debug mode (:- debug.) to get more detail.\n\n"",
    ""api:status"": ""api:server_error"",
    ""end"": ""2021-01-21T16:40:32.348Z"",
    ""start"": ""2021-01-21T16:40:32.348Z"",
    ""duration"": ""2021-01-21T16:40:32.348Z""
  }
}",2021-01-22T15:23:32Z,kghenderson,https://github.com/terminusdb/terminusdb/issues/150#issuecomment-765481167,"ok, probably not relevant, but when launching the appimage i'm getting an stderr that a port is in use [6363] and another ""error"" that a database is already initialized in the storage directory. I checked the ports with ss -tl and it wasn't in use. Thinking this could have something to do with the storage directory. I copied the AppImage to another directory and started it from there. This time it starts without the port usage or storage errors, but has the same SSL cloning issue. I was still signed in though, so something is being cached somewhere. I'm not sure where that is yet, but I'll keep plugging.
Here's a screenshot and a clean log for you :-)

httpd.log","ok probably not relevant but when launch the appimage -PRON- be get an stderr that a port be in use [ 6363 ] and another "" error "" that a database be already initialize in the storage directory i check the port with ss -tl and -PRON- be not in use thinking this could have something to do with the storage directory i copy the appimage to another directory and start -PRON- from there this time -PRON- start without the port usage or storage error but have the same ssl clone issue i be still sign in though so something be be cache somewhere -PRON- be not sure where that be yet but -PRON- will keep plug here be a screenshot and a clean log for -PRON- - ) httpdlog",0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,150,2021-01-21T16:46:09Z,kghenderson,cloning ssl error,https://github.com/terminusdb/terminusdb/issues/150,"Describe the bug
Attempting to clone the Bikes Tutorial DB from TerminusHub using the Terminus desktop app on Fedora 33 Linux.
Repository fails to clone with error:
Error: ssl_error('1416F086','SSL routines',tls_process_server_certificate,'certificate verify failed') [46] throw(error(ssl_error('1416F086','SSL routines',tls_process_server_certificate,'certificate verify failed'),_12004)) [44] catch(http:http_protocol_hook(https,...,<stream>(0x7f8110010150,0x7f81100106a0),_12064,...),error(ssl_error('1416F086','SSL routines',tls_process_server_certificate,'certificate verify failed'),_12072),http_open:(...,...)) at /tmp/.mount_.org.cdCakCk/usr/lib/swi-prolog/boot/init.pl:531 [43] http_open:try_http_proxy(direct,[uri('https://hub.terminusdb.com/api/pack/kevin/bikes'),...|...],_12134,'<garbage_collected>') at /tmp/.mount_.org.cdCakCk/usr/lib/swi-prolog/library/http/http_open.pl:448 [41] http_client:http_get('https://hub.terminusdb.com/api/pack/kevin/bikes',_12190,[post(...),...|...]) at /tmp/.mount_.org.cdCakCk/usr/lib/swi-prolog/library/http/http_client.pl:139 [39] db_fetch:authorized_fetch('Bearer eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCIsImtpZCI6InhrMEZTOTdVWVFvbHB4ODREeFlpeCJ9.eyJodHRwOi8vdGVybWludXNkYi5jb20vc2NoZW1hL3N5c3RlbSNhZ2VudF9uYW1lIjoia2doZW5kZXJzb24iLCJodHRwOi8vdGVybWludXNkYi5jb20vc2NoZW1hL3N5c3RlbSN1c2VyX2lkZW50aWZpZXIiOiJrZXZpbkBrZ2hlbmRlcnNvbi5jb20iLCJpc3MiOiJodHRwczovL3Rlcm1pbnVzZGIuZXUuYXV0aDAuY29tLyIsInN1YiI6ImF1dGgwfDVmZWRmZTIwMzkxOWJjMDA3NmE2Y2U2OSIsImF1ZCI6WyJodHRwczovL3Rlcm1pbnVzZGIuY29tL2h1YnNlcnZpY2VzIiwiaHR0cHM6Ly90ZXJtaW51c2RiLmV1LmF1dGgwLmNvbS91c2VyaW5mbyJdLCJpYXQiOjE2MTEyNDcxMDksImV4cCI6MTYxMTI4MzEwOSwiYXpwIjoiNEVZSUhRYVZMZ3dpNVY1bTJnUk5ZZHRwWlpmYnd0RHoiLCJzY29wZSI6Im9wZW5pZCBwcm9maWxlIGVtYWlsIn0.hD6DoOAMj790oN3yzcXWatGR7mbn3oto_vd7kmWy_mobmXwNTQVmOUwNr2Ha-Yd1r6ysqfwipl7aNR-FG3GbmwNyVOleZL8THoiF8jp1p6jKsanzhGUw4Q_EDPNWv7brcdkoWtcKZqEg1ZshE7AZhaOZUoLYMX64Cq_uKe6Y7tcKmGf34CoD-pBu6I56s9OGMdk8gS0CWm4FwJYVfYppUjvdbnrW0mY_tW3644OWwOI_N_V_X3-_wqrxj7TfHKhlvzXdcKUId3Pa9RldKF7Mn-pqJAJVkNDybvEl423kxJ73lT01i46ovoBq60-picfQLJCC0_RfqNctONg8siFpkw','<garbage_collected>',none,_12250) at /tmp/.mount_.org.cdCakCk/usr/share/terminusdb/core/api/db_fetch.pl:75 [38] '<meta-call>'('<garbage_collected>') <foreign> [37] database:with_transaction_(query_context{all_witnesses:false,authorization:'terminusdb:///system/data/admin',bindings:[],default_collection:database_descriptor{database_name:""bikes"",organization_name:""admin""},files:[],filter:type_filter{types: ...},prefixes:_12392{api:'http://terminusdb.com/schema/api#',doc:'terminusdb:///repository/data/',layer:'http://terminusdb.com/schema/layer#',owl:'http://www.w3.org/2002/07/owl#',rdf:'http://www.w3.org/1999/02/22-rdf-syntax-ns#',rdfs:'http://www.w3.org/2000/01/rdf-schema#',ref:'http://terminusdb.com/schema/ref#',repo:'http://terminusdb.com/schema/repository#',system:'http://terminusdb.com/schema/system#',terminus:'http://terminusdb.com/schema/system#',vio:'http://terminusdb.com/schema/vio#',woql:'http://terminusdb.com/schema/woql#',xdd:'http://terminusdb.com/schema/xdd#',xsd:'http://www.w3.org/2001/XMLSchema#'},selected:[],system:system_descriptor{},transaction_objects:[...],update_guard:_12366,write_graph:repo_graph{database_name:""bikes"",name:""main"",organization_name:""admin"",type:instance}},db_fetch:(...,...),_12324) at /tmp/.mount_.org.cdCakCk/usr/share/terminusdb/core/transaction/database.pl:213 [36] setup_call_catcher_cleanup(database:true,database:with_transaction_(...,...,_12554),_12532,database:(...,...)) at /tmp/.mount_.org.cdCakCk/usr/lib/swi-prolog/boot/init.pl:612 [32] db_clone:clone_('<garbage_collected>','terminusdb:///system/data/admin',admin,bikes,'<garbage_collected>','<garbage_collected>',false,'<garbage_collected>','<garbage_collected>',_12620) at /tmp/.mount_.org.cdCakCk/usr/share/terminusdb/core/api/db_clone.pl:55 [31] setup_call_catcher_cleanup(db_clone:true,db_clone:clone_(...,'terminusdb:///system/data/admin',admin,bikes,""TerminusDB Bikes Tutorial"",""This is the knowledge graph produced by the TerminusDB Bikes Tutorial which is commonly used as an introduction to TerminusDB. This database contains the final results in a format that you can clone complete with all of the data and structure intact so that you don't have to do it yourself. "",false,""https://hub.terminusdb.com/kevin/bikes"",...,_12690),exception(_12694),db_clone:(...;true)) at /tmp/.mount_.org.cdCakCk/usr/lib/swi-prolog/boot/init.pl:612 [30] db_clone:clone('<garbage_collected>','terminusdb:///system/data/admin',admin,bikes,'<garbage_collected>','<garbage_collected>',false,'<garbage_collected>','<garbage_collected>','<garbage_collected>') at /tmp/.mount_.org.cdCakCk/usr/share/terminusdb/core/api/db_clone.pl:17 [29] '<meta-call>'('<garbage_collected>') <foreign> [28] catch(routes:(...,...),error(ssl_error('1416F086','SSL routines',tls_process_server_certificate,'certificate verify failed'),context(_12860,_12862)),routes:do_or_die(...,...)) at /tmp/.mount_.org.cdCakCk/usr/lib/swi-prolog/boot/init.pl:530 [27] catch_with_backtrace('<garbage_collected>','<garbage_collected>','<garbage_collected>') at /tmp/.mount_.org.cdCakCk/usr/lib/swi-prolog/boot/init.pl:580 Note: some frames are missing due to last-call optimization. Re-run your program in debug mode (:- debug.) to get more detail.less
{
  ""data"": {
    ""api:message"": ""Error: ssl_error('1416F086','SSL routines',tls_process_server_certificate,'certificate verify failed')\n  [46] throw(error(ssl_error('1416F086','SSL routines',tls_process_server_certificate,'certificate verify failed'),_12004))\n  [44] catch(http:http_protocol_hook(https,...,<stream>(0x7f8110010150,0x7f81100106a0),_12064,...),error(ssl_error('1416F086','SSL routines',tls_process_server_certificate,'certificate verify failed'),_12072),http_open:(...,...)) at /tmp/.mount_.org.cdCakCk/usr/lib/swi-prolog/boot/init.pl:531\n  [43] http_open:try_http_proxy(direct,[uri('https://hub.terminusdb.com/api/pack/kevin/bikes'),...|...],_12134,'<garbage_collected>') at /tmp/.mount_.org.cdCakCk/usr/lib/swi-prolog/library/http/http_open.pl:448\n  [41] http_client:http_get('https://hub.terminusdb.com/api/pack/kevin/bikes',_12190,[post(...),...|...]) at /tmp/.mount_.org.cdCakCk/usr/lib/swi-prolog/library/http/http_client.pl:139\n  [39] db_fetch:authorized_fetch('Bearer eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCIsImtpZCI6InhrMEZTOTdVWVFvbHB4ODREeFlpeCJ9.eyJodHRwOi8vdGVybWludXNkYi5jb20vc2NoZW1hL3N5c3RlbSNhZ2VudF9uYW1lIjoia2doZW5kZXJzb24iLCJodHRwOi8vdGVybWludXNkYi5jb20vc2NoZW1hL3N5c3RlbSN1c2VyX2lkZW50aWZpZXIiOiJrZXZpbkBrZ2hlbmRlcnNvbi5jb20iLCJpc3MiOiJodHRwczovL3Rlcm1pbnVzZGIuZXUuYXV0aDAuY29tLyIsInN1YiI6ImF1dGgwfDVmZWRmZTIwMzkxOWJjMDA3NmE2Y2U2OSIsImF1ZCI6WyJodHRwczovL3Rlcm1pbnVzZGIuY29tL2h1YnNlcnZpY2VzIiwiaHR0cHM6Ly90ZXJtaW51c2RiLmV1LmF1dGgwLmNvbS91c2VyaW5mbyJdLCJpYXQiOjE2MTEyNDcxMDksImV4cCI6MTYxMTI4MzEwOSwiYXpwIjoiNEVZSUhRYVZMZ3dpNVY1bTJnUk5ZZHRwWlpmYnd0RHoiLCJzY29wZSI6Im9wZW5pZCBwcm9maWxlIGVtYWlsIn0.hD6DoOAMj790oN3yzcXWatGR7mbn3oto_vd7kmWy_mobmXwNTQVmOUwNr2Ha-Yd1r6ysqfwipl7aNR-FG3GbmwNyVOleZL8THoiF8jp1p6jKsanzhGUw4Q_EDPNWv7brcdkoWtcKZqEg1ZshE7AZhaOZUoLYMX64Cq_uKe6Y7tcKmGf34CoD-pBu6I56s9OGMdk8gS0CWm4FwJYVfYppUjvdbnrW0mY_tW3644OWwOI_N_V_X3-_wqrxj7TfHKhlvzXdcKUId3Pa9RldKF7Mn-pqJAJVkNDybvEl423kxJ73lT01i46ovoBq60-picfQLJCC0_RfqNctONg8siFpkw','<garbage_collected>',none,_12250) at /tmp/.mount_.org.cdCakCk/usr/share/terminusdb/core/api/db_fetch.pl:75\n  [38] '<meta-call>'('<garbage_collected>') <foreign>\n  [37] database:with_transaction_(query_context{all_witnesses:false,authorization:'terminusdb:///system/data/admin',bindings:[],default_collection:database_descriptor{database_name:\""bikes\"",organization_name:\""admin\""},files:[],filter:type_filter{types: ...},prefixes:_12392{api:'http://terminusdb.com/schema/api#',doc:'terminusdb:///repository/data/',layer:'http://terminusdb.com/schema/layer#',owl:'http://www.w3.org/2002/07/owl#',rdf:'http://www.w3.org/1999/02/22-rdf-syntax-ns#',rdfs:'http://www.w3.org/2000/01/rdf-schema#',ref:'http://terminusdb.com/schema/ref#',repo:'http://terminusdb.com/schema/repository#',system:'http://terminusdb.com/schema/system#',terminus:'http://terminusdb.com/schema/system#',vio:'http://terminusdb.com/schema/vio#',woql:'http://terminusdb.com/schema/woql#',xdd:'http://terminusdb.com/schema/xdd#',xsd:'http://www.w3.org/2001/XMLSchema#'},selected:[],system:system_descriptor{},transaction_objects:[...],update_guard:_12366,write_graph:repo_graph{database_name:\""bikes\"",name:\""main\"",organization_name:\""admin\"",type:instance}},db_fetch:(...,...),_12324) at /tmp/.mount_.org.cdCakCk/usr/share/terminusdb/core/transaction/database.pl:213\n  [36] setup_call_catcher_cleanup(database:true,database:with_transaction_(...,...,_12554),_12532,database:(...,...)) at /tmp/.mount_.org.cdCakCk/usr/lib/swi-prolog/boot/init.pl:612\n  [32] db_clone:clone_('<garbage_collected>','terminusdb:///system/data/admin',admin,bikes,'<garbage_collected>','<garbage_collected>',false,'<garbage_collected>','<garbage_collected>',_12620) at /tmp/.mount_.org.cdCakCk/usr/share/terminusdb/core/api/db_clone.pl:55\n  [31] setup_call_catcher_cleanup(db_clone:true,db_clone:clone_(...,'terminusdb:///system/data/admin',admin,bikes,\""TerminusDB Bikes Tutorial\"",\""This is the knowledge graph produced by the TerminusDB Bikes Tutorial which is commonly used as an introduction to TerminusDB. This database contains the final results in a format that you can clone complete with all of the data and structure intact so that you don't have to do it yourself. \"",false,\""https://hub.terminusdb.com/kevin/bikes\"",...,_12690),exception(_12694),db_clone:(...;true)) at /tmp/.mount_.org.cdCakCk/usr/lib/swi-prolog/boot/init.pl:612\n  [30] db_clone:clone('<garbage_collected>','terminusdb:///system/data/admin',admin,bikes,'<garbage_collected>','<garbage_collected>',false,'<garbage_collected>','<garbage_collected>','<garbage_collected>') at /tmp/.mount_.org.cdCakCk/usr/share/terminusdb/core/api/db_clone.pl:17\n  [29] '<meta-call>'('<garbage_collected>') <foreign>\n  [28] catch(routes:(...,...),error(ssl_error('1416F086','SSL routines',tls_process_server_certificate,'certificate verify failed'),context(_12860,_12862)),routes:do_or_die(...,...)) at /tmp/.mount_.org.cdCakCk/usr/lib/swi-prolog/boot/init.pl:530\n  [27] catch_with_backtrace('<garbage_collected>','<garbage_collected>','<garbage_collected>') at /tmp/.mount_.org.cdCakCk/usr/lib/swi-prolog/boot/init.pl:580\n\nNote: some frames are missing due to last-call optimization.\nRe-run your program in debug mode (:- debug.) to get more detail.\n\n"",
    ""api:status"": ""api:server_error"",
    ""end"": ""2021-01-21T16:40:32.348Z"",
    ""start"": ""2021-01-21T16:40:32.348Z"",
    ""duration"": ""2021-01-21T16:40:32.348Z""
  }
}",2021-01-22T15:58:33Z,rrooij,https://github.com/terminusdb/terminusdb/issues/150#issuecomment-765506821,"I am able to reproduce your problem in a Fedora Docker container. The problem is specifically caused because the AppImage contains a version of SWI Prolog which uses a Debian base. The Debian SWI Prolog points to a non existing trusted certificates directory.
On Debian:
?- system_root_certificates(X).
X = [<ssl_certificate>(0x561a3dda5180), <ssl_certificate>(0x561a3dce1d90), <ssl_certificate>(0x561a3dce1800), <ssl_certificate>(0x561a3dda8620), <ssl_certificate>(0x561a3ddaa4a0), <ssl_certificate>(0x561a3ddabb40), <ssl_certificate>(0x561a3ddab7f0), <ssl_certificate>(0x561a3ddaedb0), <ssl_certificate>(0x561a3ddaffe0)|...].

?- current_prolog_flag(system_cacert_filename, X).
X = '/etc/ssl/certs/ca-certificates.crt'.

On Fedora:
?- system_root_certificates(X).
X = [].

?- current_prolog_flag(system_cacert_filename, X).
X = '/etc/ssl/certs/ca-certificates.crt'.",i be able to reproduce -PRON- problem in a fedora docker container the problem be specifically cause because the appimage contain a version of swi prolog which use a debian base the debian swi prolog point to a non exist trusted certificate directory on debian - system_root_certificates(x ) x = [ < ssl_certificate>(0x561a3dda5180 ) < ssl_certificate>(0x561a3dce1d90 ) < ssl_certificate>(0x561a3dce1800 ) < ssl_certificate>(0x561a3dda8620 ) < ssl_certificate>(0x561a3ddaa4a0 ) < ssl_certificate>(0x561a3ddabb40 ) < ssl_certificate>(0x561a3ddab7f0 ) < ssl_certificate>(0x561a3ddaedb0 ) < ssl_certificate>(0x561a3ddaffe0)| ] - current_prolog_flag(system_cacert_filename x ) x = ' /etc / ssl / cert / ca - certificatescrt ' on fedora - system_root_certificates(x ) x = [ ] - current_prolog_flag(system_cacert_filename x ) x = ' /etc / ssl / cert / ca - certificatescrt ',0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,150,2021-01-21T16:46:09Z,kghenderson,cloning ssl error,https://github.com/terminusdb/terminusdb/issues/150,"Describe the bug
Attempting to clone the Bikes Tutorial DB from TerminusHub using the Terminus desktop app on Fedora 33 Linux.
Repository fails to clone with error:
Error: ssl_error('1416F086','SSL routines',tls_process_server_certificate,'certificate verify failed') [46] throw(error(ssl_error('1416F086','SSL routines',tls_process_server_certificate,'certificate verify failed'),_12004)) [44] catch(http:http_protocol_hook(https,...,<stream>(0x7f8110010150,0x7f81100106a0),_12064,...),error(ssl_error('1416F086','SSL routines',tls_process_server_certificate,'certificate verify failed'),_12072),http_open:(...,...)) at /tmp/.mount_.org.cdCakCk/usr/lib/swi-prolog/boot/init.pl:531 [43] http_open:try_http_proxy(direct,[uri('https://hub.terminusdb.com/api/pack/kevin/bikes'),...|...],_12134,'<garbage_collected>') at /tmp/.mount_.org.cdCakCk/usr/lib/swi-prolog/library/http/http_open.pl:448 [41] http_client:http_get('https://hub.terminusdb.com/api/pack/kevin/bikes',_12190,[post(...),...|...]) at /tmp/.mount_.org.cdCakCk/usr/lib/swi-prolog/library/http/http_client.pl:139 [39] db_fetch:authorized_fetch('Bearer eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCIsImtpZCI6InhrMEZTOTdVWVFvbHB4ODREeFlpeCJ9.eyJodHRwOi8vdGVybWludXNkYi5jb20vc2NoZW1hL3N5c3RlbSNhZ2VudF9uYW1lIjoia2doZW5kZXJzb24iLCJodHRwOi8vdGVybWludXNkYi5jb20vc2NoZW1hL3N5c3RlbSN1c2VyX2lkZW50aWZpZXIiOiJrZXZpbkBrZ2hlbmRlcnNvbi5jb20iLCJpc3MiOiJodHRwczovL3Rlcm1pbnVzZGIuZXUuYXV0aDAuY29tLyIsInN1YiI6ImF1dGgwfDVmZWRmZTIwMzkxOWJjMDA3NmE2Y2U2OSIsImF1ZCI6WyJodHRwczovL3Rlcm1pbnVzZGIuY29tL2h1YnNlcnZpY2VzIiwiaHR0cHM6Ly90ZXJtaW51c2RiLmV1LmF1dGgwLmNvbS91c2VyaW5mbyJdLCJpYXQiOjE2MTEyNDcxMDksImV4cCI6MTYxMTI4MzEwOSwiYXpwIjoiNEVZSUhRYVZMZ3dpNVY1bTJnUk5ZZHRwWlpmYnd0RHoiLCJzY29wZSI6Im9wZW5pZCBwcm9maWxlIGVtYWlsIn0.hD6DoOAMj790oN3yzcXWatGR7mbn3oto_vd7kmWy_mobmXwNTQVmOUwNr2Ha-Yd1r6ysqfwipl7aNR-FG3GbmwNyVOleZL8THoiF8jp1p6jKsanzhGUw4Q_EDPNWv7brcdkoWtcKZqEg1ZshE7AZhaOZUoLYMX64Cq_uKe6Y7tcKmGf34CoD-pBu6I56s9OGMdk8gS0CWm4FwJYVfYppUjvdbnrW0mY_tW3644OWwOI_N_V_X3-_wqrxj7TfHKhlvzXdcKUId3Pa9RldKF7Mn-pqJAJVkNDybvEl423kxJ73lT01i46ovoBq60-picfQLJCC0_RfqNctONg8siFpkw','<garbage_collected>',none,_12250) at /tmp/.mount_.org.cdCakCk/usr/share/terminusdb/core/api/db_fetch.pl:75 [38] '<meta-call>'('<garbage_collected>') <foreign> [37] database:with_transaction_(query_context{all_witnesses:false,authorization:'terminusdb:///system/data/admin',bindings:[],default_collection:database_descriptor{database_name:""bikes"",organization_name:""admin""},files:[],filter:type_filter{types: ...},prefixes:_12392{api:'http://terminusdb.com/schema/api#',doc:'terminusdb:///repository/data/',layer:'http://terminusdb.com/schema/layer#',owl:'http://www.w3.org/2002/07/owl#',rdf:'http://www.w3.org/1999/02/22-rdf-syntax-ns#',rdfs:'http://www.w3.org/2000/01/rdf-schema#',ref:'http://terminusdb.com/schema/ref#',repo:'http://terminusdb.com/schema/repository#',system:'http://terminusdb.com/schema/system#',terminus:'http://terminusdb.com/schema/system#',vio:'http://terminusdb.com/schema/vio#',woql:'http://terminusdb.com/schema/woql#',xdd:'http://terminusdb.com/schema/xdd#',xsd:'http://www.w3.org/2001/XMLSchema#'},selected:[],system:system_descriptor{},transaction_objects:[...],update_guard:_12366,write_graph:repo_graph{database_name:""bikes"",name:""main"",organization_name:""admin"",type:instance}},db_fetch:(...,...),_12324) at /tmp/.mount_.org.cdCakCk/usr/share/terminusdb/core/transaction/database.pl:213 [36] setup_call_catcher_cleanup(database:true,database:with_transaction_(...,...,_12554),_12532,database:(...,...)) at /tmp/.mount_.org.cdCakCk/usr/lib/swi-prolog/boot/init.pl:612 [32] db_clone:clone_('<garbage_collected>','terminusdb:///system/data/admin',admin,bikes,'<garbage_collected>','<garbage_collected>',false,'<garbage_collected>','<garbage_collected>',_12620) at /tmp/.mount_.org.cdCakCk/usr/share/terminusdb/core/api/db_clone.pl:55 [31] setup_call_catcher_cleanup(db_clone:true,db_clone:clone_(...,'terminusdb:///system/data/admin',admin,bikes,""TerminusDB Bikes Tutorial"",""This is the knowledge graph produced by the TerminusDB Bikes Tutorial which is commonly used as an introduction to TerminusDB. This database contains the final results in a format that you can clone complete with all of the data and structure intact so that you don't have to do it yourself. "",false,""https://hub.terminusdb.com/kevin/bikes"",...,_12690),exception(_12694),db_clone:(...;true)) at /tmp/.mount_.org.cdCakCk/usr/lib/swi-prolog/boot/init.pl:612 [30] db_clone:clone('<garbage_collected>','terminusdb:///system/data/admin',admin,bikes,'<garbage_collected>','<garbage_collected>',false,'<garbage_collected>','<garbage_collected>','<garbage_collected>') at /tmp/.mount_.org.cdCakCk/usr/share/terminusdb/core/api/db_clone.pl:17 [29] '<meta-call>'('<garbage_collected>') <foreign> [28] catch(routes:(...,...),error(ssl_error('1416F086','SSL routines',tls_process_server_certificate,'certificate verify failed'),context(_12860,_12862)),routes:do_or_die(...,...)) at /tmp/.mount_.org.cdCakCk/usr/lib/swi-prolog/boot/init.pl:530 [27] catch_with_backtrace('<garbage_collected>','<garbage_collected>','<garbage_collected>') at /tmp/.mount_.org.cdCakCk/usr/lib/swi-prolog/boot/init.pl:580 Note: some frames are missing due to last-call optimization. Re-run your program in debug mode (:- debug.) to get more detail.less
{
  ""data"": {
    ""api:message"": ""Error: ssl_error('1416F086','SSL routines',tls_process_server_certificate,'certificate verify failed')\n  [46] throw(error(ssl_error('1416F086','SSL routines',tls_process_server_certificate,'certificate verify failed'),_12004))\n  [44] catch(http:http_protocol_hook(https,...,<stream>(0x7f8110010150,0x7f81100106a0),_12064,...),error(ssl_error('1416F086','SSL routines',tls_process_server_certificate,'certificate verify failed'),_12072),http_open:(...,...)) at /tmp/.mount_.org.cdCakCk/usr/lib/swi-prolog/boot/init.pl:531\n  [43] http_open:try_http_proxy(direct,[uri('https://hub.terminusdb.com/api/pack/kevin/bikes'),...|...],_12134,'<garbage_collected>') at /tmp/.mount_.org.cdCakCk/usr/lib/swi-prolog/library/http/http_open.pl:448\n  [41] http_client:http_get('https://hub.terminusdb.com/api/pack/kevin/bikes',_12190,[post(...),...|...]) at /tmp/.mount_.org.cdCakCk/usr/lib/swi-prolog/library/http/http_client.pl:139\n  [39] db_fetch:authorized_fetch('Bearer eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCIsImtpZCI6InhrMEZTOTdVWVFvbHB4ODREeFlpeCJ9.eyJodHRwOi8vdGVybWludXNkYi5jb20vc2NoZW1hL3N5c3RlbSNhZ2VudF9uYW1lIjoia2doZW5kZXJzb24iLCJodHRwOi8vdGVybWludXNkYi5jb20vc2NoZW1hL3N5c3RlbSN1c2VyX2lkZW50aWZpZXIiOiJrZXZpbkBrZ2hlbmRlcnNvbi5jb20iLCJpc3MiOiJodHRwczovL3Rlcm1pbnVzZGIuZXUuYXV0aDAuY29tLyIsInN1YiI6ImF1dGgwfDVmZWRmZTIwMzkxOWJjMDA3NmE2Y2U2OSIsImF1ZCI6WyJodHRwczovL3Rlcm1pbnVzZGIuY29tL2h1YnNlcnZpY2VzIiwiaHR0cHM6Ly90ZXJtaW51c2RiLmV1LmF1dGgwLmNvbS91c2VyaW5mbyJdLCJpYXQiOjE2MTEyNDcxMDksImV4cCI6MTYxMTI4MzEwOSwiYXpwIjoiNEVZSUhRYVZMZ3dpNVY1bTJnUk5ZZHRwWlpmYnd0RHoiLCJzY29wZSI6Im9wZW5pZCBwcm9maWxlIGVtYWlsIn0.hD6DoOAMj790oN3yzcXWatGR7mbn3oto_vd7kmWy_mobmXwNTQVmOUwNr2Ha-Yd1r6ysqfwipl7aNR-FG3GbmwNyVOleZL8THoiF8jp1p6jKsanzhGUw4Q_EDPNWv7brcdkoWtcKZqEg1ZshE7AZhaOZUoLYMX64Cq_uKe6Y7tcKmGf34CoD-pBu6I56s9OGMdk8gS0CWm4FwJYVfYppUjvdbnrW0mY_tW3644OWwOI_N_V_X3-_wqrxj7TfHKhlvzXdcKUId3Pa9RldKF7Mn-pqJAJVkNDybvEl423kxJ73lT01i46ovoBq60-picfQLJCC0_RfqNctONg8siFpkw','<garbage_collected>',none,_12250) at /tmp/.mount_.org.cdCakCk/usr/share/terminusdb/core/api/db_fetch.pl:75\n  [38] '<meta-call>'('<garbage_collected>') <foreign>\n  [37] database:with_transaction_(query_context{all_witnesses:false,authorization:'terminusdb:///system/data/admin',bindings:[],default_collection:database_descriptor{database_name:\""bikes\"",organization_name:\""admin\""},files:[],filter:type_filter{types: ...},prefixes:_12392{api:'http://terminusdb.com/schema/api#',doc:'terminusdb:///repository/data/',layer:'http://terminusdb.com/schema/layer#',owl:'http://www.w3.org/2002/07/owl#',rdf:'http://www.w3.org/1999/02/22-rdf-syntax-ns#',rdfs:'http://www.w3.org/2000/01/rdf-schema#',ref:'http://terminusdb.com/schema/ref#',repo:'http://terminusdb.com/schema/repository#',system:'http://terminusdb.com/schema/system#',terminus:'http://terminusdb.com/schema/system#',vio:'http://terminusdb.com/schema/vio#',woql:'http://terminusdb.com/schema/woql#',xdd:'http://terminusdb.com/schema/xdd#',xsd:'http://www.w3.org/2001/XMLSchema#'},selected:[],system:system_descriptor{},transaction_objects:[...],update_guard:_12366,write_graph:repo_graph{database_name:\""bikes\"",name:\""main\"",organization_name:\""admin\"",type:instance}},db_fetch:(...,...),_12324) at /tmp/.mount_.org.cdCakCk/usr/share/terminusdb/core/transaction/database.pl:213\n  [36] setup_call_catcher_cleanup(database:true,database:with_transaction_(...,...,_12554),_12532,database:(...,...)) at /tmp/.mount_.org.cdCakCk/usr/lib/swi-prolog/boot/init.pl:612\n  [32] db_clone:clone_('<garbage_collected>','terminusdb:///system/data/admin',admin,bikes,'<garbage_collected>','<garbage_collected>',false,'<garbage_collected>','<garbage_collected>',_12620) at /tmp/.mount_.org.cdCakCk/usr/share/terminusdb/core/api/db_clone.pl:55\n  [31] setup_call_catcher_cleanup(db_clone:true,db_clone:clone_(...,'terminusdb:///system/data/admin',admin,bikes,\""TerminusDB Bikes Tutorial\"",\""This is the knowledge graph produced by the TerminusDB Bikes Tutorial which is commonly used as an introduction to TerminusDB. This database contains the final results in a format that you can clone complete with all of the data and structure intact so that you don't have to do it yourself. \"",false,\""https://hub.terminusdb.com/kevin/bikes\"",...,_12690),exception(_12694),db_clone:(...;true)) at /tmp/.mount_.org.cdCakCk/usr/lib/swi-prolog/boot/init.pl:612\n  [30] db_clone:clone('<garbage_collected>','terminusdb:///system/data/admin',admin,bikes,'<garbage_collected>','<garbage_collected>',false,'<garbage_collected>','<garbage_collected>','<garbage_collected>') at /tmp/.mount_.org.cdCakCk/usr/share/terminusdb/core/api/db_clone.pl:17\n  [29] '<meta-call>'('<garbage_collected>') <foreign>\n  [28] catch(routes:(...,...),error(ssl_error('1416F086','SSL routines',tls_process_server_certificate,'certificate verify failed'),context(_12860,_12862)),routes:do_or_die(...,...)) at /tmp/.mount_.org.cdCakCk/usr/lib/swi-prolog/boot/init.pl:530\n  [27] catch_with_backtrace('<garbage_collected>','<garbage_collected>','<garbage_collected>') at /tmp/.mount_.org.cdCakCk/usr/lib/swi-prolog/boot/init.pl:580\n\nNote: some frames are missing due to last-call optimization.\nRe-run your program in debug mode (:- debug.) to get more detail.\n\n"",
    ""api:status"": ""api:server_error"",
    ""end"": ""2021-01-21T16:40:32.348Z"",
    ""start"": ""2021-01-21T16:40:32.348Z"",
    ""duration"": ""2021-01-21T16:40:32.348Z""
  }
}",2021-01-22T16:29:50Z,kghenderson,https://github.com/terminusdb/terminusdb/issues/150#issuecomment-765531097,Nice find 👍,nice find 👍,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,150,2021-01-21T16:46:09Z,kghenderson,cloning ssl error,https://github.com/terminusdb/terminusdb/issues/150,"Describe the bug
Attempting to clone the Bikes Tutorial DB from TerminusHub using the Terminus desktop app on Fedora 33 Linux.
Repository fails to clone with error:
Error: ssl_error('1416F086','SSL routines',tls_process_server_certificate,'certificate verify failed') [46] throw(error(ssl_error('1416F086','SSL routines',tls_process_server_certificate,'certificate verify failed'),_12004)) [44] catch(http:http_protocol_hook(https,...,<stream>(0x7f8110010150,0x7f81100106a0),_12064,...),error(ssl_error('1416F086','SSL routines',tls_process_server_certificate,'certificate verify failed'),_12072),http_open:(...,...)) at /tmp/.mount_.org.cdCakCk/usr/lib/swi-prolog/boot/init.pl:531 [43] http_open:try_http_proxy(direct,[uri('https://hub.terminusdb.com/api/pack/kevin/bikes'),...|...],_12134,'<garbage_collected>') at /tmp/.mount_.org.cdCakCk/usr/lib/swi-prolog/library/http/http_open.pl:448 [41] http_client:http_get('https://hub.terminusdb.com/api/pack/kevin/bikes',_12190,[post(...),...|...]) at /tmp/.mount_.org.cdCakCk/usr/lib/swi-prolog/library/http/http_client.pl:139 [39] db_fetch:authorized_fetch('Bearer eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCIsImtpZCI6InhrMEZTOTdVWVFvbHB4ODREeFlpeCJ9.eyJodHRwOi8vdGVybWludXNkYi5jb20vc2NoZW1hL3N5c3RlbSNhZ2VudF9uYW1lIjoia2doZW5kZXJzb24iLCJodHRwOi8vdGVybWludXNkYi5jb20vc2NoZW1hL3N5c3RlbSN1c2VyX2lkZW50aWZpZXIiOiJrZXZpbkBrZ2hlbmRlcnNvbi5jb20iLCJpc3MiOiJodHRwczovL3Rlcm1pbnVzZGIuZXUuYXV0aDAuY29tLyIsInN1YiI6ImF1dGgwfDVmZWRmZTIwMzkxOWJjMDA3NmE2Y2U2OSIsImF1ZCI6WyJodHRwczovL3Rlcm1pbnVzZGIuY29tL2h1YnNlcnZpY2VzIiwiaHR0cHM6Ly90ZXJtaW51c2RiLmV1LmF1dGgwLmNvbS91c2VyaW5mbyJdLCJpYXQiOjE2MTEyNDcxMDksImV4cCI6MTYxMTI4MzEwOSwiYXpwIjoiNEVZSUhRYVZMZ3dpNVY1bTJnUk5ZZHRwWlpmYnd0RHoiLCJzY29wZSI6Im9wZW5pZCBwcm9maWxlIGVtYWlsIn0.hD6DoOAMj790oN3yzcXWatGR7mbn3oto_vd7kmWy_mobmXwNTQVmOUwNr2Ha-Yd1r6ysqfwipl7aNR-FG3GbmwNyVOleZL8THoiF8jp1p6jKsanzhGUw4Q_EDPNWv7brcdkoWtcKZqEg1ZshE7AZhaOZUoLYMX64Cq_uKe6Y7tcKmGf34CoD-pBu6I56s9OGMdk8gS0CWm4FwJYVfYppUjvdbnrW0mY_tW3644OWwOI_N_V_X3-_wqrxj7TfHKhlvzXdcKUId3Pa9RldKF7Mn-pqJAJVkNDybvEl423kxJ73lT01i46ovoBq60-picfQLJCC0_RfqNctONg8siFpkw','<garbage_collected>',none,_12250) at /tmp/.mount_.org.cdCakCk/usr/share/terminusdb/core/api/db_fetch.pl:75 [38] '<meta-call>'('<garbage_collected>') <foreign> [37] database:with_transaction_(query_context{all_witnesses:false,authorization:'terminusdb:///system/data/admin',bindings:[],default_collection:database_descriptor{database_name:""bikes"",organization_name:""admin""},files:[],filter:type_filter{types: ...},prefixes:_12392{api:'http://terminusdb.com/schema/api#',doc:'terminusdb:///repository/data/',layer:'http://terminusdb.com/schema/layer#',owl:'http://www.w3.org/2002/07/owl#',rdf:'http://www.w3.org/1999/02/22-rdf-syntax-ns#',rdfs:'http://www.w3.org/2000/01/rdf-schema#',ref:'http://terminusdb.com/schema/ref#',repo:'http://terminusdb.com/schema/repository#',system:'http://terminusdb.com/schema/system#',terminus:'http://terminusdb.com/schema/system#',vio:'http://terminusdb.com/schema/vio#',woql:'http://terminusdb.com/schema/woql#',xdd:'http://terminusdb.com/schema/xdd#',xsd:'http://www.w3.org/2001/XMLSchema#'},selected:[],system:system_descriptor{},transaction_objects:[...],update_guard:_12366,write_graph:repo_graph{database_name:""bikes"",name:""main"",organization_name:""admin"",type:instance}},db_fetch:(...,...),_12324) at /tmp/.mount_.org.cdCakCk/usr/share/terminusdb/core/transaction/database.pl:213 [36] setup_call_catcher_cleanup(database:true,database:with_transaction_(...,...,_12554),_12532,database:(...,...)) at /tmp/.mount_.org.cdCakCk/usr/lib/swi-prolog/boot/init.pl:612 [32] db_clone:clone_('<garbage_collected>','terminusdb:///system/data/admin',admin,bikes,'<garbage_collected>','<garbage_collected>',false,'<garbage_collected>','<garbage_collected>',_12620) at /tmp/.mount_.org.cdCakCk/usr/share/terminusdb/core/api/db_clone.pl:55 [31] setup_call_catcher_cleanup(db_clone:true,db_clone:clone_(...,'terminusdb:///system/data/admin',admin,bikes,""TerminusDB Bikes Tutorial"",""This is the knowledge graph produced by the TerminusDB Bikes Tutorial which is commonly used as an introduction to TerminusDB. This database contains the final results in a format that you can clone complete with all of the data and structure intact so that you don't have to do it yourself. "",false,""https://hub.terminusdb.com/kevin/bikes"",...,_12690),exception(_12694),db_clone:(...;true)) at /tmp/.mount_.org.cdCakCk/usr/lib/swi-prolog/boot/init.pl:612 [30] db_clone:clone('<garbage_collected>','terminusdb:///system/data/admin',admin,bikes,'<garbage_collected>','<garbage_collected>',false,'<garbage_collected>','<garbage_collected>','<garbage_collected>') at /tmp/.mount_.org.cdCakCk/usr/share/terminusdb/core/api/db_clone.pl:17 [29] '<meta-call>'('<garbage_collected>') <foreign> [28] catch(routes:(...,...),error(ssl_error('1416F086','SSL routines',tls_process_server_certificate,'certificate verify failed'),context(_12860,_12862)),routes:do_or_die(...,...)) at /tmp/.mount_.org.cdCakCk/usr/lib/swi-prolog/boot/init.pl:530 [27] catch_with_backtrace('<garbage_collected>','<garbage_collected>','<garbage_collected>') at /tmp/.mount_.org.cdCakCk/usr/lib/swi-prolog/boot/init.pl:580 Note: some frames are missing due to last-call optimization. Re-run your program in debug mode (:- debug.) to get more detail.less
{
  ""data"": {
    ""api:message"": ""Error: ssl_error('1416F086','SSL routines',tls_process_server_certificate,'certificate verify failed')\n  [46] throw(error(ssl_error('1416F086','SSL routines',tls_process_server_certificate,'certificate verify failed'),_12004))\n  [44] catch(http:http_protocol_hook(https,...,<stream>(0x7f8110010150,0x7f81100106a0),_12064,...),error(ssl_error('1416F086','SSL routines',tls_process_server_certificate,'certificate verify failed'),_12072),http_open:(...,...)) at /tmp/.mount_.org.cdCakCk/usr/lib/swi-prolog/boot/init.pl:531\n  [43] http_open:try_http_proxy(direct,[uri('https://hub.terminusdb.com/api/pack/kevin/bikes'),...|...],_12134,'<garbage_collected>') at /tmp/.mount_.org.cdCakCk/usr/lib/swi-prolog/library/http/http_open.pl:448\n  [41] http_client:http_get('https://hub.terminusdb.com/api/pack/kevin/bikes',_12190,[post(...),...|...]) at /tmp/.mount_.org.cdCakCk/usr/lib/swi-prolog/library/http/http_client.pl:139\n  [39] db_fetch:authorized_fetch('Bearer eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCIsImtpZCI6InhrMEZTOTdVWVFvbHB4ODREeFlpeCJ9.eyJodHRwOi8vdGVybWludXNkYi5jb20vc2NoZW1hL3N5c3RlbSNhZ2VudF9uYW1lIjoia2doZW5kZXJzb24iLCJodHRwOi8vdGVybWludXNkYi5jb20vc2NoZW1hL3N5c3RlbSN1c2VyX2lkZW50aWZpZXIiOiJrZXZpbkBrZ2hlbmRlcnNvbi5jb20iLCJpc3MiOiJodHRwczovL3Rlcm1pbnVzZGIuZXUuYXV0aDAuY29tLyIsInN1YiI6ImF1dGgwfDVmZWRmZTIwMzkxOWJjMDA3NmE2Y2U2OSIsImF1ZCI6WyJodHRwczovL3Rlcm1pbnVzZGIuY29tL2h1YnNlcnZpY2VzIiwiaHR0cHM6Ly90ZXJtaW51c2RiLmV1LmF1dGgwLmNvbS91c2VyaW5mbyJdLCJpYXQiOjE2MTEyNDcxMDksImV4cCI6MTYxMTI4MzEwOSwiYXpwIjoiNEVZSUhRYVZMZ3dpNVY1bTJnUk5ZZHRwWlpmYnd0RHoiLCJzY29wZSI6Im9wZW5pZCBwcm9maWxlIGVtYWlsIn0.hD6DoOAMj790oN3yzcXWatGR7mbn3oto_vd7kmWy_mobmXwNTQVmOUwNr2Ha-Yd1r6ysqfwipl7aNR-FG3GbmwNyVOleZL8THoiF8jp1p6jKsanzhGUw4Q_EDPNWv7brcdkoWtcKZqEg1ZshE7AZhaOZUoLYMX64Cq_uKe6Y7tcKmGf34CoD-pBu6I56s9OGMdk8gS0CWm4FwJYVfYppUjvdbnrW0mY_tW3644OWwOI_N_V_X3-_wqrxj7TfHKhlvzXdcKUId3Pa9RldKF7Mn-pqJAJVkNDybvEl423kxJ73lT01i46ovoBq60-picfQLJCC0_RfqNctONg8siFpkw','<garbage_collected>',none,_12250) at /tmp/.mount_.org.cdCakCk/usr/share/terminusdb/core/api/db_fetch.pl:75\n  [38] '<meta-call>'('<garbage_collected>') <foreign>\n  [37] database:with_transaction_(query_context{all_witnesses:false,authorization:'terminusdb:///system/data/admin',bindings:[],default_collection:database_descriptor{database_name:\""bikes\"",organization_name:\""admin\""},files:[],filter:type_filter{types: ...},prefixes:_12392{api:'http://terminusdb.com/schema/api#',doc:'terminusdb:///repository/data/',layer:'http://terminusdb.com/schema/layer#',owl:'http://www.w3.org/2002/07/owl#',rdf:'http://www.w3.org/1999/02/22-rdf-syntax-ns#',rdfs:'http://www.w3.org/2000/01/rdf-schema#',ref:'http://terminusdb.com/schema/ref#',repo:'http://terminusdb.com/schema/repository#',system:'http://terminusdb.com/schema/system#',terminus:'http://terminusdb.com/schema/system#',vio:'http://terminusdb.com/schema/vio#',woql:'http://terminusdb.com/schema/woql#',xdd:'http://terminusdb.com/schema/xdd#',xsd:'http://www.w3.org/2001/XMLSchema#'},selected:[],system:system_descriptor{},transaction_objects:[...],update_guard:_12366,write_graph:repo_graph{database_name:\""bikes\"",name:\""main\"",organization_name:\""admin\"",type:instance}},db_fetch:(...,...),_12324) at /tmp/.mount_.org.cdCakCk/usr/share/terminusdb/core/transaction/database.pl:213\n  [36] setup_call_catcher_cleanup(database:true,database:with_transaction_(...,...,_12554),_12532,database:(...,...)) at /tmp/.mount_.org.cdCakCk/usr/lib/swi-prolog/boot/init.pl:612\n  [32] db_clone:clone_('<garbage_collected>','terminusdb:///system/data/admin',admin,bikes,'<garbage_collected>','<garbage_collected>',false,'<garbage_collected>','<garbage_collected>',_12620) at /tmp/.mount_.org.cdCakCk/usr/share/terminusdb/core/api/db_clone.pl:55\n  [31] setup_call_catcher_cleanup(db_clone:true,db_clone:clone_(...,'terminusdb:///system/data/admin',admin,bikes,\""TerminusDB Bikes Tutorial\"",\""This is the knowledge graph produced by the TerminusDB Bikes Tutorial which is commonly used as an introduction to TerminusDB. This database contains the final results in a format that you can clone complete with all of the data and structure intact so that you don't have to do it yourself. \"",false,\""https://hub.terminusdb.com/kevin/bikes\"",...,_12690),exception(_12694),db_clone:(...;true)) at /tmp/.mount_.org.cdCakCk/usr/lib/swi-prolog/boot/init.pl:612\n  [30] db_clone:clone('<garbage_collected>','terminusdb:///system/data/admin',admin,bikes,'<garbage_collected>','<garbage_collected>',false,'<garbage_collected>','<garbage_collected>','<garbage_collected>') at /tmp/.mount_.org.cdCakCk/usr/share/terminusdb/core/api/db_clone.pl:17\n  [29] '<meta-call>'('<garbage_collected>') <foreign>\n  [28] catch(routes:(...,...),error(ssl_error('1416F086','SSL routines',tls_process_server_certificate,'certificate verify failed'),context(_12860,_12862)),routes:do_or_die(...,...)) at /tmp/.mount_.org.cdCakCk/usr/lib/swi-prolog/boot/init.pl:530\n  [27] catch_with_backtrace('<garbage_collected>','<garbage_collected>','<garbage_collected>') at /tmp/.mount_.org.cdCakCk/usr/lib/swi-prolog/boot/init.pl:580\n\nNote: some frames are missing due to last-call optimization.\nRe-run your program in debug mode (:- debug.) to get more detail.\n\n"",
    ""api:status"": ""api:server_error"",
    ""end"": ""2021-01-21T16:40:32.348Z"",
    ""start"": ""2021-01-21T16:40:32.348Z"",
    ""duration"": ""2021-01-21T16:40:32.348Z""
  }
}",2021-01-29T13:25:39Z,rrooij,https://github.com/terminusdb/terminusdb/issues/150#issuecomment-769802873,We fixed the issue on dev. Eventually it should be included in the release.,-PRON- fix the issue on dev eventually -PRON- should be include in the release,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,150,2021-01-21T16:46:09Z,kghenderson,cloning ssl error,https://github.com/terminusdb/terminusdb/issues/150,"Describe the bug
Attempting to clone the Bikes Tutorial DB from TerminusHub using the Terminus desktop app on Fedora 33 Linux.
Repository fails to clone with error:
Error: ssl_error('1416F086','SSL routines',tls_process_server_certificate,'certificate verify failed') [46] throw(error(ssl_error('1416F086','SSL routines',tls_process_server_certificate,'certificate verify failed'),_12004)) [44] catch(http:http_protocol_hook(https,...,<stream>(0x7f8110010150,0x7f81100106a0),_12064,...),error(ssl_error('1416F086','SSL routines',tls_process_server_certificate,'certificate verify failed'),_12072),http_open:(...,...)) at /tmp/.mount_.org.cdCakCk/usr/lib/swi-prolog/boot/init.pl:531 [43] http_open:try_http_proxy(direct,[uri('https://hub.terminusdb.com/api/pack/kevin/bikes'),...|...],_12134,'<garbage_collected>') at /tmp/.mount_.org.cdCakCk/usr/lib/swi-prolog/library/http/http_open.pl:448 [41] http_client:http_get('https://hub.terminusdb.com/api/pack/kevin/bikes',_12190,[post(...),...|...]) at /tmp/.mount_.org.cdCakCk/usr/lib/swi-prolog/library/http/http_client.pl:139 [39] db_fetch:authorized_fetch('Bearer eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCIsImtpZCI6InhrMEZTOTdVWVFvbHB4ODREeFlpeCJ9.eyJodHRwOi8vdGVybWludXNkYi5jb20vc2NoZW1hL3N5c3RlbSNhZ2VudF9uYW1lIjoia2doZW5kZXJzb24iLCJodHRwOi8vdGVybWludXNkYi5jb20vc2NoZW1hL3N5c3RlbSN1c2VyX2lkZW50aWZpZXIiOiJrZXZpbkBrZ2hlbmRlcnNvbi5jb20iLCJpc3MiOiJodHRwczovL3Rlcm1pbnVzZGIuZXUuYXV0aDAuY29tLyIsInN1YiI6ImF1dGgwfDVmZWRmZTIwMzkxOWJjMDA3NmE2Y2U2OSIsImF1ZCI6WyJodHRwczovL3Rlcm1pbnVzZGIuY29tL2h1YnNlcnZpY2VzIiwiaHR0cHM6Ly90ZXJtaW51c2RiLmV1LmF1dGgwLmNvbS91c2VyaW5mbyJdLCJpYXQiOjE2MTEyNDcxMDksImV4cCI6MTYxMTI4MzEwOSwiYXpwIjoiNEVZSUhRYVZMZ3dpNVY1bTJnUk5ZZHRwWlpmYnd0RHoiLCJzY29wZSI6Im9wZW5pZCBwcm9maWxlIGVtYWlsIn0.hD6DoOAMj790oN3yzcXWatGR7mbn3oto_vd7kmWy_mobmXwNTQVmOUwNr2Ha-Yd1r6ysqfwipl7aNR-FG3GbmwNyVOleZL8THoiF8jp1p6jKsanzhGUw4Q_EDPNWv7brcdkoWtcKZqEg1ZshE7AZhaOZUoLYMX64Cq_uKe6Y7tcKmGf34CoD-pBu6I56s9OGMdk8gS0CWm4FwJYVfYppUjvdbnrW0mY_tW3644OWwOI_N_V_X3-_wqrxj7TfHKhlvzXdcKUId3Pa9RldKF7Mn-pqJAJVkNDybvEl423kxJ73lT01i46ovoBq60-picfQLJCC0_RfqNctONg8siFpkw','<garbage_collected>',none,_12250) at /tmp/.mount_.org.cdCakCk/usr/share/terminusdb/core/api/db_fetch.pl:75 [38] '<meta-call>'('<garbage_collected>') <foreign> [37] database:with_transaction_(query_context{all_witnesses:false,authorization:'terminusdb:///system/data/admin',bindings:[],default_collection:database_descriptor{database_name:""bikes"",organization_name:""admin""},files:[],filter:type_filter{types: ...},prefixes:_12392{api:'http://terminusdb.com/schema/api#',doc:'terminusdb:///repository/data/',layer:'http://terminusdb.com/schema/layer#',owl:'http://www.w3.org/2002/07/owl#',rdf:'http://www.w3.org/1999/02/22-rdf-syntax-ns#',rdfs:'http://www.w3.org/2000/01/rdf-schema#',ref:'http://terminusdb.com/schema/ref#',repo:'http://terminusdb.com/schema/repository#',system:'http://terminusdb.com/schema/system#',terminus:'http://terminusdb.com/schema/system#',vio:'http://terminusdb.com/schema/vio#',woql:'http://terminusdb.com/schema/woql#',xdd:'http://terminusdb.com/schema/xdd#',xsd:'http://www.w3.org/2001/XMLSchema#'},selected:[],system:system_descriptor{},transaction_objects:[...],update_guard:_12366,write_graph:repo_graph{database_name:""bikes"",name:""main"",organization_name:""admin"",type:instance}},db_fetch:(...,...),_12324) at /tmp/.mount_.org.cdCakCk/usr/share/terminusdb/core/transaction/database.pl:213 [36] setup_call_catcher_cleanup(database:true,database:with_transaction_(...,...,_12554),_12532,database:(...,...)) at /tmp/.mount_.org.cdCakCk/usr/lib/swi-prolog/boot/init.pl:612 [32] db_clone:clone_('<garbage_collected>','terminusdb:///system/data/admin',admin,bikes,'<garbage_collected>','<garbage_collected>',false,'<garbage_collected>','<garbage_collected>',_12620) at /tmp/.mount_.org.cdCakCk/usr/share/terminusdb/core/api/db_clone.pl:55 [31] setup_call_catcher_cleanup(db_clone:true,db_clone:clone_(...,'terminusdb:///system/data/admin',admin,bikes,""TerminusDB Bikes Tutorial"",""This is the knowledge graph produced by the TerminusDB Bikes Tutorial which is commonly used as an introduction to TerminusDB. This database contains the final results in a format that you can clone complete with all of the data and structure intact so that you don't have to do it yourself. "",false,""https://hub.terminusdb.com/kevin/bikes"",...,_12690),exception(_12694),db_clone:(...;true)) at /tmp/.mount_.org.cdCakCk/usr/lib/swi-prolog/boot/init.pl:612 [30] db_clone:clone('<garbage_collected>','terminusdb:///system/data/admin',admin,bikes,'<garbage_collected>','<garbage_collected>',false,'<garbage_collected>','<garbage_collected>','<garbage_collected>') at /tmp/.mount_.org.cdCakCk/usr/share/terminusdb/core/api/db_clone.pl:17 [29] '<meta-call>'('<garbage_collected>') <foreign> [28] catch(routes:(...,...),error(ssl_error('1416F086','SSL routines',tls_process_server_certificate,'certificate verify failed'),context(_12860,_12862)),routes:do_or_die(...,...)) at /tmp/.mount_.org.cdCakCk/usr/lib/swi-prolog/boot/init.pl:530 [27] catch_with_backtrace('<garbage_collected>','<garbage_collected>','<garbage_collected>') at /tmp/.mount_.org.cdCakCk/usr/lib/swi-prolog/boot/init.pl:580 Note: some frames are missing due to last-call optimization. Re-run your program in debug mode (:- debug.) to get more detail.less
{
  ""data"": {
    ""api:message"": ""Error: ssl_error('1416F086','SSL routines',tls_process_server_certificate,'certificate verify failed')\n  [46] throw(error(ssl_error('1416F086','SSL routines',tls_process_server_certificate,'certificate verify failed'),_12004))\n  [44] catch(http:http_protocol_hook(https,...,<stream>(0x7f8110010150,0x7f81100106a0),_12064,...),error(ssl_error('1416F086','SSL routines',tls_process_server_certificate,'certificate verify failed'),_12072),http_open:(...,...)) at /tmp/.mount_.org.cdCakCk/usr/lib/swi-prolog/boot/init.pl:531\n  [43] http_open:try_http_proxy(direct,[uri('https://hub.terminusdb.com/api/pack/kevin/bikes'),...|...],_12134,'<garbage_collected>') at /tmp/.mount_.org.cdCakCk/usr/lib/swi-prolog/library/http/http_open.pl:448\n  [41] http_client:http_get('https://hub.terminusdb.com/api/pack/kevin/bikes',_12190,[post(...),...|...]) at /tmp/.mount_.org.cdCakCk/usr/lib/swi-prolog/library/http/http_client.pl:139\n  [39] db_fetch:authorized_fetch('Bearer eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCIsImtpZCI6InhrMEZTOTdVWVFvbHB4ODREeFlpeCJ9.eyJodHRwOi8vdGVybWludXNkYi5jb20vc2NoZW1hL3N5c3RlbSNhZ2VudF9uYW1lIjoia2doZW5kZXJzb24iLCJodHRwOi8vdGVybWludXNkYi5jb20vc2NoZW1hL3N5c3RlbSN1c2VyX2lkZW50aWZpZXIiOiJrZXZpbkBrZ2hlbmRlcnNvbi5jb20iLCJpc3MiOiJodHRwczovL3Rlcm1pbnVzZGIuZXUuYXV0aDAuY29tLyIsInN1YiI6ImF1dGgwfDVmZWRmZTIwMzkxOWJjMDA3NmE2Y2U2OSIsImF1ZCI6WyJodHRwczovL3Rlcm1pbnVzZGIuY29tL2h1YnNlcnZpY2VzIiwiaHR0cHM6Ly90ZXJtaW51c2RiLmV1LmF1dGgwLmNvbS91c2VyaW5mbyJdLCJpYXQiOjE2MTEyNDcxMDksImV4cCI6MTYxMTI4MzEwOSwiYXpwIjoiNEVZSUhRYVZMZ3dpNVY1bTJnUk5ZZHRwWlpmYnd0RHoiLCJzY29wZSI6Im9wZW5pZCBwcm9maWxlIGVtYWlsIn0.hD6DoOAMj790oN3yzcXWatGR7mbn3oto_vd7kmWy_mobmXwNTQVmOUwNr2Ha-Yd1r6ysqfwipl7aNR-FG3GbmwNyVOleZL8THoiF8jp1p6jKsanzhGUw4Q_EDPNWv7brcdkoWtcKZqEg1ZshE7AZhaOZUoLYMX64Cq_uKe6Y7tcKmGf34CoD-pBu6I56s9OGMdk8gS0CWm4FwJYVfYppUjvdbnrW0mY_tW3644OWwOI_N_V_X3-_wqrxj7TfHKhlvzXdcKUId3Pa9RldKF7Mn-pqJAJVkNDybvEl423kxJ73lT01i46ovoBq60-picfQLJCC0_RfqNctONg8siFpkw','<garbage_collected>',none,_12250) at /tmp/.mount_.org.cdCakCk/usr/share/terminusdb/core/api/db_fetch.pl:75\n  [38] '<meta-call>'('<garbage_collected>') <foreign>\n  [37] database:with_transaction_(query_context{all_witnesses:false,authorization:'terminusdb:///system/data/admin',bindings:[],default_collection:database_descriptor{database_name:\""bikes\"",organization_name:\""admin\""},files:[],filter:type_filter{types: ...},prefixes:_12392{api:'http://terminusdb.com/schema/api#',doc:'terminusdb:///repository/data/',layer:'http://terminusdb.com/schema/layer#',owl:'http://www.w3.org/2002/07/owl#',rdf:'http://www.w3.org/1999/02/22-rdf-syntax-ns#',rdfs:'http://www.w3.org/2000/01/rdf-schema#',ref:'http://terminusdb.com/schema/ref#',repo:'http://terminusdb.com/schema/repository#',system:'http://terminusdb.com/schema/system#',terminus:'http://terminusdb.com/schema/system#',vio:'http://terminusdb.com/schema/vio#',woql:'http://terminusdb.com/schema/woql#',xdd:'http://terminusdb.com/schema/xdd#',xsd:'http://www.w3.org/2001/XMLSchema#'},selected:[],system:system_descriptor{},transaction_objects:[...],update_guard:_12366,write_graph:repo_graph{database_name:\""bikes\"",name:\""main\"",organization_name:\""admin\"",type:instance}},db_fetch:(...,...),_12324) at /tmp/.mount_.org.cdCakCk/usr/share/terminusdb/core/transaction/database.pl:213\n  [36] setup_call_catcher_cleanup(database:true,database:with_transaction_(...,...,_12554),_12532,database:(...,...)) at /tmp/.mount_.org.cdCakCk/usr/lib/swi-prolog/boot/init.pl:612\n  [32] db_clone:clone_('<garbage_collected>','terminusdb:///system/data/admin',admin,bikes,'<garbage_collected>','<garbage_collected>',false,'<garbage_collected>','<garbage_collected>',_12620) at /tmp/.mount_.org.cdCakCk/usr/share/terminusdb/core/api/db_clone.pl:55\n  [31] setup_call_catcher_cleanup(db_clone:true,db_clone:clone_(...,'terminusdb:///system/data/admin',admin,bikes,\""TerminusDB Bikes Tutorial\"",\""This is the knowledge graph produced by the TerminusDB Bikes Tutorial which is commonly used as an introduction to TerminusDB. This database contains the final results in a format that you can clone complete with all of the data and structure intact so that you don't have to do it yourself. \"",false,\""https://hub.terminusdb.com/kevin/bikes\"",...,_12690),exception(_12694),db_clone:(...;true)) at /tmp/.mount_.org.cdCakCk/usr/lib/swi-prolog/boot/init.pl:612\n  [30] db_clone:clone('<garbage_collected>','terminusdb:///system/data/admin',admin,bikes,'<garbage_collected>','<garbage_collected>',false,'<garbage_collected>','<garbage_collected>','<garbage_collected>') at /tmp/.mount_.org.cdCakCk/usr/share/terminusdb/core/api/db_clone.pl:17\n  [29] '<meta-call>'('<garbage_collected>') <foreign>\n  [28] catch(routes:(...,...),error(ssl_error('1416F086','SSL routines',tls_process_server_certificate,'certificate verify failed'),context(_12860,_12862)),routes:do_or_die(...,...)) at /tmp/.mount_.org.cdCakCk/usr/lib/swi-prolog/boot/init.pl:530\n  [27] catch_with_backtrace('<garbage_collected>','<garbage_collected>','<garbage_collected>') at /tmp/.mount_.org.cdCakCk/usr/lib/swi-prolog/boot/init.pl:580\n\nNote: some frames are missing due to last-call optimization.\nRe-run your program in debug mode (:- debug.) to get more detail.\n\n"",
    ""api:status"": ""api:server_error"",
    ""end"": ""2021-01-21T16:40:32.348Z"",
    ""start"": ""2021-01-21T16:40:32.348Z"",
    ""duration"": ""2021-01-21T16:40:32.348Z""
  }
}",2021-02-18T14:21:20Z,luke-feeney,https://github.com/terminusdb/terminusdb/issues/150#issuecomment-781377064,this is fixed! will be in the next release!,this be fix will be in the next release,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,151,2021-01-26T14:11:48Z,luke-feeney,Minimise Electron App in Windows,https://github.com/terminusdb/terminusdb/issues/151,"This is half bug half enhancement - when you minimise the electron in windows it goes to the tray and not the toolbar/taskbar. Would be much handier if it minimised to the taskbar and closed to the tray.
To Reproduce
Steps to reproduce the behavior:

Go to electron app in windows
Click on minimise
See pov

Expected behavior
Would be better if it went to the bar at the bottom

Info (please complete the following information):

OS: Windows 10
Windows Electron App

Additional context
Love the product!",2021-03-15T16:41:51Z,Cheukting,https://github.com/terminusdb/terminusdb/issues/151#issuecomment-799568188,Solved! Thanks @rrooij,solve thank @rrooij,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,152,2021-01-28T16:44:07Z,pbaille,Typo in Server documentation,https://github.com/terminusdb/terminusdb/issues/152,"One segment of the URL is missing in the template for Delete Database section
The given code is:
DELETE http://localhost:6363/api/<organization>/<dbid>
where it should be:
DELETE http://localhost:6363/api/db/<organization>/<dbid>",2021-01-28T16:45:21Z,GavinMendelGleason,https://github.com/terminusdb/terminusdb/issues/152#issuecomment-769216985,Fixed in terminusdb/terminusdb-doc@5950bd5,fix in terminusdb / terminusdb - doc@5950bd5,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,154,2021-02-03T11:21:13Z,GavinMendelGleason,"Bad error report for ""Failed to merge"" ""graph_sets_not_included""",https://github.com/terminusdb/terminusdb/issues/154,"A generic error report is given for this specific graph_sets_not_included error:
{
  ""data"": {
    ""api:message"": ""Error: graph_sets_not_included(doc:'ValidCommit_dj9578n4totlczk94086dg0lihx1urh',doc:'ValidCommit_fb4si5lv7j4bkiaspf2kjcsthehysl3')"",
    ""api:status"": ""api:server_error"",
    ""end"": ""2021-02-02T10:58:42.368Z"",
    ""start"": ""2021-02-02T10:58:42.368Z"",
    ""duration"": ""2021-02-02T10:58:42.368Z""
  }
}
To Reproduce
Create a database without a schema, create a branch with a schema. Attempt to rebase from one to the other.
Expected behavior
We should rather return a specific API JSON-LD object report for this error.",,,,,,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,155,2021-02-04T12:27:48Z,luke-feeney,Create a new structure in the docs in the repo to mirror the documentation system,https://github.com/terminusdb/terminusdb/issues/155,"We are using the tutorials, how-to guides, explanation, reference breakdown as provided here:
https://documentation.divio.com/
The docs index needs to be updated to reflect this change",2021-02-10T13:10:36Z,luke-feeney,https://github.com/terminusdb/terminusdb/issues/155#issuecomment-776695480,"This is done see here: https://terminusdb.github.io/terminusdb/#/
Always needs more work, but marking done pending revisit",this be do see here https//terminusdbgithubio / terminusdb/#/ always need more work but mark do pende revisit,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,156,2021-02-04T12:33:52Z,luke-feeney,Move existing good content into new structure,https://github.com/terminusdb/terminusdb/issues/156,"Good Content to be moved:
Tutorials
Importing and Cleaning Data From CSVs
Reference
(material to be updated with non reference to be removed)
Server
JS Client - with WOQL.js - the Definitive Guide
Python Client
How Tos
How to Import and Clean CSV Data
How to Query
How to create a schema
Python How Tos
How to Accessing TerminusHub with Bootstrap - ENVs
Discussion (to become Explanation)
Storage
Data Modeling
Querying
Research Papers",2021-02-10T13:20:42Z,luke-feeney,https://github.com/terminusdb/terminusdb/issues/156#issuecomment-776701153,"This is challenging as much of the existing content includes great stuff, but also stuff that is in the wrong place.
We should discuss - especially in terms of reference material.",this be challenging as much of the exist content include great stuff but also stuff that be in the wrong place -PRON- should discuss - especially in term of reference material,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,157,2021-02-04T12:50:11Z,luke-feeney,Pick and implement themes ,https://github.com/terminusdb/terminusdb/issues/157,Implement new docsify theme that isn't so dark (as the logo is eaten by current theme),2021-02-10T13:09:35Z,luke-feeney,https://github.com/terminusdb/terminusdb/issues/157#issuecomment-776694979,"Marking this as done given that I have picked and implemented a theme, but always could reopen.",mark this as do give that i have pick and implement a theme but always could reopen,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,158,2021-02-04T13:14:00Z,luke-feeney,Getting Started with Python Client,https://github.com/terminusdb/terminusdb/issues/158,"Assume Installation already done
Create Schema
Import Data
Query Data
Use Branching",2021-02-10T15:43:09Z,luke-feeney,https://github.com/terminusdb/terminusdb/issues/158#issuecomment-776802128,"This goes in here:
https://github.com/terminusdb/terminusdb/blob/master/docs/Intro_Tutorials/Start_With_Python.md
Or can be a comment on this issue and we can move across.
Idea it to keep it simple and get the person from 0 to 1 - installation is already included in the intro, so tutorial can just refer them back to the install instructions.
@Cheukting - one for you",this go in here https//githubcom / terminusdb / terminusdb / blob / master / doc / intro_tutorial / start_with_pythonmd or can be a comment on this issue and -PRON- can move across idea -PRON- to keep -PRON- simple and get the person from 0 to 1 - installation be already include in the intro so tutorial can just refer -PRON- back to the install instruction @cheukte - one for -PRON-,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,158,2021-02-04T13:14:00Z,luke-feeney,Getting Started with Python Client,https://github.com/terminusdb/terminusdb/issues/158,"Assume Installation already done
Create Schema
Import Data
Query Data
Use Branching",2021-02-11T15:39:28Z,Cheukting,https://github.com/terminusdb/terminusdb/issues/158#issuecomment-777585484,"TerminusDB Python client supports Python version 3.6 or above. It provides Python methods to perform database management, query database,  schema building, import and update data.
This guide assumes you already have TerminusDB installed.
Download Python
Official distribution of Python can be downloaded at python.org. There are also other popular distributions available. For example Anaconda distribution
Setup virtual environment for Python (Optional)
It is recommended to set up a separate virtual environment for your Python project. To do that, you can either use venv that is provided with your official Python distribution or other popular tools like conda, pipenv and poetry.
Install TerminusDB Client for Python
TerminusDB Client can be installed using pip
python -m pip install terminusdb-client
This provides you with the basic Python client.
Optionally, you may want to install including the WOQLDataframe which allow you to convert your query result into a pandas DataFrame
python -m pip install 'terminusdb-client[dataframe]'
For details about the versions of the Python clients and more advance installation options, please refer to the README of the GitHub repo.",terminusdb python client support python version 36 or above -PRON- provide python method to perform database management query database schema building import and update datum this guide assume -PRON- already have terminusdb instal download python official distribution of python can be download at pythonorg there be also other popular distribution available for example anaconda distribution setup virtual environment for python ( optional ) -PRON- be recommend to set up a separate virtual environment for -PRON- python project to do that -PRON- can either use venv that be provide with -PRON- official python distribution or other popular tool like conda pipenv and poetry install terminusdb client for python terminusdb client can be instal use pip python -m pip install terminusdb - client this provide -PRON- with the basic python client optionally -PRON- may want to install include the woqldataframe which allow -PRON- to convert -PRON- query result into a pandas dataframe python -m pip install ' terminusdb - client[dataframe ] ' for detail about the version of the python client and more advance installation option please refer to the readme of the github repo,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,158,2021-02-04T13:14:00Z,luke-feeney,Getting Started with Python Client,https://github.com/terminusdb/terminusdb/issues/158,"Assume Installation already done
Create Schema
Import Data
Query Data
Use Branching",2021-02-11T15:39:59Z,Cheukting,https://github.com/terminusdb/terminusdb/issues/158#issuecomment-777585917,"@luke-feeney, please review",@luke - feeney please review,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,158,2021-02-04T13:14:00Z,luke-feeney,Getting Started with Python Client,https://github.com/terminusdb/terminusdb/issues/158,"Assume Installation already done
Create Schema
Import Data
Query Data
Use Branching",2021-02-17T14:17:50Z,luke-feeney,https://github.com/terminusdb/terminusdb/issues/158#issuecomment-780586334,Thanks! That's great,thank that be great,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,159,2021-02-04T13:14:40Z,luke-feeney,Tutorial: Getting Started with JS Client,https://github.com/terminusdb/terminusdb/issues/159,"Assume Installation already done
Create Schema
Import Data
Query Data
Use Branching",2021-02-10T15:44:24Z,luke-feeney,https://github.com/terminusdb/terminusdb/issues/159#issuecomment-776802939,"This goes in here:
https://github.com/terminusdb/terminusdb/blob/master/docs/Intro_Tutorials/Start_with_JS.md
Or can be a comment on this issue and we can move across.
Idea it to keep it simple and get the person from 0 to 1 - installation is already included in the intro, so tutorial can just refer them back to the install instructions.
@Francesca-Bit - happy to take a look at it as well!",this go in here https//githubcom / terminusdb / terminusdb / blob / master / doc / intro_tutorial / start_with_jsmd or can be a comment on this issue and -PRON- can move across idea -PRON- to keep -PRON- simple and get the person from 0 to 1 - installation be already include in the intro so tutorial can just refer -PRON- back to the install instruction @francesca - bit - happy to take a look at -PRON- as well,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,159,2021-02-04T13:14:40Z,luke-feeney,Tutorial: Getting Started with JS Client,https://github.com/terminusdb/terminusdb/issues/159,"Assume Installation already done
Create Schema
Import Data
Query Data
Use Branching",2021-03-23T21:51:03Z,Francesca-Bit,https://github.com/terminusdb/terminusdb/issues/159#issuecomment-805290185,create the  document page Start with js,create the document page start with js,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,160,2021-02-04T13:15:07Z,luke-feeney,Tutorial: Getting Started with the CLI,https://github.com/terminusdb/terminusdb/issues/160,"Assume Installation already done
Create Schema
Import Data
Query Data
Use Branching",2021-02-10T15:46:03Z,luke-feeney,https://github.com/terminusdb/terminusdb/issues/160#issuecomment-776804115,"This goes in here:
https://github.com/terminusdb/terminusdb/blob/master/docs/Intro_Tutorials/Start_With_CLI.md
Or can be a comment on this issue and we can move across.
Idea it to keep it simple and get the person from 0 to 1
@GavinMendelGleason - nobody knows it better",this go in here https//githubcom / terminusdb / terminusdb / blob / master / doc / intro_tutorial / start_with_climd or can be a comment on this issue and -PRON- can move across idea -PRON- to keep -PRON- simple and get the person from 0 to 1 @gavinmendelgleason - nobody know -PRON- well,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,161,2021-02-04T13:16:42Z,luke-feeney,Tutorial: How do I use the model builder tool?,https://github.com/terminusdb/terminusdb/issues/161,"Try to give an overview and answer some specific questions:
What kind, type or how much ""data"" is stored in a particular class?
Is there an intermediate class to handle boxing/Grouping?
What's the best way to start modelling (with this GUI)
ie. creating categories to find possible groups ( using the category assembler)
*Document classes are top-level classes usually storing BLAR BLAR up to a certain granularity , the decisions for how much granularity are influenced by these factors (BLAR BLAR), because of this document classes do not reside in other document classes?
*Normal classes are ""data chunks""  used to assemble a larger view/granularity within a document class. The reason for this is to allow for BLAR BLAR connections to be attached to the normal class and allows for BLAR BLAR  .
*Entry Classes, as you might expect the name says it all as this does BLAR BLAR
entry classes allow for filtering and datatype limits for BLAR BLAR, this is important for BLAR BLAR as these often help with this requirement of BLAR BLAR.
*Choice Classes, are a favourite for BLAR BLAR as these allow for BLAR BLAR , and are often used by those in the know. An example of this is when BLAR BLAR .
Connecting this all together is where the relationships come in and is based off your data analysis, relationships should obviously be between things that are related; Helpful methods for determining this is BLAR BLAR. and as you can clearly see by the above classes this granularity and potential flow can be easily achieved, here are some examples.",2021-02-10T15:49:02Z,luke-feeney,https://github.com/terminusdb/terminusdb/issues/161#issuecomment-776806186,"This is a new tutorial for the tutorial repo - it is really a cover all to try to capture a bunch of different questions that have come up in the discord.
https://github.com/terminusdb/terminusdb-tutorials
@Francesca-Bit - I think you'd be best at this and it would mean that you don't get so many data modelling questions in the server!",this be a new tutorial for the tutorial repo - -PRON- be really a cover all to try to capture a bunch of different question that have come up in the discord https//githubcom / terminusdb / terminusdb - tutorial @francesca - bit - i think -PRON- 'd be good at this and -PRON- would mean that -PRON- do not get so many datum model question in the server,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,162,2021-02-04T13:17:54Z,luke-feeney,Move 'How-Tos' into tutorial repo,https://github.com/terminusdb/terminusdb/issues/162,"These are really tutorials and need to be moved to that repo:
https://terminusdb.com/docs/how-tos/query/
https://terminusdb.com/docs/how-tos/schema/",2021-02-11T17:45:22Z,luke-feeney,https://github.com/terminusdb/terminusdb/issues/162#issuecomment-777671429,"Ok - have recreated these two as query and schema intro tutorials:
https://github.com/terminusdb/terminusdb/blob/master/docs/Intro_Tutorials/START_QUERY.md
https://github.com/terminusdb/terminusdb/blob/master/docs/Intro_Tutorials/START_SCHEMA.md
As original author could you pass over an eye @GavinMendelGleason
Might extract the mathematical operations bit and put it in a How-To",ok - have recreate these two as query and schema intro tutorials https//githubcom / terminusdb / terminusdb / blob / master / doc / intro_tutorial / start_querymd https//githubcom / terminusdb / terminusdb / blob / master / doc / intro_tutorial / start_schemamd as original author could -PRON- pass over an eye @gavinmendelgleason may extract the mathematical operation bite and put -PRON- in a how - to,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,163,2021-02-04T13:24:12Z,luke-feeney,How-To: get started writing a client in TerminusDB,https://github.com/terminusdb/terminusdb/issues/163,"How to get started writing a client in TerminusDB
All the basic steps to get started including links to curl.md",2021-02-10T15:59:11Z,luke-feeney,https://github.com/terminusdb/terminusdb/issues/163#issuecomment-776813475,"this is just a short how to get started writing a CLI for TerminusDB.
Goes in here:
https://github.com/terminusdb/terminusdb/blob/master/docs/How_To/write_client.md
This curl is useful marker:
https://github.com/terminusdb/terminusdb/blob/master/docs/CURL.md
Remember How-To so just steps with assumption of basic understanding. (Might be useful to talk to Pierre or Chris in the discord server to understand their challenges when writing a client.
@GavinMendelGleason - another one for you!",this be just a short how to get start write a cli for terminusdb go in here https//githubcom / terminusdb / terminusdb / blob / master / docs / how_to / write_clientmd this curl be useful marker https//githubcom / terminusdb / terminusdb / blob / master / doc / curlmd remember how - to so just step with assumption of basic understanding ( may be useful to talk to pierre or chris in the discord server to understand -PRON- challenge when write a client @gavinmendelgleason - another one for -PRON-,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,164,2021-02-04T13:25:05Z,luke-feeney,How-To: spin up an application in TerminusDB,https://github.com/terminusdb/terminusdb/issues/164,"How to spin up an application in TerminusDB
This is basically the ToDo application by @dmytri without the explanations",2021-02-10T16:02:46Z,luke-feeney,https://github.com/terminusdb/terminusdb/issues/164#issuecomment-776816055,"In here:
https://github.com/terminusdb/terminusdb/blob/master/docs/How_To/application.md
This is effectively taking the TODO application example prepared by @dmytri and boiling it down to the 'How-To' steps. So making it a few short jumps. Not 100% sure if this is better as a tutorial, but lets try as a really simple  How-To.
https://github.com/dmytri/terminusdb-todoapp
@dmytri - continue the winning streak",in here https//githubcom / terminusdb / terminusdb / blob / master / docs / how_to / applicationmd this be effectively take the todo application example prepare by @dmytri and boil -PRON- down to the ' how - to ' step so make -PRON- a few short jump not 100 % sure if this be well as a tutorial but let try as a really simple how - to https//githubcom / dmytri / terminusdb - todoapp @dmytri - continue the win streak,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,165,2021-02-04T13:25:51Z,luke-feeney,How-To: visualise a random schema in TerminusDB,https://github.com/terminusdb/terminusdb/issues/165,How can I import and visualise a schema in TerminusDB?,2021-02-10T18:15:58Z,luke-feeney,https://github.com/terminusdb/terminusdb/issues/165#issuecomment-776910161,"Goes in here:
https://github.com/terminusdb/terminusdb/blob/master/docs/How_To/visualize_schema.md
Maybe there is no good answer? Maybe we just need a workaround... but maybe not!
@KittyJose - for you1",go in here https//githubcom / terminusdb / terminusdb / blob / master / docs / how_to / visualize_schemamd maybe there be no good answer maybe -PRON- just need a workaround but maybe not @kittyjose - for you1,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,165,2021-02-04T13:25:51Z,luke-feeney,How-To: visualise a random schema in TerminusDB,https://github.com/terminusdb/terminusdb/issues/165,How can I import and visualise a schema in TerminusDB?,2021-02-25T15:02:57Z,KittyJose,https://github.com/terminusdb/terminusdb/issues/165#issuecomment-785967797,Once you have a schema in the visualizing part is in here https://terminusdb.github.io/terminusdb/#/How_To/visualize_schema,once -PRON- have a schema in the visualizing part be in here https//terminusdbgithubio / terminusdb/#/how_to / visualize_schema,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,166,2021-02-04T13:26:15Z,luke-feeney,How-To: do a pack operation in TerminusDB,https://github.com/terminusdb/terminusdb/issues/166,How can I do a pack operation in TerminusDB,2021-02-10T18:17:48Z,luke-feeney,https://github.com/terminusdb/terminusdb/issues/166#issuecomment-776911271,"Here is what we had
The pack operation exists now but you need to access it through the HTTP API. It is designed in order to collect all deltas on a specific branch for push or pull operations.

If I understand you correctly, you’d like a dump of only a single branch that would then be replayable later. All of the information required for this is in the pack operation.

unpack and rebase are designed to utilise these packs in order to “play them back”.

It would be possible to expose this in a user friendly way from the UI but it would be helpful to know a bit more about the workflow you imagine. Can you describe in a bit of detail your usage scenario? If it is of high importance we can certainly make it appear quickly in the roadmap 

But I think we have more now. Can we do a simple 'How-To'
@GavinMendelGleason - those are your words up there!","here be what -PRON- have the pack operation exist now but -PRON- need to access -PRON- through the http api -PRON- be design in order to collect all delta on a specific branch for push or pull operation if i understand -PRON- correctly -PRON- ’d like a dump of only a single branch that would then be replayable later all of the information require for this be in the pack operation unpack and rebase be design to utilise these pack in order to "" play -PRON- back "" -PRON- would be possible to expose this in a user friendly way from the ui but -PRON- would be helpful to know a bit more about the workflow -PRON- imagine can -PRON- describe in a bit of detail -PRON- usage scenario if -PRON- be of high importance -PRON- can certainly make -PRON- appear quickly in the roadmap but i think -PRON- have more now can -PRON- do a simple ' how - to ' @gavinmendelgleason - those be -PRON- word up there",0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,167,2021-02-04T13:26:55Z,luke-feeney,How-To: do a delta roll up in TerminusDB,https://github.com/terminusdb/terminusdb/issues/167,"How do I do a delta roll up in TerminusDB
(not why I want one - just how I can do it)",2021-02-10T18:22:16Z,luke-feeney,https://github.com/terminusdb/terminusdb/issues/167#issuecomment-776914118,"Aha! The famous delta roll ups. Step by step guide needed
Goes in here:
https://github.com/terminusdb/terminusdb/blob/master/docs/How_To/delta_rollup.md
@matko - the father of the delta roll up",aha the famous delta roll up step by step guide need go in here https//githubcom / terminusdb / terminusdb / blob / master / docs / how_to / delta_rollupmd @matko - the father of the delta roll up,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,167,2021-02-04T13:26:55Z,luke-feeney,How-To: do a delta roll up in TerminusDB,https://github.com/terminusdb/terminusdb/issues/167,"How do I do a delta roll up in TerminusDB
(not why I want one - just how I can do it)",2021-02-18T13:17:01Z,matko,https://github.com/terminusdb/terminusdb/issues/167#issuecomment-781336682,"This is tricky.
The way we have set things up now, we never really expect a user to directly do a delta rollup. Instead, we have an API endpoint to 'optimize' some database. If appropriate, this will do a delta rollup.
We do not have any way for the end user to do it directly though!",this be tricky the way -PRON- have set thing up now -PRON- never really expect a user to directly do a delta rollup instead -PRON- have an api endpoint to ' optimize ' some database if appropriate this will do a delta rollup -PRON- do not have any way for the end user to do -PRON- directly though,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,167,2021-02-04T13:26:55Z,luke-feeney,How-To: do a delta roll up in TerminusDB,https://github.com/terminusdb/terminusdb/issues/167,"How do I do a delta roll up in TerminusDB
(not why I want one - just how I can do it)",2021-02-18T13:20:59Z,GavinMendelGleason,https://github.com/terminusdb/terminusdb/issues/167#issuecomment-781338924,"We should just say what it is, and then document that they can get one somehow magically using optimize.",-PRON- should just say what -PRON- be and then document that -PRON- can get one somehow magically use optimize,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,167,2021-02-04T13:26:55Z,luke-feeney,How-To: do a delta roll up in TerminusDB,https://github.com/terminusdb/terminusdb/issues/167,"How do I do a delta roll up in TerminusDB
(not why I want one - just how I can do it)",2021-02-18T13:30:22Z,matko,https://github.com/terminusdb/terminusdb/issues/167#issuecomment-781344777,"IMO we should reverse it: document optimize, and mention that this could do a delta rollup.",imo -PRON- should reverse -PRON- document optimize and mention that this could do a delta rollup,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,167,2021-02-04T13:26:55Z,luke-feeney,How-To: do a delta roll up in TerminusDB,https://github.com/terminusdb/terminusdb/issues/167,"How do I do a delta roll up in TerminusDB
(not why I want one - just how I can do it)",2021-02-18T13:31:24Z,luke-feeney,https://github.com/terminusdb/terminusdb/issues/167#issuecomment-781345360,that makes sense to me - I'll rename,that make sense to -PRON- - -PRON- will rename,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,167,2021-02-04T13:26:55Z,luke-feeney,How-To: do a delta roll up in TerminusDB,https://github.com/terminusdb/terminusdb/issues/167,"How do I do a delta roll up in TerminusDB
(not why I want one - just how I can do it)",2021-02-19T16:59:07Z,luke-feeney,https://github.com/terminusdb/terminusdb/issues/167#issuecomment-782203548,made that change in the navigation - it is now 'How To Optimize your DB',make that change in the navigation - -PRON- be now ' how to optimize -PRON- db ',0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,168,2021-02-04T13:29:09Z,luke-feeney,How-To: revert in TerminusDB,https://github.com/terminusdb/terminusdb/issues/168,"How do I revert in TerminusDB?
Probably best to cover all the possible supported approached",2021-02-10T18:39:17Z,luke-feeney,https://github.com/terminusdb/terminusdb/issues/168#issuecomment-776924527,"Ok - so would be good to give a How To on the various approaches.
Goes in here:
https://github.com/terminusdb/terminusdb/blob/master/docs/How_To/revert.md
@KittyJose - another one for you",ok - so would be good to give a how to on the various approach go in here https//githubcom / terminusdb / terminusdb / blob / master / docs / how_to / revertmd @kittyjose - another one for -PRON-,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,168,2021-02-04T13:29:09Z,luke-feeney,How-To: revert in TerminusDB,https://github.com/terminusdb/terminusdb/issues/168,"How do I revert in TerminusDB?
Probably best to cover all the possible supported approached",2021-02-24T16:23:19Z,KittyJose,https://github.com/terminusdb/terminusdb/issues/168#issuecomment-785196092,"This feature is on our to-do list, and is coming soon",this feature be on -PRON- to - do list and be come soon,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,169,2021-02-04T13:29:34Z,luke-feeney,How-To: select a Commit message?,https://github.com/terminusdb/terminusdb/issues/169,How do I select a Commit message?,2021-02-10T18:43:37Z,luke-feeney,https://github.com/terminusdb/terminusdb/issues/169#issuecomment-776927156,"This is the final note I am writing after about 50 or 60, so I'm relieved to be getting here...
This is again a question that came up in the server...
https://github.com/terminusdb/terminusdb/blob/master/docs/How_To/select_commit.md
@matko - I haven't sent one in a while!!!",this be the final note i be write after about 50 or 60 so -PRON- be relieve to be get here this be again a question that come up in the server https//githubcom / terminusdb / terminusdb / blob / master / docs / how_to / select_commitmd @matko - i have not send one in a while,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,169,2021-02-04T13:29:34Z,luke-feeney,How-To: select a Commit message?,https://github.com/terminusdb/terminusdb/issues/169,How do I select a Commit message?,2021-02-25T14:25:59Z,matko,https://github.com/terminusdb/terminusdb/issues/169#issuecomment-785933035,"I'm confused. select a commit message where? Like, in the desktop client? Through the API? And when?",-PRON- be confuse select a commit message where like in the desktop client through the api and when,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,169,2021-02-04T13:29:34Z,luke-feeney,How-To: select a Commit message?,https://github.com/terminusdb/terminusdb/issues/169,How do I select a Commit message?,2021-02-25T14:30:20Z,luke-feeney,https://github.com/terminusdb/terminusdb/issues/169#issuecomment-785936025,"Not 100% sure - came up in the server and I copied across - through the API I'd say - and not sure when. This it is somebody trying to see a specific commit or commit message.
If nonsense, just rewrite as some other how-to that does something you think is useful!",not 100 % sure - come up in the server and i copy across - through the api -PRON- 'd say - and not sure when this -PRON- be somebody try to see a specific commit or commit message if nonsense just rewrite as some other how - to that do something -PRON- think be useful,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,170,2021-02-04T13:29:57Z,luke-feeney,How-To: update previous inserted data?,https://github.com/terminusdb/terminusdb/issues/170,How to update previous inserted data?,2021-02-10T18:42:09Z,luke-feeney,https://github.com/terminusdb/terminusdb/issues/170#issuecomment-776926243,"This is a question that came up in the forum.
Goes in there:
https://github.com/terminusdb/terminusdb/blob/master/docs/How_To/update_data.md
Quick how to using preferred approach @GavinMendelGleason",this be a question that come up in the forum go in there https//githubcom / terminusdb / terminusdb / blob / master / docs / how_to / update_datamd quick how to use preferred approach @gavinmendelgleason,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,171,2021-02-04T13:30:26Z,luke-feeney,How-To: merge back into main? ,https://github.com/terminusdb/terminusdb/issues/171,How do I merge back into main?,2021-02-10T18:40:52Z,luke-feeney,https://github.com/terminusdb/terminusdb/issues/171#issuecomment-776925504,"I think this is a console How-To, but I suppose it could be CLI or anywhere? Liverpool or Rome?
Goes in here:
https://github.com/terminusdb/terminusdb/blob/master/docs/How_To/merge_main.md
@KittyJose could you take a look?",i think this be a console how - to but i suppose -PRON- could be cli or anywhere liverpool or rome go in here https//githubcom / terminusdb / terminusdb / blob / master / docs / how_to / merge_mainmd @kittyjose could -PRON- take a look,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,171,2021-02-04T13:30:26Z,luke-feeney,How-To: merge back into main? ,https://github.com/terminusdb/terminusdb/issues/171,How do I merge back into main?,2021-02-24T15:47:50Z,KittyJose,https://github.com/terminusdb/terminusdb/issues/171#issuecomment-785170533,"I have added this bit on how to merge to main using Javascript client under
https://github.com/terminusdb/terminusdb/blob/master/docs/How_To/merge_main.md",i have add this bit on how to merge to main use javascript client under https//githubcom / terminusdb / terminusdb / blob / master / docs / how_to / merge_mainmd,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,172,2021-02-04T13:30:47Z,luke-feeney,How-To: set up a remote from TerminusDB? ,https://github.com/terminusdb/terminusdb/issues/172,How do I set up a remote from TerminusDB?,2021-02-10T18:24:55Z,luke-feeney,https://github.com/terminusdb/terminusdb/issues/172#issuecomment-776915737,"this is a good issue and one that has come up a few times
Most recent answer:
Yes - the only part that is tied up to hub is the desktop console - for reasons of security, it's difficult to do peer-to-peer from a desktop install, but the DB itself is 100% peer-to-peer and not tied up to Terminus Hub and can be used anywhere.  We would very much like to develop a server / peer to peer console / distro which provided UI support for fully decentralised secure collaboration networks unconnected from TerminusHub but we need this to be a community led project rather than a company led one for it to really work.  From a commercial point of view, in the short term the company is increasingly focused on providing cloud based orchestrations of TerminusDBs to provide high-assurance continuous integration engine services.  The database itself and the collaboration services are a long term investment in the decentralised network of the future which will be funded by, but independent of, the commercial operations.

Basically we need a simple How To for this
@rrooij into the breach once again?",this be a good issue and one that have come up a few time most recent answer yes - the only part that be tie up to hub be the desktop console - for reason of security -PRON- be difficult to do peer - to - peer from a desktop install but the db -PRON- be 100 % peer - to - peer and not tie up to terminus hub and can be use anywhere -PRON- would very much like to develop a server / peer to peer console / distro which provide ui support for fully decentralise secure collaboration network unconnected from terminushub but -PRON- need this to be a community lead project rather than a company lead one for -PRON- to really work from a commercial point of view in the short term the company be increasingly focused on provide cloud base orchestration of terminusdbs to provide high - assurance continuous integration engine service the database -PRON- and the collaboration service be a long term investment in the decentralised network of the future which will be fund by but independent of the commercial operation basically -PRON- need a simple how to for this @rrooij into the breach once again,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,172,2021-02-04T13:30:47Z,luke-feeney,How-To: set up a remote from TerminusDB? ,https://github.com/terminusdb/terminusdb/issues/172,How do I set up a remote from TerminusDB?,2021-02-11T16:28:01Z,rrooij,https://github.com/terminusdb/terminusdb/issues/172#issuecomment-777619922,Done!,do,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,172,2021-02-04T13:30:47Z,luke-feeney,How-To: set up a remote from TerminusDB? ,https://github.com/terminusdb/terminusdb/issues/172,How do I set up a remote from TerminusDB?,2021-02-11T16:31:18Z,luke-feeney,https://github.com/terminusdb/terminusdb/issues/172#issuecomment-777622074,Hero!,hero,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,173,2021-02-04T13:31:25Z,luke-feeney,How-To: save a query? ,https://github.com/terminusdb/terminusdb/issues/173,"How do I save a query?
If you can't do directly yet, include the work around. That is as good as golden.",2021-02-10T17:10:51Z,luke-feeney,https://github.com/terminusdb/terminusdb/issues/173#issuecomment-776868835,"I don't think we have this implemented yet, but comes up quite a bit in the server, so would be good to give a work around of some kind.
Goes here:
https://github.com/terminusdb/terminusdb/blob/master/docs/How_To/save_query.md
@KittyJose - for u!",i do not think -PRON- have this implement yet but come up quite a bit in the server so would be good to give a work around of some kind go here https//githubcom / terminusdb / terminusdb / blob / master / docs / how_to / save_querymd @kittyjose - for u,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,173,2021-02-04T13:31:25Z,luke-feeney,How-To: save a query? ,https://github.com/terminusdb/terminusdb/issues/173,"How do I save a query?
If you can't do directly yet, include the work around. That is as good as golden.",2021-02-24T21:12:48Z,KittyJose,https://github.com/terminusdb/terminusdb/issues/173#issuecomment-785387709,"working example - https://terminusdb.github.io/terminusdb/#/How_To/save_query
working tutorial - https://github.com/terminusdb/terminusdb-tutorials/tree/master/woql",work example - https//terminusdbgithubio / terminusdb/#/how_to / save_query work tutorial - https//githubcom / terminusdb / terminusdb - tutorial / tree / master / woql,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,173,2021-02-04T13:31:25Z,luke-feeney,How-To: save a query? ,https://github.com/terminusdb/terminusdb/issues/173,"How do I save a query?
If you can't do directly yet, include the work around. That is as good as golden.",2021-02-24T21:12:59Z,KittyJose,https://github.com/terminusdb/terminusdb/issues/173#issuecomment-785387811,"working example - https://terminusdb.github.io/terminusdb/#/How_To/save_query
working tutorial - https://github.com/terminusdb/terminusdb-tutorials/tree/master/woql
Closing issue",work example - https//terminusdbgithubio / terminusdb/#/how_to / save_query work tutorial - https//githubcom / terminusdb / terminusdb - tutorial / tree / master / woql closing issue,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,174,2021-02-04T13:31:50Z,luke-feeney,How-To: import the schema.org schemas into TerminusDB? ,https://github.com/terminusdb/terminusdb/issues/174,How do I import the schema.org schemas into TerminusDB?,2021-02-10T17:05:15Z,luke-feeney,https://github.com/terminusdb/terminusdb/issues/174#issuecomment-776865200,"I know you have done plenty of extensive work on this @Cheukting, so maybe you could boil it down into a How To?
Goes in here:
https://github.com/terminusdb/terminusdb/blob/master/docs/How_To/import_schemaorg.md
If it is too long for a How-To, we could make a tutorial, but would be nice to try to boil down to the bare minimum even if it doesn't do everything",i know -PRON- have do plenty of extensive work on this @cheukting so maybe -PRON- could boil -PRON- down into a how to go in here https//githubcom / terminusdb / terminusdb / blob / master / docs / how_to / import_schemaorgmd if -PRON- be too long for a how - to -PRON- could make a tutorial but would be nice to try to boil down to the bare minimum even if -PRON- do not do everything,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,174,2021-02-04T13:31:50Z,luke-feeney,How-To: import the schema.org schemas into TerminusDB? ,https://github.com/terminusdb/terminusdb/issues/174,How do I import the schema.org schemas into TerminusDB?,2021-02-11T15:46:28Z,Cheukting,https://github.com/terminusdb/terminusdb/issues/174#issuecomment-777590998,"I think it makes more sense to have it in a tutorial, but I think we will eventually have the Schema.org uploaded as a public database on Hub so people can just pull it form Hub and use it.",i think -PRON- make more sense to have -PRON- in a tutorial but i think -PRON- will eventually have the schemaorg upload as a public database on hub so people can just pull -PRON- form hub and use -PRON-,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,174,2021-02-04T13:31:50Z,luke-feeney,How-To: import the schema.org schemas into TerminusDB? ,https://github.com/terminusdb/terminusdb/issues/174,How do I import the schema.org schemas into TerminusDB?,2021-02-11T16:23:33Z,luke-feeney,https://github.com/terminusdb/terminusdb/issues/174#issuecomment-777616987,That works for me - I'll remove the How-To and we can just leave it in the tutorial repo.,that work for -PRON- - -PRON- will remove the how - to and -PRON- can just leave -PRON- in the tutorial repo,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,175,2021-02-04T13:32:13Z,luke-feeney,How-To: get around certificate issues in the Python Client? ,https://github.com/terminusdb/terminusdb/issues/175,How do I get around certificate issues in the Python Client?,2021-02-10T16:53:54Z,luke-feeney,https://github.com/terminusdb/terminusdb/issues/175#issuecomment-776856189,"some people have had some issues with this, so might be good to clarify!
goes in here:
https://github.com/terminusdb/terminusdb/blob/master/docs/How_To/certificate_python.md
Quick How-To - if it doesn't make sense replace with something that does
@Cheukting - one for you",some people have have some issue with this so may be good to clarify go in here https//githubcom / terminusdb / terminusdb / blob / master / docs / how_to / certificate_pythonmd quick how - to - if -PRON- do not make sense replace with something that do @cheukting - one for -PRON-,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,175,2021-02-04T13:32:13Z,luke-feeney,How-To: get around certificate issues in the Python Client? ,https://github.com/terminusdb/terminusdb/issues/175,How do I get around certificate issues in the Python Client?,2021-02-11T16:23:18Z,Cheukting,https://github.com/terminusdb/terminusdb/issues/175#issuecomment-777616816,"With https connection, a certificate is required. However, if you are running TerminusDB locally (on 127.0.0.1) you will have to by-pass this check. Simply apply the keyword argument insecure=True when creating the WOQLClient. For example: client=WOQLClient(""https://127.0.0.1:6363"", insecure=True)","with https connection a certificate be require however if -PRON- be run terminusdb locally ( on 127001 ) -PRON- will have to by - pass this check simply apply the keyword argument insecure = true when create the woqlclient for example client = woqlclient(""https//1270016363 "" insecure = true )",0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,175,2021-02-04T13:32:13Z,luke-feeney,How-To: get around certificate issues in the Python Client? ,https://github.com/terminusdb/terminusdb/issues/175,How do I get around certificate issues in the Python Client?,2021-02-11T16:23:38Z,Cheukting,https://github.com/terminusdb/terminusdb/issues/175#issuecomment-777617053,@kevinchekovfeeney would you please have a look to see if the above is correct?,@kevinchekovfeeney would -PRON- please have a look to see if the above be correct,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,175,2021-02-04T13:32:13Z,luke-feeney,How-To: get around certificate issues in the Python Client? ,https://github.com/terminusdb/terminusdb/issues/175,How do I get around certificate issues in the Python Client?,2021-02-17T14:18:23Z,luke-feeney,https://github.com/terminusdb/terminusdb/issues/175#issuecomment-780586689,Going ahead with this pending @kevinchekovfeeney review,go ahead with this pende @kevinchekovfeeney review,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,175,2021-02-04T13:32:13Z,luke-feeney,How-To: get around certificate issues in the Python Client? ,https://github.com/terminusdb/terminusdb/issues/175,How do I get around certificate issues in the Python Client?,2021-03-15T16:55:05Z,Cheukting,https://github.com/terminusdb/terminusdb/issues/175#issuecomment-799578329,@luke-feeney I think you can just copy it over and use it.,@luke - feeney i think -PRON- can just copy -PRON- over and use -PRON-,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,176,2021-02-04T13:32:40Z,luke-feeney,How-To: connect my angular app that I want to run in the browser and connect directly to my local TerminusDB?,https://github.com/terminusdb/terminusdb/issues/176,How do I connect my angular app that I want to run in the browser and connect directly to my local TerminusDB?,2021-02-10T18:20:31Z,luke-feeney,https://github.com/terminusdb/terminusdb/issues/176#issuecomment-776912934,"This is a solid How To. Connect to x and go.
https://github.com/terminusdb/terminusdb/blob/master/docs/How_To/angular_app.md
@rrooij can you set out the simple steps?",this be a solid how to connect to x and go https//githubcom / terminusdb / terminusdb / blob / master / docs / how_to / angular_appmd @rrooij can -PRON- set out the simple step,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,176,2021-02-04T13:32:40Z,luke-feeney,How-To: connect my angular app that I want to run in the browser and connect directly to my local TerminusDB?,https://github.com/terminusdb/terminusdb/issues/176,How do I connect my angular app that I want to run in the browser and connect directly to my local TerminusDB?,2021-03-08T19:24:43Z,rrooij,https://github.com/terminusdb/terminusdb/issues/176#issuecomment-793010737,I want to make it a regular JS How To instead of a specific Angular one. Once you know how to connect with the JavaScript client it should be straightforward for JS frameworks as well.,i want to make -PRON- a regular js how to instead of a specific angular one once -PRON- know how to connect with the javascript client -PRON- should be straightforward for js framework as well,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,177,2021-02-04T13:33:15Z,luke-feeney,How-To: find some json-ld query examples?,https://github.com/terminusdb/terminusdb/issues/177,"How do I find some json-ld query examples?
This is simple - but people want to know how!",2021-02-10T18:09:39Z,luke-feeney,https://github.com/terminusdb/terminusdb/issues/177#issuecomment-776906362,"I think i can write this one myself - given the extract from the discord:
You should be able to retrieve the json-ld of any python query via the .dict() function - you can write whatever query you want in python and this will give you the json-ld equivalent as a dict
goes in here:
https://github.com/terminusdb/terminusdb/blob/master/docs/How_To/jsonld_query.md
@kevinchekovfeeney - do we need more?",i think i can write this one -PRON- - give the extract from the discord -PRON- should be able to retrieve the json - ld of any python query via the dict ( ) function - -PRON- can write whatever query -PRON- want in python and this will give -PRON- the json - ld equivalent as a dict go in here https//githubcom / terminusdb / terminusdb / blob / master / docs / how_to / jsonld_querymd @kevinchekovfeeney - do -PRON- need more,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,178,2021-02-04T13:33:47Z,luke-feeney,How-To: load Turtle into a DB? ,https://github.com/terminusdb/terminusdb/issues/178,"How do I load Turtle into a DB?
This one has come up a few times",2021-02-10T18:27:35Z,luke-feeney,https://github.com/terminusdb/terminusdb/issues/178#issuecomment-776917334,"I think this is under discussion in the server at this very second. Reference to this tutorial:
https://github.com/terminusdb/terminusdb-tutorials/tree/master/woql
The How To goes in here:
https://github.com/terminusdb/terminusdb/blob/master/docs/How_To/load_turtle.md
@GavinMendelGleason as you are busy answering now",i think this be under discussion in the server at this very second reference to this tutorial https//githubcom / terminusdb / terminusdb - tutorial / tree / master / woql the how to go in here https//githubcom / terminusdb / terminusdb / blob / master / docs / how_to / load_turtlemd @gavinmendelgleason as -PRON- be busy answer now,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,179,2021-02-04T13:38:00Z,luke-feeney,How-To: handle merge conflicts?,https://github.com/terminusdb/terminusdb/issues/179,How do I handle merge conflicts?,2021-02-10T18:35:35Z,luke-feeney,https://github.com/terminusdb/terminusdb/issues/179#issuecomment-776922392,"This is a bit of a How To issue. We just need to spell out the work around until we have a better solution.
In here:
https://github.com/terminusdb/terminusdb/blob/master/docs/How_To/merge_conflict.md
@GavinMendelGleason - I think you have an idea of what we might say?",this be a bit of a how to issue -PRON- just need to spell out the work around until -PRON- have a well solution in here https//githubcom / terminusdb / terminusdb / blob / master / docs / how_to / merge_conflictmd @gavinmendelgleason - i think -PRON- have an idea of what -PRON- may say,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,179,2021-02-04T13:38:00Z,luke-feeney,How-To: handle merge conflicts?,https://github.com/terminusdb/terminusdb/issues/179,How do I handle merge conflicts?,2021-03-08T12:12:51Z,henryjcee,https://github.com/terminusdb/terminusdb/issues/179#issuecomment-792715374,"Hi @luke-feeney, I'm Henry and I'm currently working on Merge Mamba. If you're having trouble with merge conflicts you might find it helps to avoid them in future and it'll always be free for open source projects.
I'll be adding new features over the next few months so if there are any adjacent problems that your project would love a tool to help with, just let me know and I'll try and fit them into the roadmap.",hi @luke - feeney -PRON- be henry and -PRON- be currently work on merge mamba if -PRON- be have trouble with merge conflict -PRON- may find -PRON- help to avoid -PRON- in future and -PRON- will always be free for open source project -PRON- will be add new feature over the next few month so if there be any adjacent problem that -PRON- project would love a tool to help with just let -PRON- know and -PRON- will try and fit -PRON- into the roadmap,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,179,2021-02-04T13:38:00Z,luke-feeney,How-To: handle merge conflicts?,https://github.com/terminusdb/terminusdb/issues/179,How do I handle merge conflicts?,2021-03-08T12:26:23Z,luke-feeney,https://github.com/terminusdb/terminusdb/issues/179#issuecomment-792722528,"Hey @henryjcee ! Cool project. This thread is about merge conflicts when merging databases. TerminusDB is structured a bit like git, but is for data.
We have some issues with merging large databases and dealing with conflicts. We'd love a tool to deal with that, but fear it might not be an adjacent problem!",hey @henryjcee cool project this thread be about merge conflict when merge database terminusdb be structure a bit like git but be for datum -PRON- have some issue with merge large database and deal with conflict -PRON- 'd love a tool to deal with that but fear -PRON- may not be an adjacent problem,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,179,2021-02-04T13:38:00Z,luke-feeney,How-To: handle merge conflicts?,https://github.com/terminusdb/terminusdb/issues/179,How do I handle merge conflicts?,2021-03-08T12:32:40Z,henryjcee,https://github.com/terminusdb/terminusdb/issues/179#issuecomment-792725763,"We have some issues with merging large databases and dealing with conflicts. We'd love a tool to deal with that, but fear it might not be an adjacent problem!

Ah, major apologies I misunderstood the thread! Happy to delete and apologies for hijacking your issue.
H",-PRON- have some issue with merge large database and deal with conflict -PRON- 'd love a tool to deal with that but fear -PRON- may not be an adjacent problem ah major apology i misunderstand the thread happy to delete and apology for hijack -PRON- issue h,2,1
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,179,2021-02-04T13:38:00Z,luke-feeney,How-To: handle merge conflicts?,https://github.com/terminusdb/terminusdb/issues/179,How do I handle merge conflicts?,2021-03-08T12:36:21Z,luke-feeney,https://github.com/terminusdb/terminusdb/issues/179#issuecomment-792727434,"don't worry about it at all! will check out merge mamba anyhow as could help on the code deployment side.
good luck",do not worry about -PRON- at all will check out merge mamba anyhow as could help on the code deployment side good luck,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,179,2021-02-04T13:38:00Z,luke-feeney,How-To: handle merge conflicts?,https://github.com/terminusdb/terminusdb/issues/179,How do I handle merge conflicts?,2021-09-07T20:17:55Z,GavinMendelGleason,https://github.com/terminusdb/terminusdb/issues/179#issuecomment-914596015,I will close this as we've a plan for simplified merge API.,i will close this as -PRON- have a plan for simplified merge api,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,180,2021-02-04T13:38:36Z,luke-feeney,How-To: delete in TerminusDB? ,https://github.com/terminusdb/terminusdb/issues/180,How does deleting work?,2021-02-10T18:29:30Z,luke-feeney,https://github.com/terminusdb/terminusdb/issues/180#issuecomment-776918741,"just need to give the method as it currently stands. Step by step How To
https://github.com/terminusdb/terminusdb/blob/master/docs/How_To/delete.md
@Cheukting can you take a look?",just need to give the method as -PRON- currently stand step by step how to https//githubcom / terminusdb / terminusdb / blob / master / docs / how_to / deletemd @cheukting can -PRON- take a look,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,180,2021-02-04T13:38:36Z,luke-feeney,How-To: delete in TerminusDB? ,https://github.com/terminusdb/terminusdb/issues/180,How does deleting work?,2021-02-11T22:54:11Z,Cheukting,https://github.com/terminusdb/terminusdb/issues/180#issuecomment-777846718,"Delete a database with console
In the home page of the database that you want to delete, click on the delete button (see example below)

Then, type out the database id to confirm (see example below)

Delete database via Python client
The WOQLClient provide a delete_database method to do so. For example:
>>> client = WOQLClient(""https://127.0.0.1:6363/"")
>>> client.delete_database(""pybike"")","delete a database with console in the home page of the database that -PRON- want to delete click on the delete button ( see example below ) then type out the database -PRON- d to confirm ( see example below ) delete database via python client the woqlclient provide a delete_database method to do so for example > > > client = woqlclient(""https//1270016363/ "" ) > > > clientdelete_database(""pybike "" )",0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,180,2021-02-04T13:38:36Z,luke-feeney,How-To: delete in TerminusDB? ,https://github.com/terminusdb/terminusdb/issues/180,How does deleting work?,2021-02-11T22:54:56Z,Cheukting,https://github.com/terminusdb/terminusdb/issues/180#issuecomment-777847108,"@luke-feeney, please review",@luke - feeney please review,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,180,2021-02-04T13:38:36Z,luke-feeney,How-To: delete in TerminusDB? ,https://github.com/terminusdb/terminusdb/issues/180,How does deleting work?,2021-02-17T14:16:11Z,luke-feeney,https://github.com/terminusdb/terminusdb/issues/180#issuecomment-780585332,Thanks!,thanks,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,181,2021-02-04T13:39:04Z,luke-feeney,How-To: use branches for deduplication? ,https://github.com/terminusdb/terminusdb/issues/181,How do I use branches for deduplication?,2021-02-10T18:11:11Z,luke-feeney,https://github.com/terminusdb/terminusdb/issues/181#issuecomment-776907250,"Not sure where to start on this one - was an issue in the server...
https://github.com/terminusdb/terminusdb/blob/master/docs/How_To/branch_deduplication.md
@GavinMendelGleason - is this for you?",not sure where to start on this one - be an issue in the server https//githubcom / terminusdb / terminusdb / blob / master / docs / how_to / branch_deduplicationmd @gavinmendelgleason - be this for -PRON-,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,181,2021-02-04T13:39:04Z,luke-feeney,How-To: use branches for deduplication? ,https://github.com/terminusdb/terminusdb/issues/181,How do I use branches for deduplication?,2021-09-07T20:18:55Z,GavinMendelGleason,https://github.com/terminusdb/terminusdb/issues/181#issuecomment-914597647,This is no longer relevant.,this be no long relevant,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,182,2021-02-04T13:39:26Z,luke-feeney,How-To: get access to deltas directly? ,https://github.com/terminusdb/terminusdb/issues/182,How do I get access to deltas directly?,2021-02-10T18:34:11Z,luke-feeney,https://github.com/terminusdb/terminusdb/issues/182#issuecomment-776921571,"Hmmmm. This is a good one. Great for a How-To. Not sure how to approach thou (client, console etc.)
Goes in here:
https://github.com/terminusdb/terminusdb/blob/master/docs/How_To/access_deltas.md
Can you take a look @Francesca-Bit",hmmmm this be a good one great for a how - to not sure how to approach thou ( client console etc ) go in here https//githubcom / terminusdb / terminusdb / blob / master / docs / how_to / access_deltasmd can -PRON- take a look @francesca - bit,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,182,2021-02-04T13:39:26Z,luke-feeney,How-To: get access to deltas directly? ,https://github.com/terminusdb/terminusdb/issues/182,How do I get access to deltas directly?,2021-03-05T11:49:19Z,Francesca-Bit,https://github.com/terminusdb/terminusdb/issues/182#issuecomment-791370197,this document has been removed,this document have be remove,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,183,2021-02-04T13:39:49Z,luke-feeney,How-To: get the container at startup to not drop me straight into an SWI-prolog repl and then bail out. ,https://github.com/terminusdb/terminusdb/issues/183,How do I get the container at startup to not drop me straight into an SWI-prolog repl and then bail out.,2021-02-10T16:51:29Z,luke-feeney,https://github.com/terminusdb/terminusdb/issues/183#issuecomment-776854558,"This was a questions that came up in the forum.
Goes in here:
https://github.com/terminusdb/terminusdb/blob/master/docs/How_To/start_container.md
Would be good to give a basic answer
@rrooij - could you take a look",this be a question that come up in the forum go in here https//githubcom / terminusdb / terminusdb / blob / master / docs / how_to / start_containermd would be good to give a basic answer @rrooij - could -PRON- take a look,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,183,2021-02-04T13:39:49Z,luke-feeney,How-To: get the container at startup to not drop me straight into an SWI-prolog repl and then bail out. ,https://github.com/terminusdb/terminusdb/issues/183,How do I get the container at startup to not drop me straight into an SWI-prolog repl and then bail out.,2021-02-11T15:51:53Z,rrooij,https://github.com/terminusdb/terminusdb/issues/183#issuecomment-777594893,Made general instructions on how to run it from a container.,make general instruction on how to run -PRON- from a container,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,184,2021-02-04T13:40:10Z,luke-feeney,How-To: import JSON-LD from WOQL,https://github.com/terminusdb/terminusdb/issues/184,How do I import JSON-LD from WOQL,2021-02-10T16:45:51Z,luke-feeney,https://github.com/terminusdb/terminusdb/issues/184#issuecomment-776848792,"not 100% sure if this works as separate How-To, but I picked up the question from the forum.
Goes in here:
https://github.com/terminusdb/terminusdb/blob/master/docs/How_To/jsonld_woql.md
@kevinchekovfeeney - another one for you",not 100 % sure if this work as separate how - to but i pick up the question from the forum go in here https//githubcom / terminusdb / terminusdb / blob / master / docs / how_to / jsonld_woqlmd @kevinchekovfeeney - another one for -PRON-,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,184,2021-02-04T13:40:10Z,luke-feeney,How-To: import JSON-LD from WOQL,https://github.com/terminusdb/terminusdb/issues/184,How do I import JSON-LD from WOQL,2021-02-24T19:22:10Z,kevinchekovfeeney,https://github.com/terminusdb/terminusdb/issues/184#issuecomment-785314676,This is answered here: https://github.com/terminusdb/terminusdb/blob/master/docs/Explanation/DOCINTERFACE.md,this be answer here https//githubcom / terminusdb / terminusdb / blob / master / docs / explanation / docinterfacemd,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,184,2021-02-04T13:40:10Z,luke-feeney,How-To: import JSON-LD from WOQL,https://github.com/terminusdb/terminusdb/issues/184,How do I import JSON-LD from WOQL,2021-03-01T09:32:24Z,luke-feeney,https://github.com/terminusdb/terminusdb/issues/184#issuecomment-787804373,"I might be confused, but should I remove the woql json-ld How To as it is answered in the document interface explanation? Or have files become mixed up? @kevinchekovfeeney",i may be confused but should i remove the woql json - ld how to as -PRON- be answer in the document interface explanation or have file become mix up @kevinchekovfeeney,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,185,2021-02-04T13:40:55Z,luke-feeney,How-To: import prov ontology? ,https://github.com/terminusdb/terminusdb/issues/185,How do I import prov ontology?  https://www.w3.org/TR/prov-o/,2021-02-10T16:39:20Z,luke-feeney,https://github.com/terminusdb/terminusdb/issues/185#issuecomment-776843347,"This is a question that has come up in the discord a few times and is worth addressing
Here is place:
https://github.com/terminusdb/terminusdb/blob/master/docs/How_To/prov_ontology.md
@kevinchekovfeeney - a how-to!",this be a question that have come up in the discord a few time and be worth address here be place https//githubcom / terminusdb / terminusdb / blob / master / docs / how_to / prov_ontologymd @kevinchekovfeeney - a how - to,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,186,2021-02-04T13:43:56Z,luke-feeney,How To: Set ENV variable in Bootstrap? ,https://github.com/terminusdb/terminusdb/issues/186,"ENV variables that can be set by either setting ENV variables locally, or by creating a file called ENV that will be read on the start of the script.
This needs to be reconstituted as a How-To:
https://terminusdb.com/docs/reference/bootstrap",2021-02-10T16:04:41Z,luke-feeney,https://github.com/terminusdb/terminusdb/issues/186#issuecomment-776817453,"I did this one! Not sure if it is all still correct so somebody might check:
https://terminusdb.github.io/terminusdb/#/How_To/ENV
Will you take a look @rrooij to see if something more/less is needed.",i do this one not sure if -PRON- be all still correct so somebody may check https//terminusdbgithubio / terminusdb/#/how_to / env will -PRON- take a look @rrooij to see if something more / less be need,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,186,2021-02-04T13:43:56Z,luke-feeney,How To: Set ENV variable in Bootstrap? ,https://github.com/terminusdb/terminusdb/issues/186,"ENV variables that can be set by either setting ENV variables locally, or by creating a file called ENV that will be read on the start of the script.
This needs to be reconstituted as a How-To:
https://terminusdb.com/docs/reference/bootstrap",2021-02-17T14:32:48Z,rrooij,https://github.com/terminusdb/terminusdb/issues/186#issuecomment-780595953,"Looked it through, looks good!",look -PRON- through look good,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,187,2021-02-04T13:45:23Z,luke-feeney,How-To: use RegEx in TerminusDB?,https://github.com/terminusdb/terminusdb/issues/187,"How do I use RegEx?
Workarounds or complex answers are fine",2021-02-10T16:29:34Z,luke-feeney,https://github.com/terminusdb/terminusdb/issues/187#issuecomment-776835749,"Goes in here:
https://github.com/terminusdb/terminusdb/blob/master/docs/How_To/use_regex.md
Would be nice to have a short general How-To, but not sure if that is possible.
@GavinMendelGleason - another one for you I think",go in here https//githubcom / terminusdb / terminusdb / blob / master / docs / how_to / use_regexmd would be nice to have a short general how - to but not sure if that be possible @gavinmendelgleason - another one for -PRON- i think,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,188,2021-02-04T13:46:07Z,luke-feeney,How-To: use my preferred 'graph' query?,https://github.com/terminusdb/terminusdb/issues/188,"How can I do graph query x? (shortest path, 5 hops etc.)
Workarounds and complex answers are fine.",2021-02-10T16:28:24Z,luke-feeney,https://github.com/terminusdb/terminusdb/issues/188#issuecomment-776834922,"We don't really have a very good answer to this - I suppose we could put in something about some graph queries, put in a coming soon, or we could just scrap the How-To?
Goes in here:
https://github.com/terminusdb/terminusdb/blob/master/docs/How_To/graph_query.md
@GavinMendelGleason views would be helpful",-PRON- do not really have a very good answer to this - i suppose -PRON- could put in something about some graph query put in a come soon or -PRON- could just scrap the how - to go in here https//githubcom / terminusdb / terminusdb / blob / master / docs / how_to / graph_querymd @gavinmendelgleason view would be helpful,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,189,2021-02-04T13:46:57Z,luke-feeney,Explanation: Unification – what is it and why do I care?,https://github.com/terminusdb/terminusdb/issues/189,"Unification – what is it and why do I care, lets get in the weeds?
800 words please! Lots of links if deep is needed",2021-02-10T13:08:39Z,luke-feeney,https://github.com/terminusdb/terminusdb/issues/189#issuecomment-776694483,"This is in here:
https://github.com/terminusdb/terminusdb/blob/master/docs/Explanation/unification.md
Or as a comment on this issue
This is one of the ideas that can act as a barrier to entry for SQL or others that are engaging in the terminusdb world.
Just needs some discussion of what it is, why it is important, and how it differs from outer approaches.
@dmytri - I know you have traveled this road so would be great to get from that perspective.",this be in here https//githubcom / terminusdb / terminusdb / blob / master / docs / explanation / unificationmd or as a comment on this issue this be one of the idea that can act as a barrier to entry for sql or other that be engage in the terminusdb world just need some discussion of what -PRON- be why -PRON- be important and how -PRON- differ from outer approach @dmytri - i know -PRON- have travel this road so would be great to get from that perspective,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,190,2021-02-04T13:47:53Z,luke-feeney,Explanation: Datalog –what is one of those? And what has it got to do with TerminusDB?,https://github.com/terminusdb/terminusdb/issues/190,"Datalog –what is that?
Hard to do in 800 words - but lets discuss",2021-02-10T13:03:19Z,luke-feeney,https://github.com/terminusdb/terminusdb/issues/190#issuecomment-776691489,"In here:
https://github.com/terminusdb/terminusdb/blob/master/docs/Explanation/DATALOG.md
Or as comment on this issue
This would be really nice way to discuss origin of datalog and growth in current use. Why it is on the uptick (think Datomic, Grakn & Crux) and how we take advantage of powerful query.
Think this could be really important avenue into explaining the fundamental thinking that goes into our DB.
@GavinMendelGleason - has to be you.",in here https//githubcom / terminusdb / terminusdb / blob / master / docs / explanation / datalogmd or as comment on this issue this would be really nice way to discuss origin of datalog and growth in current use why -PRON- be on the uptick ( think datomic grakn & crux ) and how -PRON- take advantage of powerful query think this could be really important avenue into explain the fundamental thinking that go into -PRON- db @gavinmendelgleason - have to be -PRON-,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,191,2021-02-04T13:50:00Z,luke-feeney,Explanation: How you should model your data? What's the best way. ,https://github.com/terminusdb/terminusdb/issues/191,"How you should model your data?
TerminusDB and modelling thinking to the rescue in less than 800 words of discussion.",2021-02-10T13:01:05Z,luke-feeney,https://github.com/terminusdb/terminusdb/issues/191#issuecomment-776690163,"This goes in here:
https://github.com/terminusdb/terminusdb/blob/master/docs/Explanation/MODELHOW.md
Or can be a comment on this issue
Basically this is an extension of this blog:
https://terminusdb.com/blog/2020/11/19/model-builder-and-data-modeling/
But with some commentary on how data modelling works in other contexts.
@Francesca-Bit - data modeller!",this go in here https//githubcom / terminusdb / terminusdb / blob / master / docs / explanation / modelhowmd or can be a comment on this issue basically this be an extension of this blog https//terminusdbcom / blog/2020/11/19 / model - builder - and - data - modeling/ but with some commentary on how datum model work in other contexts @francesca - bit - data modeller,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,191,2021-02-04T13:50:00Z,luke-feeney,Explanation: How you should model your data? What's the best way. ,https://github.com/terminusdb/terminusdb/issues/191,"How you should model your data?
TerminusDB and modelling thinking to the rescue in less than 800 words of discussion.",2021-02-26T11:41:14Z,Francesca-Bit,https://github.com/terminusdb/terminusdb/issues/191#issuecomment-786596192,finished!!! I think,finish i think,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,192,2021-02-04T13:50:32Z,luke-feeney,Explanation: How should you understand provenience and why does it matter? ,https://github.com/terminusdb/terminusdb/issues/192,"How should you understand provenience and why does it matter?
Important issue I think.",2021-02-10T12:53:47Z,luke-feeney,https://github.com/terminusdb/terminusdb/issues/192#issuecomment-776686400,"Start by apologizing for my use of provenience instead of provenance!
This is here:
https://github.com/terminusdb/terminusdb/blob/master/docs/Explanation/PROVENANCE.md
Or add to this issue as a comment.
What is needed is a general discussion of provenance and some comparisons of our approach versus others. Why not use the prov ontology. What about data lineage is that the same? Why should anybody care!
@kevinchekovfeeney - this is your alley.",start by apologize for -PRON- use of provenience instead of provenance this be here https//githubcom / terminusdb / terminusdb / blob / master / docs / explanation / provenancemd or add to this issue as a comment what be need be a general discussion of provenance and some comparison of -PRON- approach versus other why not use the prov ontology what about data lineage be that the same why should anybody care @kevinchekovfeeney - this be -PRON- alley,1,1
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,193,2021-02-04T13:51:14Z,luke-feeney,Explanation: What ACID means for TerminusDB?,https://github.com/terminusdb/terminusdb/issues/193,"What ACID means for TerminusDB?
Important to spell out our understanding and how we conform to that understanding.",2021-02-10T12:38:42Z,luke-feeney,https://github.com/terminusdb/terminusdb/issues/193#issuecomment-776678736,"This goes in here:
https://github.com/terminusdb/terminusdb/blob/master/docs/Explanation/ACID.md
or as a comment on this issue.
TerminusDB claims to be ACID. What is that, why is it important. Inside each concept there is a lot. Break down what it means for us.
@GavinMendelGleason - not sure anybody else could write this!",this go in here https//githubcom / terminusdb / terminusdb / blob / master / docs / explanation / acidmd or as a comment on this issue terminusdb claim to be acid what be that why be -PRON- important inside each concept there be a lot break down what -PRON- mean for -PRON- @gavinmendelgleason - not sure anybody else could write this,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,194,2021-02-04T13:52:10Z,luke-feeney,Explanation: what is the point of immuatable? Why does it help me? ,https://github.com/terminusdb/terminusdb/issues/194,What the craic with immutable in TDB?,2021-02-10T12:20:07Z,luke-feeney,https://github.com/terminusdb/terminusdb/issues/194#issuecomment-776669496,"This goes in here:
https://github.com/terminusdb/terminusdb/blob/master/docs/Explanation/IMMUTABLE.md
Or can be included as a comment on the issue.
There are a few immutable databases out there immudb, fluree, Datomic and others.
Would be great to have a discussion of immutable, why it is important and what TerminusDB does versus others.
@GavinMendelGleason - have at it.",this go in here https//githubcom / terminusdb / terminusdb / blob / master / docs / explanation / immutablemd or can be include as a comment on the issue there be a few immutable database out there immudb fluree datomic and other would be great to have a discussion of immutable why -PRON- be important and what terminusdb do versus other @gavinmendelgleason - have at -PRON-,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,194,2021-02-04T13:52:10Z,luke-feeney,Explanation: what is the point of immuatable? Why does it help me? ,https://github.com/terminusdb/terminusdb/issues/194,What the craic with immutable in TDB?,2021-02-10T12:46:24Z,luke-feeney,https://github.com/terminusdb/terminusdb/issues/194#issuecomment-776682746,"Good article: https://adlrocha.substack.com/p/adlrocha-immutable-databases
And HN discussion: https://news.ycombinator.com/item?id=23290769
Could be useful for framing",good article https//adlrochasubstackcom / p / adlrocha - immutable - database and hn discussion https//newsycombinatorcom / itemid=23290769 could be useful for frame,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,195,2021-02-04T13:54:43Z,luke-feeney,Explanation: Why we hate locks and what we did about it ,https://github.com/terminusdb/terminusdb/issues/195,"Why we hate locks and what we did about it
Nobody likes locks - but most aren't willing to do anything about it",2021-02-10T12:12:35Z,luke-feeney,https://github.com/terminusdb/terminusdb/issues/195#issuecomment-776665639,"This goes here:
https://github.com/terminusdb/terminusdb/blob/master/docs/Explanation/LOCKS.md
Or as a comment on this thread.
Idea here is a continuation of the below comment from the discord server, but with a bit more discussion of locks in general, why others use them and why we don't think that's great.
'TerminusDB writes are append-only. every write will create a new layer of data containing all the changes made. Therefore, we have no need for data locks. Multiple writes can be active at once, with their own view of the data, making modifications to it.
A 'commit' in this model means writing a label file to point to the newest layer. The committer locks this file, ensures that the current label contains the version they expect, write their new version of the label, then unlocks. If the committer encounters a different label version than they expected (which happens in case of multiple concurrent writers), the transaction is retried. We can do this because transactions have no side-effects.
There is no chance for deadlocks. At most, a committer will be holding on to one file lock. Therefore, there is no situation where hold-and-wait will happen, and therefore, no lock cycles can occur.'
@matko - continue your fine work",this go here https//githubcom / terminusdb / terminusdb / blob / master / docs / explanation / locksmd or as a comment on this thread idea here be a continuation of the below comment from the discord server but with a bit more discussion of lock in general why other use -PRON- and why -PRON- do not think that be great ' terminusdb write be append - only every write will create a new layer of datum contain all the change make therefore -PRON- have no need for datum lock multiple write can be active at once with -PRON- own view of the datum make modification to -PRON- a ' commit ' in this model mean write a label file to point to the new layer the committer lock this file ensure that the current label contain the version -PRON- expect write -PRON- new version of the label then unlock if the committer encounter a different label version than -PRON- expect ( which happen in case of multiple concurrent writer ) the transaction be retry -PRON- can do this because transaction have no side - effect there be no chance for deadlock at most a committer will be hold on to one file lock therefore there be no situation where hold - and - wait will happen and therefore no lock cycle can occur ' @matko - continue -PRON- fine work,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,196,2021-02-04T13:55:05Z,luke-feeney,Explanation: What’s a delta roll up? And why would I need one?,https://github.com/terminusdb/terminusdb/issues/196,What’s a delta roll up? And why would I need one?,2021-02-10T12:09:46Z,luke-feeney,https://github.com/terminusdb/terminusdb/issues/196#issuecomment-776664142,"This goes in here:
https://github.com/terminusdb/terminusdb/blob/master/docs/Explanation/DELTA.md
Or can be provided as reply to issue.
This is a slightly more discursive and evergreen version of this:
https://terminusdb.com/blog/2020/09/15/delta-rollup/
@matko - best person for the job",this go in here https//githubcom / terminusdb / terminusdb / blob / master / docs / explanation / deltamd or can be provide as reply to issue this be a slightly more discursive and evergreen version of this https//terminusdbcom / blog/2020/09/15 / delta - rollup/ @matko - good person for the job,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,196,2021-02-04T13:55:05Z,luke-feeney,Explanation: What’s a delta roll up? And why would I need one?,https://github.com/terminusdb/terminusdb/issues/196,What’s a delta roll up? And why would I need one?,2021-02-25T12:30:29Z,matko,https://github.com/terminusdb/terminusdb/issues/196#issuecomment-785861215,It is done.,-PRON- be do,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,197,2021-02-04T13:55:33Z,luke-feeney,Explanation: What is this document interface all about? What should I do with it?,https://github.com/terminusdb/terminusdb/issues/197,"What is this document interface all about? What should I do with it? (TerminusDB 4.0 has full surfability, clickability and editability of database documents through the console. It is a wiki or catalogue of all your data that you can edit in place.)",2021-02-10T11:59:05Z,luke-feeney,https://github.com/terminusdb/terminusdb/issues/197#issuecomment-776658802,"This is a great thing to explain. I think this - combined with the data catalog type features - is a super power, so would be great to give it a good showing. Would be great to contrast with other approaches to the problem.
Goes in here:
https://github.com/terminusdb/terminusdb/blob/master/docs/Explanation/DOCINTERFACE.md
Or as reply to this issue.
@kevinchekovfeeney - another one for you. I am happy to work on this also as very powerful I think.",this be a great thing to explain i think this - combine with the data catalog type feature - be a super power so would be great to give -PRON- a good showing would be great to contrast with other approach to the problem go in here https//githubcom / terminusdb / terminusdb / blob / master / docs / explanation / docinterfacemd or as reply to this issue @kevinchekovfeeney - another one for -PRON- i be happy to work on this also as very powerful i think,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,198,2021-02-04T13:56:01Z,luke-feeney,Explanation: What do YOU mean by low-code?,https://github.com/terminusdb/terminusdb/issues/198,"What do YOU mean by low-code?
What does that mean to TerminusDB",2021-02-10T11:53:48Z,luke-feeney,https://github.com/terminusdb/terminusdb/issues/198#issuecomment-776656096,"This goes in here:
https://github.com/terminusdb/terminusdb/blob/master/docs/Explanation/LOWCODE.md
Or you can write as a reply to this issue and we will copy across
Discussion of what low code is and what it means to us - and what TerminusDB wants to achieve with the console interfacing with the way people really work. General and workflow on console oriented.
@KittyJose - you rule the console, give this a try!",this go in here https//githubcom / terminusdb / terminusdb / blob / master / docs / explanation / lowcodemd or -PRON- can write as a reply to this issue and -PRON- will copy across discussion of what low code be and what -PRON- mean to -PRON- - and what terminusdb want to achieve with the console interface with the way people really work general and workflow on console orient @kittyjose - -PRON- rule the console give this a try,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,198,2021-02-04T13:56:01Z,luke-feeney,Explanation: What do YOU mean by low-code?,https://github.com/terminusdb/terminusdb/issues/198,"What do YOU mean by low-code?
What does that mean to TerminusDB",2021-02-25T16:18:49Z,KittyJose,https://github.com/terminusdb/terminusdb/issues/198#issuecomment-786022755,available in https://terminusdb.github.io/terminusdb/#/Explanation/LOWCODE,available in https//terminusdbgithubio / terminusdb/#/explanation / lowcode,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,199,2021-02-04T13:56:28Z,luke-feeney,Explanation: Closed world – tell me more?,https://github.com/terminusdb/terminusdb/issues/199,Closed world – tell me more?,2021-02-10T11:50:24Z,luke-feeney,https://github.com/terminusdb/terminusdb/issues/199#issuecomment-776654386,"Needs to go in here:
https://github.com/terminusdb/terminusdb/blob/master/docs/Explanation/CLOSED.md
Would be great to include links to the proofs and the code.
https://terminusdb.com/t/papers/OWL-formalisation.pdf
https://github.com/GavinMendelGleason/OWL
Explanation of why we did it and why it is better (or comparable) than SHAQL/other approaches
@kevinchekovfeeney - a job for you",need to go in here https//githubcom / terminusdb / terminusdb / blob / master / docs / explanation / closedmd would be great to include link to the proof and the code https//terminusdbcom / t / paper / owl - formalisationpdf https//githubcom / gavinmendelgleason / owl explanation of why -PRON- do -PRON- and why -PRON- be well ( or comparable ) than shaql / other approach @kevinchekovfeeney - a job for -PRON-,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,200,2021-02-04T13:57:23Z,luke-feeney,Explanation: What is with all these graphs?   ,https://github.com/terminusdb/terminusdb/issues/200,"What is with all these graphs?
Explanation in 800 words as to why there are all these graphs and why it is important that the structure is like this.",2021-02-10T11:46:47Z,luke-feeney,https://github.com/terminusdb/terminusdb/issues/200#issuecomment-776652709,"Needs to go in here:
https://github.com/terminusdb/terminusdb/blob/master/docs/Explanation/GRAPHS.md
Or be included in response to this issue so we can copy across.
This is basically covering the below linked material in summary and setting out why this approach is best versus other possible approaches. Use the link.
https://assets.terminusdb.com/presentations/Building_a_Native_Revision_Control_Graph_DB_from_Scratch.pdf
@GavinMendelGleason - best man for job",need to go in here https//githubcom / terminusdb / terminusdb / blob / master / docs / explanation / graphsmd or be include in response to this issue so -PRON- can copy across this be basically cover the below link material in summary and set out why this approach be good versus other possible approach use the link https//assetsterminusdbcom / presentation / building_a_native_revision_control_graph_db_from_scratchpdf @gavinmendelgleason - good man for job,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,201,2021-02-04T16:16:25Z,KittyJose,merge fails on attempt to merge branch XYZ to a branch advanced from the same branch XYZ ,https://github.com/terminusdb/terminusdb/issues/201,"Describe the bug
merge fails on attempt to merge branch XYZ to a branch advanced from the same branch XYZ
To Reproduce
Steps to generate rebase bug =>

create branch dev (from main) => do something like add a doctype into dev
create another branch qa (from dev) =>   do something like add a doctype into qa
then go to dev branch and merge the current dev branch to qa branch

Info (please complete the following information):
Error: 500
@type: ""api:ErrorResponse""
api:error: {@type: ""api:APIEndpointFailed""}
api:message: ""Failed to run the API endpoint goal rebase_handler('admin/nwDBWITHGAVIN/local/branch/qa')""
api:status: ""api:failure""",2021-02-04T16:17:29Z,KittyJose,https://github.com/terminusdb/terminusdb/issues/201#issuecomment-773428388,removing additional info,remove additional info,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,202,2021-02-05T14:54:33Z,Cheukting,Checking Canary release on Mac desktop app image,https://github.com/terminusdb/terminusdb/issues/202,"Describe the bug
fail to publish to hub
To Reproduce
Steps to reproduce the behavior:

Clone the bank example from the public repo
Make a clone to the local clone, rename it
Make some changes to the db
Publish the new local db to Hub

Expected behavior
It should work as a new public db under my account
Screenshots
Error: remote_connection_error(_22728{'@type':""api:ErrorResponse"",'api:error':_22748{'@type':""api:IncorrectAuthenticationError""},'api:message':""Incorrect authentication information"",'api:status':""api:failure""}) [33] throw(error(remote_connection_error(...),_22802)) [31] '<meta-call>'('<garbage_collected>') <foreign> [30] database:with_transaction_(query_context{all_witnesses:false,authorization:'terminusdb:///system/data/admin',bindings:[],default_collection:database_descriptor{database_name:""bank_balance_example_cheuk"",organization_name:""admin""},files:[],filter:type_filter{types: ...},prefixes:_22948{api:'http://terminusdb.com/schema/api#',doc:'terminusdb:///repository/data/',layer:'http://terminusdb.com/schema/layer#',owl:'http://www.w3.org/2002/07/owl#',rdf:'http://www.w3.org/1999/02/22-rdf-syntax-ns#',rdfs:'http://www.w3.org/2000/01/rdf-schema#',ref:'http://terminusdb.com/schema/ref#',repo:'http://terminusdb.com/schema/repository#',system:'http://terminusdb.com/schema/system#',terminus:'http://terminusdb.com/schema/system#',vio:'http://terminusdb.com/schema/vio#',woql:'http://terminusdb.com/schema/woql#',xdd:'http://terminusdb.com/schema/xdd#',xsd:'http://www.w3.org/2001/XMLSchema#'},selected:[],system:system_descriptor{},transaction_objects:[...],update_guard:true,write_graph:repo_graph{database_name:""bank_balance_example_cheuk"",name:""main"",organization_name:""admin"",type:instance}},db_fetch:(...,...),_22880) at /Applications/TerminusDB.app/Contents/Resources/app/public/terminusdb-server/src/core/transaction/database.pl:213 [29] setup_call_catcher_cleanup(database:true,database:with_transaction_(...,...,_23110),_23088,database:(...,...)) at /Applications/TerminusDB.app/Contents/Resources/app/public/SWI-Prolog.app/Contents/swipl/boot/init.pl:614 [25] '<meta-call>'('<garbage_collected>') <foreign> [24] catch(routes:(...,...),error(remote_connection_error(...),context(_23216,_23218)),routes:do_or_die(...,...)) at /Applications/TerminusDB.app/Contents/Resources/app/public/SWI-Prolog.app/Contents/swipl/boot/init.pl:532 [23] catch_with_backtrace('<garbage_collected>','<garbage_collected>','<garbage_collected>') at /Applications/TerminusDB.app/Contents/Resources/app/public/SWI-Prolog.app/Contents/swipl/boot/init.pl:582 Note: some frames are missing due to last-call optimization. Re-run your program in debug mode (:- debug.) to get more detail.

Info (please complete the following information):

OS: Mac Catalina 10.15.7
Desktop app (canary-tag)

Additional context
Add any other context about the problem here.",2021-02-12T13:33:06Z,Cheukting,https://github.com/terminusdb/terminusdb/issues/202#issuecomment-778197507,Still fail to push to hub on canary of 2020-02-12,still fail to push to hub on canary of 2020 - 02 - 12,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,203,2021-02-05T15:10:20Z,GavinMendelGleason,Failed to pull from recently shared database.,https://github.com/terminusdb/terminusdb/issues/203,"Describe the bug
Can't pull for newly created database shared to hub.
To Reproduce
Create a database. Share to hub. Create a commit (for instance by changing schema). Pull from remote.
Expected behavior
No error
Screenshots
Failed to pull updates from origin main to local main (0.899 seconds)
Error: divergent_history(""mqw31zl9b55u6qfi2s6ooi6espji9nr"",[""czy9sc1vnsr1s9bcs4tubkohisns3em""],[]) [36] throw(error(divergent_history(""mqw31zl9b55u6qfi2s6ooi6espji9nr"",...,[]),_13892)) [35] '<meta-call>'(db_fast_forward:(...,...)) <foreign> [34] database:with_transaction_(query_context{all_witnesses:false,authorization:'terminusdb:///system/data/admin',bindings:[],default_collection:repository_descriptor{database_descriptor: ...,repository_name:""local""},files:[],filter:type_filter{types: ...},prefixes:_14054{api:'http://terminusdb.com/schema/api#',doc:'terminusdb:///commits/data/',layer:'http://terminusdb.com/schema/layer#',owl:'http://www.w3.org/2002/07/owl#',rdf:'http://www.w3.org/1999/02/22-rdf-syntax-ns#',rdfs:'http://www.w3.org/2000/01/rdf-schema#',ref:'http://terminusdb.com/schema/ref#',repo:'http://terminusdb.com/schema/repository#',system:'http://terminusdb.com/schema/system#',terminus:'http://terminusdb.com/schema/system#',vio:'http://terminusdb.com/schema/vio#',woql:'http://terminusdb.com/schema/woql#',xdd:'http://terminusdb.com/schema/xdd#',xsd:'http://www.w3.org/2001/XMLSchema#'},selected:[],system:system_descriptor{},transaction_objects:[...],update_guard:true,write_graph:commit_graph{database_name:""some_kinda_database"",name:""main"",organization_name:""admin"",repository_name:""local"",type:instance}},db_fast_forward:(...,...),_13986) at /tmp/.mount_.org.cx6j7ga/usr/share/terminusdb/src/core/transaction/database.pl:213 [33] setup_call_catcher_cleanup(database:true,database:with_transaction_(...,...,_14220),_14198,database:(...,...)) at /tmp/.mount_.org.cx6j7ga/usr/lib/swi-prolog/boot/init.pl:614 [29] '<meta-call>'('<garbage_collected>') <foreign> [28] catch(db_pull:(...,...),error(divergent_history(""mqw31zl9b55u6qfi2s6ooi6espji9nr"",...,[]),context(_14330,_14332)),db_pull:(...,...)) at /tmp/.mount_.org.cx6j7ga/usr/lib/swi-prolog/boot/init.pl:532 [27] catch_with_backtrace(db_pull:(...,...),error(divergent_history(""mqw31zl9b55u6qfi2s6ooi6espji9nr"",...,[]),context(_14408,_14410)),db_pull:(...,...)) at /tmp/.mount_.org.cx6j7ga/usr/lib/swi-prolog/boot/init.pl:582 Note: some frames are missing due to last-call optimization. Re-run your program in debug mode (:- debug.) to get more detail.less
{
  ""data"": {
    ""api:message"": ""Error: divergent_history(\""mqw31zl9b55u6qfi2s6ooi6espji9nr\"",[\""czy9sc1vnsr1s9bcs4tubkohisns3em\""],[])\n  [36] throw(error(divergent_history(\""mqw31zl9b55u6qfi2s6ooi6espji9nr\"",...,[]),_13892))\n  [35] '<meta-call>'(db_fast_forward:(...,...)) <foreign>\n  [34] database:with_transaction_(query_context{all_witnesses:false,authorization:'terminusdb:///system/data/admin',bindings:[],default_collection:repository_descriptor{database_descriptor: ...,repository_name:\""local\""},files:[],filter:type_filter{types: ...},prefixes:_14054{api:'http://terminusdb.com/schema/api#',doc:'terminusdb:///commits/data/',layer:'http://terminusdb.com/schema/layer#',owl:'http://www.w3.org/2002/07/owl#',rdf:'http://www.w3.org/1999/02/22-rdf-syntax-ns#',rdfs:'http://www.w3.org/2000/01/rdf-schema#',ref:'http://terminusdb.com/schema/ref#',repo:'http://terminusdb.com/schema/repository#',system:'http://terminusdb.com/schema/system#',terminus:'http://terminusdb.com/schema/system#',vio:'http://terminusdb.com/schema/vio#',woql:'http://terminusdb.com/schema/woql#',xdd:'http://terminusdb.com/schema/xdd#',xsd:'http://www.w3.org/2001/XMLSchema#'},selected:[],system:system_descriptor{},transaction_objects:[...],update_guard:true,write_graph:commit_graph{database_name:\""some_kinda_database\"",name:\""main\"",organization_name:\""admin\"",repository_name:\""local\"",type:instance}},db_fast_forward:(...,...),_13986) at /tmp/.mount_.org.cx6j7ga/usr/share/terminusdb/src/core/transaction/database.pl:213\n  [33] setup_call_catcher_cleanup(database:true,database:with_transaction_(...,...,_14220),_14198,database:(...,...)) at /tmp/.mount_.org.cx6j7ga/usr/lib/swi-prolog/boot/init.pl:614\n  [29] '<meta-call>'('<garbage_collected>') <foreign>\n  [28] catch(db_pull:(...,...),error(divergent_history(\""mqw31zl9b55u6qfi2s6ooi6espji9nr\"",...,[]),context(_14330,_14332)),db_pull:(...,...)) at /tmp/.mount_.org.cx6j7ga/usr/lib/swi-prolog/boot/init.pl:532\n  [27] catch_with_backtrace(db_pull:(...,...),error(divergent_history(\""mqw31zl9b55u6qfi2s6ooi6espji9nr\"",...,[]),context(_14408,_14410)),db_pull:(...,...)) at /tmp/.mount_.org.cx6j7ga/usr/lib/swi-prolog/boot/init.pl:582\n\nNote: some frames are missing due to last-call optimization.\nRe-run your program in debug mode (:- debug.) to get more detail.\n\n"",
    ""api:status"": ""api:server_error"",
    ""end"": ""2021-02-05T15:04:21.170Z"",
    ""start"": ""2021-02-05T15:04:21.170Z"",
    ""duration"": ""2021-02-05T15:04:21.170Z""
  }
}

Info (please complete the following information):

OS: Ubuntu 20.04
AppImage electron client for 'canary' from 5-Feb desktop application",2021-02-16T15:42:04Z,GavinMendelGleason,https://github.com/terminusdb/terminusdb/issues/203#issuecomment-779920735,Fixed in 822b7f9,fix in 822b7f9,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,204,2021-02-05T15:12:21Z,luke-feeney,Can't Import CSV,https://github.com/terminusdb/terminusdb/issues/204,"Tried to create a database using a CSV and got the error in the image. Looks like it is a UTF-8 thing, but should be a fairly standard CSV. This is CSV:
https://data.gov.ie/dataset/crimes-at-garda-stations-level-2010-2016
Error in inserting CSV (0.43 seconds)
Error: terminus_store_rust_error('CSV parse error: record 13 (line 13, field: 1, byte: 12746): invalid utf-8: invalid UTF-8 in field 1 near byte index 4') [36] terminus_store:csv_builder('garda_stations.csv','/tmp/swipl_7_28',<builder 7b8c7f92d58a903911ec386337b9319a3441162e>,<builder 3dbd717570fcec13805c98de8dc0737991f1731d>,'terminusdb:///data/','terminusdb:///schema#',true,false) [33] '$apply':forall(api_csv:member(...,...),api_csv:(...;...)) at /usr/lib/swipl/boot/apply.pl:52 [31] database:with_transaction_(query_context{all_witnesses:false,authorization:'terminusdb:///system/data/admin',bindings:[],commit_info:_16224{author:""Luke@datachemist.com"",message:""create database with CSVs""},default_collection:branch_descriptor{branch_name:""main"",repository_descriptor: ...},files:[...],filter:type_name_filter{names: ...,type:instance},prefixes:_16108{api:'http://terminusdb.com/schema/api#',doc:'terminusdb:///data/',layer:'http://terminusdb.com/schema/layer#',owl:'http://www.w3.org/2002/07/owl#',rdf:'http://www.w3.org/1999/02/22-rdf-syntax-ns#',rdfs:'http://www.w3.org/2000/01/rdf-schema#',ref:'http://terminusdb.com/schema/ref#',repo:'http://terminusdb.com/schema/repository#',scm:'terminusdb:///schema#',system:'http://terminusdb.com/schema/system#',terminus:'http://terminusdb.com/schema/system#',vio:'http://terminusdb.com/schema/vio#',woql:'http://terminusdb.com/schema/woql#',xdd:'http://terminusdb.com/schema/xdd#',xsd:'http://www.w3.org/2001/XMLSchema#'},selected:[],system:transaction_object{descriptor: ...,inference_objects: ...,instance_objects: ...,schema_objects: ...},transaction_objects:[...],update_guard:_16072,write_graph:branch_graph{branch_name:""main"",database_name:""frogman"",name:""main"",organization_name:""admin"",repository_name:""local"",type:instance}},api_csv:(...,...),_16022) at /app/terminusdb/src/core/transaction/database.pl:213 [30] setup_call_catcher_cleanup(database:true,database:with_transaction_(...,...,_16306),_16284,database:(...,...)) at /usr/lib/swipl/boot/init.pl:614 [25] '<meta-call>'('<garbage_collected>') <foreign> [24] catch(routes:(...,...),error(terminus_store_rust_error('CSV parse error: record 13 (line 13, field: 1, byte: 12746): invalid utf-8: invalid UTF-8 in field 1 near byte index 4'),context(_16412,_16414)),routes:do_or_die(...,...)) at /usr/lib/swipl/boot/init.pl:532 [23] catch_with_backtrace('<garbage_collected>','<garbage_collected>','<garbage_collected>') at /usr/lib/swipl/boot/init.pl:582 Note: some frames are missing due to last-call optimization. Re-run your program in debug mode (:- debug.) to get more detail. less

{
  ""data"": {
    ""api:message"": ""Error: terminus_store_rust_error('CSV parse error: record 13 (line 13, field: 1, byte: 12746): invalid utf-8: invalid UTF-8 in field 1 near byte index 4')\n  [36] terminus_store:csv_builder('garda_stations.csv','/tmp/swipl_7_28',<builder 7b8c7f92d58a903911ec386337b9319a3441162e>,<builder 3dbd717570fcec13805c98de8dc0737991f1731d>,'terminusdb:///data/','terminusdb:///schema#',true,false)\n  [33] '$apply':forall(api_csv:member(...,...),api_csv:(...;...)) at /usr/lib/swipl/boot/apply.pl:52\n  [31] database:with_transaction_(query_context{all_witnesses:false,authorization:'terminusdb:///system/data/admin',bindings:[],commit_info:_16224{author:\""Luke@datachemist.com\"",message:\""create database with CSVs\""},default_collection:branch_descriptor{branch_name:\""main\"",repository_descriptor: ...},files:[...],filter:type_name_filter{names: ...,type:instance},prefixes:_16108{api:'http://terminusdb.com/schema/api#',doc:'terminusdb:///data/',layer:'http://terminusdb.com/schema/layer#',owl:'http://www.w3.org/2002/07/owl#',rdf:'http://www.w3.org/1999/02/22-rdf-syntax-ns#',rdfs:'http://www.w3.org/2000/01/rdf-schema#',ref:'http://terminusdb.com/schema/ref#',repo:'http://terminusdb.com/schema/repository#',scm:'terminusdb:///schema#',system:'http://terminusdb.com/schema/system#',terminus:'http://terminusdb.com/schema/system#',vio:'http://terminusdb.com/schema/vio#',woql:'http://terminusdb.com/schema/woql#',xdd:'http://terminusdb.com/schema/xdd#',xsd:'http://www.w3.org/2001/XMLSchema#'},selected:[],system:transaction_object{descriptor: ...,inference_objects: ...,instance_objects: ...,schema_objects: ...},transaction_objects:[...],update_guard:_16072,write_graph:branch_graph{branch_name:\""main\"",database_name:\""frogman\"",name:\""main\"",organization_name:\""admin\"",repository_name:\""local\"",type:instance}},api_csv:(...,...),_16022) at /app/terminusdb/src/core/transaction/database.pl:213\n  [30] setup_call_catcher_cleanup(database:true,database:with_transaction_(...,...,_16306),_16284,database:(...,...)) at /usr/lib/swipl/boot/init.pl:614\n  [25] '<meta-call>'('<garbage_collected>') <foreign>\n  [24] catch(routes:(...,...),error(terminus_store_rust_error('CSV parse error: record 13 (line 13, field: 1, byte: 12746): invalid utf-8: invalid UTF-8 in field 1 near byte index 4'),context(_16412,_16414)),routes:do_or_die(...,...)) at /usr/lib/swipl/boot/init.pl:532\n  [23] catch_with_backtrace('<garbage_collected>','<garbage_collected>','<garbage_collected>') at /usr/lib/swipl/boot/init.pl:582\n\nNote: some frames are missing due to last-call optimization.\nRe-run your program in debug mode (:- debug.) to get more detail.\n\n"",
    ""api:status"": ""api:server_error"",
    ""end"": ""2021-02-05T15:10:21.307Z"",
    ""start"": ""2021-02-05T15:10:21.307Z"",
    ""duration"": ""2021-02-05T15:10:21.307Z""
  }
}


To Reproduce
Steps to reproduce the behavior:

try to import this CSV",2021-02-11T15:14:38Z,GavinMendelGleason,https://github.com/terminusdb/terminusdb/issues/204#issuecomment-777565670,"gavin@titan:~/Documents/terminusdb/csv[]$ file -i garda_stations.csv 
garda_stations.csv: application/csv; charset=iso-8859-1
Unfortunately we do not currently allow encodings other than utf-8. We intend to include automatic detection and conversion in a future release but this isn't ready yet.
The workaround for now is to convert the file from iso-8859-1 to utf-8 using some utility.  For instance:
gavin@titan:~/Documents/terminusdb/csv[]$ iconv -f iso-8859-1 -t utf-8 -o garda_stations-utf8.csv garda_stations.csv",gavin@titan~/document / terminusdb / csv[]$ file -i garda_stationscsv garda_stationscsv application / csv charset = iso-8859 - 1 unfortunately -PRON- do not currently allow encoding other than utf-8 -PRON- intend to include automatic detection and conversion in a future release but this be not ready yet the workaround for now be to convert the file from iso-8859 - 1 to utf-8 use some utility for instance gavin@titan~/documents / terminusdb / csv[]$ iconv -f iso-8859 - 1 -t utf-8 -o garda_station - utf8csv garda_stationscsv,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,204,2021-02-05T15:12:21Z,luke-feeney,Can't Import CSV,https://github.com/terminusdb/terminusdb/issues/204,"Tried to create a database using a CSV and got the error in the image. Looks like it is a UTF-8 thing, but should be a fairly standard CSV. This is CSV:
https://data.gov.ie/dataset/crimes-at-garda-stations-level-2010-2016
Error in inserting CSV (0.43 seconds)
Error: terminus_store_rust_error('CSV parse error: record 13 (line 13, field: 1, byte: 12746): invalid utf-8: invalid UTF-8 in field 1 near byte index 4') [36] terminus_store:csv_builder('garda_stations.csv','/tmp/swipl_7_28',<builder 7b8c7f92d58a903911ec386337b9319a3441162e>,<builder 3dbd717570fcec13805c98de8dc0737991f1731d>,'terminusdb:///data/','terminusdb:///schema#',true,false) [33] '$apply':forall(api_csv:member(...,...),api_csv:(...;...)) at /usr/lib/swipl/boot/apply.pl:52 [31] database:with_transaction_(query_context{all_witnesses:false,authorization:'terminusdb:///system/data/admin',bindings:[],commit_info:_16224{author:""Luke@datachemist.com"",message:""create database with CSVs""},default_collection:branch_descriptor{branch_name:""main"",repository_descriptor: ...},files:[...],filter:type_name_filter{names: ...,type:instance},prefixes:_16108{api:'http://terminusdb.com/schema/api#',doc:'terminusdb:///data/',layer:'http://terminusdb.com/schema/layer#',owl:'http://www.w3.org/2002/07/owl#',rdf:'http://www.w3.org/1999/02/22-rdf-syntax-ns#',rdfs:'http://www.w3.org/2000/01/rdf-schema#',ref:'http://terminusdb.com/schema/ref#',repo:'http://terminusdb.com/schema/repository#',scm:'terminusdb:///schema#',system:'http://terminusdb.com/schema/system#',terminus:'http://terminusdb.com/schema/system#',vio:'http://terminusdb.com/schema/vio#',woql:'http://terminusdb.com/schema/woql#',xdd:'http://terminusdb.com/schema/xdd#',xsd:'http://www.w3.org/2001/XMLSchema#'},selected:[],system:transaction_object{descriptor: ...,inference_objects: ...,instance_objects: ...,schema_objects: ...},transaction_objects:[...],update_guard:_16072,write_graph:branch_graph{branch_name:""main"",database_name:""frogman"",name:""main"",organization_name:""admin"",repository_name:""local"",type:instance}},api_csv:(...,...),_16022) at /app/terminusdb/src/core/transaction/database.pl:213 [30] setup_call_catcher_cleanup(database:true,database:with_transaction_(...,...,_16306),_16284,database:(...,...)) at /usr/lib/swipl/boot/init.pl:614 [25] '<meta-call>'('<garbage_collected>') <foreign> [24] catch(routes:(...,...),error(terminus_store_rust_error('CSV parse error: record 13 (line 13, field: 1, byte: 12746): invalid utf-8: invalid UTF-8 in field 1 near byte index 4'),context(_16412,_16414)),routes:do_or_die(...,...)) at /usr/lib/swipl/boot/init.pl:532 [23] catch_with_backtrace('<garbage_collected>','<garbage_collected>','<garbage_collected>') at /usr/lib/swipl/boot/init.pl:582 Note: some frames are missing due to last-call optimization. Re-run your program in debug mode (:- debug.) to get more detail. less

{
  ""data"": {
    ""api:message"": ""Error: terminus_store_rust_error('CSV parse error: record 13 (line 13, field: 1, byte: 12746): invalid utf-8: invalid UTF-8 in field 1 near byte index 4')\n  [36] terminus_store:csv_builder('garda_stations.csv','/tmp/swipl_7_28',<builder 7b8c7f92d58a903911ec386337b9319a3441162e>,<builder 3dbd717570fcec13805c98de8dc0737991f1731d>,'terminusdb:///data/','terminusdb:///schema#',true,false)\n  [33] '$apply':forall(api_csv:member(...,...),api_csv:(...;...)) at /usr/lib/swipl/boot/apply.pl:52\n  [31] database:with_transaction_(query_context{all_witnesses:false,authorization:'terminusdb:///system/data/admin',bindings:[],commit_info:_16224{author:\""Luke@datachemist.com\"",message:\""create database with CSVs\""},default_collection:branch_descriptor{branch_name:\""main\"",repository_descriptor: ...},files:[...],filter:type_name_filter{names: ...,type:instance},prefixes:_16108{api:'http://terminusdb.com/schema/api#',doc:'terminusdb:///data/',layer:'http://terminusdb.com/schema/layer#',owl:'http://www.w3.org/2002/07/owl#',rdf:'http://www.w3.org/1999/02/22-rdf-syntax-ns#',rdfs:'http://www.w3.org/2000/01/rdf-schema#',ref:'http://terminusdb.com/schema/ref#',repo:'http://terminusdb.com/schema/repository#',scm:'terminusdb:///schema#',system:'http://terminusdb.com/schema/system#',terminus:'http://terminusdb.com/schema/system#',vio:'http://terminusdb.com/schema/vio#',woql:'http://terminusdb.com/schema/woql#',xdd:'http://terminusdb.com/schema/xdd#',xsd:'http://www.w3.org/2001/XMLSchema#'},selected:[],system:transaction_object{descriptor: ...,inference_objects: ...,instance_objects: ...,schema_objects: ...},transaction_objects:[...],update_guard:_16072,write_graph:branch_graph{branch_name:\""main\"",database_name:\""frogman\"",name:\""main\"",organization_name:\""admin\"",repository_name:\""local\"",type:instance}},api_csv:(...,...),_16022) at /app/terminusdb/src/core/transaction/database.pl:213\n  [30] setup_call_catcher_cleanup(database:true,database:with_transaction_(...,...,_16306),_16284,database:(...,...)) at /usr/lib/swipl/boot/init.pl:614\n  [25] '<meta-call>'('<garbage_collected>') <foreign>\n  [24] catch(routes:(...,...),error(terminus_store_rust_error('CSV parse error: record 13 (line 13, field: 1, byte: 12746): invalid utf-8: invalid UTF-8 in field 1 near byte index 4'),context(_16412,_16414)),routes:do_or_die(...,...)) at /usr/lib/swipl/boot/init.pl:532\n  [23] catch_with_backtrace('<garbage_collected>','<garbage_collected>','<garbage_collected>') at /usr/lib/swipl/boot/init.pl:582\n\nNote: some frames are missing due to last-call optimization.\nRe-run your program in debug mode (:- debug.) to get more detail.\n\n"",
    ""api:status"": ""api:server_error"",
    ""end"": ""2021-02-05T15:10:21.307Z"",
    ""start"": ""2021-02-05T15:10:21.307Z"",
    ""duration"": ""2021-02-05T15:10:21.307Z""
  }
}


To Reproduce
Steps to reproduce the behavior:

try to import this CSV",2021-02-18T14:25:38Z,luke-feeney,https://github.com/terminusdb/terminusdb/issues/204#issuecomment-781380059,Thanks for reply - closing issue on basis that there is a short term work around and a longer term solution.,thank for reply - closing issue on basis that there be a short term work around and a long term solution,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,205,2021-02-06T09:24:40Z,GavinMendelGleason,Can't quit TerminusDB desktop on Linux,https://github.com/terminusdb/terminusdb/issues/205,"Describe the bug
After starting and quitting TerminusDB you will see the Terminus icon in the application tray. Quitting the application here does not help. Only killing the application from the command line works.
To Reproduce
This can be reproduced simply by starting the application on linux and attempting to close.
Info (please complete the following information):

OS: Ubuntu 20.04.2 LTS
Running 'canary' from 5-Feb desktop application",2021-02-11T14:31:37Z,luke-feeney,https://github.com/terminusdb/terminusdb/issues/205#issuecomment-777501945,Is this a feature of a bug - do we WANT people quitting?,be this a feature of a bug - do -PRON- want people quit,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,205,2021-02-06T09:24:40Z,GavinMendelGleason,Can't quit TerminusDB desktop on Linux,https://github.com/terminusdb/terminusdb/issues/205,"Describe the bug
After starting and quitting TerminusDB you will see the Terminus icon in the application tray. Quitting the application here does not help. Only killing the application from the command line works.
To Reproduce
This can be reproduced simply by starting the application on linux and attempting to close.
Info (please complete the following information):

OS: Ubuntu 20.04.2 LTS
Running 'canary' from 5-Feb desktop application",2021-02-18T14:16:01Z,kaaloo,https://github.com/terminusdb/terminusdb/issues/205#issuecomment-781373458,Quitting working fine for me on Ubuntu 20.10.,quit work fine for -PRON- on ubuntu 2010,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,207,2021-02-10T13:15:46Z,luke-feeney,New Docs Site Break,https://github.com/terminusdb/terminusdb/issues/207,Create new top level break on the documentation page of the TerminusDB website. Something like the below image. The TerminusHub break goes to existing docs (soon to be updated) the TerminusDB goes to the new docsify site,2021-02-10T13:16:06Z,luke-feeney,https://github.com/terminusdb/terminusdb/issues/207#issuecomment-776698415,This is for @KittyJose ;-),this be for @kittyjose - ),0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,208,2021-02-10T15:52:03Z,luke-feeney,Tutorial: Getting Started with Console,https://github.com/terminusdb/terminusdb/issues/208,"Assume Installation already done
Create Schema
Import Data
Query Data
Use Branching
This goes in here:
https://github.com/terminusdb/terminusdb/blob/master/docs/Intro_Tutorials/Start_with_Console.md
Or can be a comment on this issue and we can move across.
Idea it to keep it simple and get the person from 0 to 1 - installation is already included in the intro, so tutorial can just refer them back to the install instructions. (but maybe we should include console specific thing there?)
@KittyJose - this is for you",2021-02-25T14:10:23Z,KittyJose,https://github.com/terminusdb/terminusdb/issues/208#issuecomment-785922484,Available in https://terminusdb.github.io/terminusdb/#/Intro_Tutorials/Start_With_Console,available in https//terminusdbgithubio / terminusdb/#/intro_tutorial / start_with_console,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,209,2021-02-11T13:55:25Z,luke-feeney,Roadmap Text,https://github.com/terminusdb/terminusdb/issues/209,"The main TerminusDB roadmap needs to be populated. It can be fairly sparse for the moment as we work out the priorities for the next phase, but we should include an update of the text as part of every new episode.
Here is the relevant page:
https://terminusdb.github.io/terminusdb/#/ROADMAP
@dmytri - you might take a look at putting in some text",,,,,,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,210,2021-02-11T14:00:45Z,luke-feeney,Tutorial Repo,https://github.com/terminusdb/terminusdb/issues/210,"The README in the tutorials repo needs to be updated to reflect all the great content we have. Seems a shame to leave it hidden in folders without bringing it out for the world to see!
https://github.com/terminusdb/terminusdb-tutorials
@Cheukting - you did all the work getting it to this point! Can you give a short update?",2021-02-12T14:09:28Z,Cheukting,https://github.com/terminusdb/terminusdb/issues/210#issuecomment-778217690,Done!,do,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,211,2021-02-11T14:28:51Z,dmytri,Make console work off line after first load,https://github.com/terminusdb/terminusdb/issues/211,Persistent access to the Internet should not be required to access the database console. Use Service Workers to allow off line operation.,,,,,,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,212,2021-02-11T14:41:24Z,luke-feeney,Model Builder Commit Message ,https://github.com/terminusdb/terminusdb/issues/212,"Commit message from the model builder is saying ""commit by JSclient"" which is not helpful. We should have something lke ""update from model builder""",2021-02-12T15:44:00Z,Francesca-Bit,https://github.com/terminusdb/terminusdb/issues/212#issuecomment-778271403,message changed,message change,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,213,2021-02-12T13:20:25Z,Cheukting,"Console, Canary (2021-02-12) - Hub repo delete button",https://github.com/terminusdb/terminusdb/issues/213,"Describe the bug
A clear and concise description of what the bug is.
To Reproduce
Steps to reproduce the behavior:

Go to '...'
Click on '....'
Scroll down to '....'
See error

Expected behavior
A clear and concise description of what you expected to happen.
Screenshots

Info (please complete the following information):

OS: [e.g. Ubuntu 18.04]
How did you run terminus-server (terminus-quickstart, using Docker directly, manual installation)
SWI Prolog version if you're using the manual installation

Additional context
Add any other context about the problem here.",2021-02-12T16:49:04Z,Francesca-Bit,https://github.com/terminusdb/terminusdb/issues/213#issuecomment-778309131,fix css,fix css,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,214,2021-02-12T13:20:55Z,luke-feeney,Canary Bootstrap - WOQL.eval doesn't work! ,https://github.com/terminusdb/terminusdb/issues/214,"Tried to run through the bank balance example and got the below result

Works fine in master.",2021-02-12T13:35:42Z,luke-feeney,https://github.com/terminusdb/terminusdb/issues/214#issuecomment-778198837,@KittyJose confirmed that also an issue in dev - works fine in master as per previous report,@kittyjose confirm that also an issue in dev - work fine in master as per previous report,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,214,2021-02-12T13:20:55Z,luke-feeney,Canary Bootstrap - WOQL.eval doesn't work! ,https://github.com/terminusdb/terminusdb/issues/214,"Tried to run through the bank balance example and got the below result

Works fine in master.",2021-02-12T15:35:31Z,Francesca-Bit,https://github.com/terminusdb/terminusdb/issues/214#issuecomment-778266534,fix it in terminusdb-client,fix -PRON- in terminusdb - client,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,215,2021-02-12T13:31:40Z,Cheukting,Console - delete origin behavior is strange,https://github.com/terminusdb/terminusdb/issues/215,"Describe the bug
delete origin behavior is strange
To Reproduce
Steps to reproduce the behavior:

Clone Bank Example on Hub
Make a clone of the local clone
Delete the 1st local clone
See the screen shot behavior

Expected behavior
Better handling?
Screenshots

It says clone of undefined

It still try to show the deleted origin
Info (please complete the following information):

OS: [e.g. Ubuntu 18.04]
How did you run terminus-server (terminus-quickstart, using Docker directly, manual installation)
SWI Prolog version if you're using the manual installation

Additional context
Add any other context about the problem here.",,,,,,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,216,2021-02-15T09:04:26Z,Francesca-Bit,"Console - run query ""read_object""  result render error",https://github.com/terminusdb/terminusdb/issues/216,"Describe the bug
the result rendering of the read_object query get an error
To Reproduce
Steps to reproduce the behavior:

Go in console query panel
Run
using(""admin/seshat/local/commit/54dhg6h4hfq8jxk5h03tlmlmbbua9oe"").read_object(""terminusdb:///data/afghazn"", ""v:Doc"")
error in result rendering",2021-02-15T09:08:08Z,Francesca-Bit,https://github.com/terminusdb/terminusdb/issues/216#issuecomment-779074886,"fix bug, problem with prefixes parameter, Terminusdb-react-component ""JSONRenderer"" component,","fix bug problem with prefix parameter terminusdb - react - component "" jsonrenderer "" component",0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,217,2021-02-15T09:28:31Z,rrooij,TERMINUSDB_CONSOLE_BASE_URL should not be hardcoded in different branches,https://github.com/terminusdb/terminusdb/issues/217,"We currently have different console versions based on the Git branch. This makes sense, as terminusdb-console is an important part of TerminusDB. However, the way it is currently done is a bit dirty. We hardcode the default URL on the different branches. For instance, dev has a different default base_url in terminus_config.pl  than canary. This leads to potential merge conflicts and makes it harder to compare differences between different branches.
Instead, we should set it at build-time somehow.",,,,,,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,218,2021-02-17T15:14:57Z,luke-feeney,Creating Place for Python Tutorials to Go,https://github.com/terminusdb/terminusdb/issues/218,Further to discussion we are going to split the JS and Python tutorials at the top level and we need a place to put the relevant tutorials that we can link to from the top level navigation,2021-02-18T23:11:19Z,Cheukting,https://github.com/terminusdb/terminusdb/issues/218#issuecomment-781695942,"This is what it will look like when the new doc for the Python client go live.

Changes are now in terminusdb-client-python dev",this be what -PRON- will look like when the new doc for the python client go live change be now in terminusdb - client - python dev,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,219,2021-02-17T15:19:37Z,luke-feeney,Explanation: WOQL - how does it go?,https://github.com/terminusdb/terminusdb/issues/219,"Going to use this text as the explanation. You happy with that @GavinMendelGleason, update here if changes needed
Fluent Style
The TerminusDB query libraries make extensive use of the fluent style to simplify the expression of complex compound queries. Many WOQL query words accept a sub-query as an argument and, rather than using a functional (Lisp-like) style of capturing containment, a style where sub-queries are appended to the initial function as a new function is preferred.
rather than using a functional style:
    select(a, b, triple(c, d, e))
the fluent style would be:
    select(a, b).triple(c, d, e)
Both styles are legal WOQL and semantically equivalent. However, the second 'fluent' style is preferred because it is easier to read and easier to write primarily becaue it greatly reduces the amount of vizual parameter matching that the reader and writer have to perform in order to verify that their query is correct.
Fluent queries are parsed left to right - functions to the right of a given function are considered as sub-queries of the first, with one important exception - conjunction.
the functional style of expresing conjunction using the WOQL and() function is straightforward and is often most useful for clarity:
    and(triple(a, b, c), triple(d, e, f))
the fluent style allows us to use any of the following forumlations with the same semantics:
    and(triple(a, b, c)).triple(d, e, f)
    triple(a, b, c).and().triple(d, e, f)
    triple(a, b, c).triple(d, e, f)
The third concise form is unambiguous in situations where the WOQL functions that are chained together do not take sub-clauses - and because conjunction is so frequently used, this short-hand form, where the and() is implicit, is convenient in many situations. However it should be used with care - the conjunction is always applied to the function immediately to the left of the '.' in the chain and not to any functions further up the chain.  If used improperly, with clauses that do take sub-clauses, it will produce improperly specified queries, in particular with negation (not) and optional functions (opt).
So, for example, the following query:
    triple(a, b, c).opt().triple(d, e, f).triple(g, h, i)
is equivalent to the following query in the functional style:
    and(
        triple(a, b, c),
        opt(
            and(
                triple(d, e, f),
                triple(g, h, i)
            )
        )
    )
It is easy to misinterpret it when you mean to express:
    and(
        triple(a, b, c),
        opt().triple(d, e, f),
        triple(g, h, i)
    )
As a general rule, if in doubt, use the functional style explicitly with and() as this makes it clear and explicit which functions are sub-clauses of other functions.
WOQL.js and JSON-LD
WOQL uses JSON-LD and a formally specified ontology to define the language and to transmit queries over the wire.  WOQL.js is designed primarily to be as easy as possible for programmers to write because JSON-LD is itself tedious for humans to read and write. All WOQL.js queries are translated into the equivalent JSON-LD format for transmission over the wire.  The WOQL.js json() function can be used to translate any WOQL query from its JSON-LD format to and from it's WOQL.js equivalent (a WOQLQuery() object). If passed a JSON-LD argument, it will generate the equivalent WOQLQuery() object, if passed no argument, it will return the JSON-LD equivalent of the WOQLQuery(), in general the following semantic identity should always hold:
let wjs = new WOQLQuery().json(json_ld)
json_ld == wjs.json()
Embedding JSON-LD directly in WOQL.js
It is possible to use JSON-LD interchangably within WOQL.js - wherever a WOQL function or argument can be accepted directly in WOQL.js, the JSON-LD equivalent can also be supplied. So, for example, the following two WOQL statements are identical:
triple(a, b, 1) == triple(a, b, {""@type"": ""xsd:integer"", ""@value"": 1})
There should never be a situation in which it is necessary to use JSON-LD directly - WOQL.js is sufficiently rich to express all queries that are expressible in the underlying JSON-LD, however it can be convenient to embed JSON-LD in queries in some cases.
WOQL Variables
With the exception of resource identifiers which are used to specify the graphs against which operations such as queries are carried out (functions: using, with, into, from), WOQL allows variables to be substituted for any of the arguments to all WOQL functions. WOQL variables follow the logic of unification - borrowed from the Prolog engine which implements WOQL within TerminusDB.  That is to say that each valid value for a variable, as constrained by the totality of the query, will produce a new row in the results, and when there are multiple variables, the rows that are returned will be the cartesian product of all the possible combinations of variables values in the query.
In WOQL.js, there are 3 distinct ways of expressing variables within queries. All are semantically equivalent, although the first is generally preferred as it is easier to type and it is easier to distinguish variables from constants at a glance due to the lack of quote marks around the variables
1
    let [a, b, c] = vars('a', 'b', 'c')
    triple(a, b, c)
2
    triple('v:a', 'v:b', 'v:c')
3
    triple({'@type': 'woql:Variable', 'woql:variable_name': {""@type"": 'xsd:string', '@value': 'a'}} ....)
WOQL uses the formal logical approach to variables known as unification - this allows most WOQL functions to serve as both pattern matchers and pattern generators, depending on whether a variable or constant is provided as an argument. If a variable is provided, WOQL will generate all possible valid solutions which fill the variable value. If a constant is provided, WOQL will match only those solutions with exactly that value. With the exception of resource identifiers, WOQL functions accept either variables or constants in virtually all of their arguments.
Prefixes in WOQL.js
Internally, TerminusDB uses strict RDF rules to represent all data. This means that all identifiers and properties are represented by IRIs (which are a superset of URLs). However, IRIs are difficult to remember and tedious to type. RDF in general gets around this problem by allowing prefixed forms as shorthand - so for example, we can use ""rdf:type"" rather than ""http://obscure.w3c.url/with/embedded/dates#type"". TerminusDB defines a set of standard prefixes which are availabe to use and also allows users to extend this by adding their own prefix designations to the system. The set of standard prefixes includes the basic language elements (rdf, rdfs, owl), datatype elements (xsd, xdd) and internal namespaces (ref, repo, system, vio). It also pre-defines two prefixes for user-use - the 'doc' prefix for instance data IRIs and the 'scm' prefix for schema IRIs. So we can write ""doc:X"" or ""scm:X"" and this will always resolve to a valid IRI in all databases.
WOQL goes a step beyond supporting prefixes and automatically applies prefixes wherever possible allowing users to specify prefixes only when necessary.
The default prefixes are applied in the following way
- ""doc"" applies to woql:subject (first argument to triple) where instance data IRIs are normally what is required
- ""scm"" applies to woql:predicate and other arguments (sub, type) where schema elements are normally required
- when standard predicates are used with no prefix (label, type, comment, subClassOf, domain, range) the standard correct prefixes are applied
- otherwise if no prefix is applied a string is assumed
WOQL Functions
The JSON-LD form of WOQL supports a well-defined set of functions (woql:Triple, woql:Regexp...) - in WOQL.js these functions are known as primitives. WOQL.js supports all of these primite functions and adds several extensions on top - functions that compose multiple primitives, functions that compose partial primitives and can be chained together, and simple helper functions to make it easier to format the arguments correctly. The table below shows the full range of functions supported by WOQL.js and groups them together into categories to make it easier to find the required function for specific problems.",2021-02-25T14:12:00Z,luke-feeney,https://github.com/terminusdb/terminusdb/issues/219#issuecomment-785923639,ok have used this text - hope it makes sense,ok have use this text - hope -PRON- make sense,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,220,2021-02-17T16:08:34Z,luke-feeney,Reference: CLI,https://github.com/terminusdb/terminusdb/issues/220,"We need material for the CLI reference section in the new TerminusDB docs. Not sure where you want to put it, but we need a link for the main navigation.
Could you have a look @GavinMendelGleason",2021-02-17T16:35:16Z,GavinMendelGleason,https://github.com/terminusdb/terminusdb/issues/220#issuecomment-780683737,This one? https://github.com/terminusdb/terminusdb/blob/dev/docs/CLI.md,this one https//githubcom / terminusdb / terminusdb / blob / dev / doc / climd,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,220,2021-02-17T16:08:34Z,luke-feeney,Reference: CLI,https://github.com/terminusdb/terminusdb/issues/220,"We need material for the CLI reference section in the new TerminusDB docs. Not sure where you want to put it, but we need a link for the main navigation.
Could you have a look @GavinMendelGleason",2021-02-17T16:36:21Z,luke-feeney,https://github.com/terminusdb/terminusdb/issues/220#issuecomment-780684454,That's the one - thanks!,that be the one - thank,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,220,2021-02-17T16:08:34Z,luke-feeney,Reference: CLI,https://github.com/terminusdb/terminusdb/issues/220,"We need material for the CLI reference section in the new TerminusDB docs. Not sure where you want to put it, but we need a link for the main navigation.
Could you have a look @GavinMendelGleason",2021-02-17T16:37:36Z,luke-feeney,https://github.com/terminusdb/terminusdb/issues/220#issuecomment-780685259,me working in master and stuff happening in dev makes discovery a little tricky - but I'll withdraw once we get released and then workflow can return to dev first!,-PRON- work in master and stuff happen in dev make discovery a little tricky - but -PRON- will withdraw once -PRON- get release and then workflow can return to dev first,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,221,2021-02-17T16:10:27Z,luke-feeney,Reference: Store,https://github.com/terminusdb/terminusdb/issues/221,"We need to include the reference material for store in the main navigation. Suspect it is best to keep it in the store repo and just link to that from the main navigation.
Is it currently somewhere? Or does it need to be marshaled somehow?
@matko - master of store",2021-02-18T13:15:23Z,matko,https://github.com/terminusdb/terminusdb/issues/221#issuecomment-781335632,The main store reference is up on docs.rs. Would it be ok to just link that?,the main store reference be up on docsr would -PRON- be ok to just link that,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,221,2021-02-17T16:10:27Z,luke-feeney,Reference: Store,https://github.com/terminusdb/terminusdb/issues/221,"We need to include the reference material for store in the main navigation. Suspect it is best to keep it in the store repo and just link to that from the main navigation.
Is it currently somewhere? Or does it need to be marshaled somehow?
@matko - master of store",2021-02-18T13:34:08Z,GavinMendelGleason,https://github.com/terminusdb/terminusdb/issues/221#issuecomment-781347037,Yup,yup,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,221,2021-02-17T16:10:27Z,luke-feeney,Reference: Store,https://github.com/terminusdb/terminusdb/issues/221,"We need to include the reference material for store in the main navigation. Suspect it is best to keep it in the store repo and just link to that from the main navigation.
Is it currently somewhere? Or does it need to be marshaled somehow?
@matko - master of store",2021-02-19T17:05:56Z,luke-feeney,https://github.com/terminusdb/terminusdb/issues/221#issuecomment-782207786,Ok - used that link - was there another store reference thing that was mentioned? Or are we good with the link?,ok - use that link - be there another store reference thing that be mention or be -PRON- good with the link,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,221,2021-02-17T16:10:27Z,luke-feeney,Reference: Store,https://github.com/terminusdb/terminusdb/issues/221,"We need to include the reference material for store in the main navigation. Suspect it is best to keep it in the store repo and just link to that from the main navigation.
Is it currently somewhere? Or does it need to be marshaled somehow?
@matko - master of store",2021-02-25T13:39:55Z,matko,https://github.com/terminusdb/terminusdb/issues/221#issuecomment-785901759,IMO the documentation up on docs.rs is /the/ reference. So I think that is good. Unless you think it is not.,imo the documentation up on docsr be /the/ reference so i think that be good unless -PRON- think -PRON- be not,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,221,2021-02-17T16:10:27Z,luke-feeney,Reference: Store,https://github.com/terminusdb/terminusdb/issues/221,"We need to include the reference material for store in the main navigation. Suspect it is best to keep it in the store repo and just link to that from the main navigation.
Is it currently somewhere? Or does it need to be marshaled somehow?
@matko - master of store",2021-02-25T13:44:52Z,luke-feeney,https://github.com/terminusdb/terminusdb/issues/221#issuecomment-785904999,that's fine for me - have used that link. I just thought that @GavinMendelGleason had mentioned something else! Will close issue.,that be fine for -PRON- - have use that link i just think that @gavinmendelgleason have mention something else will close issue,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,222,2021-02-18T14:07:53Z,rrooij,AppImage and Desktop have fixed ENV variables,https://github.com/terminusdb/terminusdb/issues/222,"Even if the user sets the ENV variables themselves, the AppImage launcher and the Electron app will override those. It should instead check if the ENVs are not set, and in that case, set them to the default ones.",2021-03-15T16:47:09Z,Cheukting,https://github.com/terminusdb/terminusdb/issues/222#issuecomment-799572163,Fixed!,fix,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,223,2021-02-19T16:44:35Z,luke-feeney,Documentation: Python Intro Tutorial,https://github.com/terminusdb/terminusdb/issues/223,"This should be an introductory tutorial and not an installation guide:
https://terminusdb.github.io/terminusdb/#/Intro_Tutorials/Start_With_Python
The idea is to give people who have already installed the DB and the client a chance to try out the tool in an easy tutorial - ingest, query, schema and maybe some additional operation. But a simple getting started thing.
Could you take a look @Cheukting",2021-02-19T17:08:19Z,Cheukting,https://github.com/terminusdb/terminusdb/issues/223#issuecomment-782209196,"TerminusDB Client Python
The terminusdb_client library consists of 4 major components: WOQLClient, WOQLQuery, WOQLViews and WOQLDataFrame (optional, included only if installed with the [dataframe] option). In this guide, we will show the basic usage of each component. For detail documentation of the Python client, please refer to the full documentation. For a full example of using all the code shown below, see the bike example in Jupyter notebook.
WOQLClient
A connection object that allows you to connect with the TerminusDB (local or remote) with basic auth. To create an client object, you will have to provide a URL to the server.
from terminusdb_client import WOQLClient
client = WOQLClient(""https://127.0.0.1:6363/, insecure=True"")
For connection to a local server, we also have to set the insecure flag to True.
The most common way to use the client is to 1) connected to an existing database or 2) create a brand new database.
To connect to an existing database:
client.connect(key=""root"", account=""admin"", user=""admin"", db=""example_db"")
To create a new database:
client.connect(key=""root"", account=""admin"", user=""admin"")
client.create_database(""example_db"")
WOQLQuery
The WOQLQuery object provides a query building tool for WOQL query, instead of JSON-LD -- the native format of WOQL. Most WOQLQuery object methods return a WOQLQuery object that is chainable, one of the exceptions is execute which will send the query built with the client provided.
from terminusdb_client import WOQLQuery as wq
conditions = [wq().triple(""v:Journey"", ""type"", ""scm:Journey""),
              wq().triple(""v:Journey"", ""start_station"", ""v:Start""),
              wq().opt().triple(""v:Start"", ""label"", ""v:Start_Label""),
              wq().triple(""v:Journey"", ""end_station"", ""v:End""),
              wq().opt().triple(""v:End"", ""label"", ""v:End_Label""),
              wq().triple(""v:Journey"", ""journey_bicycle"", ""v:Bike"")]
query = wq().select(""v:Start"", ""v:Start_Label"", ""v:End"",  ""v:End_Label"").woql_and(*conditions)
result = query.execute(client)
WOQLViews
The WOQLViews object provides a tool for visualizing your query result in an interactive graph in Jupyter notebook. You can customize the look of the nodes and the edges.
from terminusdb_client import WOQLView
view = WOQLView()
view.edges([""Start"", ""End""])
view.node(""Start_Label"", ""End_Label"").hidden(True)
view.node(""End"").color([53,105,53]).icon({""color"": [255,0,0], ""unicode"": ""🏁""}).text(""v:End_Label"").size(25).charge(-10)
view.node(""Start"").icon({""color"": [255,0,0], ""label"": True}).text(""v:Start_Label"").size(25).collision_radius(10)
view.edge(""Start"", ""End"").weight(100)
view.show(result)
which will result in something like this
 <iframe src=""https://assets.terminusdb.com/docs/bike-graph.html"" width=""100%"" height=""600px""></iframe>
WOQLDataFrame
WOQLDataFrame is an optional component in terminusdb-client. If you would like to include the installation of the DataFrame module, you need to install it with the [dataframe] option when you do pip install.
With WOQLDataFrame you can easily convert the result from the query, which is in JSON format, into a pandas DataFrame.
from terminusdb_client import WOQLDataFrame
WOQLDataFrame.query_to_df(result)
which will give you something like","terminusdb client python the terminusdb_client library consist of 4 major component woqlclient woqlquery woqlviews and woqldataframe ( optional include only if instal with the [ dataframe ] option ) in this guide -PRON- will show the basic usage of each component for detail documentation of the python client please refer to the full documentation for a full example of use all the code show below see the bike example in jupyter notebook woqlclient a connection object that allow -PRON- to connect with the terminusdb ( local or remote ) with basic auth to create an client object -PRON- will have to provide a url to the server from terminusdb_client import woqlclient client = woqlclient(""https//1270016363/ insecure = true "" ) for connection to a local server -PRON- also have to set the insecure flag to true the most common way to use the client be to 1 ) connect to an exist database or 2 ) create a brand new database to connect to an exist database clientconnect(key=""root "" account=""admin "" user=""admin "" db=""example_db "" ) to create a new database clientconnect(key=""root "" account=""admin "" user=""admin "" ) clientcreate_database(""example_db "" ) woqlquery the woqlquery object provide a query building tool for woql query instead of json - ld -- the native format of woql most woqlquery object method return a woqlquery object that be chainable one of the exception be execute which will send the query build with the client provide from terminusdb_client import woqlquery as wq condition = [ wq()triple(""vjourney "" "" type "" "" scmjourney "" ) wq()triple(""vjourney "" "" start_station "" "" vstart "" ) wq()opt()triple(""vstart "" "" label "" "" vstart_label "" ) wq()triple(""vjourney "" "" end_station "" "" vend "" ) wq()opt()triple(""vend "" "" label "" "" vend_label "" ) wq()triple(""vjourney "" "" journey_bicycle "" "" vbike "" ) ] query = wq()select(""vstart "" "" vstart_label "" "" vend "" "" vend_label"")woql_and(*condition ) result = queryexecute(client ) woqlview the woqlviews object provide a tool for visualize -PRON- query result in an interactive graph in jupyter notebook -PRON- can customize the look of the node and the edge from terminusdb_client import woqlview view = woqlview ( ) viewedges([""start "" "" end "" ] ) viewnode(""start_label "" "" end_label"")hidden(true ) viewnode(""end"")color([5310553])icon({""color "" [ 25500 ] "" unicode "" "" 🏁 "" } ) text(""vend_label"")size(25)charge(-10 ) viewnode(""start"")icon({""color "" [ 25500 ] "" label "" true})text(""vstart_label"")size(25)collision_radius(10 ) viewedge(""start "" "" end"")weight(100 ) viewshow(result ) which will result in something like this < iframe src=""https//assetsterminusdbcom / doc / bike - graphhtml "" width=""100 % "" height=""600px""></iframe > woqldataframe woqldataframe be an optional component in terminusdb - client if -PRON- would like to include the installation of the dataframe module -PRON- need to install -PRON- with the [ dataframe ] option when -PRON- do pip install with woqldataframe -PRON- can easily convert the result from the query which be in json format into a pandas dataframe from terminusdb_client import woqldataframe woqldataframequery_to_df(result ) which will give -PRON- something like",0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,223,2021-02-19T16:44:35Z,luke-feeney,Documentation: Python Intro Tutorial,https://github.com/terminusdb/terminusdb/issues/223,"This should be an introductory tutorial and not an installation guide:
https://terminusdb.github.io/terminusdb/#/Intro_Tutorials/Start_With_Python
The idea is to give people who have already installed the DB and the client a chance to try out the tool in an easy tutorial - ingest, query, schema and maybe some additional operation. But a simple getting started thing.
Could you take a look @Cheukting",2021-02-19T23:20:26Z,Cheukting,https://github.com/terminusdb/terminusdb/issues/223#issuecomment-782443820,"Please try if the iframe works after putting it in place, if not let me know and I will replace it with a static png",please try if the iframe work after put -PRON- in place if not let -PRON- know and i will replace -PRON- with a static png,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,223,2021-02-19T16:44:35Z,luke-feeney,Documentation: Python Intro Tutorial,https://github.com/terminusdb/terminusdb/issues/223,"This should be an introductory tutorial and not an installation guide:
https://terminusdb.github.io/terminusdb/#/Intro_Tutorials/Start_With_Python
The idea is to give people who have already installed the DB and the client a chance to try out the tool in an easy tutorial - ingest, query, schema and maybe some additional operation. But a simple getting started thing.
Could you take a look @Cheukting",2021-02-25T14:06:10Z,luke-feeney,https://github.com/terminusdb/terminusdb/issues/223#issuecomment-785919664,Looks good iframe seems to work,look good iframe seem to work,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,224,2021-02-25T10:28:21Z,Francesca-Bit,Console - collaborator interface - Remove collaborator ,https://github.com/terminusdb/terminusdb/issues/224,add a new functionality to remove individual collaborators from the databases.,,,,,,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,225,2021-02-25T10:35:01Z,luke-feeney,Explanation: TerminusDB Vocab,https://github.com/terminusdb/terminusdb/issues/225,"This is a short guide to all the TerminusDB vocab - each concept with a short description and example under the word. Think this could be really useful for users - and we get these questions a lot.
@Francesca-Bit suggested these as a start:
Triple......
Object: ....
Abstract.....
Document......

Please add more to this thread! We can build as we develop.
Place for the text:
https://github.com/terminusdb/terminusdb/blob/master/docs/Explanation/VOCABULARY.md
@Francesca-Bit will you get the ball rolling.",,,,,,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,226,2021-02-25T12:35:57Z,Francesca-Bit,Database - problem with  xsd:date type,https://github.com/terminusdb/terminusdb/issues/226,"Describe the bug
I get an error when I try to add a xsd:date type property
To Reproduce
Steps to reproduce the behavior:
1  Create a Document Person with a xsd:date Property
2. Go to console - query panel and run the query below
WOQL.and(
  WOQL.add_triple(""doc:elena"", ""type"", ""scm:Person""),
  WOQL.add_triple(""doc:elena"", ""label"", ""Elena""),
  WOQL.add_triple(""doc:elena"", ""lifespan_start"", WOQL.literal(""2012-01-01"",""xsd:date""))
  )


See error :

Syntax error foundless
{
  ""data"": {
    ""api:message"": ""Error: woql_syntax_error(_7326{'@type':'http://terminusdb.com/schema/woql#Datatype','http://terminusdb.com/schema/woql#datatype':_7338{'@type':'http://www.w3.org/2001/XMLSchema#date','@value':\""2012-01-01\""}},['http://terminusdb.com/schema/woql#object','http://terminusdb.com/schema/woql#query','http://terminusdb.com/schema/woql#query_list'],_7326{'@type':'http://terminusdb.com/schema/woql#Datatype','http://terminusdb.com/schema/woql#datatype':_7338{'@type':'http://www.w3.org/2001/XMLSchema#date','@value':\""2012-01-01\""}})\n  [38] throw(error(woql_syntax_error(...,...,...),_7414))\n  [36] ......",2021-03-12T12:11:14Z,GavinMendelGleason,https://github.com/terminusdb/terminusdb/issues/226#issuecomment-797452013,Fixed in e28b1f4,fix in e28b1f4,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,227,2021-02-25T15:26:08Z,KittyJose,How to import a schema ,https://github.com/terminusdb/terminusdb/issues/227,"How to import schema using insert_triples
also good idea to include how to copy schema from another db, the below codes erroring out for me
using(""admin/peas/graph/schema/main"").triple(""v:A"", ""v:B"", ""v:C"").using(""admin/blah/graph/schema/main"").update_triple(""v:A"", ""v:B"", ""v:C"")",2021-02-26T12:00:27Z,KittyJose,https://github.com/terminusdb/terminusdb/issues/227#issuecomment-786605009,Please refer https://terminusdb.github.io/terminusdb/#/How_To/import_schema,please refer https//terminusdbgithubio / terminusdb/#/how_to / import_schema,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,228,2021-02-26T11:21:53Z,luke-feeney,Reference: White List of XSD datatypes ,https://github.com/terminusdb/terminusdb/issues/228,"A list of all the XSD datatypes that work in TerminusDB
We will add useful additions as we go:
https://github.com/terminusdb/terminusdb/tree/master/docs/reference",2021-02-26T11:22:12Z,luke-feeney,https://github.com/terminusdb/terminusdb/issues/228#issuecomment-786587364,"Sorry in here:
https://github.com/terminusdb/terminusdb/blob/master/docs/reference/XSD_WHITELIST.md",sorry in here https//githubcom / terminusdb / terminusdb / blob / master / doc / reference / xsd_whitelistmd,1,1
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,228,2021-02-26T11:21:53Z,luke-feeney,Reference: White List of XSD datatypes ,https://github.com/terminusdb/terminusdb/issues/228,"A list of all the XSD datatypes that work in TerminusDB
We will add useful additions as we go:
https://github.com/terminusdb/terminusdb/tree/master/docs/reference",2021-03-18T01:07:13Z,sachinbhutani,https://github.com/terminusdb/terminusdb/issues/228#issuecomment-801540825,Does uuid deserve its own data type instead of string?,do uuid deserve -PRON- own data type instead of string,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,229,2021-02-26T11:55:38Z,Francesca-Bit,Console - model builder - problem with enum element and property cardinality,https://github.com/terminusdb/terminusdb/issues/229,"** Describe the bug **
removing the cardinality or enum element using the model builder creates problems in the database schema
** To play **
Steps to reproduce the behavior:

Go to Console Model Builder
Add a new enum element
Save
Delete the enum element
see in the Owl diagram in triples, there are elements not removed well
//same for cardinality",2021-03-02T15:13:20Z,Francesca-Bit,https://github.com/terminusdb/terminusdb/issues/229#issuecomment-788980707,fix - dev - terminusdb-react-components,fix - dev - terminusdb - react - component,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,230,2021-02-26T12:35:54Z,luke-feeney,Windows Desktop Console Out of Place Element in GUI,https://github.com/terminusdb/terminusdb/issues/230,"Describe the bug
On the clone screen the search for publisher id box is over the recommendations element when the interface is not at maximum size.
To Reproduce
download windows version
click restore down button on window
view overlap
Expected behavior
that elements do not overlap
Screenshots
If applicable, add screenshots to help explain your problem.

Info (please complete the following information):
Windows",2021-03-02T15:22:47Z,KittyJose,https://github.com/terminusdb/terminusdb/issues/230#issuecomment-788987841,Overlapping issue fixed in dev,overlap issue fix in dev,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,231,2021-02-26T12:54:34Z,GavinMendelGleason,Timestamp offset parsing is sometimes wrong,https://github.com/terminusdb/terminusdb/issues/231,"Describe the bug
Sometimes offset parsing of timestamps in xsd:dateTime and xsd:dateTimeStamp are not parsed with the appropriate offsets for time zones.
To Reproduce
Try to insert, and extract the following date:
{ ""@value"": ""2004-04-12T13:20:00-05:00"",
   ""@type"": ""xsd:dateTimeStamp""}
Expected behavior
Result type after a round-trip should be isomorphic (if not written canonically)
Info (please complete the following information):

TerminusDB 4.2",2021-03-12T12:09:12Z,GavinMendelGleason,https://github.com/terminusdb/terminusdb/issues/231#issuecomment-797450944,Fixed in e28b1f4,fix in e28b1f4,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,232,2021-02-26T16:14:42Z,MrRaymondLee,What's New Popup,https://github.com/terminusdb/terminusdb/issues/232,To have a What's New button or option so people are aware of new features to try or changes that might impact their work.,,,,,,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,233,2021-02-26T16:17:56Z,MrRaymondLee,Displaying the Document ID when viewing a document,https://github.com/terminusdb/terminusdb/issues/233,"The table of Documents doesn't allow clipboarding of Document ID.  When I click to view the object, it would be nice to see the Document ID in the table instead of getting it from the URL.",2021-03-02T15:34:13Z,KittyJose,https://github.com/terminusdb/terminusdb/issues/233#issuecomment-788996135,"Hi MrRaymondLee, We may add a clipboard feature in to the table in future. In the meanwhile instead of getting the Document ID from the URL when you click to view the object the ID appears just below the Console Tab along with the type.

Likewise you can even copy paste while in json view",hi mrraymondlee -PRON- may add a clipboard feature in to the table in future in the meanwhile instead of get the document -PRON- d from the url when -PRON- click to view the object the -PRON- d appear just below the console tab along with the type likewise -PRON- can even copy paste while in json view,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,233,2021-02-26T16:17:56Z,MrRaymondLee,Displaying the Document ID when viewing a document,https://github.com/terminusdb/terminusdb/issues/233,"The table of Documents doesn't allow clipboarding of Document ID.  When I click to view the object, it would be nice to see the Document ID in the table instead of getting it from the URL.",2021-03-16T10:47:54Z,KittyJose,https://github.com/terminusdb/terminusdb/issues/233#issuecomment-800153518,"Hi MrRaymondLee, So I have added a copy button to the doc view, its in dev as of now, after some testing will release it out yea. Closing this ticket as of now.",hi mrraymondlee so i have add a copy button to the doc view -PRON- in dev as of now after some testing will release -PRON- out yea close this ticket as of now,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,234,2021-02-26T16:38:08Z,MrRaymondLee,Warn on delete,https://github.com/terminusdb/terminusdb/issues/234,"When deleting instances, it should warn if there is established relationship.  For example, if I have a Team and Member and within Member I have a link to Team.   If I delete the team instance, it should warn me that there is instances linked to that team.  The API clients too should throw an error and have a force boolean argument to delete the instance.",,,,,,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,235,2021-02-26T16:52:09Z,MrRaymondLee,Sticky layout,https://github.com/terminusdb/terminusdb/issues/235,"When saving a schema using the schema builder, it would be nice to allow the saving of nodes.  In complex schemas, it may be desirable for users to find their documents and objects if they could be positioned in a certain area of the screen to group them and those positions be persisted next time they view the schema.",,,,,,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,238,2021-03-01T14:00:58Z,luke-feeney,Do we need a 'contribution guide' specifically for the JS client?,https://github.com/terminusdb/terminusdb/issues/238,"We have a general guide (for server), a python guide and a store guide, so we probably need a guide for people that want to contribute to the JS client.",2021-07-16T12:42:46Z,yashasvimisra2798,https://github.com/terminusdb/terminusdb/issues/238#issuecomment-881419059,"Hey, @luke-feeney I would like to work on this issue. Can you elaborate more on what shall I add to the guide?",hey @luke - feeney i would like to work on this issue can -PRON- elaborate more on what shall i add to the guide,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,239,2021-03-02T16:48:31Z,rrooij,API: WOQL Queries not working when application/json charset=utf-8 is set as content-type,https://github.com/terminusdb/terminusdb/issues/239,"Describe the bug
The API works fine when you set Content-Type: application/json but not Content-Type: application/json; charset=utf-8 which is used by a lot of HTTP clients. It will give a bad request error.
To Reproduce
In this case, replace test with some other database.
curl -k -XPOST -H 'Content-Type: application/json; charset=utf-8' -u admin:root 'https://127.0.0.1:6363/api/woql/admin/test/local/branch/main' -d '{""query"":{""woql:query"":{""woql:predicate"":{""woql:node"":""rdf:type"",""@type"":""woql:Node""},""woql:object"":{""woql:node"":""system:Database"",""@type"":""woql:Node""},""@type"":""woql:Triple"",""woql:subject"":{""woql:variable_name"":{""@value"":""X"",""@type"":""xsd:string""},""@type"":""woql:Variable""}},""@type"":""woql:Using"",""woql:collection"":{""@value"":""_system"",""@type"":""xsd:string""}}}'

vs
curl -k -XPOST -H 'Content-Type: application/json' -u admin:root 'https://127.0.0.1:6363/api/woql/admin/test/local/branch/main' -d '{""query"":{""woql:query"":{""woql:predicate"":{""woql:node"":""rdf:type"",""@type"":""woql:Node""},""woql:object"":{""woql:node"":""system:Database"",""@type"":""woql:Node""},""@type"":""woql:Triple"",""woql:subject"":{""woql:variable_name"":{""@value"":""X"",""@type"":""xsd:string""},""@type"":""woql:Variable""}},""@type"":""woql:Using"",""woql:collection"":{""@value"":""_system"",""@type"":""xsd:string""}}}'

Info

OS: Debian Testing
Manual compiled master version
SWIPL 8.3",2021-03-12T13:15:43Z,GavinMendelGleason,https://github.com/terminusdb/terminusdb/issues/239#issuecomment-797483118,Fixed in 1d71133,fix in 1d71133,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,240,2021-03-05T16:12:25Z,matko,Better error reporting from server on woql_syntax_error,https://github.com/terminusdb/terminusdb/issues/240,"Not all errors from terminusdb are clear. This is especially the case when an error is thrown from prolog or rust, which is then not handled. We need to improve this.",2021-08-23T14:00:15Z,spl,https://github.com/terminusdb/terminusdb/issues/240#issuecomment-903798704,We'll open up new issues for specific instances of this.,-PRON- will open up new issue for specific instance of this,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,241,2021-03-06T04:19:28Z,yu-eric,README redirects to dead documentation link,https://github.com/terminusdb/terminusdb/issues/241,"Describe the typo/ misinformation
The ""Documentation"" section has a link that redirects to https://terminusdb.com/documentation/ which returns 404 NOT FOUND
Expected changes
Change this link to https://terminusdb.com/docs/",2021-03-06T10:05:00Z,rrooij,https://github.com/terminusdb/terminusdb/issues/241#issuecomment-791906188,Good catch! It should indeed be updated with the new documentation URLs.,good catch -PRON- should indeed be update with the new documentation urls,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,241,2021-03-06T04:19:28Z,yu-eric,README redirects to dead documentation link,https://github.com/terminusdb/terminusdb/issues/241,"Describe the typo/ misinformation
The ""Documentation"" section has a link that redirects to https://terminusdb.com/documentation/ which returns 404 NOT FOUND
Expected changes
Change this link to https://terminusdb.com/docs/",2021-03-06T10:09:09Z,rrooij,https://github.com/terminusdb/terminusdb/issues/241#issuecomment-791906617,Fixed in 2de02d9,fix in 2de02d9,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,242,2021-03-08T01:09:15Z,jbennettgit,xsd:gYear and xdd:gYearRange quietly fail to cast.,https://github.com/terminusdb/terminusdb/issues/242,"Ubuntu 18.04 running bootstrap version 4.1.2 in docker container, via python client:
The following queries:
WOQLQuery().cast('0123','xsd:gYear','v:Value').execute(client)
WOQLQuery().cast('0123','xdd:gYearRange','v:Value').execute(client)
WOQLQuery().cast('[0123,0456]','xdd:gYearRange','v:Value').execute(client)
etc.
All return the following.  No API error, no complaints:
{'@type': 'api:WoqlResponse', 'api:status': 'api:success', 'api:variable_names': ['Value'], 'bindings': [], 'deletes': 0, 'inserts': 0, 'transaction_retry_count': 0}",2021-03-09T15:06:10Z,GavinMendelGleason,https://github.com/terminusdb/terminusdb/issues/242#issuecomment-794018626,"gYear, gYearRange, and other date objects which are not dateTime are currently not working in 4.2. We have a whitelist of xsd types which are known to work at: https://github.com/terminusdb/terminusdb/blob/master/docs/reference/XSD_WHITELIST.md
I realise that these are pretty important for Seshat and since they worked in previous iterations they were pretty pervasive, so I'll make sure to get them working again, hopefully in the next patch to 4.2.",gyear gyearrange and other date object which be not datetime be currently not work in 42 -PRON- have a whitelist of xsd type which be know to work at https//githubcom / terminusdb / terminusdb / blob / master / doc / reference / xsd_whitelistmd i realise that these be pretty important for seshat and since -PRON- work in previous iteration -PRON- be pretty pervasive so -PRON- will make sure to get -PRON- work again hopefully in the next patch to 42,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,242,2021-03-08T01:09:15Z,jbennettgit,xsd:gYear and xdd:gYearRange quietly fail to cast.,https://github.com/terminusdb/terminusdb/issues/242,"Ubuntu 18.04 running bootstrap version 4.1.2 in docker container, via python client:
The following queries:
WOQLQuery().cast('0123','xsd:gYear','v:Value').execute(client)
WOQLQuery().cast('0123','xdd:gYearRange','v:Value').execute(client)
WOQLQuery().cast('[0123,0456]','xdd:gYearRange','v:Value').execute(client)
etc.
All return the following.  No API error, no complaints:
{'@type': 'api:WoqlResponse', 'api:status': 'api:success', 'api:variable_names': ['Value'], 'bindings': [], 'deletes': 0, 'inserts': 0, 'transaction_retry_count': 0}",2021-03-12T11:54:35Z,GavinMendelGleason,https://github.com/terminusdb/terminusdb/issues/242#issuecomment-797443812,Fixed in e28b1f4,fix in e28b1f4,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,243,2021-03-08T01:12:24Z,jbennettgit,xsd:nonNegativeInteger permits negative integers on cast,https://github.com/terminusdb/terminusdb/issues/243,"Ubuntu 18.04 running bootstrap version 4.1.2 in docker container, via python client:
WOQLQuery().cast('-1','xsd:nonNegativeInteger','v:Value').execute(client)
returns:
{'@type': 'api:WoqlResponse', 'api:status': 'api:success', 'api:variable_names': ['Value'], 'bindings': [{'Value': {'@type': 'http://www.w3.org/2001/XMLSchema#nonNegativeInteger', '@value': -1}}], 'deletes': 0, 'inserts': 0, 'transaction_retry_count': 0}
Expected an error of some sort.",2021-03-09T15:02:55Z,GavinMendelGleason,https://github.com/terminusdb/terminusdb/issues/243#issuecomment-794014608,That is indeed silly - fixing for a patch now.,that be indeed silly - fix for a patch now,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,243,2021-03-08T01:12:24Z,jbennettgit,xsd:nonNegativeInteger permits negative integers on cast,https://github.com/terminusdb/terminusdb/issues/243,"Ubuntu 18.04 running bootstrap version 4.1.2 in docker container, via python client:
WOQLQuery().cast('-1','xsd:nonNegativeInteger','v:Value').execute(client)
returns:
{'@type': 'api:WoqlResponse', 'api:status': 'api:success', 'api:variable_names': ['Value'], 'bindings': [{'Value': {'@type': 'http://www.w3.org/2001/XMLSchema#nonNegativeInteger', '@value': -1}}], 'deletes': 0, 'inserts': 0, 'transaction_retry_count': 0}
Expected an error of some sort.",2021-03-12T11:54:51Z,GavinMendelGleason,https://github.com/terminusdb/terminusdb/issues/243#issuecomment-797443929,Fixed in e28b1f4,fix in e28b1f4,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,245,2021-03-08T01:24:06Z,jbennettgit,Casting to unknown types does not generate expected api:BadCast error,https://github.com/terminusdb/terminusdb/issues/245,"Ubuntu 18.04 running bootstrap version 4.1.2 in docker container, via python client:


WOQLQuery().cast('0123','unknown_type','v:Value').execute(client)
yields:
{'@type': 'api:WoqlResponse', 'api:status': 'api:success', 'api:variable_names': ['Value'], 'bindings': [], 'deletes': 0, 'inserts': 0, 'transaction_retry_count': 0}


I expected a complaint.",2021-03-09T15:03:18Z,GavinMendelGleason,https://github.com/terminusdb/terminusdb/issues/245#issuecomment-794015092,Adding a patch which will throw an error,add a patch which will throw an error,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,245,2021-03-08T01:24:06Z,jbennettgit,Casting to unknown types does not generate expected api:BadCast error,https://github.com/terminusdb/terminusdb/issues/245,"Ubuntu 18.04 running bootstrap version 4.1.2 in docker container, via python client:


WOQLQuery().cast('0123','unknown_type','v:Value').execute(client)
yields:
{'@type': 'api:WoqlResponse', 'api:status': 'api:success', 'api:variable_names': ['Value'], 'bindings': [], 'deletes': 0, 'inserts': 0, 'transaction_retry_count': 0}


I expected a complaint.",2021-03-12T11:53:53Z,GavinMendelGleason,https://github.com/terminusdb/terminusdb/issues/245#issuecomment-797443452,Fixed in e28b1f4,fix in e28b1f4,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,248,2021-03-08T14:54:21Z,Cheukting,Add commit ID to API responses,https://github.com/terminusdb/terminusdb/issues/248,Server giving commit id in the returned JSON when there are updates,2021-09-08T02:13:17Z,spl,https://github.com/terminusdb/terminusdb/issues/248#issuecomment-914830631,"I've updated the title to reflect that we think it would be good to include the commit ID in any API response involving a query or update of the data in the database. (I'm not sure if that is all API endpoints.) The idea is that a client can see the source commit for the response and can act on that, if desired.",-PRON- have update the title to reflect that -PRON- think -PRON- would be good to include the commit -PRON- d in any api response involve a query or update of the datum in the database ( -PRON- be not sure if that be all api endpoint ) the idea be that a client can see the source commit for the response and can act on that if desire,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,248,2021-03-08T14:54:21Z,Cheukting,Add commit ID to API responses,https://github.com/terminusdb/terminusdb/issues/248,Server giving commit id in the returned JSON when there are updates,2021-09-08T05:17:54Z,GavinMendelGleason,https://github.com/terminusdb/terminusdb/issues/248#issuecomment-914925166,Presumably we intend to return the commit id in the response headers so as not to break API?,presumably -PRON- intend to return the commit -PRON- d in the response header so as not to break api,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,248,2021-03-08T14:54:21Z,Cheukting,Add commit ID to API responses,https://github.com/terminusdb/terminusdb/issues/248,Server giving commit id in the returned JSON when there are updates,2021-09-08T05:59:42Z,spl,https://github.com/terminusdb/terminusdb/issues/248#issuecomment-914941513,"Presumably we intend to return the commit id in the response headers so as not to break API?

That's a good idea.",presumably -PRON- intend to return the commit -PRON- d in the response header so as not to break api that be a good idea,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,248,2021-03-08T14:54:21Z,Cheukting,Add commit ID to API responses,https://github.com/terminusdb/terminusdb/issues/248,Server giving commit id in the returned JSON when there are updates,2021-09-08T06:22:37Z,GavinMendelGleason,https://github.com/terminusdb/terminusdb/issues/248#issuecomment-914953370,"For WOQL query responses it probably will not matter, but for the document interface I don't know where else we would put it...",for woql query response -PRON- probably will not matter but for the document interface i do not know where else -PRON- would put -PRON-,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,249,2021-03-08T15:20:09Z,matko,Casting rewrite,https://github.com/terminusdb/terminusdb/issues/249,"Various bugs and issues related to casting will be resolved by refactoring how casting is done.
This concerns #79, #242, #243, #244, #245, #246, #247.",2021-03-12T11:53:20Z,GavinMendelGleason,https://github.com/terminusdb/terminusdb/issues/249#issuecomment-797443202,Completed in e28b1f4,complete in e28b1f4,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,250,2021-03-08T15:25:42Z,Cheukting,Automatically add cards in projects,https://github.com/terminusdb/terminusdb/issues/250,Automatically add cards in projects,2021-03-08T15:28:18Z,Cheukting,https://github.com/terminusdb/terminusdb/issues/250#issuecomment-792831769,ok it's done,ok -PRON- be do,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,251,2021-03-08T22:24:30Z,Cheukting,Adding triage label automatically,https://github.com/terminusdb/terminusdb/issues/251,Adding triage label automatically,,,,,,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,252,2021-03-09T10:17:42Z,GavinMendelGleason,"Casting to xsd:anyURI, casts to xdd:url",https://github.com/terminusdb/terminusdb/issues/252,"Describe the bug
typecast('http://vocab.getty.edu/aat/300011908', 'http://www.w3.org/2001/XMLSchema#anyURI', [], X).
X = 'http://vocab.getty.edu/aat/300011908'^^'http://terminusdb.com/schema/xdd#url'.
Info (please complete the following information):

TerminusDB 4.0",2021-03-12T11:52:48Z,GavinMendelGleason,https://github.com/terminusdb/terminusdb/issues/252#issuecomment-797442957,Fixed in e28b1f4,fix in e28b1f4,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,253,2021-03-09T21:13:04Z,jbennettgit,Performance degradation during batched inserts,https://github.com/terminusdb/terminusdb/issues/253,"Ubuntu 18.04 running bootstrap version 4.1.2 in docker container, using python client.  Time to insert a polity's data from a test version of a seshat csv increases linearly even if the number of inserts (batched by polity) stays roughly the same.  Attempts to squash() after each execute() commit only increased the incremental and total time.
Separately attempts to view the final, committed db in the TerminusDB console hangs with a spinning icon.
To reproduce:

Pull our seshat test code using git clone https://github.com/MajidBenam/seshat-3store.git
cd seshat-3store
cp -f equinox_full.csv equinox.csv
python3 define_seshat_schema.py
python3 insert_from_csv.py

Step 3 ensures you use the full 47K line csv file, not the little test equinox.csv.
Step 4 should take around 7-8s to define the schema.
During step 5 the initial set of commits, which batch insertions by polity, show ~450 inserts happening in ~1s.  But after 20 or so polity commits things really start to slow and by the end each polity takes ~10s to commit.  We expect roughly constant time per insert.

If you re-run step 4, it will delete the db and rebuild it (and the schema) for easy testing.
There will be various complaints and WARNINGs during step 5 which you can ignore.
If you run step 5 once and then run step 5 immediately again (re-asserting the same data so causing deletes as well as inserts) quite often there is an initial 'hang' of a few 10s of seconds at the beginning during the commit of first true polity.

Partial example output from step 5 enclosed below.  Each 'Committing' line reports the number of inserts (i:), deletes (d:) and elapsed time for the execute() call.
Parsing data from equinox.csv
Committing data for Code book i:6 d:0 0.15s
WARNING: Pre-casting '1790000:490000' from AfDurrn|Social Complexity variables|Social Scale|Polity territory to a range: likely ill-formed!
Committing data for AfDurrn i:453 d:0 0.92s
WARNING: Pre-casting '60000:80000' from AfGhurd|Social Complexity variables|Social Scale|Polity territory to a range: likely ill-formed!
WARNING: Pre-casting '900000:1100000' from AfGhurd|Social Complexity variables|Social Scale|Polity territory to a range: likely ill-formed!
WARNING: Pre-casting '1600000:1700000' from AfGhurd|Social Complexity variables|Social Scale|Polity territory to a range: likely ill-formed!
Committing data for AfGhurd i:485 d:0 0.92s
WARNING: Pre-casting '600000:800000' from AfGrBct|Social Complexity variables|Social Scale|Polity territory to a range: likely ill-formed!
WARNING: Pre-casting '1500000:2000000' from AfGrBct|Social Complexity variables|Social Scale|Polity Population to a range: likely ill-formed!
WARNING: Pre-casting '25000:50000' from AfGrBct|Social Complexity variables|Social Scale|Population of the largest settlement to a range: likely ill-formed!
Committing data for AfGrBct i:506 d:0 1.00s
WARNING: Pre-casting '2750000:3000000' from AfHepht|Social Complexity variables|Social Scale|Polity territory to a range: likely ill-formed!
Committing data for AfHepht i:568 d:0 1.11s
....
Committing data for YeQasmi i:414 d:0 13.88s
WARNING: Pre-casting '30000:40000' from YeQatab|Social Complexity variables|Social Scale|Population of the largest settlement to a range: likely ill-formed!
Committing data for YeQatab i:300 d:0 9.37s
WARNING: Unable to pre-cast 'suspected unknown' from YeRasul|Warfare variables|Military Technologies|Long walls to integer; assuming 0
Committing data for YeRasul i:441 d:0 12.71s
Committing data for YeSaRay i:222 d:0 7.17s
WARNING: Pre-casting '30000:40000' from YeSabaC|Social Complexity variables|Social Scale|Population of the largest settlement to a range: likely ill-formed!
Committing data for YeSabaC i:279 d:0 9.17s
WARNING: Unable to pre-cast 'suspected unknown' from YeTahir|Social Complexity variables|Social Scale|Polity territory to decimal; assuming 0.0
WARNING: Unable to pre-cast 'suspected unknown' from YeTahir|Social Complexity variables|Social Scale|Polity Population to decimal; assuming 0.0
WARNING: Unable to pre-cast 'suspected unknown' from YeTahir|Social Complexity variables|Social Scale|Population of the largest settlement to decimal; assuming 0.0
WARNING: Unable to pre-cast 'suspected unknown' from YeTahir|Warfare variables|Military Technologies|Long walls to integer; assuming 0
Committing data for YeTahir i:435 d:0 13.29s
WARNING: Pre-casting '250000:350000' from YeWarLd|Social Complexity variables|Social Scale|Polity territory to a range: likely ill-formed!
WARNING: Unable to pre-cast 'suspected unknown' from YeWarLd|Warfare variables|Military Technologies|Long walls to integer; assuming 0
Committing data for YeWarLd i:456 d:0 12.78s
WARNING: Unable to pre-cast 'suspected unknown' from YeZiyad|Warfare variables|Military Technologies|Long walls to integer; assuming 0
Committing data for YeZiyad i:402 d:0 11.15s
Total assertions: 374 requiring inserts: 169681 deletes: 0
Total failed commits: 0
Execution time: 2499.8s",2021-03-10T08:21:32Z,rrooij,https://github.com/terminusdb/terminusdb/issues/253#issuecomment-795071193,"I see that you are trying to insert on each row. It is better to do a bulk insert with: post, remote or file. post will send the CSV to the server, remote fetches it from a remote source and file will search for a local file on the same system that TerminusDB is running on.
An example can be found here: https://github.com/terminusdb/terminusdb-tutorials/blob/master/bike-tutorial/python/bike-tutorial.py#L49",i see that -PRON- be try to insert on each row -PRON- be well to do a bulk insert with post remote or file post will send the csv to the server remote fetch -PRON- from a remote source and file will search for a local file on the same system that terminusdb be run on an example can be find here https//githubcom / terminusdb / terminusdb - tutorial / blob / master / bike - tutorial / python / bike - tutorialpy#l49,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,253,2021-03-09T21:13:04Z,jbennettgit,Performance degradation during batched inserts,https://github.com/terminusdb/terminusdb/issues/253,"Ubuntu 18.04 running bootstrap version 4.1.2 in docker container, using python client.  Time to insert a polity's data from a test version of a seshat csv increases linearly even if the number of inserts (batched by polity) stays roughly the same.  Attempts to squash() after each execute() commit only increased the incremental and total time.
Separately attempts to view the final, committed db in the TerminusDB console hangs with a spinning icon.
To reproduce:

Pull our seshat test code using git clone https://github.com/MajidBenam/seshat-3store.git
cd seshat-3store
cp -f equinox_full.csv equinox.csv
python3 define_seshat_schema.py
python3 insert_from_csv.py

Step 3 ensures you use the full 47K line csv file, not the little test equinox.csv.
Step 4 should take around 7-8s to define the schema.
During step 5 the initial set of commits, which batch insertions by polity, show ~450 inserts happening in ~1s.  But after 20 or so polity commits things really start to slow and by the end each polity takes ~10s to commit.  We expect roughly constant time per insert.

If you re-run step 4, it will delete the db and rebuild it (and the schema) for easy testing.
There will be various complaints and WARNINGs during step 5 which you can ignore.
If you run step 5 once and then run step 5 immediately again (re-asserting the same data so causing deletes as well as inserts) quite often there is an initial 'hang' of a few 10s of seconds at the beginning during the commit of first true polity.

Partial example output from step 5 enclosed below.  Each 'Committing' line reports the number of inserts (i:), deletes (d:) and elapsed time for the execute() call.
Parsing data from equinox.csv
Committing data for Code book i:6 d:0 0.15s
WARNING: Pre-casting '1790000:490000' from AfDurrn|Social Complexity variables|Social Scale|Polity territory to a range: likely ill-formed!
Committing data for AfDurrn i:453 d:0 0.92s
WARNING: Pre-casting '60000:80000' from AfGhurd|Social Complexity variables|Social Scale|Polity territory to a range: likely ill-formed!
WARNING: Pre-casting '900000:1100000' from AfGhurd|Social Complexity variables|Social Scale|Polity territory to a range: likely ill-formed!
WARNING: Pre-casting '1600000:1700000' from AfGhurd|Social Complexity variables|Social Scale|Polity territory to a range: likely ill-formed!
Committing data for AfGhurd i:485 d:0 0.92s
WARNING: Pre-casting '600000:800000' from AfGrBct|Social Complexity variables|Social Scale|Polity territory to a range: likely ill-formed!
WARNING: Pre-casting '1500000:2000000' from AfGrBct|Social Complexity variables|Social Scale|Polity Population to a range: likely ill-formed!
WARNING: Pre-casting '25000:50000' from AfGrBct|Social Complexity variables|Social Scale|Population of the largest settlement to a range: likely ill-formed!
Committing data for AfGrBct i:506 d:0 1.00s
WARNING: Pre-casting '2750000:3000000' from AfHepht|Social Complexity variables|Social Scale|Polity territory to a range: likely ill-formed!
Committing data for AfHepht i:568 d:0 1.11s
....
Committing data for YeQasmi i:414 d:0 13.88s
WARNING: Pre-casting '30000:40000' from YeQatab|Social Complexity variables|Social Scale|Population of the largest settlement to a range: likely ill-formed!
Committing data for YeQatab i:300 d:0 9.37s
WARNING: Unable to pre-cast 'suspected unknown' from YeRasul|Warfare variables|Military Technologies|Long walls to integer; assuming 0
Committing data for YeRasul i:441 d:0 12.71s
Committing data for YeSaRay i:222 d:0 7.17s
WARNING: Pre-casting '30000:40000' from YeSabaC|Social Complexity variables|Social Scale|Population of the largest settlement to a range: likely ill-formed!
Committing data for YeSabaC i:279 d:0 9.17s
WARNING: Unable to pre-cast 'suspected unknown' from YeTahir|Social Complexity variables|Social Scale|Polity territory to decimal; assuming 0.0
WARNING: Unable to pre-cast 'suspected unknown' from YeTahir|Social Complexity variables|Social Scale|Polity Population to decimal; assuming 0.0
WARNING: Unable to pre-cast 'suspected unknown' from YeTahir|Social Complexity variables|Social Scale|Population of the largest settlement to decimal; assuming 0.0
WARNING: Unable to pre-cast 'suspected unknown' from YeTahir|Warfare variables|Military Technologies|Long walls to integer; assuming 0
Committing data for YeTahir i:435 d:0 13.29s
WARNING: Pre-casting '250000:350000' from YeWarLd|Social Complexity variables|Social Scale|Polity territory to a range: likely ill-formed!
WARNING: Unable to pre-cast 'suspected unknown' from YeWarLd|Warfare variables|Military Technologies|Long walls to integer; assuming 0
Committing data for YeWarLd i:456 d:0 12.78s
WARNING: Unable to pre-cast 'suspected unknown' from YeZiyad|Warfare variables|Military Technologies|Long walls to integer; assuming 0
Committing data for YeZiyad i:402 d:0 11.15s
Total assertions: 374 requiring inserts: 169681 deletes: 0
Total failed commits: 0
Execution time: 2499.8s",2021-03-10T19:19:09Z,jbennettgit,https://github.com/terminusdb/terminusdb/issues/253#issuecomment-795943306,"I am familiar with that example.  However while the data source is a spreadsheet the data structures being asserted are not simple (as Kevin and Gavin know) and different numbers of inserts (and deletes) are made depending on the information on each row.  Further the values of many properties are class instances themselves and have to be managed.  So we cannot use the csv support that comes native from TerminusDB (and eventually we'll be using a different front end source for the data in any case).
An original version tried to make a set of inserts and deletes per spreadsheet line but that was really slow (it would have taken over 2 days for the full spreadsheet).  This version attempts to batch all the inserts and deletes according to each 'polity' which is a natural unit for our application. This reduced the time for the full spreadsheet to <1hr to make nearly 170K inserts over 374 polities. If each polity commit took around 1s (as it does at the start) then I expect the whole operation to take around 6m.
I suspect, with no evidence, that the problem is the following.  In our application we will be asserting  possibly several triples for a property on a polity.  When we are updating that property with its new values we need to ensure we delete the old triples first (so we don't have residual, possibly incorrect data left over).  So we do something like (assume existing polity 'AfDurn'):
triple('v:Polity_ID','original_Name','Afdurn').
triple('v:Polity_ID','Polity territory','v:Old_Values_1').
delete_triple('v:Polity_ID','Polity territory','v:Old_Values_1').
delete_object('v:Old_Values_1').
...code to insert new Polity territory triples on v:Polity_ID..
The 2nd triple() finds the previous Polity territory values, the delete_triple() gets rid of them, and the delete_object() gets rid of the now disconnected _Values instances that held on to the old values (so we don't clutter the DB and don't inadvertently have naming collisions with otherwise anonymous Value instances).  I think what is happening is that as more triples are added for all the other polities the 2nd triple might just take longer and longer to do the match (unless there is an index on Polity_ID).  We do this idiom once for each property on the polity we are updating (~20 properties or so per execute, each with their own v:Old_Values_n variable).
If there is a better way, let us know. thanks.",i be familiar with that example however while the data source be a spreadsheet the data structure be assert be not simple ( as kevin and gavin know ) and different number of insert ( and delete ) be make depend on the information on each row far the value of many property be class instance -PRON- and have to be manage so -PRON- can not use the csv support that come native from terminusdb ( and eventually -PRON- will be use a different front end source for the datum in any case ) an original version try to make a set of insert and delete per spreadsheet line but that be really slow ( -PRON- would have take over 2 day for the full spreadsheet ) this version attempt to batch all the insert and delete accord to each ' polity ' which be a natural unit for -PRON- application this reduce the time for the full spreadsheet to < 1hr to make nearly 170k insert over 374 polity if each polity commit take around 1 ( as -PRON- do at the start ) then i expect the whole operation to take around 6 m i suspect with no evidence that the problem be the following in -PRON- application -PRON- will be assert possibly several triple for a property on a polity when -PRON- be update that property with -PRON- new value -PRON- need to ensure -PRON- delete the old triple first ( so -PRON- do not have residual possibly incorrect datum leave over ) so -PRON- do something like ( assume exist polity ' afdurn ' ) triple('vpolity_id''original_name''afdurn ' ) triple('vpolity_id''polity territory''vold_values_1 ' ) delete_triple('vpolity_id''polity territory''vold_values_1 ' ) delete_object('vold_values_1 ' ) code to insert new polity territory triple on vpolity_id the 2nd triple ( ) find the previous polity territory value the delete_triple ( ) get rid of -PRON- and the delete_object ( ) get rid of the now disconnect _ value instance that hold on to the old value ( so -PRON- do not clutter the db and do not inadvertently have name collision with otherwise anonymous value instance ) i think what be happen be that as more triple be add for all the other polity the 2nd triple may just take long and long to do the match ( unless there be an index on polity_id ) -PRON- do this idiom once for each property on the polity -PRON- be update ( ~20 property or so per execute each with -PRON- own vold_values_n variable ) if there be a well way let -PRON- know thank,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,253,2021-03-09T21:13:04Z,jbennettgit,Performance degradation during batched inserts,https://github.com/terminusdb/terminusdb/issues/253,"Ubuntu 18.04 running bootstrap version 4.1.2 in docker container, using python client.  Time to insert a polity's data from a test version of a seshat csv increases linearly even if the number of inserts (batched by polity) stays roughly the same.  Attempts to squash() after each execute() commit only increased the incremental and total time.
Separately attempts to view the final, committed db in the TerminusDB console hangs with a spinning icon.
To reproduce:

Pull our seshat test code using git clone https://github.com/MajidBenam/seshat-3store.git
cd seshat-3store
cp -f equinox_full.csv equinox.csv
python3 define_seshat_schema.py
python3 insert_from_csv.py

Step 3 ensures you use the full 47K line csv file, not the little test equinox.csv.
Step 4 should take around 7-8s to define the schema.
During step 5 the initial set of commits, which batch insertions by polity, show ~450 inserts happening in ~1s.  But after 20 or so polity commits things really start to slow and by the end each polity takes ~10s to commit.  We expect roughly constant time per insert.

If you re-run step 4, it will delete the db and rebuild it (and the schema) for easy testing.
There will be various complaints and WARNINGs during step 5 which you can ignore.
If you run step 5 once and then run step 5 immediately again (re-asserting the same data so causing deletes as well as inserts) quite often there is an initial 'hang' of a few 10s of seconds at the beginning during the commit of first true polity.

Partial example output from step 5 enclosed below.  Each 'Committing' line reports the number of inserts (i:), deletes (d:) and elapsed time for the execute() call.
Parsing data from equinox.csv
Committing data for Code book i:6 d:0 0.15s
WARNING: Pre-casting '1790000:490000' from AfDurrn|Social Complexity variables|Social Scale|Polity territory to a range: likely ill-formed!
Committing data for AfDurrn i:453 d:0 0.92s
WARNING: Pre-casting '60000:80000' from AfGhurd|Social Complexity variables|Social Scale|Polity territory to a range: likely ill-formed!
WARNING: Pre-casting '900000:1100000' from AfGhurd|Social Complexity variables|Social Scale|Polity territory to a range: likely ill-formed!
WARNING: Pre-casting '1600000:1700000' from AfGhurd|Social Complexity variables|Social Scale|Polity territory to a range: likely ill-formed!
Committing data for AfGhurd i:485 d:0 0.92s
WARNING: Pre-casting '600000:800000' from AfGrBct|Social Complexity variables|Social Scale|Polity territory to a range: likely ill-formed!
WARNING: Pre-casting '1500000:2000000' from AfGrBct|Social Complexity variables|Social Scale|Polity Population to a range: likely ill-formed!
WARNING: Pre-casting '25000:50000' from AfGrBct|Social Complexity variables|Social Scale|Population of the largest settlement to a range: likely ill-formed!
Committing data for AfGrBct i:506 d:0 1.00s
WARNING: Pre-casting '2750000:3000000' from AfHepht|Social Complexity variables|Social Scale|Polity territory to a range: likely ill-formed!
Committing data for AfHepht i:568 d:0 1.11s
....
Committing data for YeQasmi i:414 d:0 13.88s
WARNING: Pre-casting '30000:40000' from YeQatab|Social Complexity variables|Social Scale|Population of the largest settlement to a range: likely ill-formed!
Committing data for YeQatab i:300 d:0 9.37s
WARNING: Unable to pre-cast 'suspected unknown' from YeRasul|Warfare variables|Military Technologies|Long walls to integer; assuming 0
Committing data for YeRasul i:441 d:0 12.71s
Committing data for YeSaRay i:222 d:0 7.17s
WARNING: Pre-casting '30000:40000' from YeSabaC|Social Complexity variables|Social Scale|Population of the largest settlement to a range: likely ill-formed!
Committing data for YeSabaC i:279 d:0 9.17s
WARNING: Unable to pre-cast 'suspected unknown' from YeTahir|Social Complexity variables|Social Scale|Polity territory to decimal; assuming 0.0
WARNING: Unable to pre-cast 'suspected unknown' from YeTahir|Social Complexity variables|Social Scale|Polity Population to decimal; assuming 0.0
WARNING: Unable to pre-cast 'suspected unknown' from YeTahir|Social Complexity variables|Social Scale|Population of the largest settlement to decimal; assuming 0.0
WARNING: Unable to pre-cast 'suspected unknown' from YeTahir|Warfare variables|Military Technologies|Long walls to integer; assuming 0
Committing data for YeTahir i:435 d:0 13.29s
WARNING: Pre-casting '250000:350000' from YeWarLd|Social Complexity variables|Social Scale|Polity territory to a range: likely ill-formed!
WARNING: Unable to pre-cast 'suspected unknown' from YeWarLd|Warfare variables|Military Technologies|Long walls to integer; assuming 0
Committing data for YeWarLd i:456 d:0 12.78s
WARNING: Unable to pre-cast 'suspected unknown' from YeZiyad|Warfare variables|Military Technologies|Long walls to integer; assuming 0
Committing data for YeZiyad i:402 d:0 11.15s
Total assertions: 374 requiring inserts: 169681 deletes: 0
Total failed commits: 0
Execution time: 2499.8s",2021-03-12T14:41:16Z,GavinMendelGleason,https://github.com/terminusdb/terminusdb/issues/253#issuecomment-797531820,"Hello Jim, I'll look at this this weekend.",hello jim -PRON- will look at this this weekend,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,253,2021-03-09T21:13:04Z,jbennettgit,Performance degradation during batched inserts,https://github.com/terminusdb/terminusdb/issues/253,"Ubuntu 18.04 running bootstrap version 4.1.2 in docker container, using python client.  Time to insert a polity's data from a test version of a seshat csv increases linearly even if the number of inserts (batched by polity) stays roughly the same.  Attempts to squash() after each execute() commit only increased the incremental and total time.
Separately attempts to view the final, committed db in the TerminusDB console hangs with a spinning icon.
To reproduce:

Pull our seshat test code using git clone https://github.com/MajidBenam/seshat-3store.git
cd seshat-3store
cp -f equinox_full.csv equinox.csv
python3 define_seshat_schema.py
python3 insert_from_csv.py

Step 3 ensures you use the full 47K line csv file, not the little test equinox.csv.
Step 4 should take around 7-8s to define the schema.
During step 5 the initial set of commits, which batch insertions by polity, show ~450 inserts happening in ~1s.  But after 20 or so polity commits things really start to slow and by the end each polity takes ~10s to commit.  We expect roughly constant time per insert.

If you re-run step 4, it will delete the db and rebuild it (and the schema) for easy testing.
There will be various complaints and WARNINGs during step 5 which you can ignore.
If you run step 5 once and then run step 5 immediately again (re-asserting the same data so causing deletes as well as inserts) quite often there is an initial 'hang' of a few 10s of seconds at the beginning during the commit of first true polity.

Partial example output from step 5 enclosed below.  Each 'Committing' line reports the number of inserts (i:), deletes (d:) and elapsed time for the execute() call.
Parsing data from equinox.csv
Committing data for Code book i:6 d:0 0.15s
WARNING: Pre-casting '1790000:490000' from AfDurrn|Social Complexity variables|Social Scale|Polity territory to a range: likely ill-formed!
Committing data for AfDurrn i:453 d:0 0.92s
WARNING: Pre-casting '60000:80000' from AfGhurd|Social Complexity variables|Social Scale|Polity territory to a range: likely ill-formed!
WARNING: Pre-casting '900000:1100000' from AfGhurd|Social Complexity variables|Social Scale|Polity territory to a range: likely ill-formed!
WARNING: Pre-casting '1600000:1700000' from AfGhurd|Social Complexity variables|Social Scale|Polity territory to a range: likely ill-formed!
Committing data for AfGhurd i:485 d:0 0.92s
WARNING: Pre-casting '600000:800000' from AfGrBct|Social Complexity variables|Social Scale|Polity territory to a range: likely ill-formed!
WARNING: Pre-casting '1500000:2000000' from AfGrBct|Social Complexity variables|Social Scale|Polity Population to a range: likely ill-formed!
WARNING: Pre-casting '25000:50000' from AfGrBct|Social Complexity variables|Social Scale|Population of the largest settlement to a range: likely ill-formed!
Committing data for AfGrBct i:506 d:0 1.00s
WARNING: Pre-casting '2750000:3000000' from AfHepht|Social Complexity variables|Social Scale|Polity territory to a range: likely ill-formed!
Committing data for AfHepht i:568 d:0 1.11s
....
Committing data for YeQasmi i:414 d:0 13.88s
WARNING: Pre-casting '30000:40000' from YeQatab|Social Complexity variables|Social Scale|Population of the largest settlement to a range: likely ill-formed!
Committing data for YeQatab i:300 d:0 9.37s
WARNING: Unable to pre-cast 'suspected unknown' from YeRasul|Warfare variables|Military Technologies|Long walls to integer; assuming 0
Committing data for YeRasul i:441 d:0 12.71s
Committing data for YeSaRay i:222 d:0 7.17s
WARNING: Pre-casting '30000:40000' from YeSabaC|Social Complexity variables|Social Scale|Population of the largest settlement to a range: likely ill-formed!
Committing data for YeSabaC i:279 d:0 9.17s
WARNING: Unable to pre-cast 'suspected unknown' from YeTahir|Social Complexity variables|Social Scale|Polity territory to decimal; assuming 0.0
WARNING: Unable to pre-cast 'suspected unknown' from YeTahir|Social Complexity variables|Social Scale|Polity Population to decimal; assuming 0.0
WARNING: Unable to pre-cast 'suspected unknown' from YeTahir|Social Complexity variables|Social Scale|Population of the largest settlement to decimal; assuming 0.0
WARNING: Unable to pre-cast 'suspected unknown' from YeTahir|Warfare variables|Military Technologies|Long walls to integer; assuming 0
Committing data for YeTahir i:435 d:0 13.29s
WARNING: Pre-casting '250000:350000' from YeWarLd|Social Complexity variables|Social Scale|Polity territory to a range: likely ill-formed!
WARNING: Unable to pre-cast 'suspected unknown' from YeWarLd|Warfare variables|Military Technologies|Long walls to integer; assuming 0
Committing data for YeWarLd i:456 d:0 12.78s
WARNING: Unable to pre-cast 'suspected unknown' from YeZiyad|Warfare variables|Military Technologies|Long walls to integer; assuming 0
Committing data for YeZiyad i:402 d:0 11.15s
Total assertions: 374 requiring inserts: 169681 deletes: 0
Total failed commits: 0
Execution time: 2499.8s",2021-03-12T16:37:40Z,jbennettgit,https://github.com/terminusdb/terminusdb/issues/253#issuecomment-797607988,"Thanks Gavin!  Let me know if you have any questions.

BTW I did some more experiments and gathered more timing data.

As I mentioned in my report follow-on we ‘batch’ all property updates on a per polity basis to reduce the calls to execute().  The structure of each polity query over all looks like:

qp,[qf,qv+]+

where

- qp is the fragment of the query that either looks up the existing polity object or creates it (via insert(), etc.)
- qf is the fragment that finds and deletes the old triples (and the scoped objects if needed) for the property
- qv are the query fragments that assert the new value(s) for the property

I attempted to isolate the location(s) of the timing issues. Each time I ran the experiment I did the following thing:

terminusdb-container stop
terminusdb-container rm (deleting the full store)
terminusdb-container run
python3 define_seshat_schema.py

So for each experiment the database had nothing in it. As a consequence I could:

1. Just run the qp fragments: timing starts at 0.14s and grows to 0.5s after 374 queries. I assume the increased timing reflects the work needed to both create and index the object(s) plus journaling.
2. Run a modified qp only that just does the insert without looking to see if the polity already exists: Same timing result as experiment 1.

Of course in both there are only very few triples in the db (374 objects and 374 triples) but the timing trend is clear.

3. Run qp,[qv]+, that is, drop the qf flushing fragments (since I knew the db was clean, nothing to flush).  Minor timing improvement but calls were still taking 10s at the end.  So my previous conjecture about it being the flushing lookup code looks incorrect and it is something about the qv inserts.

4. Run qp then run qp,[qf,qv+]+ that is do the probe/insert for the polity twice in succession (as separate execute() calls).  I expected the 2nd qp to simply match the results of the first qp, which did the creation. In this case, the timings on the first qp increased from 0.14s to 0.76 (1/2 again as much as experiment 1, probably consistent with also many more triples for prior qvs being in the db).  But the timings on the 2nd query doubled(!) so that by the end they were taking ~20s!  I have no idea why running the first qp should make the second full query take even longer.
…
 On Mar 12, 2021, at 06:41 , Gavin Mendel-Gleason ***@***.***> wrote:


 Hello Jim, I'll look at this this weekend.

 —
 You are receiving this because you authored the thread.
 Reply to this email directly, view it on GitHub, or unsubscribe.",thanks gavin let -PRON- know if -PRON- have any question btw i do some more experiment and gather more timing datum as i mention in -PRON- report follow - on -PRON- ' batch ' all property update on a per polity basis to reduce the call to execute ( ) the structure of each polity query over all look like qp[qfqv+]+ where - qp be the fragment of the query that either look up the exist polity object or create -PRON- ( via insert ( ) etc ) - qf be the fragment that find and delete the old triple ( and the scope object if need ) for the property - qv be the query fragment that assert the new value(s ) for the property i attempt to isolate the location(s ) of the timing issue each time i run the experiment i do the follow thing terminusdb - container stop terminusdb - container rm ( delete the full store ) terminusdb - container run python3 define_seshat_schemapy so for each experiment the database have nothing in -PRON- as a consequence i could 1 just run the qp fragment timing start at 014 and grow to 05 after 374 query i assume the increased timing reflect the work need to both create and index the object(s ) plus journale 2 run a modified qp only that just do the insert without look to see if the polity already exist same timing result as experiment 1 of course in both there be only very few triple in the db ( 374 object and 374 triple ) but the timing trend be clear 3 run qp[qv]+ that be drop the qf flush fragment ( since i know the db be clean nothing to flush ) minor timing improvement but call be still take 10 at the end so -PRON- previous conjecture about -PRON- be the flush lookup code look incorrect and -PRON- be something about the qv insert 4 run qp then run qp[qfqv+]+ that be do the probe / insert for the polity twice in succession ( as separate execute ( ) call ) i expect the 2nd qp to simply match the result of the first qp which do the creation in this case the timing on the first qp increase from 014 to 076 ( 1/2 again as much as experiment 1 probably consistent with also many more triple for prior qvs be in the db ) but the timing on the 2nd query double ( ) so that by the end -PRON- be take ~20s i have no idea why run the first qp should make the second full query take even long … on mar 12 2021 at 0641 gavin mendel - gleason * * * @ * * * * * * > write hello jim -PRON- will look at this this weekend — -PRON- be receive this because -PRON- author the thread reply to this email directly view -PRON- on github or unsubscribe,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,253,2021-03-09T21:13:04Z,jbennettgit,Performance degradation during batched inserts,https://github.com/terminusdb/terminusdb/issues/253,"Ubuntu 18.04 running bootstrap version 4.1.2 in docker container, using python client.  Time to insert a polity's data from a test version of a seshat csv increases linearly even if the number of inserts (batched by polity) stays roughly the same.  Attempts to squash() after each execute() commit only increased the incremental and total time.
Separately attempts to view the final, committed db in the TerminusDB console hangs with a spinning icon.
To reproduce:

Pull our seshat test code using git clone https://github.com/MajidBenam/seshat-3store.git
cd seshat-3store
cp -f equinox_full.csv equinox.csv
python3 define_seshat_schema.py
python3 insert_from_csv.py

Step 3 ensures you use the full 47K line csv file, not the little test equinox.csv.
Step 4 should take around 7-8s to define the schema.
During step 5 the initial set of commits, which batch insertions by polity, show ~450 inserts happening in ~1s.  But after 20 or so polity commits things really start to slow and by the end each polity takes ~10s to commit.  We expect roughly constant time per insert.

If you re-run step 4, it will delete the db and rebuild it (and the schema) for easy testing.
There will be various complaints and WARNINGs during step 5 which you can ignore.
If you run step 5 once and then run step 5 immediately again (re-asserting the same data so causing deletes as well as inserts) quite often there is an initial 'hang' of a few 10s of seconds at the beginning during the commit of first true polity.

Partial example output from step 5 enclosed below.  Each 'Committing' line reports the number of inserts (i:), deletes (d:) and elapsed time for the execute() call.
Parsing data from equinox.csv
Committing data for Code book i:6 d:0 0.15s
WARNING: Pre-casting '1790000:490000' from AfDurrn|Social Complexity variables|Social Scale|Polity territory to a range: likely ill-formed!
Committing data for AfDurrn i:453 d:0 0.92s
WARNING: Pre-casting '60000:80000' from AfGhurd|Social Complexity variables|Social Scale|Polity territory to a range: likely ill-formed!
WARNING: Pre-casting '900000:1100000' from AfGhurd|Social Complexity variables|Social Scale|Polity territory to a range: likely ill-formed!
WARNING: Pre-casting '1600000:1700000' from AfGhurd|Social Complexity variables|Social Scale|Polity territory to a range: likely ill-formed!
Committing data for AfGhurd i:485 d:0 0.92s
WARNING: Pre-casting '600000:800000' from AfGrBct|Social Complexity variables|Social Scale|Polity territory to a range: likely ill-formed!
WARNING: Pre-casting '1500000:2000000' from AfGrBct|Social Complexity variables|Social Scale|Polity Population to a range: likely ill-formed!
WARNING: Pre-casting '25000:50000' from AfGrBct|Social Complexity variables|Social Scale|Population of the largest settlement to a range: likely ill-formed!
Committing data for AfGrBct i:506 d:0 1.00s
WARNING: Pre-casting '2750000:3000000' from AfHepht|Social Complexity variables|Social Scale|Polity territory to a range: likely ill-formed!
Committing data for AfHepht i:568 d:0 1.11s
....
Committing data for YeQasmi i:414 d:0 13.88s
WARNING: Pre-casting '30000:40000' from YeQatab|Social Complexity variables|Social Scale|Population of the largest settlement to a range: likely ill-formed!
Committing data for YeQatab i:300 d:0 9.37s
WARNING: Unable to pre-cast 'suspected unknown' from YeRasul|Warfare variables|Military Technologies|Long walls to integer; assuming 0
Committing data for YeRasul i:441 d:0 12.71s
Committing data for YeSaRay i:222 d:0 7.17s
WARNING: Pre-casting '30000:40000' from YeSabaC|Social Complexity variables|Social Scale|Population of the largest settlement to a range: likely ill-formed!
Committing data for YeSabaC i:279 d:0 9.17s
WARNING: Unable to pre-cast 'suspected unknown' from YeTahir|Social Complexity variables|Social Scale|Polity territory to decimal; assuming 0.0
WARNING: Unable to pre-cast 'suspected unknown' from YeTahir|Social Complexity variables|Social Scale|Polity Population to decimal; assuming 0.0
WARNING: Unable to pre-cast 'suspected unknown' from YeTahir|Social Complexity variables|Social Scale|Population of the largest settlement to decimal; assuming 0.0
WARNING: Unable to pre-cast 'suspected unknown' from YeTahir|Warfare variables|Military Technologies|Long walls to integer; assuming 0
Committing data for YeTahir i:435 d:0 13.29s
WARNING: Pre-casting '250000:350000' from YeWarLd|Social Complexity variables|Social Scale|Polity territory to a range: likely ill-formed!
WARNING: Unable to pre-cast 'suspected unknown' from YeWarLd|Warfare variables|Military Technologies|Long walls to integer; assuming 0
Committing data for YeWarLd i:456 d:0 12.78s
WARNING: Unable to pre-cast 'suspected unknown' from YeZiyad|Warfare variables|Military Technologies|Long walls to integer; assuming 0
Committing data for YeZiyad i:402 d:0 11.15s
Total assertions: 374 requiring inserts: 169681 deletes: 0
Total failed commits: 0
Execution time: 2499.8s",2021-08-09T13:21:06Z,AstroChelonian,https://github.com/terminusdb/terminusdb/issues/253#issuecomment-895217457,This will be solved by the new terminus release.,this will be solve by the new terminus release,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,254,2021-03-12T11:52:24Z,Cheukting,Problem with get_csv from the Python client,https://github.com/terminusdb/terminusdb/issues/254,"Describe the bug
A clear and concise description of what the bug is.
To Reproduce
Steps to reproduce the behavior:
running get_csv while testing the Python client
Expected behavior
No error or a useful error message in json format
Screenshots
If applicable, add screenshots to help explain your problem.
Info (please complete the following information):

OS: [e.g. Ubuntu 18.04]
How did you run terminus-server (terminus-quickstart, using Docker directly, manual installation)
SWI Prolog version if you're using the manual installation

Additional context
terminusdb_client.woqlclient.errors.DatabaseError: {
E                 ""api:message"":""Error: woql_syntax_error(badly_formed_ast((t(v('Var623'),rdfs:label,'employee_file.csv'@en),t(v('Var623'),csv:csv_column,v('Var624')),t(v('Var624'),csv:csv_column_name,v('Var625')^^v('Var626')),t(v('Var624'),csv:csv_column_index,v('Var627')^^v('Var628')))))\n  [39] throw(error(woql_syntax_error(...),_6748))\n  [37] woql_compile:compile_query((t(...,...,...),...,...),_6790,query_context{all_witnesses:false,authorization:'terminusdb:///system/data/admin',bindings:[...|...],default_collection:branch_descriptor{branch_name:\""main\"",repository_descriptor: ...},files:[],filter:type_name_filter{names: ...,type:'None'},prefixes:_6896{api:'http://terminusdb.com/schema/api#',csv:'terminusdb:///schema#',doc:'terminusdb:///data/',layer:'http://terminusdb.com/schema/layer#',owl:'http://www.w3.org/2002/07/owl#',rdf:'http://www.w3.org/1999/02/22-rdf-syntax-ns#',rdfs:'http://www.w3.org/2000/01/rdf-schema#',ref:'http://terminusdb.com/schema/ref#',repo:'http://terminusdb.com/schema/repository#',scm:'terminusdb:///schema#',system:'http://terminusdb.com/schema/system#',terminus:'http://terminusdb.com/schema/system#',vio:'http://terminusdb.com/schema/vio#',woql:'http://terminusdb.com/schema/woql#',xdd:'http://terminusdb.com/schema/xdd#',xsd:'http://www.w3.org/2001/XMLSchema#'},selected:[],system:transaction_object{descriptor: ...,inference_objects: ...,instance_objects: ...,schema_objects: ...},transaction_objects:[...],update_guard:_6832,write_graph:branch_graph{branch_name:\""main\"",database_name:\""test_csv\"",name:\""main\"",organization_name:\""admin\"",repository_name:\""local\"",type:instance}},_6794) at /app/terminusdb/src/core/query/woql_compile.pl:591\n  [36] ask:ask_ast(query_context{all_witnesses:false,authorization:'terminusdb:///system/data/admin',bindings:[...|...],default_collection:branch_descriptor{branch_name:\""main\"",repository_descriptor: ...},files:[],filter:type_name_filter{names: ...,type:'None'},prefixes:_6896{api:'http://terminusdb.com/schema/api#',csv:'terminusdb:///schema#',doc:'terminusdb:///data/',layer:'http://terminusdb.com/schema/layer#',owl:'http://www.w3.org/2002/07/owl#',rdf:'http://www.w3.org/1999/02/22-rdf-syntax-ns#',rdfs:'http://www.w3.org/2000/01/rdf-schema#',ref:'http://terminusdb.com/schema/ref#',repo:'http://terminusdb.com/schema/repository#',scm:'terminusdb:///schema#',system:'http://terminusdb.com/schema/system#',terminus:'http://terminusdb.com/schema/system#',vio:'http://terminusdb.com/schema/vio#',woql:'http://terminusdb.com/schema/woql#',xdd:'http://terminusdb.com/schema/xdd#',xsd:'http://www.w3.org/2001/XMLSchema#'},selected:[],system:transaction_object{descriptor: ...,inference_objects: ...,instance_objects: ...,schema_objects: ...},transaction_objects:[...],update_guard:_7076,write_graph:branch_graph{branch_name:\""main\"",database_name:\""test_csv\"",name:\""main\"",organization_name:\""admin\"",repository_name:\""local\"",type:instance}},(t(...,...,...),...,...),_7058) at /app/terminusdb/src/core/query/ask.pl:239\n  [33] '<meta-call>'(api_csv:(...,...)) <foreign>\n  [32] '$bags':findall_loop(_7380-_7382-_7376,api_csv:(...,...),_7368,[]) at /usr/lib/swipl/boot/bags.pl:99\n  [31] setup_call_catcher_cleanup('$bags':'$new_findall_bag','$bags':findall_loop(...,...,_7450,[]),_7428,'$bags':'$destroy_findall_bag') at /usr/lib/swipl/boot/init.pl:614\n  [27] api_csv:csv_columns('employee_file.csv',query_context{all_witnesses:false,authorization:'terminusdb:///system/data/admin',bindings:[],default_collection:branch_descriptor{branch_name:\""main\"",repository_descriptor: ...},files:[],filter:type_name_filter{names: ...,type:'None'},prefixes:_6896{api:'http://terminusdb.com/schema/api#',csv:'terminusdb:///schema#',doc:'terminusdb:///data/',layer:'http://terminusdb.com/schema/layer#',owl:'http://www.w3.org/2002/07/owl#',rdf:'http://www.w3.org/1999/02/22-rdf-syntax-ns#',rdfs:'http://www.w3.org/2000/01/rdf-schema#',ref:'http://terminusdb.com/schema/ref#',repo:'http://terminusdb.com/schema/repository#',scm:'terminusdb:///schema#',system:'http://terminusdb.com/schema/system#',terminus:'http://terminusdb.com/schema/system#',vio:'http://terminusdb.com/schema/vio#',woql:'http://terminusdb.com/schema/woql#',xdd:'http://terminusdb.com/schema/xdd#',xsd:'http://www.w3.org/2001/XMLSchema#'},selected:[],system:transaction_object{descriptor: ...,inference_objects: ...,instance_objects: ...,schema_objects: ...},transaction_objects:[...],update_guard:_7516,write_graph:branch_graph{branch_name:\""main\"",database_name:\""test_csv\"",name:\""main\"",organization_name:\""admin\"",repository_name:\""local\"",type:instance}},_7498) at /app/terminusdb/src/core/api/api_csv.pl:271\n  [26] api_csv:csv_dump('<garbage_collected>','terminusdb:///system/data/admin','admin/test_csv/local/branch/main/None/None','employee_file.csv','/tmp/swipl_28_21',_7746{}) at /app/terminusdb/src/core/api/api_csv.pl:238\n  [25] '<meta-call>'('<garbage_collected>') <foreign>\n  [24] catch(routes:(...,...),error(woql_syntax_error(...),context(_7840,_7842)),routes:do_or_die(...,...)) at /usr/lib/swipl/boot/init.pl:532\n  [23] catch_with_backtrace('<garbage_collected>','<garbage_collected>','<garbage_collected>') at /usr/lib/swipl/boot/init.pl:582\n\nNote: some frames are missing due to last-call optimization.\nRe-run your program in debug mode (:- debug.) to get more detail.\n\n"",
E                 ""api:status"":""api:server_error""
E               }",2021-03-18T12:03:33Z,GavinMendelGleason,https://github.com/terminusdb/terminusdb/issues/254#issuecomment-801872460,Fixed in v1.0.0 of the Python client,fix in v100 of the python client,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,255,2021-03-08T04:10:21Z,teleological,"xsd:integer properties export to turtle, but can't be imported",https://github.com/terminusdb/terminusdb/issues/255,"Describe the bug
xsd:integer instance property values won't round-trip via turtle: They export to turtle as bare integer literals (e.g. 1) but are rejected (""Not a well formed integer"") on import. String with cast (""1""^^xsd:integer) is similarly rejected.
Note however that if the type is declared as xsd:nonNegativeInteger in the schema, a string with cast (""1""^^xsd:nonNegativeInteger) is accepted on import.
To Reproduce

Import these triples into the schema graph:

scm:foo
  a owl:Class ;
  rdfs:label ""foo"" ;
  rdfs:subClassOf terminus:Document .
                  
scm:bar
  a owl:DatatypeProperty ;
  rdfs:domain scm:foo ;
  rdfs:label ""bar"" ;
  rdfs:range xsd:integer .



Using the HTML interface, create a foo document with value bar = 1


Dump the instance triples. Should look something like this:


doc:foo_doc
  a scm:foo ;
  rdfs:label ""foo"" ;
  scm:bar 1 .


Try to import the turtle triples as displayed. Using the python client, I got this error:

terminusdb_client.woqlclient.errors.APIError: {
  ""@type"":""api:TriplesErrorResponse"",
  ""api:error"": {
    ""@type"":""api:SchemaValidationError"",
    ""api:witness"": {
      ""@type"":""vio:ViolationWithDatatypeObject"",
      ""vio:base_type"": {""@type"":""xsd:string"", ""@value"":""xsd:integer""},
      ""vio:literal"": {""@type"":""xsd:anySimpleType"", ""@value"":""1""},
      ""vio:message"":""Not a well formed integer."",
      ""vio:property"": {""@type"":""xsd:anyURI"", ""@value"":""terminusdb:///schema#bar""}
    }
  },
  ""api:message"":""Schema did not validate after this update"",
  ""api:status"":""api:failure""
}

Submitting the web form with no changes after clicking ""edit"" also raises a ""Not a well formed integer"" error.
Expected behavior
I assume that a bare integer literal should be recognized as valid turtle on import as well as export.
Info (please complete the following information):

OS: Mac OS X 11.2
How did you run terminus-server: Downloaded app from website (console 4.2.2, server 4.2.0)

Additional context
I love this project. Hi, Gavin!",2021-03-09T15:06:46Z,GavinMendelGleason,https://github.com/terminusdb/terminusdb/issues/255#issuecomment-794019440,"Hello Riley! I'll have a patch for this in the next bugfix release, hopefully in the next couple of weeks.",hello riley -PRON- will have a patch for this in the next bugfix release hopefully in the next couple of week,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,255,2021-03-08T04:10:21Z,teleological,"xsd:integer properties export to turtle, but can't be imported",https://github.com/terminusdb/terminusdb/issues/255,"Describe the bug
xsd:integer instance property values won't round-trip via turtle: They export to turtle as bare integer literals (e.g. 1) but are rejected (""Not a well formed integer"") on import. String with cast (""1""^^xsd:integer) is similarly rejected.
Note however that if the type is declared as xsd:nonNegativeInteger in the schema, a string with cast (""1""^^xsd:nonNegativeInteger) is accepted on import.
To Reproduce

Import these triples into the schema graph:

scm:foo
  a owl:Class ;
  rdfs:label ""foo"" ;
  rdfs:subClassOf terminus:Document .
                  
scm:bar
  a owl:DatatypeProperty ;
  rdfs:domain scm:foo ;
  rdfs:label ""bar"" ;
  rdfs:range xsd:integer .



Using the HTML interface, create a foo document with value bar = 1


Dump the instance triples. Should look something like this:


doc:foo_doc
  a scm:foo ;
  rdfs:label ""foo"" ;
  scm:bar 1 .


Try to import the turtle triples as displayed. Using the python client, I got this error:

terminusdb_client.woqlclient.errors.APIError: {
  ""@type"":""api:TriplesErrorResponse"",
  ""api:error"": {
    ""@type"":""api:SchemaValidationError"",
    ""api:witness"": {
      ""@type"":""vio:ViolationWithDatatypeObject"",
      ""vio:base_type"": {""@type"":""xsd:string"", ""@value"":""xsd:integer""},
      ""vio:literal"": {""@type"":""xsd:anySimpleType"", ""@value"":""1""},
      ""vio:message"":""Not a well formed integer."",
      ""vio:property"": {""@type"":""xsd:anyURI"", ""@value"":""terminusdb:///schema#bar""}
    }
  },
  ""api:message"":""Schema did not validate after this update"",
  ""api:status"":""api:failure""
}

Submitting the web form with no changes after clicking ""edit"" also raises a ""Not a well formed integer"" error.
Expected behavior
I assume that a bare integer literal should be recognized as valid turtle on import as well as export.
Info (please complete the following information):

OS: Mac OS X 11.2
How did you run terminus-server: Downloaded app from website (console 4.2.2, server 4.2.0)

Additional context
I love this project. Hi, Gavin!",2021-03-09T15:40:39Z,luke-feeney,https://github.com/terminusdb/terminusdb/issues/255#issuecomment-794062648,love the additional context!,love the additional context,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,255,2021-03-08T04:10:21Z,teleological,"xsd:integer properties export to turtle, but can't be imported",https://github.com/terminusdb/terminusdb/issues/255,"Describe the bug
xsd:integer instance property values won't round-trip via turtle: They export to turtle as bare integer literals (e.g. 1) but are rejected (""Not a well formed integer"") on import. String with cast (""1""^^xsd:integer) is similarly rejected.
Note however that if the type is declared as xsd:nonNegativeInteger in the schema, a string with cast (""1""^^xsd:nonNegativeInteger) is accepted on import.
To Reproduce

Import these triples into the schema graph:

scm:foo
  a owl:Class ;
  rdfs:label ""foo"" ;
  rdfs:subClassOf terminus:Document .
                  
scm:bar
  a owl:DatatypeProperty ;
  rdfs:domain scm:foo ;
  rdfs:label ""bar"" ;
  rdfs:range xsd:integer .



Using the HTML interface, create a foo document with value bar = 1


Dump the instance triples. Should look something like this:


doc:foo_doc
  a scm:foo ;
  rdfs:label ""foo"" ;
  scm:bar 1 .


Try to import the turtle triples as displayed. Using the python client, I got this error:

terminusdb_client.woqlclient.errors.APIError: {
  ""@type"":""api:TriplesErrorResponse"",
  ""api:error"": {
    ""@type"":""api:SchemaValidationError"",
    ""api:witness"": {
      ""@type"":""vio:ViolationWithDatatypeObject"",
      ""vio:base_type"": {""@type"":""xsd:string"", ""@value"":""xsd:integer""},
      ""vio:literal"": {""@type"":""xsd:anySimpleType"", ""@value"":""1""},
      ""vio:message"":""Not a well formed integer."",
      ""vio:property"": {""@type"":""xsd:anyURI"", ""@value"":""terminusdb:///schema#bar""}
    }
  },
  ""api:message"":""Schema did not validate after this update"",
  ""api:status"":""api:failure""
}

Submitting the web form with no changes after clicking ""edit"" also raises a ""Not a well formed integer"" error.
Expected behavior
I assume that a bare integer literal should be recognized as valid turtle on import as well as export.
Info (please complete the following information):

OS: Mac OS X 11.2
How did you run terminus-server: Downloaded app from website (console 4.2.2, server 4.2.0)

Additional context
I love this project. Hi, Gavin!",2021-03-12T13:46:40Z,GavinMendelGleason,https://github.com/terminusdb/terminusdb/issues/255#issuecomment-797500002,Fixed in e28b1f4,fix in e28b1f4,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,256,2021-03-17T16:16:38Z,jbennettgit,<Response 200> object returned from execute() call,https://github.com/terminusdb/terminusdb/issues/256,"Per Gavin, updated to dev server (see below) via docker container running on Ubuntu 18.04.  Also updated, via pip, the python terminusdb client version: woql.version. version = 0.6.1.
Unable to find image 'terminusdb/terminusdb-server:dev' locally
dev: Pulling from terminusdb/terminusdb-server
...
Digest: sha256:c963490d7ff605e3ddf1e94ea0df6f4f8c8ce7c6871714f9cbe61714c1b3f7db
Status: Downloaded newer image for terminusdb/terminusdb-server:dev
terminusdb-server container started https://127.0.0.1:6363/
Running our insertion_from_csv code, which worked previously, the 2nd and subsequent calls to execute() of a query return not a dict with bindings but a <Response 200> object which appears to be something from the intermediate http traffic with the server.
python3 insert_from_csv.py
/home/ubuntu/.local/lib/python3.6/site-packages/terminusdb_client/woqlview/woql_view.py:9: UserWarning: woqlview need to be used in Jupyter notebook.
warnings.warn(msg)
Parsing data from equinox.csv
Committing data for Code book i:6 d:0 0.30s << works once and result dict returns 6 inserts, no deletes
WARNING: Pre-casting '1790000:490000' from AfDurrn|Social Complexity variables|Social Scale|Polity territory to a range: likely ill-formed!
WARNING: Pre-casting '3:4' from AfDurrn|Social Complexity variables|Hierarchical Complexity|Settlement hierarchy to a range: likely ill-formed!
WARNING: Pre-casting '1:2' from AfDurrn|Social Complexity variables|Hierarchical Complexity|Religious levels to a range: likely ill-formed!
Execution ERROR while Committing data for AfDurrn after 1.11s -- skipped  << caught exception during 2nd execute()
/usr/lib/python3/dist-packages/apport/report.py:13: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
import fnmatch, glob, traceback, errno, sys, atexit, locale, imp, stat
Traceback (most recent call last):
File ""insert_from_csv.py"", line 39, in execute_commit
result = q.execute(client,commit_msg=msg)
File ""/home/ubuntu/.local/lib/python3.6/site-packages/terminusdb_client/woqlquery/woql_query.py"", line 448, in execute
return client.query(self, commit_msg, file_dict=file_dict)
File ""/home/ubuntu/.local/lib/python3.6/site-packages/terminusdb_client/woqlclient/woqlClient.py"", line 1025, in query
if result.get(""inserts"") or result.get(""deletes""):
AttributeError: 'Response' object has no attribute 'get'
During handling of the above exception, another exception occurred:
Traceback (most recent call last):
File ""insert_from_csv.py"", line 277, in 
assert_seshat_row(Polity, Variable, Value_From, Value_To, Date_From, Date_To, Fact_Type, Value_Note)
File ""insert_from_csv.py"", line 71, in assert_seshat_row
execute_commit(polity_query)
File ""insert_from_csv.py"", line 58, in execute_commit
print(f""{exception.msg}"")
Without dicts with bindings being consistently returned we are unable to continue development.",2021-03-19T15:42:45Z,GavinMendelGleason,https://github.com/terminusdb/terminusdb/issues/256#issuecomment-802925573,I can't reproduce this with v1.0 of the python client. This might be a bug in an older version?,i can not reproduce this with v10 of the python client this may be a bug in an old version,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,256,2021-03-17T16:16:38Z,jbennettgit,<Response 200> object returned from execute() call,https://github.com/terminusdb/terminusdb/issues/256,"Per Gavin, updated to dev server (see below) via docker container running on Ubuntu 18.04.  Also updated, via pip, the python terminusdb client version: woql.version. version = 0.6.1.
Unable to find image 'terminusdb/terminusdb-server:dev' locally
dev: Pulling from terminusdb/terminusdb-server
...
Digest: sha256:c963490d7ff605e3ddf1e94ea0df6f4f8c8ce7c6871714f9cbe61714c1b3f7db
Status: Downloaded newer image for terminusdb/terminusdb-server:dev
terminusdb-server container started https://127.0.0.1:6363/
Running our insertion_from_csv code, which worked previously, the 2nd and subsequent calls to execute() of a query return not a dict with bindings but a <Response 200> object which appears to be something from the intermediate http traffic with the server.
python3 insert_from_csv.py
/home/ubuntu/.local/lib/python3.6/site-packages/terminusdb_client/woqlview/woql_view.py:9: UserWarning: woqlview need to be used in Jupyter notebook.
warnings.warn(msg)
Parsing data from equinox.csv
Committing data for Code book i:6 d:0 0.30s << works once and result dict returns 6 inserts, no deletes
WARNING: Pre-casting '1790000:490000' from AfDurrn|Social Complexity variables|Social Scale|Polity territory to a range: likely ill-formed!
WARNING: Pre-casting '3:4' from AfDurrn|Social Complexity variables|Hierarchical Complexity|Settlement hierarchy to a range: likely ill-formed!
WARNING: Pre-casting '1:2' from AfDurrn|Social Complexity variables|Hierarchical Complexity|Religious levels to a range: likely ill-formed!
Execution ERROR while Committing data for AfDurrn after 1.11s -- skipped  << caught exception during 2nd execute()
/usr/lib/python3/dist-packages/apport/report.py:13: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
import fnmatch, glob, traceback, errno, sys, atexit, locale, imp, stat
Traceback (most recent call last):
File ""insert_from_csv.py"", line 39, in execute_commit
result = q.execute(client,commit_msg=msg)
File ""/home/ubuntu/.local/lib/python3.6/site-packages/terminusdb_client/woqlquery/woql_query.py"", line 448, in execute
return client.query(self, commit_msg, file_dict=file_dict)
File ""/home/ubuntu/.local/lib/python3.6/site-packages/terminusdb_client/woqlclient/woqlClient.py"", line 1025, in query
if result.get(""inserts"") or result.get(""deletes""):
AttributeError: 'Response' object has no attribute 'get'
During handling of the above exception, another exception occurred:
Traceback (most recent call last):
File ""insert_from_csv.py"", line 277, in 
assert_seshat_row(Polity, Variable, Value_From, Value_To, Date_From, Date_To, Fact_Type, Value_Note)
File ""insert_from_csv.py"", line 71, in assert_seshat_row
execute_commit(polity_query)
File ""insert_from_csv.py"", line 58, in execute_commit
print(f""{exception.msg}"")
Without dicts with bindings being consistently returned we are unable to continue development.",2021-03-19T16:19:39Z,jbennettgit,https://github.com/terminusdb/terminusdb/issues/256#issuecomment-802951197,"Perhaps.  I tried to update via pip before submitting the bug but that yielded v 0.6.1 and maintained the bug.  What is the proper technique to update to the dev version of the client?  Pull from git and install directly?  Just want to follow best practice and not make things worse.  Let me know.
…
 On Mar 19, 2021, at 08:43 , Gavin Mendel-Gleason ***@***.***> wrote:


 I can't reproduce this with v1.0 of the python client. This might be a bug in an older version?

 —
 You are receiving this because you authored the thread.
 Reply to this email directly, view it on GitHub, or unsubscribe.",perhaps i try to update via pip before submit the bug but that yield v 061 and maintain the bug what be the proper technique to update to the dev version of the client pull from git and install directly just want to follow good practice and not make thing bad let -PRON- know … on mar 19 2021 at 0843 gavin mendel - gleason * * * @ * * * * * * > write i can not reproduce this with v10 of the python client this may be a bug in an old version — -PRON- be receive this because -PRON- author the thread reply to this email directly view -PRON- on github or unsubscribe,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,256,2021-03-17T16:16:38Z,jbennettgit,<Response 200> object returned from execute() call,https://github.com/terminusdb/terminusdb/issues/256,"Per Gavin, updated to dev server (see below) via docker container running on Ubuntu 18.04.  Also updated, via pip, the python terminusdb client version: woql.version. version = 0.6.1.
Unable to find image 'terminusdb/terminusdb-server:dev' locally
dev: Pulling from terminusdb/terminusdb-server
...
Digest: sha256:c963490d7ff605e3ddf1e94ea0df6f4f8c8ce7c6871714f9cbe61714c1b3f7db
Status: Downloaded newer image for terminusdb/terminusdb-server:dev
terminusdb-server container started https://127.0.0.1:6363/
Running our insertion_from_csv code, which worked previously, the 2nd and subsequent calls to execute() of a query return not a dict with bindings but a <Response 200> object which appears to be something from the intermediate http traffic with the server.
python3 insert_from_csv.py
/home/ubuntu/.local/lib/python3.6/site-packages/terminusdb_client/woqlview/woql_view.py:9: UserWarning: woqlview need to be used in Jupyter notebook.
warnings.warn(msg)
Parsing data from equinox.csv
Committing data for Code book i:6 d:0 0.30s << works once and result dict returns 6 inserts, no deletes
WARNING: Pre-casting '1790000:490000' from AfDurrn|Social Complexity variables|Social Scale|Polity territory to a range: likely ill-formed!
WARNING: Pre-casting '3:4' from AfDurrn|Social Complexity variables|Hierarchical Complexity|Settlement hierarchy to a range: likely ill-formed!
WARNING: Pre-casting '1:2' from AfDurrn|Social Complexity variables|Hierarchical Complexity|Religious levels to a range: likely ill-formed!
Execution ERROR while Committing data for AfDurrn after 1.11s -- skipped  << caught exception during 2nd execute()
/usr/lib/python3/dist-packages/apport/report.py:13: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
import fnmatch, glob, traceback, errno, sys, atexit, locale, imp, stat
Traceback (most recent call last):
File ""insert_from_csv.py"", line 39, in execute_commit
result = q.execute(client,commit_msg=msg)
File ""/home/ubuntu/.local/lib/python3.6/site-packages/terminusdb_client/woqlquery/woql_query.py"", line 448, in execute
return client.query(self, commit_msg, file_dict=file_dict)
File ""/home/ubuntu/.local/lib/python3.6/site-packages/terminusdb_client/woqlclient/woqlClient.py"", line 1025, in query
if result.get(""inserts"") or result.get(""deletes""):
AttributeError: 'Response' object has no attribute 'get'
During handling of the above exception, another exception occurred:
Traceback (most recent call last):
File ""insert_from_csv.py"", line 277, in 
assert_seshat_row(Polity, Variable, Value_From, Value_To, Date_From, Date_To, Fact_Type, Value_Note)
File ""insert_from_csv.py"", line 71, in assert_seshat_row
execute_commit(polity_query)
File ""insert_from_csv.py"", line 58, in execute_commit
print(f""{exception.msg}"")
Without dicts with bindings being consistently returned we are unable to continue development.",2021-03-19T17:19:55Z,Cheukting,https://github.com/terminusdb/terminusdb/issues/256#issuecomment-802989427,@jbennettgit could you try upgrading to the latest released version by pip install -U terminusdb_client you could check the version you are using by doing a pip list,@jbennettgit could -PRON- try upgrade to the late release version by pip install -u terminusdb_client -PRON- could check the version -PRON- be use by do a pip list,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,256,2021-03-17T16:16:38Z,jbennettgit,<Response 200> object returned from execute() call,https://github.com/terminusdb/terminusdb/issues/256,"Per Gavin, updated to dev server (see below) via docker container running on Ubuntu 18.04.  Also updated, via pip, the python terminusdb client version: woql.version. version = 0.6.1.
Unable to find image 'terminusdb/terminusdb-server:dev' locally
dev: Pulling from terminusdb/terminusdb-server
...
Digest: sha256:c963490d7ff605e3ddf1e94ea0df6f4f8c8ce7c6871714f9cbe61714c1b3f7db
Status: Downloaded newer image for terminusdb/terminusdb-server:dev
terminusdb-server container started https://127.0.0.1:6363/
Running our insertion_from_csv code, which worked previously, the 2nd and subsequent calls to execute() of a query return not a dict with bindings but a <Response 200> object which appears to be something from the intermediate http traffic with the server.
python3 insert_from_csv.py
/home/ubuntu/.local/lib/python3.6/site-packages/terminusdb_client/woqlview/woql_view.py:9: UserWarning: woqlview need to be used in Jupyter notebook.
warnings.warn(msg)
Parsing data from equinox.csv
Committing data for Code book i:6 d:0 0.30s << works once and result dict returns 6 inserts, no deletes
WARNING: Pre-casting '1790000:490000' from AfDurrn|Social Complexity variables|Social Scale|Polity territory to a range: likely ill-formed!
WARNING: Pre-casting '3:4' from AfDurrn|Social Complexity variables|Hierarchical Complexity|Settlement hierarchy to a range: likely ill-formed!
WARNING: Pre-casting '1:2' from AfDurrn|Social Complexity variables|Hierarchical Complexity|Religious levels to a range: likely ill-formed!
Execution ERROR while Committing data for AfDurrn after 1.11s -- skipped  << caught exception during 2nd execute()
/usr/lib/python3/dist-packages/apport/report.py:13: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
import fnmatch, glob, traceback, errno, sys, atexit, locale, imp, stat
Traceback (most recent call last):
File ""insert_from_csv.py"", line 39, in execute_commit
result = q.execute(client,commit_msg=msg)
File ""/home/ubuntu/.local/lib/python3.6/site-packages/terminusdb_client/woqlquery/woql_query.py"", line 448, in execute
return client.query(self, commit_msg, file_dict=file_dict)
File ""/home/ubuntu/.local/lib/python3.6/site-packages/terminusdb_client/woqlclient/woqlClient.py"", line 1025, in query
if result.get(""inserts"") or result.get(""deletes""):
AttributeError: 'Response' object has no attribute 'get'
During handling of the above exception, another exception occurred:
Traceback (most recent call last):
File ""insert_from_csv.py"", line 277, in 
assert_seshat_row(Polity, Variable, Value_From, Value_To, Date_From, Date_To, Fact_Type, Value_Note)
File ""insert_from_csv.py"", line 71, in assert_seshat_row
execute_commit(polity_query)
File ""insert_from_csv.py"", line 58, in execute_commit
print(f""{exception.msg}"")
Without dicts with bindings being consistently returned we are unable to continue development.",2021-03-19T17:40:35Z,jbennettgit,https://github.com/terminusdb/terminusdb/issues/256#issuecomment-803001181,"Thanks that did the trick.  The current client works with the dev server on my version now.  No more <Response> objects.
…
 On Mar 19, 2021, at 10:20 , Cheuk Ting Ho ***@***.***> wrote:


 @jbennettgit could you try upgrading to the latest released version by pip install -U terminusdb_client

 —
 You are receiving this because you were mentioned.
 Reply to this email directly, view it on GitHub, or unsubscribe.",thank that do the trick the current client work with the dev server on -PRON- version now no more < response > object … on mar 19 2021 at 1020 cheuk ting ho * * * @ * * * * * * > write @jbennettgit could -PRON- try upgrade to the late release version by pip install -u terminusdb_client — -PRON- be receive this because -PRON- be mention reply to this email directly view -PRON- on github or unsubscribe,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,256,2021-03-17T16:16:38Z,jbennettgit,<Response 200> object returned from execute() call,https://github.com/terminusdb/terminusdb/issues/256,"Per Gavin, updated to dev server (see below) via docker container running on Ubuntu 18.04.  Also updated, via pip, the python terminusdb client version: woql.version. version = 0.6.1.
Unable to find image 'terminusdb/terminusdb-server:dev' locally
dev: Pulling from terminusdb/terminusdb-server
...
Digest: sha256:c963490d7ff605e3ddf1e94ea0df6f4f8c8ce7c6871714f9cbe61714c1b3f7db
Status: Downloaded newer image for terminusdb/terminusdb-server:dev
terminusdb-server container started https://127.0.0.1:6363/
Running our insertion_from_csv code, which worked previously, the 2nd and subsequent calls to execute() of a query return not a dict with bindings but a <Response 200> object which appears to be something from the intermediate http traffic with the server.
python3 insert_from_csv.py
/home/ubuntu/.local/lib/python3.6/site-packages/terminusdb_client/woqlview/woql_view.py:9: UserWarning: woqlview need to be used in Jupyter notebook.
warnings.warn(msg)
Parsing data from equinox.csv
Committing data for Code book i:6 d:0 0.30s << works once and result dict returns 6 inserts, no deletes
WARNING: Pre-casting '1790000:490000' from AfDurrn|Social Complexity variables|Social Scale|Polity territory to a range: likely ill-formed!
WARNING: Pre-casting '3:4' from AfDurrn|Social Complexity variables|Hierarchical Complexity|Settlement hierarchy to a range: likely ill-formed!
WARNING: Pre-casting '1:2' from AfDurrn|Social Complexity variables|Hierarchical Complexity|Religious levels to a range: likely ill-formed!
Execution ERROR while Committing data for AfDurrn after 1.11s -- skipped  << caught exception during 2nd execute()
/usr/lib/python3/dist-packages/apport/report.py:13: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
import fnmatch, glob, traceback, errno, sys, atexit, locale, imp, stat
Traceback (most recent call last):
File ""insert_from_csv.py"", line 39, in execute_commit
result = q.execute(client,commit_msg=msg)
File ""/home/ubuntu/.local/lib/python3.6/site-packages/terminusdb_client/woqlquery/woql_query.py"", line 448, in execute
return client.query(self, commit_msg, file_dict=file_dict)
File ""/home/ubuntu/.local/lib/python3.6/site-packages/terminusdb_client/woqlclient/woqlClient.py"", line 1025, in query
if result.get(""inserts"") or result.get(""deletes""):
AttributeError: 'Response' object has no attribute 'get'
During handling of the above exception, another exception occurred:
Traceback (most recent call last):
File ""insert_from_csv.py"", line 277, in 
assert_seshat_row(Polity, Variable, Value_From, Value_To, Date_From, Date_To, Fact_Type, Value_Note)
File ""insert_from_csv.py"", line 71, in assert_seshat_row
execute_commit(polity_query)
File ""insert_from_csv.py"", line 58, in execute_commit
print(f""{exception.msg}"")
Without dicts with bindings being consistently returned we are unable to continue development.",2021-03-19T18:16:23Z,Cheukting,https://github.com/terminusdb/terminusdb/issues/256#issuecomment-803021276,You are welcome :-),-PRON- be welcome - ),0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,257,2021-03-17T20:49:28Z,sachinbhutani,create datatypes (xsd) for sequence (auto-increment integer) and uuid,https://github.com/terminusdb/terminusdb/issues/257,"Is your feature request related to a problem? Please describe.
A clear and concise description of what the problem is. Ex. I'm always frustrated when [...]
need auto increment sequence number
need auto generated uuid
Describe the solution you'd like
A clear and concise description of what you want to happen.
schema definition should allow to include a property which can be a unique sequence number (int or bigint) which can be auto-incremented on each insert.
Describe alternatives you've considered
A clear and concise description of any alternative solutions or features you've considered.
A query may be created to first read the highest id and then update new id with max id + 1, however it would not be possible to insert multiple records in a single query this way
use case for uuid: application may generate a uuid and then populate it into the database, or may require the database to generate a uuid, when application is creating a uuid it may be stored as a string, but there should also be an option for the uuid to be generated from the database and provided back to the application.
Additional context
Add any other context or screenshots about the feature request here.
It would be great if  an auto(increment)  function can be provided like cardinality, which can be attached to existing datatype positiveInteger or integer",,,,,,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,258,2021-03-19T17:11:54Z,Francesca-Bit,Console/hub-console - class frames/document interface - delete property button dosen't work,https://github.com/terminusdb/terminusdb/issues/258,"Describe the bug
when I try to delete a document property it dosen't work
To Reproduce
Steps to reproduce the behavior:

Go to 'document interface.'
Click on 'plus button'
choose a new document
remove a property using the ""-"" button
you can not add the property any more



if you have 2 property of the same type it remove both",2021-03-23T15:15:49Z,Cheukting,https://github.com/terminusdb/terminusdb/issues/258#issuecomment-804987513,@KittyJose is this done? Can I close it?,@kittyjose be this do can i close -PRON-,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,258,2021-03-19T17:11:54Z,Francesca-Bit,Console/hub-console - class frames/document interface - delete property button dosen't work,https://github.com/terminusdb/terminusdb/issues/258,"Describe the bug
when I try to delete a document property it dosen't work
To Reproduce
Steps to reproduce the behavior:

Go to 'document interface.'
Click on 'plus button'
choose a new document
remove a property using the ""-"" button
you can not add the property any more



if you have 2 property of the same type it remove both",2021-03-25T09:52:34Z,KittyJose,https://github.com/terminusdb/terminusdb/issues/258#issuecomment-806513255,"Fix in dev, will release to canary today :)",fix in dev will release to canary today ),0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,261,2021-03-21T00:00:14Z,matko,Allow delay of commit until a coordinator gives the go-ahead,https://github.com/terminusdb/terminusdb/issues/261,"When multiple components besides terminusdb need to update together, some sort of coordination is necessary to ensure that commits only happen when all components are able to do so. One of the algorithms to do this is the two-phase protocol, but there are a few other options around as well.
The common requirement to make any of them work is to have some concept of a commit in terminusdb which is only visible from one transaction, and then some way to make that transaction definite afterwards. We'll need to think this out a bit, but here's a few requirements I can think of now:

it has to be possible to open a transaction as a persistent connection.
the transaction goes through a bunch of stages:
-- initialized
-- pre-committed
-- finalized

From initialized, it should be possible to do any number of queries, the final one of which will be an update query making modifications, at which point the transaction goes to pre-committed.
When in pre-committed state, more read queries are allowed, returning the updated data, but any concurrent transaction on another connection should still see the old data. A final 'finalize' command should be given to make the changes permanent and finalize the transaction. Additionally, a 'rollback' could be given to forget about the pre-commit and undo any locks. Closing the transaction connection or crashing would effectively be the same thing.
While in pre-committed phase, no concurrent transaction should be allowed to write. (We could accomplish this by locking the relevant file labels, thus blocking any concurrent write transaction, but we need to give this a think). This creates a window between pre-committed and finalized where other components can run and do their own updates. After they're all done, the final go-ahead can be given to make changes permanent.
This should probably be enough basis to implement support for many common transaction coordinators, but that's again a thing to really look into.
This feature requests came up as part of a discussion in the community discord on how to coordinate between some amazon storage and terminusdb.",2021-03-21T00:36:33Z,matko,https://github.com/terminusdb/terminusdb/issues/261#issuecomment-803491334,"given issue terminusdb/terminusdb-client-python#182 it may actually be worthwhile to support long-lived transactions that allow multiple write commits as well. The main issue with this though would be that essentially, any concurrent writer needs to be blocked from doing a commit until the long-lived transaction is done, and this may be too much overhead. So we may not want to encourage this as the 'default' way of doing transactions.
Something to think about.",give issue terminusdb / terminusdb - client - python#182 -PRON- may actually be worthwhile to support long - live transaction that allow multiple write commit as well the main issue with this though would be that essentially any concurrent writer need to be block from do a commit until the long - live transaction be do and this may be too much overhead so -PRON- may not want to encourage this as the ' default ' way of do transaction something to think about,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,261,2021-03-21T00:00:14Z,matko,Allow delay of commit until a coordinator gives the go-ahead,https://github.com/terminusdb/terminusdb/issues/261,"When multiple components besides terminusdb need to update together, some sort of coordination is necessary to ensure that commits only happen when all components are able to do so. One of the algorithms to do this is the two-phase protocol, but there are a few other options around as well.
The common requirement to make any of them work is to have some concept of a commit in terminusdb which is only visible from one transaction, and then some way to make that transaction definite afterwards. We'll need to think this out a bit, but here's a few requirements I can think of now:

it has to be possible to open a transaction as a persistent connection.
the transaction goes through a bunch of stages:
-- initialized
-- pre-committed
-- finalized

From initialized, it should be possible to do any number of queries, the final one of which will be an update query making modifications, at which point the transaction goes to pre-committed.
When in pre-committed state, more read queries are allowed, returning the updated data, but any concurrent transaction on another connection should still see the old data. A final 'finalize' command should be given to make the changes permanent and finalize the transaction. Additionally, a 'rollback' could be given to forget about the pre-commit and undo any locks. Closing the transaction connection or crashing would effectively be the same thing.
While in pre-committed phase, no concurrent transaction should be allowed to write. (We could accomplish this by locking the relevant file labels, thus blocking any concurrent write transaction, but we need to give this a think). This creates a window between pre-committed and finalized where other components can run and do their own updates. After they're all done, the final go-ahead can be given to make changes permanent.
This should probably be enough basis to implement support for many common transaction coordinators, but that's again a thing to really look into.
This feature requests came up as part of a discussion in the community discord on how to coordinate between some amazon storage and terminusdb.",2021-03-21T00:41:20Z,Cheukting,https://github.com/terminusdb/terminusdb/issues/261#issuecomment-803491759,"I think it is not just a problem for the Python client I agree. There should be some mechanism to lock concurrent update access to the database I think.
Unless it's a read-only mode we will have issue if there can be multiple clients accessing the same database at the same time.",i think -PRON- be not just a problem for the python client i agree there should be some mechanism to lock concurrent update access to the database i think unless -PRON- be a read - only mode -PRON- will have issue if there can be multiple client access the same database at the same time,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,261,2021-03-21T00:00:14Z,matko,Allow delay of commit until a coordinator gives the go-ahead,https://github.com/terminusdb/terminusdb/issues/261,"When multiple components besides terminusdb need to update together, some sort of coordination is necessary to ensure that commits only happen when all components are able to do so. One of the algorithms to do this is the two-phase protocol, but there are a few other options around as well.
The common requirement to make any of them work is to have some concept of a commit in terminusdb which is only visible from one transaction, and then some way to make that transaction definite afterwards. We'll need to think this out a bit, but here's a few requirements I can think of now:

it has to be possible to open a transaction as a persistent connection.
the transaction goes through a bunch of stages:
-- initialized
-- pre-committed
-- finalized

From initialized, it should be possible to do any number of queries, the final one of which will be an update query making modifications, at which point the transaction goes to pre-committed.
When in pre-committed state, more read queries are allowed, returning the updated data, but any concurrent transaction on another connection should still see the old data. A final 'finalize' command should be given to make the changes permanent and finalize the transaction. Additionally, a 'rollback' could be given to forget about the pre-commit and undo any locks. Closing the transaction connection or crashing would effectively be the same thing.
While in pre-committed phase, no concurrent transaction should be allowed to write. (We could accomplish this by locking the relevant file labels, thus blocking any concurrent write transaction, but we need to give this a think). This creates a window between pre-committed and finalized where other components can run and do their own updates. After they're all done, the final go-ahead can be given to make changes permanent.
This should probably be enough basis to implement support for many common transaction coordinators, but that's again a thing to really look into.
This feature requests came up as part of a discussion in the community discord on how to coordinate between some amazon storage and terminusdb.",2021-03-22T10:47:49Z,matko,https://github.com/terminusdb/terminusdb/issues/261#issuecomment-803965606,"An alternative strategy could be that we introduce the concept of transaction branches - branches tracked in a special local repository that are associated with some transaction ID. This would allow the whole system to be a lot more stateless - rather than keeping a connection open, queries could send along a transaction ID and any server node should be able to locate the transaction branch and work with it.
There are (at least) two complications to solve here:

how exactly do we merge back this branch when the transaction is done? If we're lucky, this is just a fast-forward, but other stuff could have happened inbetween. We are currently not clever enough to auto-merge, so it may be the case that we need to simply fail any multi-commit transaction if other commits happened in the meantime. In that case, clients need to be prepared to handle failing commits.
How do we clean up transactions? Ideally transactions get auto-cleaned up when they close, but clients may disappear unexpectedly. So we need some sort of garbage collection that cleans up stale transactions.

I think this would be better than any scheme that uses file locks, as for simplicity in the concurrent case, optimism is almost always better, as it requires a lot less communication between nodes.",an alternative strategy could be that -PRON- introduce the concept of transaction branch - branch track in a special local repository that be associate with some transaction -PRON- d this would allow the whole system to be a lot more stateless - rather than keep a connection open query could send along a transaction -PRON- d and any server node should be able to locate the transaction branch and work with -PRON- there be ( at least ) two complication to solve here how exactly do -PRON- merge back this branch when the transaction be do if -PRON- be lucky this be just a fast - forward but other stuff could have happen inbetween -PRON- be currently not clever enough to auto - merge so -PRON- may be the case that -PRON- need to simply fail any multi - commit transaction if other commit happen in the meantime in that case client need to be prepared to handle fail commit how do -PRON- clean up transaction ideally transaction get auto - clean up when -PRON- close but client may disappear unexpectedly so -PRON- need some sort of garbage collection that clean up stale transaction i think this would be well than any scheme that use file lock as for simplicity in the concurrent case optimism be almost always well as -PRON- require a lot less communication between node,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,262,2021-03-21T01:14:31Z,matko,Fail early when loading a database that will cause the process to run out of memory,https://github.com/terminusdb/terminusdb/issues/262,"There are currently no safeguards in place that will stop one from trying to load a database that is too big. This can lead to swapping and out-of-memory crashes. It would be better if TerminusDB caught such an attempt early and returned an error back to the user, rather than actually trying to load the database.",,,,,,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,263,2021-03-22T12:39:52Z,matko,"When retrying commit queries, don't rebuild layers when it is not necessary",https://github.com/terminusdb/terminusdb/issues/263,"Currently, whenever a commit query fails, we'll attempt a retry up to four times. During this retry, we rerun the entire query.
This potentially does way too much work. We only need to rerun the bits that will result in different layers.
For ordinary branch commits, there are three cases to consider:

We are modifying a branch, and the same branch was modified concurrently. In this case, everything needs to be rebuilt.
We are modifying a branch, and the underlying commit graph was modified concurrently without modifying this branch (for example, a concurrent commit in another branch). In this case our actual commit layers are fine, but the commit and metadata graphs need to be rebuilt.
We are modifying a branch, and the underlying metadata graph was modified concurrently without modifying our repository (for example, a fetch happened, or someone updated the default prefixes). In this case we only need to rebuild the metadata graph.

So it would be good if we could be clever about this and limit the amount of work we actually end up doing. If we have already constructed our actual commit layers, retrying can be super quick and limit the opportunity for another race to happen.",,,,,,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,264,2021-03-22T12:45:16Z,matko,Allow queries to submit an expected commit id and fail when it does not match,https://github.com/terminusdb/terminusdb/issues/264,"When writing client code, it can be useful at times to be really sure that a query is executed in the state we expect a branch to be in, and to error when the database is actually in another state. I propose we add an optional parameter to the woql endpoint, commit_id. If the current commit does not match what is in commit_id, the server is to send back an error saying so, along with the newest commit id of the branch.
This would obviously only be relevant for queries on branches. We may wish to consider similar mechanisms for queries on the commit graph and metadata graph, but this is less important.
This is the dual of #248.",,,,,,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,265,2021-03-22T13:41:31Z,matko,Initialize a new database from a (remote) query,https://github.com/terminusdb/terminusdb/issues/265,"A pattern that often comes up when working with data is that the user will make a selection of data they want to work with, and then do further querying on just that data. It would be good if we could support this use case better by being able to create a new database from a woql query.
The way I imagine we can do this is have a way to construct a pack directly from a query. A pack is the thing we send over the wire when people clone a database, so if we were to construct one of these directly, we could support both the use case where we create a database locally, but more importantly, we'd also support the use case where a remote worker node queries for the selection to do further local processing.",,,,,,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,266,2021-03-22T13:44:18Z,matko,Run terminusdb in nonpersistent mode,https://github.com/terminusdb/terminusdb/issues/266,"For some use cases we don't actually need persistency at all. This is the case for example when we spawn short-lived instances of terminusdb that are just meant to do some processing.
The storage backend actually supports full in-memory mode, but we don't currently expose this in the server. We could easily support this though with some startup flag.",,,,,,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,267,2021-03-22T14:44:14Z,kaaloo,Can clone from self hosted instance but can't push changes back,https://github.com/terminusdb/terminusdb/issues/267,"Describe the bug
The TerminusDB CLI clone command can be used to clone a database that has been created on a self hosted instance onto a local store.  When attempting to push local changes back to the self hosted instance using the CLI the following stack trace occurs:
❯ ./terminusdb push admin/test --user admin --password XXXXXXXXXXXXXXXXXX
  [16] throw(error(remote_unpack_unexpected_failure(...),_23620))
  [14] catch(db_push:call(...,""https://terminusdb.mydomain.com/admin/test"",""5b609bcca4468bbc65850d3cfbb81a0300428bc4\037\\213\\b\000\\000\\000\\000\\000\\000\ÿí\235\O\214\ãV\035\ÇßL[Tº¨ì\201\\vâ\222\î\200\VTÚØïùo\212\\204\VÚ\025\\034\Ø\226\\225\ÊÂ\212\Ãz\237\íç\211\\213\cG¶³Ù\024\!¥\b\211\\vB\b©§\n\004\\207\å\200\\004\í\215\\vH qCê\221\\003\§^¹TU¥J=uû\222\\231\I&;\231\µýâ¼nú~\237\Ãä9\233\\211\g6óÉï÷}ös,ßÖ{~\020\PÓ´]ß\017\lËµôÐ\b\""ßw1Õ\r]7\211\ë\a&Ú\000\þ\034\ºãXüVÇ\216\¥\237\¾=\001\aS'6&¦iØü~lÚ6A\026\\222\À¨(iÞé\240\M9ùENnw\004\«æë¯¥YÈ¼0\016\Ê8Ki>ñü$\v~Rt\207\QP¹\017\\035\ëºm\233\ç¾þ\206\ýèëO\b\236\Ý\""\t(þú÷Ërø\222\¦\225\,\037\Äé¨\býn\220\\r´\""è³\001\Õ\022\:aùÁ\215\ÙW4]>\204\?^ã\217\\032\Äe¡\205\´¤Úws:ì{é½×]\032\$d\022\¸vÏ\214\í¬(¢×Ë¼_ö\222\Ðì\a^\234\òÿê4`Þ\200\Æ)ú×Ñ>\216\6Þ\232\ïÃó\035\\235\ÿáa3`.\211\LÝ¥\206\oº\030\\023\7Ð\003\\203\P#4\230\o`ÝGoý\220\&qxmþ3Tíwå×}\006\\001\+\bû\237\EQÁÊ¢\233\d\207\4Ïéä1û¨ò\237\ÿû£þ\033\\206\CÀ\177\Iì!@Ujû?Ì\031\·\237\\226\\002\M@\225\ÿØ8ë?6Mð_\002\5êÿü«\027\\207\èw9\213\\016\\216\\n¿7¤9KKôÇÃyé\237\?äôÓ>\205\\200\\235\`3ÿë5\001\\002\õß4u\vü\227\\004\Ô\177\u©íÿ=\232\\214\\004\'\000\ªü·\234\³ùßà=\001\ø¿}.a\033\Û&ï·,Ò5g#\a_ºsçòq[0\036\\217\»c£\233\å\207\\032\á/\232\vûå\033\¯Î[\203\\203\\220\\005\ñ\200\&\227\Ñ\033\·\206\|\016\\200\u¢<\033\t\006\<$&\035\\177\\024\'!Ëk=OQæqzÈ\237\¦nöoö¬É(.®Æi\220\\214\\212\8êÒ¸ÙwWÌ,4z²U\236\\230\y\bqÿëO\000\\210\ä\177\ËÆà¿$\240\þ«Kýþ\177\\036\øº}v\037\5¥Ò\177\ëÌñ\037\Û\201\ú/\005\»çDNÈÿ\006\\234\0\b\034\ÛéÙ\216\ïØ,ÂØ%\226\ÓãÝ\001\%nÏ7\020\ð¹¤¾ÿYá\025\ÞÐ£ák4`i0ñ\222\¸à\a_F\203\Ê\026\\240\Òÿ³ó\177\³\216\\024\ü\227\\000\ýÁ­«\177\ýæ¯\177\ñ\227\·\027\w]\204\\203\dê°©ÿ>?\bÜå_6ò\237\¬ñ_\207\ù\177\\031\<Xmþ/\""@)Zðß\213\Ó\220\Ý?\231\\017\\\Ó\v\bøo[\026\ôÿR¸¹\030\í}\t\001\ªÑ\236\ÿÅ¹o\000\\""þ;\206\\001\þËcÖòï]E\200\j4ó\177\èe\002\\023\\000\\""ùßÄPÿeð\237\\213\ÿöÿ\236\ýì\037\?þ`q\027\ä\177\\205\ØØÿ\032\\023\\000\\""ùß°¡þËàáÃ\225\MÈÿ\212\Ñ\206\ÿU\023\\000\Bý?qÀ\177\\t\234\\232\þ\203\ü¯ -ú\177\î\004\\200\\200\ÿ\016\Æ°þG\""³º\017\ù_A\032\ù\237\yÃ\002\òÿzvÔÿ¯^¿þÏ÷'?ÿï\vß[Ü\005\ù_!6ö\037\òÿ\021\;ê¿\aù_iÚð\037\ò?ÚÝþÿ½Å\bò¿\202\´è?äÿ]ôÿ\210\ç\021\ä\177\%iäÿò\032\\000\cz\217\%¬ôÊ\234\±Ê\b \220\ÿ-Ý\201\ë\177\Éàiôço\177\ò\235\/¿øÑ\213\+wßD\200\\022\´äÿc#\200\Hýç\023\\200\à¿\004\ö¯~q1\204\þ_=Úõ\177\}\004\\020\ñß&°þG\""_GÐÿ+Iýë\177\²Ã­­ÿ]»þ\017\\216\ÿÉàîr¸\aÇýÔcSÿ·µþ\027\[Pÿe0]Ý\204\K\001\)F\vþogý/\201\ú/\205\å\021\\177\8þ§ íùßòú_\035\Îÿ\221\È¬îCþW\220\fþoiýïºóÿ,8ÿG\006\Óå\020\ò¿\202\lìÿ¶Îÿ\205\ë\177\Haºº\tù_1Úð\177\\033\çÿÚ\016\Ô\177\)@þW\232\\026\ýoõü_Ý\206\ó\177\%\002\ù_Q\032\ù¿­õ¿\220\ÿ?3¦Ëá\036\4ÿê±±ÿ\220\ÿ\217\Ø}ÿgÀ[\200\b´á?ä\177\\004\ù\037\ØIZô\037\òÿ.ú\177\\004\ä\177\Eiäÿ¶Öÿ\222\5ë\177\Møüo\031\Ü]Ý\204\S\000\\024\£%ÿÛ_ÿ\213\aý¯\024\:\213\\021\ôÿ\nÒ®ÿí­ÿµ\034\Xÿ#\221\}\004\ý¿\222\Ôö?èÇIèÍ¯\002\>ò_cAY¹ìw\201\\200\ÿÄ0àú?2øÆÛ¿\232\\036\\017\\237\\201\æ_=\004\üÏ\032\ê/rý/\202\Mð_\006\×¾öÜÿ>~÷oï~¼¼ëyx\037\P\207\\206\þÏÏ\002\\224\Rÿuð_\006\Óåpï)\004\¨\206\\200\ÿí×ÿ5þc\fóÿ2\230\.\207\°þOA|Gç®c3`.\211\LÝ¥\206\oº\030\\023\7Ð\003\\203\P#4\230\o`ÝG\033\ÀßCtÇ±Î÷__úoØ³ûy÷oêÈB\022\PÜÿº¯¿\226\f!óøìo\031\g)Í''\207\z\206\QP¹\217\ª÷\177\b>úúÏÞÿáø\217\\fúe9|IÓJ\226\\017\âtT\204\~7È\006\Z\021\ôÙ\200\jÅ¤(Ùààz\026\\214\\006\,-Ñ/Çãqwlt³üP#üEÔn¿|ãÕùC\017\\212\2\217\ÓCô&ÑtGËÆÉÁµ\204\\026\\005\úýuZÒr2d7ólÈòr\202\¦Ë]ñý\036\ïéàVÁrôfJ\alõÇû\002\\002\¶\212\°ÿY\024\\025\¬^\027\XÙÿá3þ\033\\206\\005\×\177\\222\\005\\234\ô­.µý_\036\ûmÜ\004\Tù\217\í³þc\002\ùO\006\ÇõÿTaÇ½^OÓ\211\FÈ\225\<\214\®\024\\223\´¤÷¯¤ÅÁ¬\214\£ßòº¯k¼ôÏÿí¨x\207\Ù\200\Æ)z\220\P\237\%èANÓC\206\\036\ðiÂy\017\ðýèÔî b>alæ\177\½&@\240\þÃç?J\004\ê¿ºÔöÿ\036\MF\202\\023\\000\\002\õ\237\\037\ÿ\201\ü/\203\K¯ðÌ}éÎ\235\Ëg\033\\201\µ\tÿ2zc\226\Õ\033\}Ç)ö\021\ðD!î\177\ý\t\000\\221\üo\031\°þS\026\PÿÕ¥¶ÿ>-\230\à\aÀ49þ»èÿ1ø/\205\ýÿ¿r<º\000\ç\177\¨ÇÆþ·|ý\227\\023\ÿ\211\\003\ù_\006\\037\vV6/ @)Úð¿Íë¿\034\ûoÛÐÿËáîb\004\ë?\025\¤Eÿ[¹þË±ÿ\016\\237\\002\\004\ÿå1;Ï\006\Ö\177\*HCÿ\205\>\000\F¤ÿ7\035\Xÿ!\a÷+?êÌ\a\027\\236\F\200\jlî\177\õ\004\\200\\220\ÿ\026\Ô\177\\031\<|oe\023\ò¿b´â\177\Å\004\\200\Pÿ\217\¡þKá\235\Å\bò¿\202\´éÿy\023\\000\\""þ\023\8ÿG&³º\017\ù_A\232\ù/ö\001\0\220\ÿ\237\\\~zû7\177\:\036\>\vù_=6÷\037\òÿ\234\]Íÿ«\233\Ï\""@)Zñ\037\òÿîöÿÓÅ\bò¿\202\´é?äÿ\235\Íÿ³º\017\ù_A\232\ù/ö\001\0\002\þ[Ø\206\ë¿Ë`¿óÎ·Nm¾\200\\000\¥hËÿÇE\000\\221\úo\031\pý\037\)üa1\202\þ_AZö\177\m\004\\020\ñßÁ°þG\""Ï!èÿ\001\\000\\000\\224\âSÚèm\026\\000\\000\\001\\000\""),error(unhandled_status_code(500,'<!DOCTYPE html>\n<html>\n<head>\n<title>500 Internal server error</title>\n\n<meta http-equiv=""content-type"" content=""text/html; charset=UTF-8"">\n\n</head>\n<body>\n\n<h1>Internal server error</h1>\n\n<p>\ngoal unexpectedly failed: routes:auth_wrapper(tus_dispatch,[protocol(https),peer(ip(100,64,0,100)),pool(client(httpsd6363,server:http_dispatch,&lt;stream&gt;(0x55a866e46800),&lt;stream&gt;(0x55a866ceee00))),input(&lt;stream&gt;(0x55a866e46800)),method(post),request_uri(\'/api/files\'),path(\'/api/files\'),http_version(1-1),host(\'terminusdb.mydomain.com\'),x_request_id(\'4116fd5fc0860cf1ac65ca6b90632835\'),x_real_ip(\'193.32.126.222\'),x_forwarded_for(\'193.32.126.222\'),x_forwarded_host(\'terminusdb.mydomain.com\'),x_forwarded_port(\'443\'),x_forwarded_proto(https),x_scheme(https),user_agent(\'SWI-Prolog\'),upload_length(\'2222\'),upload_metadata(\'filename L3RtcC9zd2lwbF82NzEzMV8xNw==\'),tus_resumable(\'1.0.0\'),authorization(\'Basic XXXXXXXXXXXXXXXXXXXXXXXXX==\')])</p>\n\n<address>\n<a href=""http://www.swi-prolog.org"">SWI-Prolog</a> httpd at terminusdb-deployment-cbfd9698d-jq2mt\n</address>\n\n</body>\n</html>\n'),_23678),db_push:(...;...)) at /usr/lib/swi-prolog/boot/init.pl:483
  [13] db_push:push('<garbage_collected>','terminusdb:///system/data/admin','admin/test',""origin"",main,'<garbage_collected>','<garbage_collected>',_23746) at /home/luis/Code/terminusdb/terminusdb/src/core/api/db_push.pl:136
  [12] catch(cli:push(...,'terminusdb:///system/data/admin','admin/test',""origin"",main,...,...,_23804),error(remote_unpack_unexpected_failure(...),context(_23818,_23820)),cli:do_or_die(...,...)) at /usr/lib/swi-prolog/boot/init.pl:482
  [11] catch_with_backtrace('<garbage_collected>','<garbage_collected>','<garbage_collected>') at /usr/lib/swi-prolog/boot/init.pl:532

Note: some frames are missing due to last-call optimization.
Re-run your program in debug mode (:- debug.) to get more detail.
Error: remote_unpack_unexpected_failure(error(unhandled_status_code(500,'<!DOCTYPE html>\n<html>\n<head>\n<title>500 Internal server error</title>\n\n<meta http-equiv=""content-type"" content=""text/html; charset=UTF-8"">\n\n</head>\n<body>\n\n<h1>Internal server error</h1>\n\n<p>\ngoal unexpectedly failed: routes:auth_wrapper(tus_dispatch,[protocol(https),peer(ip(100,64,0,100)),pool(client(httpsd6363,server:http_dispatch,&lt;stream&gt;(0x55a866e46800),&lt;stream&gt;(0x55a866ceee00))),input(&lt;stream&gt;(0x55a866e46800)),method(post),request_uri(\'/api/files\'),path(\'/api/files\'),http_version(1-1),host(\'terminusdb.mydomain.com\'),x_request_id(\'4116fd5fc0860cf1ac65ca6b90632835\'),x_real_ip(\'193.32.126.222\'),x_forwarded_for(\'193.32.126.222\'),x_forwarded_host(\'terminusdb.mydomain.com\'),x_forwarded_port(\'443\'),x_forwarded_proto(https),x_scheme(https),user_agent(\'SWI-Prolog\'),upload_length(\'2222\'),upload_metadata(\'filename L3RtcC9zd2lwbF82NzEzMV8xNw==\'),tus_resumable(\'1.0.0\'),authorization(\'Basic XXXXXXXXXXXXXXXXXXXXXXXX==\')])</p>\n\n<address>\n<a href=""http://www.swi-prolog.org"">SWI-Prolog</a> httpd at terminusdb-deployment-cbfd9698d-jq2mt\n</address>\n\n</body>\n</html>\n'),_23564))
To Reproduce
Prerequisites:

a self-hosted instance on your domain
set the TERMINUSDB_SERVER env var as follows so that the CLI and Desktop use the same store


TERMINUSDB_SERVER_DB_PATH=$HOME/.terminusdb/db

Steps to reproduce the behavior:

Create a new database on the self-hosted instance
Clone the database locally using the TerminusDB CLI as follows:


./terminusdb clone https://terminusdb.mydomain.com/admin/test admin/test --user admin --password XXXXXXXXXXXXXX


Open the TerminusDB Desktop and add a triplet to the test database
Push the changes back to the self-hosted instance using the following:


./terminusdb push admin/test --user admin --password XXXXXXXXXXXXXX

Expected behavior
The push operation should succeed and the added local updates should be visible on the self-hosted instance.
Info (please complete the following information):

OS: self-hosted instance using the terminusdb/terminusdb-server:v4.2.0 docker image, local CLI compiled locally on Ubuntu 20.10 master branch commit f7082a4
SWI-Prolog version 8.2.1 for x86_64-linux",2021-07-12T13:36:04Z,spl,https://github.com/terminusdb/terminusdb/issues/267#issuecomment-878285632,"Sorry for the late response. Thanks for the report. We're aware of the problem. We're currently working on a new release, and we'll revisit this after that.",sorry for the late response thank for the report -PRON- be aware of the problem -PRON- be currently work on a new release and -PRON- will revisit this after that,1,1
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,268,2021-03-22T14:58:37Z,matko,Query for when a particular triple or object was last updated,https://github.com/terminusdb/terminusdb/issues/268,"We need woql words to query for when a change happened. A change can be either on the level of a single triple, or of an entire object, so we probably need two woql words.",,,,,,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,269,2021-03-22T15:33:42Z,matko,Be more selective about indexing,https://github.com/terminusdb/terminusdb/issues/269,"We build full indexes for every layer that is constructed, even when that makes absolutely no sense.
Reasons why this may make no sense:

Very small layer (linear search is always going to be faster anyway)
Layer will never be queried (if we squash later)
We only ever query in one particular direction (for example, we never touch the predicate index)

Building indexes takes time, and all that time is spent not doing useful things. We should avoid doing that.
Possible solutions:

Index lazily, when a query needs it. Disadvantage is potentially slow initial query time
Index by user request
Index according to some heuristics",,,,,,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,270,2021-03-22T15:36:18Z,matko,Single file layer format,https://github.com/terminusdb/terminusdb/issues/270,"Currently layers are stored in directories as a lot of tiny files. This creates quite a bit of overhead, especially for small layers, as file sizes have to be a multiple of 4 kilobytes on most file systems. We should move towards a single file for all layer datastructures.
We should probably exclude the rollup file from this. For ergonomics it may also be useful to keep the parent file separate, but this is not strictly necessary.",2021-03-23T12:56:59Z,matko,https://github.com/terminusdb/terminusdb/issues/270#issuecomment-804881290,This is actually a duplicate of terminusdb/terminusdb-store#11 so I'm closing it here.,this be actually a duplicate of terminusdb / terminusdb - store#11 so -PRON- be close -PRON- here,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,271,2021-03-23T11:16:33Z,matko,Better error page when store is uninitialized,https://github.com/terminusdb/terminusdb/issues/271,"When opening the console when the store has not been initialized, we get an error page with the following contents:
{
  ""@type"":""api:ErrorResponse"",
  ""api:error"": {""@type"":""api:APIEndpointFailed""},
  ""api:message"":""Failed to run the API endpoint goal console_handler"",
  ""api:status"":""api:failure""
}

This is no good. It should tell what the actual problem is, namely that the store has been uninitialized. And since we're trying to open the console here, it should show that as a nice HTML page and not as a JSON error.",,,,,,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,274,2021-03-24T11:43:42Z,GavinMendelGleason,Add metadata somewhere in order to find database creation time,https://github.com/terminusdb/terminusdb/issues/274,"Is your feature request related to a problem? Please describe.
We currently do not have an easy way to discover the creation time of a database. Instead we scan all commits in the commit graph looking for one with no parent and then use the commit time stamp.
Describe the solution you'd like
Something that is O(1)
Describe alternatives you've considered
We could either put in a tag on first commit in the commit graph, or potentially to put a timestamp in the repo graph or maybe a timestamp on the system graph.",,,,,,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,277,2021-03-25T00:14:58Z,Cheukting,Path in commit graph gives duplicate binding back,https://github.com/terminusdb/terminusdb/issues/277,"Describe the bug
I run the following query:
WOQLQuery()
            .using(""_commits"")
            .limit(limit_history)
            .select(
                ""v:cid"",
                ""v:author"",
                ""v:message"",
                ""v:timestamp"",
                ""v:cur_cid"",
                ""v:cur_author"",
                ""v:cur_message"",
                ""v:cur_timestamp"",
            )
            .triple(""v:branch"", ""ref:branch_name"", self.checkout())
            .triple(""v:branch"", ""ref:ref_commit"", ""v:commit"")
            .path(
                ""v:commit"",
                ""ref:commit_parent+"",
                ""v:target_commit"",
                ""v:path"",
            )
            .triple(""v:target_commit"", ""ref:commit_id"", ""v:cid"")
            .triple(""v:target_commit"", ""ref:commit_author"", ""v:author"")
            .triple(""v:target_commit"", ""ref:commit_message"", ""v:message"")
            .triple(""v:target_commit"", ""ref:commit_timestamp"", ""v:timestamp"")
            .triple(""v:commit"", ""ref:commit_id"", ""v:cur_cid"")
            .triple(""v:commit"", ""ref:commit_author"", ""v:cur_author"")
            .triple(""v:commit"", ""ref:commit_message"", ""v:cur_message"")
            .triple(""v:commit"", ""ref:commit_timestamp"", ""v:cur_timestamp"")
        )

And the binding coming back is giving duplicate records.
Found while running test_happy_path in Python client",2021-04-12T15:16:23Z,Cheukting,https://github.com/terminusdb/terminusdb/issues/277#issuecomment-817897357,Fixed in #294,fix in # 294,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,278,2021-03-25T00:28:22Z,Cheukting,Test issue for update auto-label,https://github.com/terminusdb/terminusdb/issues/278,,2021-03-25T00:34:42Z,Cheukting,https://github.com/terminusdb/terminusdb/issues/278#issuecomment-806272645,"It failed, seems an issue with the bot, not the setup. Already open an issue there: Renato66/auto-label#25",-PRON- fail seem an issue with the bot not the setup already open an issue there renato66 / auto - label#25,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,279,2021-03-25T08:44:51Z,andeplane,Can't find the cli tool on macOS,https://github.com/terminusdb/terminusdb/issues/279,"Describe the bug
I'm on macOS 10.15.7 and was going to check out terminusdb, and installed it by downloading .dmg and moving the executable into /Applications. I then wanted to work with the cli tool, but can't find any reference to how to install it / where it is if it is in my current installation. The README points to https://github.com/terminusdb/terminusdb/blob/master/docs/CLI.md but it is not mentioned how i get the terminusdb executable in the first place.
Seems like the code is in src/cli/main.pl, but my only attempt perl main.pl didn't work, so gave up :D
To Reproduce

Be on macOS
Download and install TerminusDB by copying .app into /Applications
Start application
Open terminal and try to run terminusdb

Expected behavior
At least some documentation on how to get the cli, or the app automatically symlink it or something.
Info (please complete the following information):
macOS 10.15.7",2021-03-25T09:30:59Z,rrooij,https://github.com/terminusdb/terminusdb/issues/279#issuecomment-806500053,"There is no compiled binary yet for MacOS unfortunately. You can build one yourself by following the instructions at: https://github.com/terminusdb/terminusdb/blob/master/docs/BUILD.md#mac-os
Unfortunately, getting a static SWI Prolog build on MacOS is a lot of work which is required for getting a decent static binary.
We don't use Perl by the way. Prolog files and Perl files share the same file extension.",there be no compile binary yet for macos unfortunately -PRON- can build one -PRON- by follow the instruction at https//githubcom / terminusdb / terminusdb / blob / master / doc / buildmd#mac - os unfortunately get a static swi prolog build on macos be a lot of work which be require for get a decent static binary -PRON- do not use perl by the way prolog file and perl file share the same file extension,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,279,2021-03-25T08:44:51Z,andeplane,Can't find the cli tool on macOS,https://github.com/terminusdb/terminusdb/issues/279,"Describe the bug
I'm on macOS 10.15.7 and was going to check out terminusdb, and installed it by downloading .dmg and moving the executable into /Applications. I then wanted to work with the cli tool, but can't find any reference to how to install it / where it is if it is in my current installation. The README points to https://github.com/terminusdb/terminusdb/blob/master/docs/CLI.md but it is not mentioned how i get the terminusdb executable in the first place.
Seems like the code is in src/cli/main.pl, but my only attempt perl main.pl didn't work, so gave up :D
To Reproduce

Be on macOS
Download and install TerminusDB by copying .app into /Applications
Start application
Open terminal and try to run terminusdb

Expected behavior
At least some documentation on how to get the cli, or the app automatically symlink it or something.
Info (please complete the following information):
macOS 10.15.7",2021-03-25T09:57:22Z,Cheukting,https://github.com/terminusdb/terminusdb/issues/279#issuecomment-806516078,Maybe we can have a simple wrapper in the Python client which you can use cli with the Python client.,maybe -PRON- can have a simple wrapper in the python client which -PRON- can use cli with the python client,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,279,2021-03-25T08:44:51Z,andeplane,Can't find the cli tool on macOS,https://github.com/terminusdb/terminusdb/issues/279,"Describe the bug
I'm on macOS 10.15.7 and was going to check out terminusdb, and installed it by downloading .dmg and moving the executable into /Applications. I then wanted to work with the cli tool, but can't find any reference to how to install it / where it is if it is in my current installation. The README points to https://github.com/terminusdb/terminusdb/blob/master/docs/CLI.md but it is not mentioned how i get the terminusdb executable in the first place.
Seems like the code is in src/cli/main.pl, but my only attempt perl main.pl didn't work, so gave up :D
To Reproduce

Be on macOS
Download and install TerminusDB by copying .app into /Applications
Start application
Open terminal and try to run terminusdb

Expected behavior
At least some documentation on how to get the cli, or the app automatically symlink it or something.
Info (please complete the following information):
macOS 10.15.7",2021-03-25T10:34:05Z,andeplane,https://github.com/terminusdb/terminusdb/issues/279#issuecomment-806539778,"Ahh it's prolog, that explains absolutely everything :D I see that GitHub also shows this (98.9% Prolog), but didn't notice.
I can work in a linux VM to explore this anyway, so no worries. I'd appreciate something about it in the readme :) Anyway, the database looks awesome. I will explore it for my company where we need version controlled data + files as a part of our services.",ahh -PRON- be prolog that explain absolutely everything d i see that github also show this ( 989 % prolog ) but do not notice i can work in a linux vm to explore this anyway so no worry -PRON- 'd appreciate something about -PRON- in the readme ) anyway the database look awesome i will explore -PRON- for -PRON- company where -PRON- need version control datum + file as a part of -PRON- service,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,279,2021-03-25T08:44:51Z,andeplane,Can't find the cli tool on macOS,https://github.com/terminusdb/terminusdb/issues/279,"Describe the bug
I'm on macOS 10.15.7 and was going to check out terminusdb, and installed it by downloading .dmg and moving the executable into /Applications. I then wanted to work with the cli tool, but can't find any reference to how to install it / where it is if it is in my current installation. The README points to https://github.com/terminusdb/terminusdb/blob/master/docs/CLI.md but it is not mentioned how i get the terminusdb executable in the first place.
Seems like the code is in src/cli/main.pl, but my only attempt perl main.pl didn't work, so gave up :D
To Reproduce

Be on macOS
Download and install TerminusDB by copying .app into /Applications
Start application
Open terminal and try to run terminusdb

Expected behavior
At least some documentation on how to get the cli, or the app automatically symlink it or something.
Info (please complete the following information):
macOS 10.15.7",2021-06-29T11:30:39Z,spl,https://github.com/terminusdb/terminusdb/issues/279#issuecomment-870514660,"@andeplane Do you use Homebrew? I'm also a macOS user, and I think it would be useful to have the CLI available through Homebrew. From my experience, it's a bit of a pain to install and use a CLI with a .dmg or .app, whereas, with Homebrew, you just brew install terminusdb-cli (or whatever it might be named) and brew upgrade to get the latest.",@andeplane do -PRON- use homebrew -PRON- be also a macos user and i think -PRON- would be useful to have the cli available through homebrew from -PRON- experience -PRON- be a bit of a pain to install and use a cli with a dmg or app whereas with homebrew -PRON- just brew install terminusdb - cli ( or whatever -PRON- may be name ) and brew upgrade to get the late,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,280,2021-03-25T15:04:30Z,KittyJose,feature to query latest updated/ created documents ,https://github.com/terminusdb/terminusdb/issues/280,feature to query latest updated/ created documents,2021-03-25T15:09:23Z,matko,https://github.com/terminusdb/terminusdb/issues/280#issuecomment-806920213,This is a duplicate of #268 so I'm closing this.,this be a duplicate of # 268 so -PRON- be close this,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,281,2021-03-27T11:36:32Z,bionicles,Possible to add more (pluggable) QLs?,https://github.com/terminusdb/terminusdb/issues/281,"Is your feature request related to a problem? Please describe.
This looks really interesting but I would want to be able to use legit graph query language such as openCypher, gremlin, datalog
Describe the solution you'd like
(terminus:DB)-[:supports]->(gql:Query language)
Note the SQL standard is evolving to eventually include GQL (that is NOT GraphQL) so if Terminus began to support a reasonably close dialect of openCypher then it could eventually become the only permissive-license open-source graph database to support this new paradigm. All the others are closed source or lack permissive license which means they couldn't really be embedded in a product
On the other hand, Gremlin is easier to implement, so that could be cranked out faster
Describe alternatives you've considered
RedisLabs Cloud supports RedisGraph now, even on the free tier. That's super compelling but I'm keen to embed a graph database in my project. I also like Rust a lot
Additional context
Oracle PGQL is perhaps the best as it has schemas to ensure data integrity https://pgql-lang.org/
The Linked Data Benchmark Council has standard tests for Graph DBs
http://ldbcouncil.org/
There's a beta cypher parser written in C with a permissive license which could be rewritten in rust https://cleishm.github.io/libcypher-parser/ and would benefit the graph DB community a great deal
GQL standard has a website https://www.gqlstandards.org/
and published scope/features document
https://s3.amazonaws.com/artifacts.opencypher.org/website/materials/sql-pg-2018-0046r3-GQL-Scope-and-Features.pdf
openCypher grammar, integration tests are available here https://github.com/opencypher/openCypher
I'm unclear how database query engines work but I'll look into it",2021-03-27T13:17:49Z,luke-feeney,https://github.com/terminusdb/terminusdb/issues/281#issuecomment-808731598,"Thanks @bionicles - query languages are always a bit fraught. WOQL is a datalog, so it is similar to graph query languages used by other upstart graph databases like Grakn, Datomic, and OpenCrux. WOQL is based on prolog so its a proper datalog. We think this is the way of the future (with python SDKs doing some of the work), especially for complex data.
We debated using SPARQL initially; however, we found SPARQL to have a number of shortcomings that we wanted to see addressed. We wanted tighter integration with OWL and schema-awareness, so if we were going to need to fundamentally alter the semantics, it didn’t seem that important to start with SPARQL. We wanted better composability and SPARQL feels quite ad-hoc.
We also assessed Cypher and found that it lacked some of the features we needed. It is also primarily geared towards property graphs and not RDF graphs. It also does not enable the smooth SDK manipulation that WOQL facilitates by using JSON-LD as a query interchange format.
We think that the Python and JavaScript SDKs for TerminusDB are the main way that users will interact with the database, so we are focused on getting them right rather than adding more QLs for the moment.
I fear that the GQL standards process will drag on forever and could end up with something that nobody really wants. Neo4j seemed to be driving it for a while, but are less prominent lately.
If we were adding another QL, I'd favor a SQL wrapper of some sort. Better to go towards the biggest possible pool (as graph query is still pretty niche).",thank @bionicles - query language be always a bit fraught woql be a datalog so -PRON- be similar to graph query language use by other upstart graph database like grakn datomic and opencrux woql be base on prolog so -PRON- a proper datalog -PRON- think this be the way of the future ( with python sdks do some of the work ) especially for complex datum -PRON- debate use sparql initially however -PRON- find sparql to have a number of shortcoming that -PRON- want to see address -PRON- want tight integration with owl and schema - awareness so if -PRON- be go to need to fundamentally alter the semantic -PRON- do not seem that important to start with sparql -PRON- want well composability and sparql feel quite ad - hoc -PRON- also assess cypher and find that -PRON- lack some of the feature -PRON- need -PRON- be also primarily gear towards property graph and not rdf graphs -PRON- also do not enable the smooth sdk manipulation that woql facilitate by use json - ld as a query interchange format -PRON- think that the python and javascript sdks for terminusdb be the main way that user will interact with the database so -PRON- be focused on get -PRON- right rather than add more qls for the moment i fear that the gql standard process will drag on forever and could end up with something that nobody really want neo4j seem to be drive -PRON- for a while but be less prominent lately if -PRON- be add another ql -PRON- 'd favor a sql wrapper of some sort better to go towards the big possible pool ( as graph query be still pretty niche ),0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,281,2021-03-27T11:36:32Z,bionicles,Possible to add more (pluggable) QLs?,https://github.com/terminusdb/terminusdb/issues/281,"Is your feature request related to a problem? Please describe.
This looks really interesting but I would want to be able to use legit graph query language such as openCypher, gremlin, datalog
Describe the solution you'd like
(terminus:DB)-[:supports]->(gql:Query language)
Note the SQL standard is evolving to eventually include GQL (that is NOT GraphQL) so if Terminus began to support a reasonably close dialect of openCypher then it could eventually become the only permissive-license open-source graph database to support this new paradigm. All the others are closed source or lack permissive license which means they couldn't really be embedded in a product
On the other hand, Gremlin is easier to implement, so that could be cranked out faster
Describe alternatives you've considered
RedisLabs Cloud supports RedisGraph now, even on the free tier. That's super compelling but I'm keen to embed a graph database in my project. I also like Rust a lot
Additional context
Oracle PGQL is perhaps the best as it has schemas to ensure data integrity https://pgql-lang.org/
The Linked Data Benchmark Council has standard tests for Graph DBs
http://ldbcouncil.org/
There's a beta cypher parser written in C with a permissive license which could be rewritten in rust https://cleishm.github.io/libcypher-parser/ and would benefit the graph DB community a great deal
GQL standard has a website https://www.gqlstandards.org/
and published scope/features document
https://s3.amazonaws.com/artifacts.opencypher.org/website/materials/sql-pg-2018-0046r3-GQL-Scope-and-Features.pdf
openCypher grammar, integration tests are available here https://github.com/opencypher/openCypher
I'm unclear how database query engines work but I'll look into it",2021-03-29T21:31:43Z,bionicles,https://github.com/terminusdb/terminusdb/issues/281#issuecomment-809728540,"What about just letting us use Prolog / Datalog instead?  They're still hard to read but at least supported more widely, and it looks like Terminus uses a lot of Prolog anyway...
WOQL looks verbose and hard to follow for me, in comparison with cypher which is elegant and easy to read. Also, WOQL only works with TerminusDB, so it's a lot harder to communicate with stakeholders because I'd be using this one-off language.
I'd love to use a temporal graph database which uses Cypher and supports static type declarative schema derived from Rust structs, as well as JSON, which is everywhere and pretty required to deal with.
The main reason I'd use a graph database is to use Cypher over SQL, and that's literally only because I prefer the syntactic sugar for joins, but I wanted to avoid Neo4j because Java sucks, and I like RedisGraph, but it doesn't support temporal queries or JSON, and has a weird license.  ArangoDB is the only db which supports graphs and json, but they don't support temporal queries and have their own one-off language AQL, and pretty scattered docs.  Datomic might be nice but alas, more Java.  Tigergraph is way too expensive and proprietary. Oracle Graph DB looks cool, since you can use a schema, but ugh, oracle, of course it's more java.  Memgraph, not great cloud offerings, more ""fully managed"" (but not managed autoscaling?). Neptune, proprietary, also ""fully managed"" (but not managed autoscaling?) and gremlin.  Janusgraph, java, gremlin.  CosmosDB, gremlin, microsoft, no thanks.
Catching my drift here?  There's no truly great open-source permissive-license strongly-typed temporal openCypher graph database. MAYBE Apache AGE, but it's quite new, I'd be scared.
I accept the fact cypher is difficult to implement since it requires a parser and assorted components.  To me, this is worth the effort, because many graph DBs now support it, and it is a simple and easy language to read.  SQL is obviously a nice feature but defeats the purpose of going toward a graph database since we have to write 3 lines of joins just to do
(a:Query)-[:like]->(this:One).
Heck, even Gremlin would be better than WOQL.  Just one man's opinion: I love the inner workings and capabilities of TerminusDB. Dislike how I'd query it because WOQL doesn't make semantic sense.
Also, I'm using Rust, is there a client for the crab language?
Anyway... It's hard to pick a database. There are many options and none really stand out to me. I'll just suck it up and use GraphQL on MySQL, at least then I can generate DDL from Rust structs and do declarative migrations with Skeema.","what about just let -PRON- use prolog / datalog instead -PRON- be still hard to read but at least support more widely and -PRON- look like terminus use a lot of prolog anyway woql look verbose and hard to follow for -PRON- in comparison with cypher which be elegant and easy to read also woql only work with terminusdb so -PRON- be a lot hard to communicate with stakeholder because -PRON- 'd be use this one - off language -PRON- 'd love to use a temporal graph database which use cypher and support static type declarative schema derive from rust struct as well as json which be everywhere and pretty require to deal with the main reason -PRON- 'd use a graph database be to use cypher over sql and that be literally only because i prefer the syntactic sugar for join but i want to avoid neo4j because java suck and i like redisgraph but -PRON- do not support temporal query or json and have a weird license arangodb be the only db which support graphs and json but -PRON- do not support temporal query and have -PRON- own one - off language aql and pretty scatter doc datomic may be nice but alas more java tigergraph be way too expensive and proprietary oracle graph db look cool since -PRON- can use a schema but ugh oracle of course -PRON- be more java memgraph not great cloud offering more "" fully manage "" ( but not manage autoscaling ) neptune proprietary also "" fully manage "" ( but not manage autoscaling ) and gremlin janusgraph java gremlin cosmosdb gremlin microsoft no thank catch -PRON- drift here there be no truly great open - source permissive - license strongly - type temporal opencypher graph database maybe apache age but -PRON- be quite new -PRON- 'd be scared i accept the fact cypher be difficult to implement since -PRON- require a parser and assorted component to -PRON- this be worth the effort because many graph dbs now support -PRON- and -PRON- be a simple and easy language to read sql be obviously a nice feature but defeat the purpose of go toward a graph database since -PRON- have to write 3 line of join just to do ( aquery)-[like]->(thisone ) heck even gremlin would be well than woql just one man 's opinion i love the inner working and capability of terminusdb dislike how -PRON- 'd query -PRON- because woql do not make semantic sense also -PRON- be use rust be there a client for the crab language anyway -PRON- be hard to pick a database there be many option and none really stand out to -PRON- -PRON- will just suck -PRON- up and use graphql on mysql at least then i can generate ddl from rust struct and do declarative migration with skeema",0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,281,2021-03-27T11:36:32Z,bionicles,Possible to add more (pluggable) QLs?,https://github.com/terminusdb/terminusdb/issues/281,"Is your feature request related to a problem? Please describe.
This looks really interesting but I would want to be able to use legit graph query language such as openCypher, gremlin, datalog
Describe the solution you'd like
(terminus:DB)-[:supports]->(gql:Query language)
Note the SQL standard is evolving to eventually include GQL (that is NOT GraphQL) so if Terminus began to support a reasonably close dialect of openCypher then it could eventually become the only permissive-license open-source graph database to support this new paradigm. All the others are closed source or lack permissive license which means they couldn't really be embedded in a product
On the other hand, Gremlin is easier to implement, so that could be cranked out faster
Describe alternatives you've considered
RedisLabs Cloud supports RedisGraph now, even on the free tier. That's super compelling but I'm keen to embed a graph database in my project. I also like Rust a lot
Additional context
Oracle PGQL is perhaps the best as it has schemas to ensure data integrity https://pgql-lang.org/
The Linked Data Benchmark Council has standard tests for Graph DBs
http://ldbcouncil.org/
There's a beta cypher parser written in C with a permissive license which could be rewritten in rust https://cleishm.github.io/libcypher-parser/ and would benefit the graph DB community a great deal
GQL standard has a website https://www.gqlstandards.org/
and published scope/features document
https://s3.amazonaws.com/artifacts.opencypher.org/website/materials/sql-pg-2018-0046r3-GQL-Scope-and-Features.pdf
openCypher grammar, integration tests are available here https://github.com/opencypher/openCypher
I'm unclear how database query engines work but I'll look into it",2021-04-06T15:35:26Z,GavinMendelGleason,https://github.com/terminusdb/terminusdb/issues/281#issuecomment-814218308,"It is possible technically to add other query languages but we have no plans to do so presently - hence I will close this ticket.
It is possible to query using WOQL embedded in prolog - we use it internally, but it would be nice to make this a more external interface. There are big advantages to having queries be an AST however, rather than strings as is the case in many other query languages, and this can enable tighter integration in your language of choice.
I'm not sure that ""WOQL doesn't make much semantic sense"" is reasonable given that it does have a clear semantics based on datalog.","-PRON- be possible technically to add other query language but -PRON- have no plan to do so presently - hence i will close this ticket -PRON- be possible to query use woql embed in prolog - -PRON- use -PRON- internally but -PRON- would be nice to make this a more external interface there be big advantage to have query be an ast however rather than string as be the case in many other query language and this can enable tight integration in -PRON- language of choice -PRON- be not sure that "" woql do not make much semantic sense "" be reasonable give that -PRON- do have a clear semantic base on datalog",0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,282,2021-03-27T12:03:52Z,Francesca-Bit,terminusdb-client-js woql-query ,https://github.com/terminusdb/terminusdb/issues/282,"Describe the bug
when  I try to insert a new document using the code below
WOQL.and(
    WOQL.doctype(""Person"")
        .label(""Person"")
        .description(""A Person’s ID number"")
        .property(""ID"", ""xsd:string"").cardinality(1)
            .label(""ID"")
        .property(""NameFirst"", ""xsd:string"").cardinality(1)
            .label(""First Name"")
        .property(""NameLast"", ""xsd:string"").cardinality(1)
            .label(""Last Name"")
)

The document get the wrong label.
To Reproduce
Steps to reproduce the behavior:

Go to 'Console'
Click on 'query interface'
Run the query
Go in the model builder interface
See the error in the label (the document got First Name instead of Person)

It Seems a problem with cardinality, if I remove cardinality it works",2021-03-29T17:29:26Z,Francesca-Bit,https://github.com/terminusdb/terminusdb/issues/282#issuecomment-809567939,fixed in terminusdb-client-js dev branch,fix in terminusdb - client - js dev branch,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,283,2021-03-28T23:05:55Z,Cheukting,List of all xsd that is supported,https://github.com/terminusdb/terminusdb/issues/283,"Do we have a list of all datatypes in the documentation? I only know the basics ones like string, double, integer etc. I think it is useful to list all of them as a reference in the documentation.",,,,,,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,284,2021-03-30T01:10:04Z,Cheukting,Server throws database_does_not_exist error with stacktrace when retrieving prefixes of a nonexistent database,https://github.com/terminusdb/terminusdb/issues/284,"When I try to connect to a database that does not exist with the Python client. It give back a DatabaseError, which is expected, what is not expected is the super long error message:
Error: database_does_not_exist(""admin"",""bank_example"")
  [36] throw(error(database_does_not_exist(""admin"",""bank_example""),_20372))
  [34] capabilities:check_descriptor_auth_(database_descriptor{database_name:""bank_example"",organization_name:""admi
n""},system:instance_read_access,'terminusdb:///system/data/admin',transaction_object{descriptor:system_descriptor{}
,inference_objects:[...],instance_objects:[...],schema_objects:[...]}) at /Applications/TerminusDB.app/Contents/Res
ources/app/public/terminusdb-server/src/core/account/capabilities.pl:373
  [30] api_prefixes:get_prefixes('admin/bank_example',transaction_object{descriptor:system_descriptor{},inference_o
bjects:[...],instance_objects:[...],schema_objects:[...]},'terminusdb:///system/data/admin',_20522) at /Application
s/TerminusDB.app/Contents/Resources/app/public/terminusdb-server/src/core/api/api_prefixes.pl:15
  [29] '<meta-call>'(routes:(...,...)) <foreign>
  [28] catch(routes:(...,...),error(database_does_not_exist(""admin"",""bank_example""),context(_20672,_20674)),routes:
do_or_die(...,...)) at /Applications/TerminusDB.app/Contents/Resources/app/public/SWI-Prolog.app/Contents/swipl/boo
t/init.pl:532
  [27] catch_with_backtrace(routes:(...,...),error(database_does_not_exist(""admin"",""bank_example""),context(_20748,_
20750)),routes:do_or_die(...,...)) at /Applications/TerminusDB.app/Contents/Resources/app/public/SWI-Prolog.app/Con
tents/swipl/boot/init.pl:582

Note: some frames are missing due to last-call optimization.
Re-run your program in debug mode (:- debug.) to get more detail.

I think only Error: database_does_not_exist(""admin"",""bank_example"") is enough to let the user know the database does not exisi.",2021-03-30T10:33:50Z,matko,https://github.com/terminusdb/terminusdb/issues/284#issuecomment-810111776,"This is happening while retrieving prefixes of a nonexistent database. Apparently we're not checking for this error condition, and it turns into an unhandled error, which comes back as a big stack trace.
We have to properly catch this error and turn into a more useful error for clients.",this be happen while retrieve prefix of a nonexistent database apparently -PRON- be not check for this error condition and -PRON- turn into an unhandled error which come back as a big stack trace -PRON- have to properly catch this error and turn into a more useful error for client,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,284,2021-03-30T01:10:04Z,Cheukting,Server throws database_does_not_exist error with stacktrace when retrieving prefixes of a nonexistent database,https://github.com/terminusdb/terminusdb/issues/284,"When I try to connect to a database that does not exist with the Python client. It give back a DatabaseError, which is expected, what is not expected is the super long error message:
Error: database_does_not_exist(""admin"",""bank_example"")
  [36] throw(error(database_does_not_exist(""admin"",""bank_example""),_20372))
  [34] capabilities:check_descriptor_auth_(database_descriptor{database_name:""bank_example"",organization_name:""admi
n""},system:instance_read_access,'terminusdb:///system/data/admin',transaction_object{descriptor:system_descriptor{}
,inference_objects:[...],instance_objects:[...],schema_objects:[...]}) at /Applications/TerminusDB.app/Contents/Res
ources/app/public/terminusdb-server/src/core/account/capabilities.pl:373
  [30] api_prefixes:get_prefixes('admin/bank_example',transaction_object{descriptor:system_descriptor{},inference_o
bjects:[...],instance_objects:[...],schema_objects:[...]},'terminusdb:///system/data/admin',_20522) at /Application
s/TerminusDB.app/Contents/Resources/app/public/terminusdb-server/src/core/api/api_prefixes.pl:15
  [29] '<meta-call>'(routes:(...,...)) <foreign>
  [28] catch(routes:(...,...),error(database_does_not_exist(""admin"",""bank_example""),context(_20672,_20674)),routes:
do_or_die(...,...)) at /Applications/TerminusDB.app/Contents/Resources/app/public/SWI-Prolog.app/Contents/swipl/boo
t/init.pl:532
  [27] catch_with_backtrace(routes:(...,...),error(database_does_not_exist(""admin"",""bank_example""),context(_20748,_
20750)),routes:do_or_die(...,...)) at /Applications/TerminusDB.app/Contents/Resources/app/public/SWI-Prolog.app/Con
tents/swipl/boot/init.pl:582

Note: some frames are missing due to last-call optimization.
Re-run your program in debug mode (:- debug.) to get more detail.

I think only Error: database_does_not_exist(""admin"",""bank_example"") is enough to let the user know the database does not exisi.",2021-04-12T15:14:34Z,Cheukting,https://github.com/terminusdb/terminusdb/issues/284#issuecomment-817895964,Fix in f921012,fix in f921012,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,286,2021-03-30T11:13:33Z,matko,Tests fail sporadically,https://github.com/terminusdb/terminusdb/issues/286,"We tried to make tests run faster by running them in parallel, but ever since, we've been running into cases where tests fail sporadically in CI. We should either undo the parallelization, or figure out the root cause, which I think is something about how the test initialization is happening.",,,,,,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,287,2021-04-06T02:57:00Z,MrRaymondLee,Horizontal Scroll in Query Results,https://github.com/terminusdb/terminusdb/issues/287,"Is your feature request related to a problem? Please describe.
When querying many properties, there is no horizontal scroll and thus one isn't able to see properties on the far right.
Describe the solution you'd like
Ability to horizontal scroll.
Describe alternatives you've considered
There isn't a comment out ability in the query so have to copy & paste into a text editor and remove properties to see the far right
properties.",2021-04-06T15:10:07Z,Cheukting,https://github.com/terminusdb/terminusdb/issues/287#issuecomment-814199174,@Francesca-Bit and @KittyJose do you want to have a lot and see how should it be triage?,@francesca - bit and @kittyjose do -PRON- want to have a lot and see how should -PRON- be triage,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,287,2021-04-06T02:57:00Z,MrRaymondLee,Horizontal Scroll in Query Results,https://github.com/terminusdb/terminusdb/issues/287,"Is your feature request related to a problem? Please describe.
When querying many properties, there is no horizontal scroll and thus one isn't able to see properties on the far right.
Describe the solution you'd like
Ability to horizontal scroll.
Describe alternatives you've considered
There isn't a comment out ability in the query so have to copy & paste into a text editor and remove properties to see the far right
properties.",2021-04-27T08:22:51Z,Francesca-Bit,https://github.com/terminusdb/terminusdb/issues/287#issuecomment-827417446,implemented,implement,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,288,2021-04-06T03:06:14Z,MrRaymondLee,WOQL query allows specifying a property that does not exist and no error is returned,https://github.com/terminusdb/terminusdb/issues/288,"Describe the bug
In WOQL query (from the web ui), if I misspell a property, there will be no error.
To Reproduce
Steps to reproduce the behavior:

Go to 'Query'
Enter a WOQL.js query specifying a property that does not exist.

Expected behavior
An error that the property (specify which one) doesn't exist.
Console version: 4.2.2
Server: 4.2.0",2021-04-06T15:07:28Z,Cheukting,https://github.com/terminusdb/terminusdb/issues/288#issuecomment-814197166,"It's designed to not raise an error if the property does not exist.
@GavinMendelGleason suggested a linter endpoint.
I will change it to a feature request.",-PRON- be design to not raise an error if the property do not exist @gavinmendelgleason suggest a linter endpoint i will change -PRON- to a feature request,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,288,2021-04-06T03:06:14Z,MrRaymondLee,WOQL query allows specifying a property that does not exist and no error is returned,https://github.com/terminusdb/terminusdb/issues/288,"Describe the bug
In WOQL query (from the web ui), if I misspell a property, there will be no error.
To Reproduce
Steps to reproduce the behavior:

Go to 'Query'
Enter a WOQL.js query specifying a property that does not exist.

Expected behavior
An error that the property (specify which one) doesn't exist.
Console version: 4.2.2
Server: 4.2.0",2021-04-06T19:45:08Z,MrRaymondLee,https://github.com/terminusdb/terminusdb/issues/288#issuecomment-814392894,"Sounds good.  If there's situations where someone builds the schema, a developer builds the queries, and the schema is changed then things will start breaking but without an error it'll be discovered late know.",sound good if there be situation where someone build the schema a developer build the query and the schema be change then thing will start break but without an error -PRON- will be discover late know,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,289,2021-04-07T08:28:26Z,KittyJose,Creating an issue to test webhook ,https://github.com/terminusdb/terminusdb/issues/289,Creating an issue to test webhook,2021-04-07T08:31:30Z,KittyJose,https://github.com/terminusdb/terminusdb/issues/289#issuecomment-814718347,closing coz test was successfull :),close coz test be successfull ),0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,290,2021-04-07T08:35:54Z,KittyJose,creating an issue to test webhook - test 2,https://github.com/terminusdb/terminusdb/issues/290,creating an issue to test webhook - test 2,2021-04-07T08:47:44Z,KittyJose,https://github.com/terminusdb/terminusdb/issues/290#issuecomment-814729197,closing this coz test was unsuccessfull have to try again,close this coz test be unsuccessfull have to try again,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,291,2021-04-07T08:52:36Z,KittyJose,creating an issue to test webhook - test 3,https://github.com/terminusdb/terminusdb/issues/291,creating an issue to test webhook - test 3,2021-04-07T08:53:18Z,KittyJose,https://github.com/terminusdb/terminusdb/issues/291#issuecomment-814733003,closing issue 3 to test web hook,close issue 3 to test web hook,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,292,2021-04-07T09:15:48Z,KittyJose,creating new issue to test web hook = test 4,https://github.com/terminusdb/terminusdb/issues/292,creating new issue to test web hook = test 4,2021-04-07T09:17:27Z,KittyJose,https://github.com/terminusdb/terminusdb/issues/292#issuecomment-814753176,closing this issue,close this issue,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,298,2021-04-13T09:58:54Z,matko,Provide a way to easily check that a database has changed,https://github.com/terminusdb/terminusdb/issues/298,"We currently have no way to quickly check that a database was updated. The only way is to actually open a database and see if new commits were added.
We could provide an easy first pass way to check that a database has changed by simply comparing the revision number of a label file. If this hasn't changed, for sure the database has not updated and we don't have to look any deeper. To support this we'd need to return the label revision number or label commit id, or a hash of it, in a header, which the client can submit back upon doing a request.
A second improvement on top of that would also include the commit graph layer or a hash of it in the headers.",,,,,,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,299,2021-04-13T13:59:42Z,mikkokotila,python basic setup fails from docs,https://github.com/terminusdb/terminusdb/issues/299,"I run:
from terminusdb_client import WOQLClient
client = WOQLClient(""https://127.0.0.1:6363/"", insecure=True)
client.connect(key=""root"", account=""admin"", user=""admin"")

And I get:
---------------------------------------------------------------------------
ConnectionRefusedError                    Traceback (most recent call last)
~/miniconda3/envs/wip/lib/python3.8/site-packages/urllib3/connection.py in _new_conn(self)
    158         try:
--> 159             conn = connection.create_connection(
    160                 (self._dns_host, self.port), self.timeout, **extra_kw

~/miniconda3/envs/wip/lib/python3.8/site-packages/urllib3/util/connection.py in create_connection(address, timeout, source_address, socket_options)
     83     if err is not None:
---> 84         raise err
     85 

~/miniconda3/envs/wip/lib/python3.8/site-packages/urllib3/util/connection.py in create_connection(address, timeout, source_address, socket_options)
     73                 sock.bind(source_address)
---> 74             sock.connect(sa)
     75             return sock

ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

NewConnectionError                        Traceback (most recent call last)
~/miniconda3/envs/wip/lib/python3.8/site-packages/urllib3/connectionpool.py in urlopen(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)
    669             # Make the request on the httplib connection object.
--> 670             httplib_response = self._make_request(
    671                 conn,

~/miniconda3/envs/wip/lib/python3.8/site-packages/urllib3/connectionpool.py in _make_request(self, conn, method, url, timeout, chunked, **httplib_request_kw)
    380         try:
--> 381             self._validate_conn(conn)
    382         except (SocketTimeout, BaseSSLError) as e:

~/miniconda3/envs/wip/lib/python3.8/site-packages/urllib3/connectionpool.py in _validate_conn(self, conn)
    975         if not getattr(conn, ""sock"", None):  # AppEngine might not have  `.sock`
--> 976             conn.connect()
    977 

~/miniconda3/envs/wip/lib/python3.8/site-packages/urllib3/connection.py in connect(self)
    307         # Add certificate verification
--> 308         conn = self._new_conn()
    309         hostname = self.host

~/miniconda3/envs/wip/lib/python3.8/site-packages/urllib3/connection.py in _new_conn(self)
    170         except SocketError as e:
--> 171             raise NewConnectionError(
    172                 self, ""Failed to establish a new connection: %s"" % e

NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x7f395037e550>: Failed to establish a new connection: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

MaxRetryError                             Traceback (most recent call last)
~/miniconda3/envs/wip/lib/python3.8/site-packages/requests/adapters.py in send(self, request, stream, timeout, verify, cert, proxies)
    438             if not chunked:
--> 439                 resp = conn.urlopen(
    440                     method=request.method,

~/miniconda3/envs/wip/lib/python3.8/site-packages/urllib3/connectionpool.py in urlopen(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)
    723 
--> 724             retries = retries.increment(
    725                 method, url, error=e, _pool=self, _stacktrace=sys.exc_info()[2]

~/miniconda3/envs/wip/lib/python3.8/site-packages/urllib3/util/retry.py in increment(self, method, url, response, error, _pool, _stacktrace)
    438         if new_retry.is_exhausted():
--> 439             raise MaxRetryError(_pool, url, error or ResponseError(cause))
    440 

MaxRetryError: HTTPSConnectionPool(host='127.0.0.1', port=6363): Max retries exceeded with url: /api (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f395037e550>: Failed to establish a new connection: [Errno 111] Connection refused'))

During handling of the above exception, another exception occurred:

ConnectionError                           Traceback (most recent call last)
<ipython-input-27-c9ec83ad448f> in <module>
----> 1 client.connect(key=""root"", account=""admin"", user=""admin"")

~/miniconda3/envs/wip/lib/python3.8/site-packages/terminusdb_client/woqlclient/woqlClient.py in connect(self, account, db, remote_auth, key, user, branch, ref, repo, **kwargs)
     96         self._connected = True
     97 
---> 98         capabilities = self._dispatch_json(""get"", self._api)
     99         self._uid = capabilities[""@id""]
    100 

~/miniconda3/envs/wip/lib/python3.8/site-packages/terminusdb_client/woqlclient/woqlClient.py in _dispatch_json(self, action, url, payload, file_list)
   1487             Dictionary convered from the json string that is passed from a successful dispatch call.
   1488         """"""
-> 1489         result = self._dispatch(action, url, payload, file_list)
   1490         return json.loads(result)
   1491 

~/miniconda3/envs/wip/lib/python3.8/site-packages/terminusdb_client/woqlclient/woqlClient.py in _dispatch(self, action, url, payload, file_list)
   1553 
   1554         if action == ""get"":
-> 1555             request_response = requests.get(
   1556                 url, headers=headers, verify=verify, params=payload
   1557             )

~/miniconda3/envs/wip/lib/python3.8/site-packages/requests/api.py in get(url, params, **kwargs)
     74 
     75     kwargs.setdefault('allow_redirects', True)
---> 76     return request('get', url, params=params, **kwargs)
     77 
     78 

~/miniconda3/envs/wip/lib/python3.8/site-packages/requests/api.py in request(method, url, **kwargs)
     59     # cases, and look like a memory leak in others.
     60     with sessions.Session() as session:
---> 61         return session.request(method=method, url=url, **kwargs)
     62 
     63 

~/miniconda3/envs/wip/lib/python3.8/site-packages/requests/sessions.py in request(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)
    528         }
    529         send_kwargs.update(settings)
--> 530         resp = self.send(prep, **send_kwargs)
    531 
    532         return resp

~/miniconda3/envs/wip/lib/python3.8/site-packages/requests/sessions.py in send(self, request, **kwargs)
    641 
    642         # Send the request
--> 643         r = adapter.send(request, **kwargs)
    644 
    645         # Total elapsed time of the request (approximately)

~/miniconda3/envs/wip/lib/python3.8/site-packages/requests/adapters.py in send(self, request, stream, timeout, verify, cert, proxies)
    514                 raise SSLError(e, request=request)
    515 
--> 516             raise ConnectionError(e, request=request)
    517 
    518         except ClosedPoolError as e:

ConnectionError: HTTPSConnectionPool(host='127.0.0.1', port=6363): Max retries exceeded with url: /api (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f395037e550>: Failed to establish a new connection: [Errno 111] Connection refused'))",2021-04-13T14:10:18Z,rrooij,https://github.com/terminusdb/terminusdb/issues/299#issuecomment-818767455,"Which distribution of TerminusDB are you running? E.g. desktop, bootstrap, docker?",which distribution of terminusdb be -PRON- run eg desktop bootstrap docker,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,299,2021-04-13T13:59:42Z,mikkokotila,python basic setup fails from docs,https://github.com/terminusdb/terminusdb/issues/299,"I run:
from terminusdb_client import WOQLClient
client = WOQLClient(""https://127.0.0.1:6363/"", insecure=True)
client.connect(key=""root"", account=""admin"", user=""admin"")

And I get:
---------------------------------------------------------------------------
ConnectionRefusedError                    Traceback (most recent call last)
~/miniconda3/envs/wip/lib/python3.8/site-packages/urllib3/connection.py in _new_conn(self)
    158         try:
--> 159             conn = connection.create_connection(
    160                 (self._dns_host, self.port), self.timeout, **extra_kw

~/miniconda3/envs/wip/lib/python3.8/site-packages/urllib3/util/connection.py in create_connection(address, timeout, source_address, socket_options)
     83     if err is not None:
---> 84         raise err
     85 

~/miniconda3/envs/wip/lib/python3.8/site-packages/urllib3/util/connection.py in create_connection(address, timeout, source_address, socket_options)
     73                 sock.bind(source_address)
---> 74             sock.connect(sa)
     75             return sock

ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

NewConnectionError                        Traceback (most recent call last)
~/miniconda3/envs/wip/lib/python3.8/site-packages/urllib3/connectionpool.py in urlopen(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)
    669             # Make the request on the httplib connection object.
--> 670             httplib_response = self._make_request(
    671                 conn,

~/miniconda3/envs/wip/lib/python3.8/site-packages/urllib3/connectionpool.py in _make_request(self, conn, method, url, timeout, chunked, **httplib_request_kw)
    380         try:
--> 381             self._validate_conn(conn)
    382         except (SocketTimeout, BaseSSLError) as e:

~/miniconda3/envs/wip/lib/python3.8/site-packages/urllib3/connectionpool.py in _validate_conn(self, conn)
    975         if not getattr(conn, ""sock"", None):  # AppEngine might not have  `.sock`
--> 976             conn.connect()
    977 

~/miniconda3/envs/wip/lib/python3.8/site-packages/urllib3/connection.py in connect(self)
    307         # Add certificate verification
--> 308         conn = self._new_conn()
    309         hostname = self.host

~/miniconda3/envs/wip/lib/python3.8/site-packages/urllib3/connection.py in _new_conn(self)
    170         except SocketError as e:
--> 171             raise NewConnectionError(
    172                 self, ""Failed to establish a new connection: %s"" % e

NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x7f395037e550>: Failed to establish a new connection: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

MaxRetryError                             Traceback (most recent call last)
~/miniconda3/envs/wip/lib/python3.8/site-packages/requests/adapters.py in send(self, request, stream, timeout, verify, cert, proxies)
    438             if not chunked:
--> 439                 resp = conn.urlopen(
    440                     method=request.method,

~/miniconda3/envs/wip/lib/python3.8/site-packages/urllib3/connectionpool.py in urlopen(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)
    723 
--> 724             retries = retries.increment(
    725                 method, url, error=e, _pool=self, _stacktrace=sys.exc_info()[2]

~/miniconda3/envs/wip/lib/python3.8/site-packages/urllib3/util/retry.py in increment(self, method, url, response, error, _pool, _stacktrace)
    438         if new_retry.is_exhausted():
--> 439             raise MaxRetryError(_pool, url, error or ResponseError(cause))
    440 

MaxRetryError: HTTPSConnectionPool(host='127.0.0.1', port=6363): Max retries exceeded with url: /api (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f395037e550>: Failed to establish a new connection: [Errno 111] Connection refused'))

During handling of the above exception, another exception occurred:

ConnectionError                           Traceback (most recent call last)
<ipython-input-27-c9ec83ad448f> in <module>
----> 1 client.connect(key=""root"", account=""admin"", user=""admin"")

~/miniconda3/envs/wip/lib/python3.8/site-packages/terminusdb_client/woqlclient/woqlClient.py in connect(self, account, db, remote_auth, key, user, branch, ref, repo, **kwargs)
     96         self._connected = True
     97 
---> 98         capabilities = self._dispatch_json(""get"", self._api)
     99         self._uid = capabilities[""@id""]
    100 

~/miniconda3/envs/wip/lib/python3.8/site-packages/terminusdb_client/woqlclient/woqlClient.py in _dispatch_json(self, action, url, payload, file_list)
   1487             Dictionary convered from the json string that is passed from a successful dispatch call.
   1488         """"""
-> 1489         result = self._dispatch(action, url, payload, file_list)
   1490         return json.loads(result)
   1491 

~/miniconda3/envs/wip/lib/python3.8/site-packages/terminusdb_client/woqlclient/woqlClient.py in _dispatch(self, action, url, payload, file_list)
   1553 
   1554         if action == ""get"":
-> 1555             request_response = requests.get(
   1556                 url, headers=headers, verify=verify, params=payload
   1557             )

~/miniconda3/envs/wip/lib/python3.8/site-packages/requests/api.py in get(url, params, **kwargs)
     74 
     75     kwargs.setdefault('allow_redirects', True)
---> 76     return request('get', url, params=params, **kwargs)
     77 
     78 

~/miniconda3/envs/wip/lib/python3.8/site-packages/requests/api.py in request(method, url, **kwargs)
     59     # cases, and look like a memory leak in others.
     60     with sessions.Session() as session:
---> 61         return session.request(method=method, url=url, **kwargs)
     62 
     63 

~/miniconda3/envs/wip/lib/python3.8/site-packages/requests/sessions.py in request(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)
    528         }
    529         send_kwargs.update(settings)
--> 530         resp = self.send(prep, **send_kwargs)
    531 
    532         return resp

~/miniconda3/envs/wip/lib/python3.8/site-packages/requests/sessions.py in send(self, request, **kwargs)
    641 
    642         # Send the request
--> 643         r = adapter.send(request, **kwargs)
    644 
    645         # Total elapsed time of the request (approximately)

~/miniconda3/envs/wip/lib/python3.8/site-packages/requests/adapters.py in send(self, request, stream, timeout, verify, cert, proxies)
    514                 raise SSLError(e, request=request)
    515 
--> 516             raise ConnectionError(e, request=request)
    517 
    518         except ClosedPoolError as e:

ConnectionError: HTTPSConnectionPool(host='127.0.0.1', port=6363): Max retries exceeded with url: /api (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f395037e550>: Failed to establish a new connection: [Errno 111] Connection refused'))",2021-04-13T14:16:14Z,mikkokotila,https://github.com/terminusdb/terminusdb/issues/299#issuecomment-818772035,I followed the docs example as it is i.e. install with pip.,i follow the doc example as -PRON- be ie install with pip,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,299,2021-04-13T13:59:42Z,mikkokotila,python basic setup fails from docs,https://github.com/terminusdb/terminusdb/issues/299,"I run:
from terminusdb_client import WOQLClient
client = WOQLClient(""https://127.0.0.1:6363/"", insecure=True)
client.connect(key=""root"", account=""admin"", user=""admin"")

And I get:
---------------------------------------------------------------------------
ConnectionRefusedError                    Traceback (most recent call last)
~/miniconda3/envs/wip/lib/python3.8/site-packages/urllib3/connection.py in _new_conn(self)
    158         try:
--> 159             conn = connection.create_connection(
    160                 (self._dns_host, self.port), self.timeout, **extra_kw

~/miniconda3/envs/wip/lib/python3.8/site-packages/urllib3/util/connection.py in create_connection(address, timeout, source_address, socket_options)
     83     if err is not None:
---> 84         raise err
     85 

~/miniconda3/envs/wip/lib/python3.8/site-packages/urllib3/util/connection.py in create_connection(address, timeout, source_address, socket_options)
     73                 sock.bind(source_address)
---> 74             sock.connect(sa)
     75             return sock

ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

NewConnectionError                        Traceback (most recent call last)
~/miniconda3/envs/wip/lib/python3.8/site-packages/urllib3/connectionpool.py in urlopen(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)
    669             # Make the request on the httplib connection object.
--> 670             httplib_response = self._make_request(
    671                 conn,

~/miniconda3/envs/wip/lib/python3.8/site-packages/urllib3/connectionpool.py in _make_request(self, conn, method, url, timeout, chunked, **httplib_request_kw)
    380         try:
--> 381             self._validate_conn(conn)
    382         except (SocketTimeout, BaseSSLError) as e:

~/miniconda3/envs/wip/lib/python3.8/site-packages/urllib3/connectionpool.py in _validate_conn(self, conn)
    975         if not getattr(conn, ""sock"", None):  # AppEngine might not have  `.sock`
--> 976             conn.connect()
    977 

~/miniconda3/envs/wip/lib/python3.8/site-packages/urllib3/connection.py in connect(self)
    307         # Add certificate verification
--> 308         conn = self._new_conn()
    309         hostname = self.host

~/miniconda3/envs/wip/lib/python3.8/site-packages/urllib3/connection.py in _new_conn(self)
    170         except SocketError as e:
--> 171             raise NewConnectionError(
    172                 self, ""Failed to establish a new connection: %s"" % e

NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x7f395037e550>: Failed to establish a new connection: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

MaxRetryError                             Traceback (most recent call last)
~/miniconda3/envs/wip/lib/python3.8/site-packages/requests/adapters.py in send(self, request, stream, timeout, verify, cert, proxies)
    438             if not chunked:
--> 439                 resp = conn.urlopen(
    440                     method=request.method,

~/miniconda3/envs/wip/lib/python3.8/site-packages/urllib3/connectionpool.py in urlopen(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)
    723 
--> 724             retries = retries.increment(
    725                 method, url, error=e, _pool=self, _stacktrace=sys.exc_info()[2]

~/miniconda3/envs/wip/lib/python3.8/site-packages/urllib3/util/retry.py in increment(self, method, url, response, error, _pool, _stacktrace)
    438         if new_retry.is_exhausted():
--> 439             raise MaxRetryError(_pool, url, error or ResponseError(cause))
    440 

MaxRetryError: HTTPSConnectionPool(host='127.0.0.1', port=6363): Max retries exceeded with url: /api (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f395037e550>: Failed to establish a new connection: [Errno 111] Connection refused'))

During handling of the above exception, another exception occurred:

ConnectionError                           Traceback (most recent call last)
<ipython-input-27-c9ec83ad448f> in <module>
----> 1 client.connect(key=""root"", account=""admin"", user=""admin"")

~/miniconda3/envs/wip/lib/python3.8/site-packages/terminusdb_client/woqlclient/woqlClient.py in connect(self, account, db, remote_auth, key, user, branch, ref, repo, **kwargs)
     96         self._connected = True
     97 
---> 98         capabilities = self._dispatch_json(""get"", self._api)
     99         self._uid = capabilities[""@id""]
    100 

~/miniconda3/envs/wip/lib/python3.8/site-packages/terminusdb_client/woqlclient/woqlClient.py in _dispatch_json(self, action, url, payload, file_list)
   1487             Dictionary convered from the json string that is passed from a successful dispatch call.
   1488         """"""
-> 1489         result = self._dispatch(action, url, payload, file_list)
   1490         return json.loads(result)
   1491 

~/miniconda3/envs/wip/lib/python3.8/site-packages/terminusdb_client/woqlclient/woqlClient.py in _dispatch(self, action, url, payload, file_list)
   1553 
   1554         if action == ""get"":
-> 1555             request_response = requests.get(
   1556                 url, headers=headers, verify=verify, params=payload
   1557             )

~/miniconda3/envs/wip/lib/python3.8/site-packages/requests/api.py in get(url, params, **kwargs)
     74 
     75     kwargs.setdefault('allow_redirects', True)
---> 76     return request('get', url, params=params, **kwargs)
     77 
     78 

~/miniconda3/envs/wip/lib/python3.8/site-packages/requests/api.py in request(method, url, **kwargs)
     59     # cases, and look like a memory leak in others.
     60     with sessions.Session() as session:
---> 61         return session.request(method=method, url=url, **kwargs)
     62 
     63 

~/miniconda3/envs/wip/lib/python3.8/site-packages/requests/sessions.py in request(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)
    528         }
    529         send_kwargs.update(settings)
--> 530         resp = self.send(prep, **send_kwargs)
    531 
    532         return resp

~/miniconda3/envs/wip/lib/python3.8/site-packages/requests/sessions.py in send(self, request, **kwargs)
    641 
    642         # Send the request
--> 643         r = adapter.send(request, **kwargs)
    644 
    645         # Total elapsed time of the request (approximately)

~/miniconda3/envs/wip/lib/python3.8/site-packages/requests/adapters.py in send(self, request, stream, timeout, verify, cert, proxies)
    514                 raise SSLError(e, request=request)
    515 
--> 516             raise ConnectionError(e, request=request)
    517 
    518         except ClosedPoolError as e:

ConnectionError: HTTPSConnectionPool(host='127.0.0.1', port=6363): Max retries exceeded with url: /api (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f395037e550>: Failed to establish a new connection: [Errno 111] Connection refused'))",2021-04-13T14:16:44Z,mikkokotila,https://github.com/terminusdb/terminusdb/issues/299#issuecomment-818772388,"lol yes, I got it, I only have the client installed :D",lol yes i get -PRON- i only have the client instal d,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,299,2021-04-13T13:59:42Z,mikkokotila,python basic setup fails from docs,https://github.com/terminusdb/terminusdb/issues/299,"I run:
from terminusdb_client import WOQLClient
client = WOQLClient(""https://127.0.0.1:6363/"", insecure=True)
client.connect(key=""root"", account=""admin"", user=""admin"")

And I get:
---------------------------------------------------------------------------
ConnectionRefusedError                    Traceback (most recent call last)
~/miniconda3/envs/wip/lib/python3.8/site-packages/urllib3/connection.py in _new_conn(self)
    158         try:
--> 159             conn = connection.create_connection(
    160                 (self._dns_host, self.port), self.timeout, **extra_kw

~/miniconda3/envs/wip/lib/python3.8/site-packages/urllib3/util/connection.py in create_connection(address, timeout, source_address, socket_options)
     83     if err is not None:
---> 84         raise err
     85 

~/miniconda3/envs/wip/lib/python3.8/site-packages/urllib3/util/connection.py in create_connection(address, timeout, source_address, socket_options)
     73                 sock.bind(source_address)
---> 74             sock.connect(sa)
     75             return sock

ConnectionRefusedError: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

NewConnectionError                        Traceback (most recent call last)
~/miniconda3/envs/wip/lib/python3.8/site-packages/urllib3/connectionpool.py in urlopen(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)
    669             # Make the request on the httplib connection object.
--> 670             httplib_response = self._make_request(
    671                 conn,

~/miniconda3/envs/wip/lib/python3.8/site-packages/urllib3/connectionpool.py in _make_request(self, conn, method, url, timeout, chunked, **httplib_request_kw)
    380         try:
--> 381             self._validate_conn(conn)
    382         except (SocketTimeout, BaseSSLError) as e:

~/miniconda3/envs/wip/lib/python3.8/site-packages/urllib3/connectionpool.py in _validate_conn(self, conn)
    975         if not getattr(conn, ""sock"", None):  # AppEngine might not have  `.sock`
--> 976             conn.connect()
    977 

~/miniconda3/envs/wip/lib/python3.8/site-packages/urllib3/connection.py in connect(self)
    307         # Add certificate verification
--> 308         conn = self._new_conn()
    309         hostname = self.host

~/miniconda3/envs/wip/lib/python3.8/site-packages/urllib3/connection.py in _new_conn(self)
    170         except SocketError as e:
--> 171             raise NewConnectionError(
    172                 self, ""Failed to establish a new connection: %s"" % e

NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x7f395037e550>: Failed to establish a new connection: [Errno 111] Connection refused

During handling of the above exception, another exception occurred:

MaxRetryError                             Traceback (most recent call last)
~/miniconda3/envs/wip/lib/python3.8/site-packages/requests/adapters.py in send(self, request, stream, timeout, verify, cert, proxies)
    438             if not chunked:
--> 439                 resp = conn.urlopen(
    440                     method=request.method,

~/miniconda3/envs/wip/lib/python3.8/site-packages/urllib3/connectionpool.py in urlopen(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)
    723 
--> 724             retries = retries.increment(
    725                 method, url, error=e, _pool=self, _stacktrace=sys.exc_info()[2]

~/miniconda3/envs/wip/lib/python3.8/site-packages/urllib3/util/retry.py in increment(self, method, url, response, error, _pool, _stacktrace)
    438         if new_retry.is_exhausted():
--> 439             raise MaxRetryError(_pool, url, error or ResponseError(cause))
    440 

MaxRetryError: HTTPSConnectionPool(host='127.0.0.1', port=6363): Max retries exceeded with url: /api (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f395037e550>: Failed to establish a new connection: [Errno 111] Connection refused'))

During handling of the above exception, another exception occurred:

ConnectionError                           Traceback (most recent call last)
<ipython-input-27-c9ec83ad448f> in <module>
----> 1 client.connect(key=""root"", account=""admin"", user=""admin"")

~/miniconda3/envs/wip/lib/python3.8/site-packages/terminusdb_client/woqlclient/woqlClient.py in connect(self, account, db, remote_auth, key, user, branch, ref, repo, **kwargs)
     96         self._connected = True
     97 
---> 98         capabilities = self._dispatch_json(""get"", self._api)
     99         self._uid = capabilities[""@id""]
    100 

~/miniconda3/envs/wip/lib/python3.8/site-packages/terminusdb_client/woqlclient/woqlClient.py in _dispatch_json(self, action, url, payload, file_list)
   1487             Dictionary convered from the json string that is passed from a successful dispatch call.
   1488         """"""
-> 1489         result = self._dispatch(action, url, payload, file_list)
   1490         return json.loads(result)
   1491 

~/miniconda3/envs/wip/lib/python3.8/site-packages/terminusdb_client/woqlclient/woqlClient.py in _dispatch(self, action, url, payload, file_list)
   1553 
   1554         if action == ""get"":
-> 1555             request_response = requests.get(
   1556                 url, headers=headers, verify=verify, params=payload
   1557             )

~/miniconda3/envs/wip/lib/python3.8/site-packages/requests/api.py in get(url, params, **kwargs)
     74 
     75     kwargs.setdefault('allow_redirects', True)
---> 76     return request('get', url, params=params, **kwargs)
     77 
     78 

~/miniconda3/envs/wip/lib/python3.8/site-packages/requests/api.py in request(method, url, **kwargs)
     59     # cases, and look like a memory leak in others.
     60     with sessions.Session() as session:
---> 61         return session.request(method=method, url=url, **kwargs)
     62 
     63 

~/miniconda3/envs/wip/lib/python3.8/site-packages/requests/sessions.py in request(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)
    528         }
    529         send_kwargs.update(settings)
--> 530         resp = self.send(prep, **send_kwargs)
    531 
    532         return resp

~/miniconda3/envs/wip/lib/python3.8/site-packages/requests/sessions.py in send(self, request, **kwargs)
    641 
    642         # Send the request
--> 643         r = adapter.send(request, **kwargs)
    644 
    645         # Total elapsed time of the request (approximately)

~/miniconda3/envs/wip/lib/python3.8/site-packages/requests/adapters.py in send(self, request, stream, timeout, verify, cert, proxies)
    514                 raise SSLError(e, request=request)
    515 
--> 516             raise ConnectionError(e, request=request)
    517 
    518         except ClosedPoolError as e:

ConnectionError: HTTPSConnectionPool(host='127.0.0.1', port=6363): Max retries exceeded with url: /api (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f395037e550>: Failed to establish a new connection: [Errno 111] Connection refused'))",2021-04-13T14:19:22Z,mikkokotila,https://github.com/terminusdb/terminusdb/issues/299#issuecomment-818774352,"So yes, I basically ""forgot"" to install TerminusDB and just jumped right into the client.","so yes i basically "" forget "" to install terminusdb and just jump right into the client",0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,303,2021-04-13T14:57:07Z,rrooij,TerminusDB list should have non-pretty print version,https://github.com/terminusdb/terminusdb/issues/303,"Running ./terminusdb list gives a nice tree, but for scripts it gets a bit cumbersome. Either we just make a plain text list or we can return a JSON.",2021-04-14T08:33:40Z,rrooij,https://github.com/terminusdb/terminusdb/issues/303#issuecomment-819338985,This is solved in 1d88d8f,this be solve in 1d88d8f,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,305,2021-04-14T08:35:23Z,rrooij,Running optimize on meta graph on empty DB crashes,https://github.com/terminusdb/terminusdb/issues/305,"Describe the bug
When creating a new DB, running optimize on its meta graph will crash.
To Reproduce
./terminusdb create 'admin/test'
./terminusdb optimize 'admin/test/_meta'
Expected behavior
Some kind of useful error message?
Error message
./terminusdb optimize 'admin/test/_meta'                                                                     dev 17h ● ⬡
  [18] api_optimize:head(<named_graph admin%7ctest>,_4568,_4570)
  [17] api_optimize:named_graph_optimize('admin|test') at /home/robin/git/terminusdb/src/core/api/api_optimize.pl:35
  [14] '<meta-call>'('<garbage_collected>') <foreign>
  [13] '$apply':forall('<garbage_collected>','<garbage_collected>') at /usr/lib/swi-prolog/boot/apply.pl:52
  [12] catch(cli:forall(...,...),error(existence_error(procedure,...),context(...,_4742)),cli:do_or_die(...,...)) at /usr/lib/swi-prolog/boot/init.pl:480
  [11] catch_with_backtrace('<garbage_collected>','<garbage_collected>','<garbage_collected>') at /usr/lib/swi-prolog/boot/init.pl:530

Note: some frames are missing due to last-call optimization.
Re-run your program in debug mode (:- debug.) to get more detail.
Error: existence_error(procedure,api_optimize:head/3)",2021-04-14T08:41:52Z,matko,https://github.com/terminusdb/terminusdb/issues/305#issuecomment-819344673,"It looks like you haven't rebuilt your terminus_store_prolog library, as it is not finding the head/3 predicate.",-PRON- look like -PRON- have not rebuild -PRON- terminus_store_prolog library as -PRON- be not find the head/3 predicate,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,305,2021-04-14T08:35:23Z,rrooij,Running optimize on meta graph on empty DB crashes,https://github.com/terminusdb/terminusdb/issues/305,"Describe the bug
When creating a new DB, running optimize on its meta graph will crash.
To Reproduce
./terminusdb create 'admin/test'
./terminusdb optimize 'admin/test/_meta'
Expected behavior
Some kind of useful error message?
Error message
./terminusdb optimize 'admin/test/_meta'                                                                     dev 17h ● ⬡
  [18] api_optimize:head(<named_graph admin%7ctest>,_4568,_4570)
  [17] api_optimize:named_graph_optimize('admin|test') at /home/robin/git/terminusdb/src/core/api/api_optimize.pl:35
  [14] '<meta-call>'('<garbage_collected>') <foreign>
  [13] '$apply':forall('<garbage_collected>','<garbage_collected>') at /usr/lib/swi-prolog/boot/apply.pl:52
  [12] catch(cli:forall(...,...),error(existence_error(procedure,...),context(...,_4742)),cli:do_or_die(...,...)) at /usr/lib/swi-prolog/boot/init.pl:480
  [11] catch_with_backtrace('<garbage_collected>','<garbage_collected>','<garbage_collected>') at /usr/lib/swi-prolog/boot/init.pl:530

Note: some frames are missing due to last-call optimization.
Re-run your program in debug mode (:- debug.) to get more detail.
Error: existence_error(procedure,api_optimize:head/3)",2021-04-14T10:40:24Z,rrooij,https://github.com/terminusdb/terminusdb/issues/305#issuecomment-819419631,That is true...,that be true,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,306,2021-04-14T13:29:40Z,erikseulean,The name of the python library is not specified,https://github.com/terminusdb/terminusdb/issues/306,"In the url here https://terminusdb.com/docs/terminusdb/#/How_To/graph_query it's mentioned that there's a Python client library, however the name of the library is not mentioned anywhere. (or at least I'm not spotting it). Consider adding the library name somewhere and the versions of Python supported.",2021-04-14T13:47:02Z,rrooij,https://github.com/terminusdb/terminusdb/issues/306#issuecomment-819531496,"You are right. The library is https://github.com/terminusdb/terminusdb-client-python/ and is called terminusdb-client on pip, but this should be mentioned.",-PRON- be right the library be https//githubcom / terminusdb / terminusdb - client - python/ and be call terminusdb - client on pip but this should be mention,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,308,2021-04-15T11:46:59Z,erikseulean,Improve error reporting ,https://github.com/terminusdb/terminusdb/issues/308,"Currently, when the input file used to load data into a database has an incorrect format, the error shows as a syntax error in the following format:
Error: error(syntax_error('PN_PREFIX expected'),stream(<stream>(0x565294943c00),5,13,175))
Consider parsing and reporting more helpful errors that could help handle issues as such.",,,,,,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,309,2021-04-19T10:38:44Z,GavinMendelGleason,Automatic object identifiers need to be generated with parent path as well as downward transitive closure,https://github.com/terminusdb/terminusdb/issues/309,"We should be generating internal object ids automatically using the path to them, as well as the downward transitive closure to ensure uniqueness and lack of sharing between document classes.
Some network hash solution for all existential insertions might also be possible.",2021-09-07T20:09:19Z,GavinMendelGleason,https://github.com/terminusdb/terminusdb/issues/309#issuecomment-914590875,Fixed in PR #499,fix in pr # 499,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,310,2021-04-19T10:56:40Z,matko,fully support nonpersistent mode,https://github.com/terminusdb/terminusdb/issues/310,"TerminusDB now has a startup flag, -m, for starting in nonpersistent mode. Unfortunately there's some missing odds and ends that make this not entirely work.
Label deletion is currently implemented with a workaround, where server knows about the on-disk file format and manually deletes labels. This should be handled by store. This is now implemented in the rust library through terminusdb/terminusdb-store#8, but not yet in the prolog library.
Similarly, layer size calculations are also done by server through knowledge of the file layout. This too should be done by the store library.",,,,,,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,312,2021-04-20T13:12:57Z,matko,organization name filter is too strict,https://github.com/terminusdb/terminusdb/issues/312,"in organization_database_name/3, there is a check that the organization name is not 'excluded'. This exclusion is implemented through a regex, which appears to be way too strict. For example, any instance of 'db' anywhere in the organization name will now throw an error.
This is way too strict. We should make this regex match only things that are truly problematic.",2021-04-20T15:08:42Z,rrooij,https://github.com/terminusdb/terminusdb/issues/312#issuecomment-823352482,Related pull request. #313,related pull request # 313,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,312,2021-04-20T13:12:57Z,matko,organization name filter is too strict,https://github.com/terminusdb/terminusdb/issues/312,"in organization_database_name/3, there is a check that the organization name is not 'excluded'. This exclusion is implemented through a regex, which appears to be way too strict. For example, any instance of 'db' anywhere in the organization name will now throw an error.
This is way too strict. We should make this regex match only things that are truly problematic.",2021-04-22T09:31:22Z,rrooij,https://github.com/terminusdb/terminusdb/issues/312#issuecomment-824688953,Solved in that pull request,solve in that pull request,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,316,2021-05-06T17:33:16Z,jamesnvc,Error visiting web interface on Ubuntu Linux,https://github.com/terminusdb/terminusdb/issues/316,"After installing the terminus server, init-ing the db, and starting the server, trying to visit the web interface gives the following error in the terminal where the server is running:
ERROR: [Thread httpsd6363_2] SSL(1408F09C) ssl3_get_record: http request

To Reproduce

In swipl: pack_install(terminus_store_prolog), pack_install(tus).
make
./terminusdb  store init --key ""foobarbaz""  (shows ""Successfully initialised database!!!"")
./terminusdb serve
open localhost:6363 in the browser
Browser fails to load the page, terminal displays ""ssl3_get_record"" error message.

Expected behavior
I would be able to view the server
Info (please complete the following information):

OS: Ubuntu 20.04.2 LTS
How did you run terminus-server: manual installation
SWI Prolog version: SWI-Prolog version 8.3.21 for x86_64-linux (installed via swivm)",2021-05-06T19:19:02Z,jamesnvc,https://github.com/terminusdb/terminusdb/issues/316#issuecomment-833795291,"Ah, I see the problem -- I was trying to connect over http, not https. Perhaps the server should be configured to redirect?",ah i see the problem -- i be try to connect over http not https perhaps the server should be configure to redirect,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,319,2021-05-13T08:41:57Z,panrobot,Incomplete build instruction for Ubuntu,https://github.com/terminusdb/terminusdb/issues/319,"I have followed Your instruction for building terminus server under Ubuntu as presented in the BUILD file. Unfortunately instruction is incomplete.

apt install swi-prolog on Ubuntu server 20.04 installs some old 7.x version which fails on installing packs (some HTTP/HTTPS transition error). I had to install swi-prolog from their Ubuntu PPA
Once installed, you will have to install one library dependency for our storage backend however a code snippet has also a line for installing a tus library.
make after git clone fails with error

swipl -t 'main,halt.' -O -q -f src/bootstrap.pl
ERROR: /home/terminus/.local/share/swi-prolog/pack/terminus_store_prolog/prolog/terminus_store.pl:68:
ERROR:    /home/terminus/.local/share/swi-prolog/pack/terminus_store_prolog/prolog/terminus_store.pl:68: Initialization goal raised exception:
ERROR:    '$open_shared_object'/3: libterminus_store: cannot open shared object file: No such file or directory

to fix this I had to go to ~/.local/share/swi-prolog/pack/terminus_store_prolog/prolog and run ./make.sh. It compiles a lot of libraries with terminus-store and termins-store-prolog at the end. The clue was resolve the hard coded shared lib mess #10 in the terminus_store_prolog repo
After all of these steps it finally works :)
Thank you very much for a great piece of software!",2021-05-13T08:48:45Z,rrooij,https://github.com/terminusdb/terminusdb/issues/319#issuecomment-840417348,"You are totally right, the instructions should be updated. The Debian version of SWI Prolog is pretty recent and I based the instructions on that one, but indeed the Ubuntu LTS versions offer an ancient version of SWI Prolog where users definitely need the version of the repo you mentioned.",-PRON- be totally right the instruction should be update the debian version of swi prolog be pretty recent and i base the instruction on that one but indeed the ubuntu lts version offer an ancient version of swi prolog where user definitely need the version of the repo -PRON- mention,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,322,2021-05-17T15:10:09Z,rrooij,Possible to delete root account and completely lock yourself out,https://github.com/terminusdb/terminusdb/issues/322,"Describe the bug
When you delete the root user, you end up with a malfunctioning system database.
To Reproduce
Steps to reproduce the behavior:

Start TerminusDB
Run: curl -k  'https://127.0.0.1:6363/api/user/admin' -u 'admin:root' -X DELETE
You are locked out of your database forever and have to start from scratch, see:

 curl -k 'https://127.0.0.1:6363/api/' -u 'admin:root'
{
  ""@type"":""api:ErrorResponse"",
  ""api:error"": {""@type"":""api:IncorrectAuthenticationError""},
  ""api:message"":""Incorrect authentication information"",
  ""api:status"":""api:failure""
}

Expected behavior
Either we give an error that this should not be possible, or we add user management to the CLI to at least provide people with a mechanism to rescue their databases.",,,,,,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,323,2021-05-17T15:24:26Z,rrooij,User management: deleting a user with JSON payload fails,https://github.com/terminusdb/terminusdb/issues/323,"Describe the bug
The api/user DELETE with payload is broken and returns:
{
  ""api:message"":""Type error for _15166{agent_name:_15168} which should be callable"",
  ""api:status"":""api:failure"",
  ""system:witnesses"": [
    {
      ""@type"":""vio:ViolationWithDatatypeObject"",
      ""vio:literal"":""_15166{agent_name:_15168}"",
      ""vio:message"":""Type error for _15166{agent_name:_15168} which should be callable"",
      ""vio:type"":""callable""
    }
  ]
}

To Reproduce
Steps to reproduce the behavior:

Create a new user

curl -k  'https://127.0.0.1:6363/api/user' -u 'admin:root' -X POST -d '{ ""agent_name"": ""test"", ""user_identifier"": ""test"", ""comment"": ""TESTUSER"" }' -H 'Content-Type: application/json'

Delete the user with the same payload

curl -k  'https://127.0.0.1:6363/api/user' -u 'admin:root' -X DELETE -d '{ ""agent_name"": ""test"", ""user_identifier"": ""test"", ""comment"": ""TESTUSER"" }' -H 'Content-Type: application/json'
Expected behavior
It should delete the newly created test user.",2021-08-09T13:20:20Z,AstroChelonian,https://github.com/terminusdb/terminusdb/issues/323#issuecomment-895216828,This will be solved by the new terminus release.,this will be solve by the new terminus release,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,324,2021-05-17T19:04:13Z,jamesnvc,Running create schema queries not working on fresh install,https://github.com/terminusdb/terminusdb/issues/324,"Describe the bug
After building TerminusDB and trying to run queries from tutorials in the web interface, I get an error stating Badly formed ast after compilation with term.
To Reproduce
Steps to reproduce the behavior:

Open the web interface at localhost:6363
Create a new database
Go to ""Query""
Run, e.g. WOQL.doctype(""scm:student"").property(""scm:name"", ""xsd:string"") (although various other queries from the tutorials have given me the same error)
Error message like Badly formed ast after compilation

Full error message
Badly formed ast after compilation with term: insert('terminusdb:///schema#student','http://www.w3.org/1999/02/22-rdf-syntax-ns#type','http://www.w3.org/2002/07/owl#Class',""schema/main""),insert('terminusdb:///schema#student','http://www.w3.org/2000/01/rdf-schema#subClassOf','http://terminusdb.com/schema/system#Document',""schema/main""),insert('terminusdb:///schema#name','http://www.w3.org/1999/02/22-rdf-syntax-ns#type','http://www.w3.org/2002/07/owl#DatatypeProperty',""schema/main""),insert('terminusdb:///schema#name','http://www.w3.org/2000/01/rdf-schema#range','http://www.w3.org/2001/XMLSchema#string',""schema/main""),insert('terminusdb:///schema#name','http://www.w3.org/2000/01/rdf-schema#domain','terminusdb:///schema#student',""schema/main"")less

{
  ""data"": {
    ""@type"": ""api:WoqlErrorResponse"",
    ""api:error"": {
      ""@type"": ""api:WOQLSyntaxError"",
      ""api:error_term"": ""insert('terminusdb:///schema#student','http://www.w3.org/1999/02/22-rdf-syntax-ns#type','http://www.w3.org/2002/07/owl#Class',\""schema/main\""),insert('terminusdb:///schema#student','http://www.w3.org/2000/01/rdf-schema#subClassOf','http://terminusdb.com/schema/system#Document',\""schema/main\""),insert('terminusdb:///schema#name','http://www.w3.org/1999/02/22-rdf-syntax-ns#type','http://www.w3.org/2002/07/owl#DatatypeProperty',\""schema/main\""),insert('terminusdb:///schema#name','http://www.w3.org/2000/01/rdf-schema#range','http://www.w3.org/2001/XMLSchema#string',\""schema/main\""),insert('terminusdb:///schema#name','http://www.w3.org/2000/01/rdf-schema#domain','terminusdb:///schema#student',\""schema/main\"")""
    },
    ""api:message"": ""Badly formed ast after compilation with term: insert('terminusdb:///schema#student','http://www.w3.org/1999/02/22-rdf-syntax-ns#type','http://www.w3.org/2002/07/owl#Class',\""schema/main\""),insert('terminusdb:///schema#student','http://www.w3.org/2000/01/rdf-schema#subClassOf','http://terminusdb.com/schema/system#Document',\""schema/main\""),insert('terminusdb:///schema#name','http://www.w3.org/1999/02/22-rdf-syntax-ns#type','http://www.w3.org/2002/07/owl#DatatypeProperty',\""schema/main\""),insert('terminusdb:///schema#name','http://www.w3.org/2000/01/rdf-schema#range','http://www.w3.org/2001/XMLSchema#string',\""schema/main\""),insert('terminusdb:///schema#name','http://www.w3.org/2000/01/rdf-schema#domain','terminusdb:///schema#student',\""schema/main\"")"",
    ""api:status"": ""api:failure"",
    ""end"": ""2021-05-17T18:54:16.812Z"",
    ""start"": ""2021-05-17T18:54:16.812Z"",
    ""duration"": ""2021-05-17T18:54:16.812Z""
  }
}


Expected behavior
The quads would be inserted
Screenshots

Info (please complete the following information):

OS: Ubuntu 20.04.2 LTS
How did you run terminus-server: manual installation
SWI Prolog version: 8.3.21 (via swivm)",2021-05-18T14:54:18Z,jamesnvc,https://github.com/terminusdb/terminusdb/issues/324#issuecomment-843241518,"I also get the same result using the python client:
client = WOQLClient(server_url='https://127.0.0.1:6363')
client.connect(key='foobarbaz', account='admin', user='admin', db='university')
WOQLQuery().doctype(""scm:student"").property(""scm:name"", ""xsd:string"").execute(client, ""student schema created."")
throws the same error:
terminusdb_client.errors.DatabaseError: Badly formed ast after compilation with term: insert('terminusdb:///schema#student','http://www.w3.org/1999/02/22-rdf-syntax-ns#type','http://www.w3.org/2002/07/owl#Class',""schema/main""),insert('terminusdb:///schema#student','http://www.w3.org/2000/01/rdf-schema#subClassOf','http://terminusdb.com/schema/system#Document',""schema/main""),insert('terminusdb:///schema#name','http://www.w3.org/1999/02/22-rdf-syntax-ns#type','http://www.w3.org/2002/07/owl#DatatypeProperty',""schema/main""),insert('terminusdb:///schema#name','http://www.w3.org/2000/01/rdf-schema#range','http://www.w3.org/2001/XMLSchema#string',""schema/main""),insert('terminusdb:///schema#name','http://www.w3.org/2000/01/rdf-schema#domain','terminusdb:///schema#student',""schema/main"")","i also get the same result use the python client client = woqlclient(server_url='https//1270016363 ' ) clientconnect(key='foobarbaz ' account='admin ' user='admin ' db='university ' ) woqlquery()doctype(""scmstudent"")property(""scmname "" "" xsdstring"")execute(client "" student schema create "" ) throw the same error terminusdb_clienterrorsdatabaseerror badly form ast after compilation with term insert('terminusdb///schema#student''http//wwww3org/1999/02/22-rdf - syntax - ns#type''http//wwww3org/2002/07 / owl#class'""schema / main"")insert('terminusdb///schema#student''http//wwww3org/2000/01 / rdf - schema#subclassof''http//terminusdbcom / schema / system#document'""schema / main"")insert('terminusdb///schema#name''http//wwww3org/1999/02/22-rdf - syntax - ns#type''http//wwww3org/2002/07 / owl#datatypeproperty'""schema / main"")insert('terminusdb///schema#name''http//wwww3org/2000/01 / rdf - schema#range''http//wwww3org/2001 / xmlschema#string'""schema / main"")insert('terminusdb///schema#name''http//wwww3org/2000/01 / rdf - schema#domain''terminusdb///schema#student'""schema / main "" )",0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,324,2021-05-17T19:04:13Z,jamesnvc,Running create schema queries not working on fresh install,https://github.com/terminusdb/terminusdb/issues/324,"Describe the bug
After building TerminusDB and trying to run queries from tutorials in the web interface, I get an error stating Badly formed ast after compilation with term.
To Reproduce
Steps to reproduce the behavior:

Open the web interface at localhost:6363
Create a new database
Go to ""Query""
Run, e.g. WOQL.doctype(""scm:student"").property(""scm:name"", ""xsd:string"") (although various other queries from the tutorials have given me the same error)
Error message like Badly formed ast after compilation

Full error message
Badly formed ast after compilation with term: insert('terminusdb:///schema#student','http://www.w3.org/1999/02/22-rdf-syntax-ns#type','http://www.w3.org/2002/07/owl#Class',""schema/main""),insert('terminusdb:///schema#student','http://www.w3.org/2000/01/rdf-schema#subClassOf','http://terminusdb.com/schema/system#Document',""schema/main""),insert('terminusdb:///schema#name','http://www.w3.org/1999/02/22-rdf-syntax-ns#type','http://www.w3.org/2002/07/owl#DatatypeProperty',""schema/main""),insert('terminusdb:///schema#name','http://www.w3.org/2000/01/rdf-schema#range','http://www.w3.org/2001/XMLSchema#string',""schema/main""),insert('terminusdb:///schema#name','http://www.w3.org/2000/01/rdf-schema#domain','terminusdb:///schema#student',""schema/main"")less

{
  ""data"": {
    ""@type"": ""api:WoqlErrorResponse"",
    ""api:error"": {
      ""@type"": ""api:WOQLSyntaxError"",
      ""api:error_term"": ""insert('terminusdb:///schema#student','http://www.w3.org/1999/02/22-rdf-syntax-ns#type','http://www.w3.org/2002/07/owl#Class',\""schema/main\""),insert('terminusdb:///schema#student','http://www.w3.org/2000/01/rdf-schema#subClassOf','http://terminusdb.com/schema/system#Document',\""schema/main\""),insert('terminusdb:///schema#name','http://www.w3.org/1999/02/22-rdf-syntax-ns#type','http://www.w3.org/2002/07/owl#DatatypeProperty',\""schema/main\""),insert('terminusdb:///schema#name','http://www.w3.org/2000/01/rdf-schema#range','http://www.w3.org/2001/XMLSchema#string',\""schema/main\""),insert('terminusdb:///schema#name','http://www.w3.org/2000/01/rdf-schema#domain','terminusdb:///schema#student',\""schema/main\"")""
    },
    ""api:message"": ""Badly formed ast after compilation with term: insert('terminusdb:///schema#student','http://www.w3.org/1999/02/22-rdf-syntax-ns#type','http://www.w3.org/2002/07/owl#Class',\""schema/main\""),insert('terminusdb:///schema#student','http://www.w3.org/2000/01/rdf-schema#subClassOf','http://terminusdb.com/schema/system#Document',\""schema/main\""),insert('terminusdb:///schema#name','http://www.w3.org/1999/02/22-rdf-syntax-ns#type','http://www.w3.org/2002/07/owl#DatatypeProperty',\""schema/main\""),insert('terminusdb:///schema#name','http://www.w3.org/2000/01/rdf-schema#range','http://www.w3.org/2001/XMLSchema#string',\""schema/main\""),insert('terminusdb:///schema#name','http://www.w3.org/2000/01/rdf-schema#domain','terminusdb:///schema#student',\""schema/main\"")"",
    ""api:status"": ""api:failure"",
    ""end"": ""2021-05-17T18:54:16.812Z"",
    ""start"": ""2021-05-17T18:54:16.812Z"",
    ""duration"": ""2021-05-17T18:54:16.812Z""
  }
}


Expected behavior
The quads would be inserted
Screenshots

Info (please complete the following information):

OS: Ubuntu 20.04.2 LTS
How did you run terminus-server: manual installation
SWI Prolog version: 8.3.21 (via swivm)",2021-05-19T20:58:18Z,jamesnvc,https://github.com/terminusdb/terminusdb/issues/324#issuecomment-844467940,"After running the query inside swipl, I was able to see that the error was happening because the schema/main graph didn't exist. I was able to manually create the graph and then things work.


Should that graph have been automatically created at some point?


It was very difficult to figure that out via the error messages, because it seems the errors in woql_compile:compile_query/4 somehow get swallowed and just sent as a generic badly_formed_ast term.",after run the query inside swipl i be able to see that the error be happen because the schema / main graph do not exist i be able to manually create the graph and then thing work should that graph have be automatically create at some point -PRON- be very difficult to figure that out via the error message because -PRON- seem the error in woql_compilecompile_query/4 somehow get swallow and just send as a generic badly_formed_ast term,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,324,2021-05-17T19:04:13Z,jamesnvc,Running create schema queries not working on fresh install,https://github.com/terminusdb/terminusdb/issues/324,"Describe the bug
After building TerminusDB and trying to run queries from tutorials in the web interface, I get an error stating Badly formed ast after compilation with term.
To Reproduce
Steps to reproduce the behavior:

Open the web interface at localhost:6363
Create a new database
Go to ""Query""
Run, e.g. WOQL.doctype(""scm:student"").property(""scm:name"", ""xsd:string"") (although various other queries from the tutorials have given me the same error)
Error message like Badly formed ast after compilation

Full error message
Badly formed ast after compilation with term: insert('terminusdb:///schema#student','http://www.w3.org/1999/02/22-rdf-syntax-ns#type','http://www.w3.org/2002/07/owl#Class',""schema/main""),insert('terminusdb:///schema#student','http://www.w3.org/2000/01/rdf-schema#subClassOf','http://terminusdb.com/schema/system#Document',""schema/main""),insert('terminusdb:///schema#name','http://www.w3.org/1999/02/22-rdf-syntax-ns#type','http://www.w3.org/2002/07/owl#DatatypeProperty',""schema/main""),insert('terminusdb:///schema#name','http://www.w3.org/2000/01/rdf-schema#range','http://www.w3.org/2001/XMLSchema#string',""schema/main""),insert('terminusdb:///schema#name','http://www.w3.org/2000/01/rdf-schema#domain','terminusdb:///schema#student',""schema/main"")less

{
  ""data"": {
    ""@type"": ""api:WoqlErrorResponse"",
    ""api:error"": {
      ""@type"": ""api:WOQLSyntaxError"",
      ""api:error_term"": ""insert('terminusdb:///schema#student','http://www.w3.org/1999/02/22-rdf-syntax-ns#type','http://www.w3.org/2002/07/owl#Class',\""schema/main\""),insert('terminusdb:///schema#student','http://www.w3.org/2000/01/rdf-schema#subClassOf','http://terminusdb.com/schema/system#Document',\""schema/main\""),insert('terminusdb:///schema#name','http://www.w3.org/1999/02/22-rdf-syntax-ns#type','http://www.w3.org/2002/07/owl#DatatypeProperty',\""schema/main\""),insert('terminusdb:///schema#name','http://www.w3.org/2000/01/rdf-schema#range','http://www.w3.org/2001/XMLSchema#string',\""schema/main\""),insert('terminusdb:///schema#name','http://www.w3.org/2000/01/rdf-schema#domain','terminusdb:///schema#student',\""schema/main\"")""
    },
    ""api:message"": ""Badly formed ast after compilation with term: insert('terminusdb:///schema#student','http://www.w3.org/1999/02/22-rdf-syntax-ns#type','http://www.w3.org/2002/07/owl#Class',\""schema/main\""),insert('terminusdb:///schema#student','http://www.w3.org/2000/01/rdf-schema#subClassOf','http://terminusdb.com/schema/system#Document',\""schema/main\""),insert('terminusdb:///schema#name','http://www.w3.org/1999/02/22-rdf-syntax-ns#type','http://www.w3.org/2002/07/owl#DatatypeProperty',\""schema/main\""),insert('terminusdb:///schema#name','http://www.w3.org/2000/01/rdf-schema#range','http://www.w3.org/2001/XMLSchema#string',\""schema/main\""),insert('terminusdb:///schema#name','http://www.w3.org/2000/01/rdf-schema#domain','terminusdb:///schema#student',\""schema/main\"")"",
    ""api:status"": ""api:failure"",
    ""end"": ""2021-05-17T18:54:16.812Z"",
    ""start"": ""2021-05-17T18:54:16.812Z"",
    ""duration"": ""2021-05-17T18:54:16.812Z""
  }
}


Expected behavior
The quads would be inserted
Screenshots

Info (please complete the following information):

OS: Ubuntu 20.04.2 LTS
How did you run terminus-server: manual installation
SWI Prolog version: 8.3.21 (via swivm)",2021-08-09T13:20:05Z,AstroChelonian,https://github.com/terminusdb/terminusdb/issues/324#issuecomment-895216627,This will be solved by the new terminus release.,this will be solve by the new terminus release,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,327,2021-05-24T22:36:32Z,borgauf,Cannot get terminus_store_prolog for terminusdb make,https://github.com/terminusdb/terminusdb/issues/327,"?- pack_install(terminus_store_prolog).
% Contacting server at http://www.swi-prolog.org/pack/query ...
ERROR: url `'https://www.swi-prolog.org/pack/query'' does not exist (status(500,Internal Server Error))
ERROR: In:
ERROR:   [18] throw(error(existence_error(url,'https://www.swi-prolog.org/pack/query'),context(_8356,...)))
ERROR:   [16] http_open:try_http_proxy(direct,[uri('http://www.swi-prolog.org/pack/query'),...|...],_8392,[post(...),...]) at /usr/local/lib/swipl/library/http/http_open.pl:425
ERROR:   [14] <meta call>
ERROR:   [13] '$sig_atomic'(prolog_pack:http_open('http://www.swi-prolog.org/pack/query',_8486,...)) <foreign>
ERROR:   [12] setup_call_catcher_cleanup(prolog_pack:http_open('http://www.swi-prolog.org/pack/query',_8530,...),prolog_pack:read_reply(_8542,_8544,_8546),_8516,prolog_pack:close(_8556)) at /usr/local/lib/swipl/boot/init.pl:469
ERROR:   [10] prolog_pack:query_pack_server(locate(terminus_store_prolog),_8588,[]) at /usr/local/lib/swipl/library/prolog_pack.pl:1733
ERROR:    [9] prolog_pack:pack_default_options(terminus_store_prolog,terminus_store_prolog,[],_8630) at /usr/local/lib/swipl/library/prolog_pack.pl:516
ERROR:    [8] prolog_pack:pack_install(terminus_store_prolog) at /usr/local/lib/swipl/library/prolog_pack.pl:457
ERROR:    [7] <user>
ERROR: 
ERROR: Note: some frames are missing due to last-call optimization.
ERROR: Re-run your program in debug mode (:- debug.) to get more detail.

I'm on Ubuntu 21.04 and had to settle for SWI-Prolog 8.0.2 due to no repo for 21.04 yet. I tried the manual install of terminus_store_prolog from github. It failed too.
> ./make.sh 
mkdir -p lib/x86_64-linux
cd rust; cargo build --release
   Compiling swipl v0.3.3
error[E0425]: cannot find function, tuple struct or tuple variant `PL_put_uint64` in this scope
    --> /home/olwegm/.cargo/registry/src/github.com-1ecc6299db9ec823/swipl-0.3.3/src/term.rs:756:18
     |
756  |         unsafe { PL_put_uint64(term.term, *self) };
     |                  ^^^^^^^^^^^^^ help: a function with a similar name exists: `PL_put_int64`
     | 
    ::: /home/olwegm/terminus_store_prolog/rust/target/release/build/swipl-fli-36a0903ae793c6ef/out/bindings.rs:6035:5
     |
6035 |     pub fn PL_put_int64(t: term_t, i: i64) -> ::std::os::raw::c_int;
     |     ---------------------------------------------------------------- similarly named function `PL_put_int64` defined here

error: aborting due to previous error",2021-05-25T07:53:21Z,rrooij,https://github.com/terminusdb/terminusdb/issues/327#issuecomment-847638696,"Unfortunately, that version of SWI Prolog is too old to work. The PL_put_int64 function was added in later versions of SWI Prolog.
See: SWI-Prolog/swipl-devel@170e88d#diff-ba0428e019fa32bd1185e69819ab77829415589fb7176aeec2be90c5f9cc1a0e
Is using the Docker image a possiblity for you?",unfortunately that version of swi prolog be too old to work the pl_put_int64 function be add in later version of swi prolog see swi - prolog / swipl - devel@170e88d#diff - ba0428e019fa32bd1185e69819ab77829415589fb7176aeec2be90c5f9cc1a0e be use the docker image a possiblity for -PRON-,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,327,2021-05-24T22:36:32Z,borgauf,Cannot get terminus_store_prolog for terminusdb make,https://github.com/terminusdb/terminusdb/issues/327,"?- pack_install(terminus_store_prolog).
% Contacting server at http://www.swi-prolog.org/pack/query ...
ERROR: url `'https://www.swi-prolog.org/pack/query'' does not exist (status(500,Internal Server Error))
ERROR: In:
ERROR:   [18] throw(error(existence_error(url,'https://www.swi-prolog.org/pack/query'),context(_8356,...)))
ERROR:   [16] http_open:try_http_proxy(direct,[uri('http://www.swi-prolog.org/pack/query'),...|...],_8392,[post(...),...]) at /usr/local/lib/swipl/library/http/http_open.pl:425
ERROR:   [14] <meta call>
ERROR:   [13] '$sig_atomic'(prolog_pack:http_open('http://www.swi-prolog.org/pack/query',_8486,...)) <foreign>
ERROR:   [12] setup_call_catcher_cleanup(prolog_pack:http_open('http://www.swi-prolog.org/pack/query',_8530,...),prolog_pack:read_reply(_8542,_8544,_8546),_8516,prolog_pack:close(_8556)) at /usr/local/lib/swipl/boot/init.pl:469
ERROR:   [10] prolog_pack:query_pack_server(locate(terminus_store_prolog),_8588,[]) at /usr/local/lib/swipl/library/prolog_pack.pl:1733
ERROR:    [9] prolog_pack:pack_default_options(terminus_store_prolog,terminus_store_prolog,[],_8630) at /usr/local/lib/swipl/library/prolog_pack.pl:516
ERROR:    [8] prolog_pack:pack_install(terminus_store_prolog) at /usr/local/lib/swipl/library/prolog_pack.pl:457
ERROR:    [7] <user>
ERROR: 
ERROR: Note: some frames are missing due to last-call optimization.
ERROR: Re-run your program in debug mode (:- debug.) to get more detail.

I'm on Ubuntu 21.04 and had to settle for SWI-Prolog 8.0.2 due to no repo for 21.04 yet. I tried the manual install of terminus_store_prolog from github. It failed too.
> ./make.sh 
mkdir -p lib/x86_64-linux
cd rust; cargo build --release
   Compiling swipl v0.3.3
error[E0425]: cannot find function, tuple struct or tuple variant `PL_put_uint64` in this scope
    --> /home/olwegm/.cargo/registry/src/github.com-1ecc6299db9ec823/swipl-0.3.3/src/term.rs:756:18
     |
756  |         unsafe { PL_put_uint64(term.term, *self) };
     |                  ^^^^^^^^^^^^^ help: a function with a similar name exists: `PL_put_int64`
     | 
    ::: /home/olwegm/terminus_store_prolog/rust/target/release/build/swipl-fli-36a0903ae793c6ef/out/bindings.rs:6035:5
     |
6035 |     pub fn PL_put_int64(t: term_t, i: i64) -> ::std::os::raw::c_int;
     |     ---------------------------------------------------------------- similarly named function `PL_put_int64` defined here

error: aborting due to previous error",2021-06-02T13:00:42Z,rrooij,https://github.com/terminusdb/terminusdb/issues/327#issuecomment-853006905,"I'm closing the issue by the way, since we don't support this version of SWI Prolog. I hope that the PPA will add support for 21.04 soon! In the meantime, you can either compile SWI Prolog yourself, use SWIVM or use the Docker image.",-PRON- be close the issue by the way since -PRON- do not support this version of swi prolog i hope that the ppa will add support for 2104 soon in the meantime -PRON- can either compile swi prolog -PRON- use swivm or use the docker image,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,329,2021-05-29T05:32:07Z,hoijnet,CSV import separator should be configurable in the format configuration,https://github.com/terminusdb/terminusdb/issues/329,"Is your feature request related to a problem? Please describe.
CSV files produced in different locales have different default separators, a comma-separated file ("","") is but one type of CSV.
In Sweden we usually use a semicolon ("";"") as separator and many CSV files use a tab as separator (""\t""). Importing CSVs requires reformatting them before import, and causes inoperability issues and friction to use with Excel. This causes quite some extra work to produce CSVs in the right format, or having to insert a converting step.
Describe the solution you'd like
I would like a CSV with semicolon to be importable with a configurable separator in the option json like so:
WOQL.get(
    WOQL.as(""Title"",""v:Title"")
        .as(""Name"", ""v:Name"")
).file(""/app/local_files/items.csv"", {type: ""csv"", separator: "";""})

Possible vs not possible:

Works currently when using the WOQL above with CSV with comma separator: {type: ""csv""}
What should work with semicolon CSV field separator: {type: ""csv"", separator: "";""}

Describe alternatives you've considered
I have considered to use ("","") as a separator, while this is possible in limited scenarios, it is a hassle, and I imagine friction for those that want to continue from the bike example to continue with their own CSV files in exploration and trying out.
It looks like the src/terminus-schema/woql.owl.ttl schema includes format_header for CSVs as an option which is not documented with the file import function (but used in examples), to add the ""separator"" with a separator character (single char) would be how it would be expected to work.
It would likely be added to the src/core/api/api_csv.pl file and use separator from the Prolog library, but being new to prolog, this is about as far as I have come in my exploration.
Additional context
It might be helpful to add support for tab separator too, unsure if that would be through a literal tab, or through the (""\t"") clear way of expressing the tab and thus causing less errors.",2021-05-29T05:47:38Z,hoijnet,https://github.com/terminusdb/terminusdb/issues/329#issuecomment-850775645,"Might be relevant to implement the following configuration options at the same time:

ignore_quotes: boolean
strip (or named strip_whitespace): boolean
skip_header (or named skip_prefixed): string for line prefix to cause exclusion",may be relevant to implement the follow configuration option at the same time ignore_quotes boolean strip ( or name strip_whitespace ) boolean skip_header ( or name skip_prefixed ) string for line prefix to cause exclusion,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,331,2021-06-02T12:32:16Z,rrooij,Tests require internet connection,https://github.com/terminusdb/terminusdb/issues/331,"Some tests require an internet connection. For instance, the bike CSV is loaded from a remote file. The best way to do these kinds of tests is to spawn a temporary HTTP file server that serves the files locally.
Tests with this problem:
jwt_auth
indexed_get
named_get
And probably some more",,,,,,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,333,2021-06-15T07:51:07Z,mikkokotila,Tibetan Corpus on Terminus,https://github.com/terminusdb/terminusdb/issues/333,"I was not sure where to put this kind of topic, so posting it here.
We have a non-profit project where we are looking for ways to make roughly 200,000 Tibetan language texts available in the most meaningful way. It is about 20,000,000 pages of text in total.
These are basically the texts that were transmitted from India to Tibet (from Sanskrit to Tibetan) over a few centuries about a thousand years ago. It almost entirely consists of various mind training and meditation manuals, and philosophical treatises. Most of the texts are no longer available in Sanskrit.
We want to store the text in tokens instead of full text because from tokens we get the full text, but from the full text, it takes time to get tokens. 
The body of the text is static i.e. reading and any preprocessing must be only done once ever.
There is rich meta-data available from several sources.
Given the way, the Tibetan language follows a particular way of encoding meaning to words, similar to Sanskrit, where the words themselves concretely connect things and topics, and the way the body of knowledge the language is used to describe is actually an honest ontology looks like a really interesting use-case for Terminus.
A typical downstream use case is where the translator wants to understand the context for a given word or a scholar wants to find documents that are related semantically or otherwise.
At the moment we are planning to build our own datastore, but it dawned on me that there might be some really interesting opportunity in building on top of Terminus.",2021-06-15T09:25:53Z,luke-feeney,https://github.com/terminusdb/terminusdb/issues/333#issuecomment-861340758,Interesting @mikkokotila - similar to one of the suggestions that came up during a recent brainstorming session (@matko was the proposer). I think it would be an interesting use case for terminus.,interesting @mikkokotila - similar to one of the suggestion that come up during a recent brainstorming session ( @matko be the proposer ) i think -PRON- would be an interesting use case for terminus,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,333,2021-06-15T07:51:07Z,mikkokotila,Tibetan Corpus on Terminus,https://github.com/terminusdb/terminusdb/issues/333,"I was not sure where to put this kind of topic, so posting it here.
We have a non-profit project where we are looking for ways to make roughly 200,000 Tibetan language texts available in the most meaningful way. It is about 20,000,000 pages of text in total.
These are basically the texts that were transmitted from India to Tibet (from Sanskrit to Tibetan) over a few centuries about a thousand years ago. It almost entirely consists of various mind training and meditation manuals, and philosophical treatises. Most of the texts are no longer available in Sanskrit.
We want to store the text in tokens instead of full text because from tokens we get the full text, but from the full text, it takes time to get tokens. 
The body of the text is static i.e. reading and any preprocessing must be only done once ever.
There is rich meta-data available from several sources.
Given the way, the Tibetan language follows a particular way of encoding meaning to words, similar to Sanskrit, where the words themselves concretely connect things and topics, and the way the body of knowledge the language is used to describe is actually an honest ontology looks like a really interesting use-case for Terminus.
A typical downstream use case is where the translator wants to understand the context for a given word or a scholar wants to find documents that are related semantically or otherwise.
At the moment we are planning to build our own datastore, but it dawned on me that there might be some really interesting opportunity in building on top of Terminus.",2021-06-15T11:24:58Z,mikkokotila,https://github.com/terminusdb/terminusdb/issues/333#issuecomment-861417127,"Beautiful proposal! How wonderful.
What would be the best way to explore this further? For example, if at some point there was an RFC or some other ""the thing to be built"" definition I think the resources for building, at least the majority of it, could come from us.","beautiful proposal how wonderful what would be the good way to explore this further for example if at some point there be an rfc or some other "" the thing to be build "" definition i think the resource for build at least the majority of -PRON- could come from -PRON-",0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,333,2021-06-15T07:51:07Z,mikkokotila,Tibetan Corpus on Terminus,https://github.com/terminusdb/terminusdb/issues/333,"I was not sure where to put this kind of topic, so posting it here.
We have a non-profit project where we are looking for ways to make roughly 200,000 Tibetan language texts available in the most meaningful way. It is about 20,000,000 pages of text in total.
These are basically the texts that were transmitted from India to Tibet (from Sanskrit to Tibetan) over a few centuries about a thousand years ago. It almost entirely consists of various mind training and meditation manuals, and philosophical treatises. Most of the texts are no longer available in Sanskrit.
We want to store the text in tokens instead of full text because from tokens we get the full text, but from the full text, it takes time to get tokens. 
The body of the text is static i.e. reading and any preprocessing must be only done once ever.
There is rich meta-data available from several sources.
Given the way, the Tibetan language follows a particular way of encoding meaning to words, similar to Sanskrit, where the words themselves concretely connect things and topics, and the way the body of knowledge the language is used to describe is actually an honest ontology looks like a really interesting use-case for Terminus.
A typical downstream use case is where the translator wants to understand the context for a given word or a scholar wants to find documents that are related semantically or otherwise.
At the moment we are planning to build our own datastore, but it dawned on me that there might be some really interesting opportunity in building on top of Terminus.",2021-06-15T12:57:10Z,matko,https://github.com/terminusdb/terminusdb/issues/333#issuecomment-861474973,"Hey there!
This is a very exciting proposal. I was actually intending to build something like this, except for the Pali canon, which is the Theravada Buddhist corpus. Maybe we can work together on a common core, though I suspect the Tibetan and Pali languages are sufficiently different that we'll need slightly different things in the end.
Right now, we're working very hard on getting a new schema language and document interface released. I'd definitely wait for this to land before attempting to implement this, as it'll make your life a lot easier. This should happen somewhere in the next few weeks.
I agree on the benefit of dealing with tokens instead of raw strings. A string is just a value. A token is a node which, besides its written representation, can have various sorts of metadata associated with it, such as a grammatical case, a translation, an etymology, etc.
The Pali Canon use case and similarities with the Tibetan Corpus
Since I put some thought into this use case from the Pali Canon perspective, I'd like to share some of my thoughts on the requirements of a graph representation of a corpus. Not everything will be relevant for your use case I imagine, but hopefully, enough will be.
Pali is a highly inflected language. This means that the same concept will be expressed with a different word depending on the grammatical role in a sentence. It'd be good if it was possible to identify the various inflections of a word and group them together, so that we can quickly query not just for the exact way a particular word is written, but also all the other inflections. I believe Tibetan is far less inflected, but I'm sure there are some similar concerns there.
As you mention in your proposal, we're essentially dealing with a non-changing body of texts. Just as in the Pali canon, I imagine the Tibetan corpus employs a lot of stock phrases which you'll find throughout different texts, and it'd be very useful if we could somehow identify instances of this, and be able to provide some scholarly annotations for such occasions. In he most extreme case, entire suttas are equivalent except for one or two words. It'd be good to have a sutta template description that makes can make such occasions explicit.
Even though the Pali canon it is a non-changing body of texts, there are slight disagreements in the Theravada Buddhist world on some aspects of the canon. Different traditions group their texts slightly different, resulting in the same text being divided over a different amount of suttas. Furthermore, there are occasionally small disagreements on the exact word used in a particular sentence between the Thai, Burmese and Sri Lankan versions of the canon. For me, it'd be good to be able to register such discrepancies in a neutral way, not picking any sides.
I do not know what the situation is with classical Tibetan, but for Pali, the Pali Canon is pretty much the authoritative collection of texts that define what the language actually is. While there is secondary Pali literature, historically grammarians have considered the canon to be leading in what correct use of Pali is, and as far as I am aware there has been no major development of the language in secondary literature. This means that the Pali Canon is not just an interesting body of texts for the content, but also a linguistic study object for the Pali language itself. It would therefore be very interesting if a graph representation of the Pali Canon facilitated grammatical analysis.
I imagine that on top of the bare tokenized sentences, a researcher could build a grammatical analysis - an alternative description of the same sentence which is deeply structured along the grammar. A similar concern is there for individual words. Words in Pali are generally formed out of some root combined with a bunch of affixes, and possibly further combined into a compound word. It should be possible for a researcher to annotate a word with a description of how this construction happened.
it'd be good if a graph representation of a corpus facilitated translation efforts. There's different sorts of ways to translate a text. One can translate word for word, one can translate a sentence at a time, or one can take a whole range of text and write a more free-form translation. It'd be good to support translation annotations for all these sorts of translations.
Finally, it'd be good to be able to classify texts in various ways. An obvious one would be a classification based on their topics, so that it'd be easy to get a list of all texts that have to do with jhana for example. Another classification could be a hypothetical moment of composition, to differentiate earlier texts from later additions.
One graph, multiple tools
There are many use cases for a graph version of a corpus. Some people may just want to browse the texts. Some people may want to do active translation. Some people may be actively searching for parallels, and will be wanting to annotate the texts when they find them. With TerminusDB, it should be possible to have multiple tools all use the same rich data in different ways. Therefore, iIn my opinion, the first task of a project like this should be figuring out a high quality data model that is able to support a wide range of use cases, rather than designing the data model around one particular use case.
Final thoughts
I'm not sure if any of these thoughts are useful to you, but I strongly suspect that our use cases are similar enough that we should at least have a conversation on this. We can discuss this further in this issue, or if you like, you can reach me directly at matthijs@terminusdb.com.",hey there this be a very exciting proposal i be actually intend to build something like this except for the pali canon which be the theravada buddhist corpus maybe -PRON- can work together on a common core though i suspect the tibetan and pali language be sufficiently different that -PRON- will need slightly different thing in the end right now -PRON- be work very hard on get a new schema language and document interface release -PRON- 'd definitely wait for this to land before attempt to implement this as -PRON- will make -PRON- life a lot easy this should happen somewhere in the next few week i agree on the benefit of deal with token instead of raw string a string be just a value a token be a node which besides -PRON- write representation can have various sort of metadata associate with -PRON- such as a grammatical case a translation an etymology etc the pali canon use case and similarity with the tibetan corpus since i put some thought into this use case from the pali canon perspective -PRON- 'd like to share some of -PRON- thought on the requirement of a graph representation of a corpus not everything will be relevant for -PRON- use case i imagine but hopefully enough will be pali be a highly inflect language this mean that the same concept will be express with a different word depend on the grammatical role in a sentence -PRON- 'd be good if -PRON- be possible to identify the various inflection of a word and group -PRON- together so that -PRON- can quickly query not just for the exact way a particular word be write but also all the other inflection i believe tibetan be far less inflected but -PRON- be sure there be some similar concern there as -PRON- mention in -PRON- proposal -PRON- be essentially deal with a non - changing body of text just as in the pali canon i imagine the tibetan corpus employ a lot of stock phrase which -PRON- will find throughout different text and -PRON- 'd be very useful if -PRON- could somehow identify instance of this and be able to provide some scholarly annotation for such occasion in -PRON- most extreme case entire sutta be equivalent except for one or two word -PRON- 'd be good to have a sutta template description that make can make such occasion explicit even though the pali canon -PRON- be a non - changing body of text there be slight disagreement in the theravada buddhist world on some aspect of the canon different tradition group -PRON- text slightly different resulting in the same text being divide over a different amount of sutta furthermore there be occasionally small disagreement on the exact word use in a particular sentence between the thai burmese and sri lankan version of the canon for -PRON- -PRON- 'd be good to be able to register such discrepancy in a neutral way not pick any side i do not know what the situation be with classical tibetan but for pali the pali canon be pretty much the authoritative collection of text that define what the language actually be while there be secondary pali literature historically grammarian have consider the canon to be lead in what correct use of pali be and as far as i be aware there have be no major development of the language in secondary literature this mean that the pali canon be not just an interesting body of text for the content but also a linguistic study object for the pali language -PRON- -PRON- would therefore be very interesting if a graph representation of the pali canon facilitate grammatical analysis i imagine that on top of the bare tokenized sentence a researcher could build a grammatical analysis - an alternative description of the same sentence which be deeply structure along the grammar a similar concern be there for individual word word in pali be generally form out of some root combine with a bunch of affix and possibly further combine into a compound word -PRON- should be possible for a researcher to annotate a word with a description of how this construction happen -PRON- 'd be good if a graph representation of a corpus facilitate translation effort there be different sort of way to translate a text one can translate word for word one can translate a sentence at a time or one can take a whole range of text and write a more free - form translation -PRON- 'd be good to support translation annotation for all these sort of translation finally -PRON- 'd be good to be able to classify text in various way an obvious one would be a classification base on -PRON- topic so that -PRON- 'd be easy to get a list of all text that have to do with jhana for example another classification could be a hypothetical moment of composition to differentiate early text from later addition one graph multiple tool there be many use case for a graph version of a corpus some people may just want to browse the text some people may want to do active translation some people may be actively search for parallel and will be want to annotate the text when -PRON- find -PRON- with terminusdb -PRON- should be possible to have multiple tool all use the same rich datum in different way therefore iin -PRON- opinion the first task of a project like this should be figure out a high quality datum model that be able to support a wide range of use case rather than design the data model around one particular use case final thought -PRON- be not sure if any of these thought be useful to -PRON- but i strongly suspect that -PRON- use case be similar enough that -PRON- should at least have a conversation on this -PRON- can discuss this further in this issue or if -PRON- like -PRON- can reach -PRON- directly at matthijs@terminusdbcom,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,333,2021-06-15T07:51:07Z,mikkokotila,Tibetan Corpus on Terminus,https://github.com/terminusdb/terminusdb/issues/333,"I was not sure where to put this kind of topic, so posting it here.
We have a non-profit project where we are looking for ways to make roughly 200,000 Tibetan language texts available in the most meaningful way. It is about 20,000,000 pages of text in total.
These are basically the texts that were transmitted from India to Tibet (from Sanskrit to Tibetan) over a few centuries about a thousand years ago. It almost entirely consists of various mind training and meditation manuals, and philosophical treatises. Most of the texts are no longer available in Sanskrit.
We want to store the text in tokens instead of full text because from tokens we get the full text, but from the full text, it takes time to get tokens. 
The body of the text is static i.e. reading and any preprocessing must be only done once ever.
There is rich meta-data available from several sources.
Given the way, the Tibetan language follows a particular way of encoding meaning to words, similar to Sanskrit, where the words themselves concretely connect things and topics, and the way the body of knowledge the language is used to describe is actually an honest ontology looks like a really interesting use-case for Terminus.
A typical downstream use case is where the translator wants to understand the context for a given word or a scholar wants to find documents that are related semantically or otherwise.
At the moment we are planning to build our own datastore, but it dawned on me that there might be some really interesting opportunity in building on top of Terminus.",2021-06-15T14:08:44Z,mikkokotila,https://github.com/terminusdb/terminusdb/issues/333#issuecomment-861531355,"@matko wonderful :)
I suspect that Panini Sanskrit was modeled closely on Pali for the part of the vocabulary that communicates Buddhadharma. This would be wonderful, as in Mahavyutpatti [1] the objective seems to be to transmit Sanskrit into Tibetan ""perfectly"". For example, in Sanskrit you would have Sadhana (means of accomplishment) and Sadhaka (the one who is accomplished), which in Tibetan became Drubtop and Drubtop (because ""ka"" from Sanskrit in Tibetan is handled without affecting the sound). So if we are lucky, Tibetan for this part is basically a ""carbon copy"" of Pali. I will research to find out more about the relationship between Pali and Sanskrit has for the Buddhadharma language.
In terms ""canonical"" vs ""non-canonical"", in the Tibetan corpus this is quite different. There is basically no other literature than Buddhadharma literature.
As a point of reference, we use https://github.com/OpenPecha as a source for the texts.
More broadly, the sharing above is very useful :) I will definitely be in touch directly. Will update status here for others to see as needed.
[1] https://en.wikipedia.org/wiki/Mah%C4%81vyutpatti","@matko wonderful ) i suspect that panini sanskrit be model closely on pali for the part of the vocabulary that communicate buddhadharma this would be wonderful as in mahavyutpatti [ 1 ] the objective seem to be to transmit sanskrit into tibetan "" perfectly "" for example in sanskrit -PRON- would have sadhana ( mean of accomplishment ) and sadhaka ( the one who be accomplish ) which in tibetan become drubtop and drubtop ( because "" ka "" from sanskrit in tibetan be handle without affect the sound ) so if -PRON- be lucky tibetan for this part be basically a "" carbon copy "" of pali i will research to find out more about the relationship between pali and sanskrit have for the buddhadharma language in term "" canonical "" vs "" non - canonical "" in the tibetan corpus this be quite different there be basically no other literature than buddhadharma literature as a point of reference -PRON- use https//githubcom / openpecha as a source for the text more broadly the sharing above be very useful ) i will definitely be in touch directly will update status here for other to see as need [ 1 ] https//enwikipediaorg / wiki / mah%c4%81vyutpatti",0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,333,2021-06-15T07:51:07Z,mikkokotila,Tibetan Corpus on Terminus,https://github.com/terminusdb/terminusdb/issues/333,"I was not sure where to put this kind of topic, so posting it here.
We have a non-profit project where we are looking for ways to make roughly 200,000 Tibetan language texts available in the most meaningful way. It is about 20,000,000 pages of text in total.
These are basically the texts that were transmitted from India to Tibet (from Sanskrit to Tibetan) over a few centuries about a thousand years ago. It almost entirely consists of various mind training and meditation manuals, and philosophical treatises. Most of the texts are no longer available in Sanskrit.
We want to store the text in tokens instead of full text because from tokens we get the full text, but from the full text, it takes time to get tokens. 
The body of the text is static i.e. reading and any preprocessing must be only done once ever.
There is rich meta-data available from several sources.
Given the way, the Tibetan language follows a particular way of encoding meaning to words, similar to Sanskrit, where the words themselves concretely connect things and topics, and the way the body of knowledge the language is used to describe is actually an honest ontology looks like a really interesting use-case for Terminus.
A typical downstream use case is where the translator wants to understand the context for a given word or a scholar wants to find documents that are related semantically or otherwise.
At the moment we are planning to build our own datastore, but it dawned on me that there might be some really interesting opportunity in building on top of Terminus.",2021-06-18T13:13:24Z,mikkokotila,https://github.com/terminusdb/terminusdb/issues/333#issuecomment-864029689,"Maybe we can work together on a common core

It looks like from TerminusDB standpoint, most useful is ""common core"" which might then extend readily to other languages as well (assuming that downstream something else is done, which could later become part of the ""common core""). I think the point I'm making is that simpler the better. But yes, this is very valuable I believe.

I'd definitely wait for this to land before attempting to implement this

Sure. This update sounds amazing, really look forward to this one!

Pali is a highly inflected language.

Yes :) It appears that in language processing there is an incredible number of things that create complications. My learning with language tech is that the simpler the implementation is, the better.
In the case of inflection, Tibetan way can be clearly witnessed here: : https://en.wikipedia.org/wiki/Classical_Tibetan#Inflection
It is a beautiful language for deterministic NLP (in the graph). See the previous message in the thread for more on possible Pali <--> Tibetan connection. There is a comprehensive corpus available for Tibetan verbs in: https://raw.githubusercontent.com/Esukhia/tibetan-verbs-database/master/db/db.csv

so that we can quickly query not just for the exact way a particular word is written, but also all the other inflections.

Yes, I think this level of ability is ""core of the core"". Then on top of that, downstream one is able to create further connections through meta-data.

I imagine the Tibetan corpus employs a lot of stock phrases which you'll find throughout different texts

Yes. Many expressions and such are there repeating frequently. There are many more that are specific to a certain practice lineage or certain kind of meditation.
The really interesting thing here, which is apparent in the Pali Canon as well, is that topics are presented as a description of an ontology (model) or a way to provide details about the topic. Everything in the corpus has been incredibly well thought out, and the way everything can be connected to everything else based on that.
For example, consider the following topics:

Four Noble Truths
Three Poisons
Karma
Ten Non-Virtues
Six Realms

Then let's consider how these connect.
In Four Noble Truths we learn about suffering. We learn that suffering is caused by Three Poisons. From there we learn that because of the Three Poisons we create negative Karma, more specifically the Ten Non-Virtues. All the negative Karmas of speech and body are explained to be caused by the non-virtues of the mind, which are the Three Poisons. The lower three of the Six Realms are explained as the psychological states we experience as a result of the Three Poisons.
In other words, the terms ""Three Poisons"", ""Three Negative Karmas of Mind"", and ""Three Lower Realms"" form a thermodynamically sound construct of knowledge. My understanding is that the entire Buddhist Canon ""clicks"" in a similar way. The entire thing is already a data model. In fact, the mother of all data models.
This is just a vague illustration of the incredible potential there is in terms of representing these Canons as a graph.

and it'd be very useful if we could somehow identify instances of this, and be able to provide some scholarly annotations for such occasions.

Yes, we are able to do this within our team. We have several topically specialized translators and one Khenpo (same as professor) working with us :) Also, there are many others with very high level understanding of these topics, that we can ask questions.

For me, it'd be good to be able to register such discrepancies in a neutral way, not picking any sides.

I think that is really interesting idea!

I do not know what the situation is with classical Tibetan, but for Pali, the Pali Canon is pretty much the authoritative collection of texts that define what the language actually is.

There are virtually no other literature in Tibetan. In the body of texts we will be working with, there will be none. It is 100% Buddhadharma.

It should be possible for a researcher to annotate a word with a description of how this construction happened.

Yes, this is very important. There should be the ability for different translators and scholars to create alternative meanings to words, and then those words should be optionally available.

Finally, it'd be good to be able to classify texts in various ways.

Yes, for the Tibetan texts, we already have quite a rich meta-data structure. For example, the era, the yana, the cycle, and so forth.

Therefore, iIn my opinion, the first task of a project like this should be figuring out a high quality data model that is able to support a wide range of use cases, rather than designing the data model around one particular use case.

Yes for sure. I think the nuance here is that maybe it is better to first implement a basic datastore capability, and then gradually add generic language features.","maybe -PRON- can work together on a common core -PRON- look like from terminusdb standpoint most useful be "" common core "" which may then extend readily to other language as well ( assume that downstream something else be do which could later become part of the "" common core "" ) i think the point -PRON- be make be that simple the well but yes this be very valuable i believe -PRON- 'd definitely wait for this to land before attempt to implement this sure this update sound amazing really look forward to this one pali be a highly inflect language yes ) -PRON- appear that in language processing there be an incredible number of thing that create complication -PRON- learning with language tech be that the simple the implementation be the well in the case of inflection tibetan way can be clearly witness here https//enwikipediaorg / wiki / classical_tibetan#inflection -PRON- be a beautiful language for deterministic nlp ( in the graph ) see the previous message in the thread for more on possible pali < -- > tibetan connection there be a comprehensive corpus available for tibetan verb in https//rawgithubusercontentcom / esukhia / tibetan - verbs - database / master / db / dbcsv so that -PRON- can quickly query not just for the exact way a particular word be write but also all the other inflection yes i think this level of ability be "" core of the core "" then on top of that downstream one be able to create further connection through meta - datum i imagine the tibetan corpus employ a lot of stock phrase which -PRON- will find throughout different text yes many expression and such be there repeat frequently there be many more that be specific to a certain practice lineage or certain kind of meditation the really interesting thing here which be apparent in the pali canon as well be that topic be present as a description of an ontology ( model ) or a way to provide detail about the topic everything in the corpus have be incredibly well think out and the way everything can be connect to everything else base on that for example consider the follow topic four noble truth three poison karma ten non - virtue six realm then let -PRON- consider how these connect in four noble truth -PRON- learn about suffer -PRON- learn that suffering be cause by three poison from there -PRON- learn that because of the three poison -PRON- create negative karma more specifically the ten non - virtue all the negative karma of speech and body be explain to be cause by the non - virtue of the mind which be the three poison the low three of the six realm be explain as the psychological state -PRON- experience as a result of the three poison in other word the term "" three poison "" "" three negative karma of mind "" and "" three low realm "" form a thermodynamically sound construct of knowledge -PRON- understanding be that the entire buddhist canon "" click "" in a similar way the entire thing be already a data model in fact the mother of all data model this be just a vague illustration of the incredible potential there be in term of represent these canon as a graph and -PRON- 'd be very useful if -PRON- could somehow identify instance of this and be able to provide some scholarly annotation for such occasion yes -PRON- be able to do this within -PRON- team -PRON- have several topically specialize translator and one khenpo ( same as professor ) work with -PRON- ) also there be many other with very high level understanding of these topic that -PRON- can ask question for -PRON- -PRON- 'd be good to be able to register such discrepancy in a neutral way not pick any side i think that be really interesting idea i do not know what the situation be with classical tibetan but for pali the pali canon be pretty much the authoritative collection of text that define what the language actually be there be virtually no other literature in tibetan in the body of text -PRON- will be work with there will be none -PRON- be 100 % buddhadharma -PRON- should be possible for a researcher to annotate a word with a description of how this construction happen yes this be very important there should be the ability for different translator and scholar to create alternative meaning to word and then those word should be optionally available finally -PRON- 'd be good to be able to classify text in various way yes for the tibetan text -PRON- already have quite a rich meta - data structure for example the era the yana the cycle and so forth therefore iin -PRON- opinion the first task of a project like this should be figure out a high quality datum model that be able to support a wide range of use case rather than design the data model around one particular use case yes for sure i think the nuance here be that maybe -PRON- be well to first implement a basic datastore capability and then gradually add generic language feature",0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,334,2021-06-17T07:50:14Z,GavinMendelGleason,Incorrect properties for key cause silent dropping,https://github.com/terminusdb/terminusdb/issues/334,"If you have a key in which the properties are incorrectly spelled, it will silently drop them leading to a keyless class.
{ ""@type"" : ""Class"", 
  ""@id"" : ""TestClass"", 
  ""@key"" : { ""@type"" : ""Lexical"", 
                   ""@feilds"" : [ ""name"" ] } }
To reproduce you can try inserting this class into a schema.
This exists in the document interface in dev branch",2021-08-26T13:50:31Z,spl,https://github.com/terminusdb/terminusdb/issues/334#issuecomment-906432306,Was this fixed in 013a606?,be this fix in 013a606,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,334,2021-06-17T07:50:14Z,GavinMendelGleason,Incorrect properties for key cause silent dropping,https://github.com/terminusdb/terminusdb/issues/334,"If you have a key in which the properties are incorrectly spelled, it will silently drop them leading to a keyless class.
{ ""@type"" : ""Class"", 
  ""@id"" : ""TestClass"", 
  ""@key"" : { ""@type"" : ""Lexical"", 
                   ""@feilds"" : [ ""name"" ] } }
To reproduce you can try inserting this class into a schema.
This exists in the document interface in dev branch",2021-08-30T14:12:30Z,spl,https://github.com/terminusdb/terminusdb/issues/334#issuecomment-908378047,Fixed as mentioned above.,fix as mention above,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,335,2021-06-18T08:44:29Z,matko,changing key strategy does not invalidate existing objects,https://github.com/terminusdb/terminusdb/issues/335,"When you change key strategy for a type in the schema, none of the existing objects in the instance data are invalidated. This means it is possible to have objects in the database that do not match the key strategy. Therefore, duplicates are not correctly detected, nor can these existing objects be modified.
When changing key strategy for a type, existing objects that do not match that key strategy should be invalidated.",,,,,,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,336,2021-06-18T14:56:32Z,GavinMendelGleason,Upcast loses information in woql document retrieval,https://github.com/terminusdb/terminusdb/issues/336,"Describe the bug
When inserting a document which has an super type for the value, we can lose information on retrieval for datatypes.
To Reproduce
Attempt to insert the following into a database using the WOQL.json schema.
{ ""@type"" : ""Substring"",
  ""string"" : { ""@type"" : ""DataValue"",
                   ""data"" : { ""@type"" : ""xsd:string"",
                                  ""@value"" : ""Test""}},
 ""before"" : { ""@type"" : ""DataValue"",
                   ""data"" : { ""@type"" : ""xsd:integer"",
                                  ""@value"" : 1}},
 ""length"" : { ""@type"" : ""DataValue"",
                  ""variable"" : ""Length""},
 ""after"" : { ""@type"" : ""DataValue"",
                ""data"" : { ""@type"" : ""xsd:integer"",
                               ""@value"" : 1}},
 ""substring"" : { ""@type"" : ""DataValue"",
                       ""variable"" : ""Substring"" }}
On extraction you will obtain:
{
  ""@id"":""Substring_76855fec5024f7283ca349d703b6d8b9625ad6ec"",
  ""@type"":""Substring"",
  ""after"": {
    ""@id"":""DataValue_c6cad64a9ce466e1446690f74a034ff7153a4639"",
    ""@type"":""DataValue"",
    ""data"": 1
  },
  ""before"": {
    ""@id"":""DataValue_c6cad64a9ce466e1446690f74a034ff7153a4639"",
    ""@type"":""DataValue"",
    ""data"": 1
  },
  ""length"": {
    ""@id"":""DataValue_1e4318564a1db6dac877ee9497689ab0511cb324"",
    ""@type"":""DataValue"",
    ""variable"":""Length""
  },
  ""string"": {
    ""@id"":""DataValue_8995245f7d5bdcd9ea99110575f9144c589eb997"",
    ""@type"":""DataValue"",
    ""data"": ""Test""
  },
  ""substring"": {
    ""@id"":""DataValue_8361dd40f8b523a0243ebf7f2b55e19ce68a752d"",
    ""@type"":""DataValue"",
    ""variable"":""Substring""
  }
}
Expected behavior
Instead we should get the following object:
{
  ""@id"":""Substring_76855fec5024f7283ca349d703b6d8b9625ad6ec"",
  ""@type"":""Substring"",
  ""after"": {
    ""@id"":""DataValue_c6cad64a9ce466e1446690f74a034ff7153a4639"",
    ""@type"":""DataValue"",
    ""data"": {""@type"":""xsd:integer"", ""@value"":1}
  },
  ""before"": {
    ""@id"":""DataValue_c6cad64a9ce466e1446690f74a034ff7153a4639"",
    ""@type"":""DataValue"",
    ""data"": {""@type"":""xsd:integer"", ""@value"":1}
  },
  ""length"": {
    ""@id"":""DataValue_1e4318564a1db6dac877ee9497689ab0511cb324"",
    ""@type"":""DataValue"",
    ""variable"":""Length""
  },
  ""string"": {
    ""@id"":""DataValue_8995245f7d5bdcd9ea99110575f9144c589eb997"",
    ""@type"":""DataValue"",
    ""data"": {""@type"":""xsd:string"", ""@value"":""Test""}
  },
  ""substring"": {
    ""@id"":""DataValue_8361dd40f8b523a0243ebf7f2b55e19ce68a752d"",
    ""@type"":""DataValue"",
    ""variable"":""Substring""
  }
}
Info (please complete the following information):

Dev branch / document interface development",,,,,,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,337,2021-06-22T14:39:26Z,somethingelseentirely,xdd:coordinate fields fail,https://github.com/terminusdb/terminusdb/issues/337,"Describe the bug
When trying to create an object with xdd:coordinate coordinates through the GUI I get the following error when trying to commit:
The value point(49.00882,8.43888) could not be cast as 'http://terminusdb.com/schema/xdd#coordinatePolyline'less
{
  ""data"": {
    ""@type"": ""api:FrameErrorResponse"",
    ""api:error"": {
      ""@type"": ""api:BadCast"",
      ""api:type"": ""http://terminusdb.com/schema/xdd#coordinatePolyline"",
      ""api:value"": ""point(49.00882,8.43888)""
    },
    ""api:message"": ""The value point(49.00882,8.43888) could not be cast as 'http://terminusdb.com/schema/xdd#coordinatePolyline'"",
    ""api:status"": ""api:failure"",
    ""end"": ""2021-06-22T14:26:39.958Z"",
    ""start"": ""2021-06-22T14:26:39.958Z"",
    ""duration"": ""2021-06-22T14:26:39.958Z""
  }
}

To Reproduce

Create a Schema with an object containing a xdd:coordinate field.
Try to create an instance of said object through the documents tab.
Get the error.

Expected behavior
The commit should succeed as it is a valid document..
Screenshots

Info (please complete the following information):

OS: Ubuntu 20.04.2 LTS
TerminusDB Desktop on Linux / appimage",2021-06-28T11:40:59Z,GavinMendelGleason,https://github.com/terminusdb/terminusdb/issues/337#issuecomment-869612681,"This is curious, I'll have a look at it.",this be curious -PRON- will have a look at -PRON-,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,337,2021-06-22T14:39:26Z,somethingelseentirely,xdd:coordinate fields fail,https://github.com/terminusdb/terminusdb/issues/337,"Describe the bug
When trying to create an object with xdd:coordinate coordinates through the GUI I get the following error when trying to commit:
The value point(49.00882,8.43888) could not be cast as 'http://terminusdb.com/schema/xdd#coordinatePolyline'less
{
  ""data"": {
    ""@type"": ""api:FrameErrorResponse"",
    ""api:error"": {
      ""@type"": ""api:BadCast"",
      ""api:type"": ""http://terminusdb.com/schema/xdd#coordinatePolyline"",
      ""api:value"": ""point(49.00882,8.43888)""
    },
    ""api:message"": ""The value point(49.00882,8.43888) could not be cast as 'http://terminusdb.com/schema/xdd#coordinatePolyline'"",
    ""api:status"": ""api:failure"",
    ""end"": ""2021-06-22T14:26:39.958Z"",
    ""start"": ""2021-06-22T14:26:39.958Z"",
    ""duration"": ""2021-06-22T14:26:39.958Z""
  }
}

To Reproduce

Create a Schema with an object containing a xdd:coordinate field.
Try to create an instance of said object through the documents tab.
Get the error.

Expected behavior
The commit should succeed as it is a valid document..
Screenshots

Info (please complete the following information):

OS: Ubuntu 20.04.2 LTS
TerminusDB Desktop on Linux / appimage",2021-07-05T11:34:51Z,matko,https://github.com/terminusdb/terminusdb/issues/337#issuecomment-874043061,"It turns out that some of the relevant code has been fixed in our canary branch, but this never made its way back into dev. This needs to be merged back. (to main, since we changed branch strategy). @GavinMendelGleason",-PRON- turn out that some of the relevant code have be fix in -PRON- canary branch but this never make -PRON- way back into dev this need to be merge back ( to main since -PRON- change branch strategy ) @gavinmendelgleason,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,337,2021-06-22T14:39:26Z,somethingelseentirely,xdd:coordinate fields fail,https://github.com/terminusdb/terminusdb/issues/337,"Describe the bug
When trying to create an object with xdd:coordinate coordinates through the GUI I get the following error when trying to commit:
The value point(49.00882,8.43888) could not be cast as 'http://terminusdb.com/schema/xdd#coordinatePolyline'less
{
  ""data"": {
    ""@type"": ""api:FrameErrorResponse"",
    ""api:error"": {
      ""@type"": ""api:BadCast"",
      ""api:type"": ""http://terminusdb.com/schema/xdd#coordinatePolyline"",
      ""api:value"": ""point(49.00882,8.43888)""
    },
    ""api:message"": ""The value point(49.00882,8.43888) could not be cast as 'http://terminusdb.com/schema/xdd#coordinatePolyline'"",
    ""api:status"": ""api:failure"",
    ""end"": ""2021-06-22T14:26:39.958Z"",
    ""start"": ""2021-06-22T14:26:39.958Z"",
    ""duration"": ""2021-06-22T14:26:39.958Z""
  }
}

To Reproduce

Create a Schema with an object containing a xdd:coordinate field.
Try to create an instance of said object through the documents tab.
Get the error.

Expected behavior
The commit should succeed as it is a valid document..
Screenshots

Info (please complete the following information):

OS: Ubuntu 20.04.2 LTS
TerminusDB Desktop on Linux / appimage",2021-08-23T13:35:04Z,spl,https://github.com/terminusdb/terminusdb/issues/337#issuecomment-903774930,"We're working on a release, and this will be fixed then. Thanks for the report!",-PRON- be work on a release and this will be fix then thank for the report,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,339,2021-06-22T15:46:43Z,somethingelseentirely,Enums are partially broken in the schema editor.,https://github.com/terminusdb/terminusdb/issues/339,"Describe the bug
When creating an enum and a field using it an error is thrown.
To Reproduce

Create a fresh db.
Create a schema containing an entity, and a document with a field using said entity.
Commit.

Expected behavior
The schema should be commited.
Screenshots

Info (please complete the following information):

OS: Ubuntu 20.04.2 LTS
TerminusDB Bootstrap

Additional Information
It is possible to commit the enum in a transaction, and then add the field in another one.",2021-08-09T13:20:37Z,AstroChelonian,https://github.com/terminusdb/terminusdb/issues/339#issuecomment-895217073,This will be solved by the new terminus release.,this will be solve by the new terminus release,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,341,2021-06-23T10:08:33Z,somethingelseentirely,"Documentation is outdated, inconsistent, and disconnected.",https://github.com/terminusdb/terminusdb/issues/341,"What's missing
The naming in the tutorials, examples, and references don't match up,
e.g. WOWL.js client initialisation.
The split into terminusdb and terminushub documentation also doesn't help,
the terminushub documentation contains references on WOQL which seem to be more up to date
than the ones in the TerminusDB docs.",2021-06-23T11:20:23Z,spl,https://github.com/terminusdb/terminusdb/issues/341#issuecomment-866752190,Thanks for taking a look at the docs. There has unfortunately been a bit of neglect there. But we're now working diligently to improve it all.,thank for take a look at the doc there have unfortunately be a bit of neglect there but -PRON- be now work diligently to improve -PRON- all,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,341,2021-06-23T10:08:33Z,somethingelseentirely,"Documentation is outdated, inconsistent, and disconnected.",https://github.com/terminusdb/terminusdb/issues/341,"What's missing
The naming in the tutorials, examples, and references don't match up,
e.g. WOWL.js client initialisation.
The split into terminusdb and terminushub documentation also doesn't help,
the terminushub documentation contains references on WOQL which seem to be more up to date
than the ones in the TerminusDB docs.",2021-09-13T14:03:48Z,spl,https://github.com/terminusdb/terminusdb/issues/341#issuecomment-918225055,Check out https://docs.terminusdb.com/!,check out https//docsterminusdbcom/,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,342,2021-06-26T11:47:16Z,mlliarm,Documentation issue on the python intro tutorial page,https://github.com/terminusdb/terminusdb/issues/342,"Describe the typo/ misinformation
Hello, I was wandering through the docs and reached this point:
https://terminusdb.github.io/terminusdb/#/Intro_Tutorials/Start_With_Python?id=woqlviews
After that it was supposed to show something in the box beneath the text ""which will result in something like this"", but all I can see is white. I guess something is wrong?
I'm using Google Chrome Version 91.0.4472.124 (Official Build) (64-bit), on Windows 10.
Expected changes
I expected to be able to see something (I guess something like a graph) in that box of white.
Screenshots
See attached image.",2021-06-26T16:02:18Z,kamleshjoshi8102,https://github.com/terminusdb/terminusdb/issues/342#issuecomment-869022633,"Describe the typo/ misinformation
Hello, I was wandering through the docs and reached this point:
https://terminusdb.github.io/terminusdb/#/Intro_Tutorials/Start_With_Python?id=woqlviews
After that it was supposed to show something in the box beneath the text ""which will result in something like this"", but all I can see is white. I guess something is wrong?
I'm using Google Chrome Version 91.0.4472.124 (Official Build) (64-bit), on Windows 10.
Expected changes
I expected to be able to see something (I guess something like a graph) in that box of white.
Screenshots
See attached image.


Hey! Can you explain a little more I can't understand
:P","describe the typo/ misinformation hello i be wander through the doc and reach this point https//terminusdbgithubio / terminusdb/#/intro_tutorial / start_with_pythonid = woqlview after that -PRON- be suppose to show something in the box beneath the text "" which will result in something like this "" but all i can see be white i guess something be wrong -PRON- be use google chrome version 9104472124 ( official build ) ( 64-bit ) on window 10 expect change i expect to be able to see something ( i guess something like a graph ) in that box of white screenshot see attach image hey can -PRON- explain a little more i can not understand p",0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,342,2021-06-26T11:47:16Z,mlliarm,Documentation issue on the python intro tutorial page,https://github.com/terminusdb/terminusdb/issues/342,"Describe the typo/ misinformation
Hello, I was wandering through the docs and reached this point:
https://terminusdb.github.io/terminusdb/#/Intro_Tutorials/Start_With_Python?id=woqlviews
After that it was supposed to show something in the box beneath the text ""which will result in something like this"", but all I can see is white. I guess something is wrong?
I'm using Google Chrome Version 91.0.4472.124 (Official Build) (64-bit), on Windows 10.
Expected changes
I expected to be able to see something (I guess something like a graph) in that box of white.
Screenshots
See attached image.",2021-06-26T16:51:57Z,spl,https://github.com/terminusdb/terminusdb/issues/342#issuecomment-869029112,"Thanks for the report! I recall somebody saying something about bintray not being available to us, and that looks like the problem.
@Cheukting Were you looking into this?",thank for the report i recall somebody say something about bintray not be available to -PRON- and that look like the problem @cheukte be -PRON- look into this,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,342,2021-06-26T11:47:16Z,mlliarm,Documentation issue on the python intro tutorial page,https://github.com/terminusdb/terminusdb/issues/342,"Describe the typo/ misinformation
Hello, I was wandering through the docs and reached this point:
https://terminusdb.github.io/terminusdb/#/Intro_Tutorials/Start_With_Python?id=woqlviews
After that it was supposed to show something in the box beneath the text ""which will result in something like this"", but all I can see is white. I guess something is wrong?
I'm using Google Chrome Version 91.0.4472.124 (Official Build) (64-bit), on Windows 10.
Expected changes
I expected to be able to see something (I guess something like a graph) in that box of white.
Screenshots
See attached image.",2021-06-27T02:45:48Z,spl,https://github.com/terminusdb/terminusdb/issues/342#issuecomment-869090970,@Cheukting Is it enough to do terminusdb/terminusdb-tutorials#32? I'm not sure if some sort of deployment is needed.,@cheukte be -PRON- enough to do terminusdb / terminusdb - tutorials#32 -PRON- be not sure if some sort of deployment be need,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,342,2021-06-26T11:47:16Z,mlliarm,Documentation issue on the python intro tutorial page,https://github.com/terminusdb/terminusdb/issues/342,"Describe the typo/ misinformation
Hello, I was wandering through the docs and reached this point:
https://terminusdb.github.io/terminusdb/#/Intro_Tutorials/Start_With_Python?id=woqlviews
After that it was supposed to show something in the box beneath the text ""which will result in something like this"", but all I can see is white. I guess something is wrong?
I'm using Google Chrome Version 91.0.4472.124 (Official Build) (64-bit), on Windows 10.
Expected changes
I expected to be able to see something (I guess something like a graph) in that box of white.
Screenshots
See attached image.",2021-06-29T11:23:56Z,spl,https://github.com/terminusdb/terminusdb/issues/342#issuecomment-870510316,Looks like this is fixed! Thanks again for the report. Let us know if you run into any other problems.,look like this be fix thank again for the report let -PRON- know if -PRON- run into any other problem,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,343,2021-06-27T02:59:47Z,spl,Bintray ,https://github.com/terminusdb/terminusdb/issues/343,"Describe the bug
We no longer use bintray for publishing but it is still mentioned in numerous places, resulting in reports like #342.
To Reproduce
See https://github.com/search?q=org%3Aterminusdb+bintray&type=code for many of the places bintray is mentioned.
Expected behavior
Bintray is not named everywhere it should not be named.
Additional context
I think we should do a full sweep of the relevant repositories and substitute every mention of bintray with something appropriate.",2021-06-28T11:31:46Z,spl,https://github.com/terminusdb/terminusdb/issues/343#issuecomment-869607692,I think https://github.com/terminusdb/terminusdb-web-assets/blob/master/docs/bike-graph.html needs to be changed to be like terminusdb/terminusdb-tutorials#32.,i think https//githubcom / terminusdb / terminusdb - web - asset / blob / master / doc / bike - graphhtml need to be change to be like terminusdb / terminusdb - tutorials#32,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,343,2021-06-27T02:59:47Z,spl,Bintray ,https://github.com/terminusdb/terminusdb/issues/343,"Describe the bug
We no longer use bintray for publishing but it is still mentioned in numerous places, resulting in reports like #342.
To Reproduce
See https://github.com/search?q=org%3Aterminusdb+bintray&type=code for many of the places bintray is mentioned.
Expected behavior
Bintray is not named everywhere it should not be named.
Additional context
I think we should do a full sweep of the relevant repositories and substitute every mention of bintray with something appropriate.",2021-06-28T12:07:10Z,spl,https://github.com/terminusdb/terminusdb/issues/343#issuecomment-869628264,"I think https://github.com/terminusdb/terminusdb-web-assets/blob/master/docs/bike-graph.html needs to be changed to be like terminusdb/terminusdb-tutorials#32.

Done by @rrooij! terminusdb/terminusdb-web-assets@0909f9c",i think https//githubcom / terminusdb / terminusdb - web - asset / blob / master / doc / bike - graphhtml need to be change to be like terminusdb / terminusdb - tutorials#32 do by @rrooij terminusdb / terminusdb - web - assets@0909f9c,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,343,2021-06-27T02:59:47Z,spl,Bintray ,https://github.com/terminusdb/terminusdb/issues/343,"Describe the bug
We no longer use bintray for publishing but it is still mentioned in numerous places, resulting in reports like #342.
To Reproduce
See https://github.com/search?q=org%3Aterminusdb+bintray&type=code for many of the places bintray is mentioned.
Expected behavior
Bintray is not named everywhere it should not be named.
Additional context
I think we should do a full sweep of the relevant repositories and substitute every mention of bintray with something appropriate.",2021-07-12T13:16:46Z,Cheukting,https://github.com/terminusdb/terminusdb/issues/343#issuecomment-878270950,@rrooij is this done and can be closed?,@rrooij be this do and can be close,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,344,2021-06-28T09:08:16Z,GavinMendelGleason,Documentation of Properties should be a single object,https://github.com/terminusdb/terminusdb/issues/344,"Is your feature request related to a problem? Please describe.
The documentation of properties has the form:
{ ...
""@documentation"" : { ""@properties"" : [ { ""asdf"" : ""fdsa""}, {""foo"" : ""bar"" } ]}
}
This should be altered to:
{ ...
""@documentation"" : { ""@properties"" :  { ""asdf"" : ""fdsa"", ""foo"" : ""bar"" }}
}
This is only relevant to the current dev branch.",,,,,,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,345,2021-06-28T11:47:06Z,matko,Change branch strategy,https://github.com/terminusdb/terminusdb/issues/345,"Currently we're developing in dev, and only merge to master when we release.
We've decided to change this to working in master (to be renamed to main) instead, and have a stable branch for the release.
This issue is for discussion of how to do this and what will go into this. @rrooij will take charge in this.",2021-06-29T11:13:10Z,spl,https://github.com/terminusdb/terminusdb/issues/345#issuecomment-870503387,"What are all the repositories that this change applies to? I think at least:

https://github.com/terminusdb/terminusdb
https://github.com/terminusdb/terminusdb-client-js
https://github.com/terminusdb/terminusdb-store",what be all the repository that this change apply to i think at least https//githubcom / terminusdb / terminusdb https//githubcom / terminusdb / terminusdb - client - js https//githubcom / terminusdb / terminusdb - store,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,345,2021-06-28T11:47:06Z,matko,Change branch strategy,https://github.com/terminusdb/terminusdb/issues/345,"Currently we're developing in dev, and only merge to master when we release.
We've decided to change this to working in master (to be renamed to main) instead, and have a stable branch for the release.
This issue is for discussion of how to do this and what will go into this. @rrooij will take charge in this.",2021-06-29T11:13:54Z,spl,https://github.com/terminusdb/terminusdb/issues/345#issuecomment-870503883,"Also, documentation such as Contributing.md (and others?) will need to be updated.",also documentation such as contributingmd ( and other ) will need to be update,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,345,2021-06-28T11:47:06Z,matko,Change branch strategy,https://github.com/terminusdb/terminusdb/issues/345,"Currently we're developing in dev, and only merge to master when we release.
We've decided to change this to working in master (to be renamed to main) instead, and have a stable branch for the release.
This issue is for discussion of how to do this and what will go into this. @rrooij will take charge in this.",2021-06-29T18:21:00Z,spl,https://github.com/terminusdb/terminusdb/issues/345#issuecomment-870815382,"This isn't strictly part of the issue, but, if we're going to change branch names in multiple repositories, should we go ahead and do it in all the repositories? Not that I want to add more work to this issue, but I think it will reduce the cognitive load in the longer term:

Assuming we eventually want main instead of master everywhere, it would be easier to change them all in one go. In particular, each of us only has to go through the branch renaming steps on our local repositories once for each clone we have.
We won't have to remember which repository uses master and which one uses main because they will all use main. I can imagine the confusion I would have if my “hand memory” cannot remember which word to type when I switch from one to the other.","this be not strictly part of the issue but if -PRON- be go to change branch name in multiple repository should -PRON- go ahead and do -PRON- in all the repository not that i want to add more work to this issue but i think -PRON- will reduce the cognitive load in the long term assume -PRON- eventually want main instead of master everywhere -PRON- would be easy to change -PRON- all in one go in particular each of -PRON- only have to go through the branch rename step on -PRON- local repository once for each clone -PRON- have -PRON- will not have to remember which repository use master and which one use main because -PRON- will all use main i can imagine the confusion i would have if -PRON- "" hand memory "" can not remember which word to type when i switch from one to the other",0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,346,2021-06-28T17:13:41Z,hoijnet,Path traversal documentation incorrectly formatted,https://github.com/terminusdb/terminusdb/issues/346,"Describe the typo/ misinformation
On the documentation about the primitives, the information for path regular expression for path traversal, in the patterns part, are missing the characters for some fields (""alternatives"" and ""any predicate"")
Expected changes
I was expecting/presuming the following and it seems to work as per expectations:

pipe should be representing the alternatives
star should be representing the any predicate option

Looks like the characters fell off, might be a good idea to write out which character it is.
Screenshots

Additional context
Will add separate issue for path traversal with star.",2021-06-29T11:09:31Z,spl,https://github.com/terminusdb/terminusdb/issues/346#issuecomment-870501115,Thanks! Looks like some easy fixes. I also see at least one typo: “predcitates.” We'll get on it!,"thank look like some easy fix i also see at least one typo "" predcitate "" -PRON- will get on -PRON-",0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,346,2021-06-28T17:13:41Z,hoijnet,Path traversal documentation incorrectly formatted,https://github.com/terminusdb/terminusdb/issues/346,"Describe the typo/ misinformation
On the documentation about the primitives, the information for path regular expression for path traversal, in the patterns part, are missing the characters for some fields (""alternatives"" and ""any predicate"")
Expected changes
I was expecting/presuming the following and it seems to work as per expectations:

pipe should be representing the alternatives
star should be representing the any predicate option

Looks like the characters fell off, might be a good idea to write out which character it is.
Screenshots

Additional context
Will add separate issue for path traversal with star.",2021-06-29T15:40:49Z,spl,https://github.com/terminusdb/terminusdb/issues/346#issuecomment-870708935,"@hoijnet For now, you can view the revised docs at https://github.com/terminusdb/terminusdb-client-js/blob/dev/docs/api/woql.js.md#path. Thanks again!",@hoijnet for now -PRON- can view the revised doc at https//githubcom / terminusdb / terminusdb - client - js / blob / dev / docs / api / woqljsmd#path thank again,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,346,2021-06-28T17:13:41Z,hoijnet,Path traversal documentation incorrectly formatted,https://github.com/terminusdb/terminusdb/issues/346,"Describe the typo/ misinformation
On the documentation about the primitives, the information for path regular expression for path traversal, in the patterns part, are missing the characters for some fields (""alternatives"" and ""any predicate"")
Expected changes
I was expecting/presuming the following and it seems to work as per expectations:

pipe should be representing the alternatives
star should be representing the any predicate option

Looks like the characters fell off, might be a good idea to write out which character it is.
Screenshots

Additional context
Will add separate issue for path traversal with star.",2021-06-29T17:21:12Z,spl,https://github.com/terminusdb/terminusdb/issues/346#issuecomment-870778020,"Or even better, as I just discovered: https://terminusdb.github.io/terminusdb-client-js/#/api/woql.js?id=woqlpathsubject-pattern-object-resultvarname-%e2%87%92-woqlquery",or even well as i just discover https//terminusdbgithubio / terminusdb - client - js/#/api / woqljsid = woqlpathsubject - pattern - object - resultvarname-%e2%87%92-woqlquery,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,346,2021-06-28T17:13:41Z,hoijnet,Path traversal documentation incorrectly formatted,https://github.com/terminusdb/terminusdb/issues/346,"Describe the typo/ misinformation
On the documentation about the primitives, the information for path regular expression for path traversal, in the patterns part, are missing the characters for some fields (""alternatives"" and ""any predicate"")
Expected changes
I was expecting/presuming the following and it seems to work as per expectations:

pipe should be representing the alternatives
star should be representing the any predicate option

Looks like the characters fell off, might be a good idea to write out which character it is.
Screenshots

Additional context
Will add separate issue for path traversal with star.",2021-06-30T04:10:28Z,hoijnet,https://github.com/terminusdb/terminusdb/issues/346#issuecomment-871080839,"Thanks for the update! It looks beautiful, helpful and much clearer! 🙏",thank for the update -PRON- look beautiful helpful and much clear 🙏,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,347,2021-06-28T17:26:04Z,hoijnet,"Path regular expression is inconsistent to expectations for repeated ""any predicate""",https://github.com/terminusdb/terminusdb/issues/347,"Describe the bug
A path regular expression ""()+"" or ""(){1,2} does not identify/traverse more than one level from source to target which would be expected considering that ""(property_name_level_1|property_name_level_2)+"" does traverse more than one level.
To Reproduce
Steps to reproduce the behavior:

Go to TerminusDB Console Query tab
Enter any of the following:

Traverses two levels: WOQL.path(""v:source"", ""(propertyname_firstlevel|propertyname_secondlevel){1,2}"", ""v:target"", ""v:traversed"")
Does not work, expecting same traversals or more than above: WOQL.path(""v:source"", ""(*){1,2}"", ""v:target"", ""v:traversed"")
Traverses two levels: WOQL.path(""v:source"", ""(propertyname_firstlevel|propertyname_secondlevel)+"", ""v:target"", ""v:traversed"")
Does not work, expecting same traversals or more than above: WOQL.path(""v:source"", ""(*)+"", ""v:target"", ""v:traversed"")


Click Run Query

Expected behavior
For the ones that work I got the correct result, traversal from source-->intermediary-->target and the path in v:traversed.
For the ones that does not work, when using ""any predicate, *"", only one level is traversed: source-->intermediary, and only the path for the source-->intermediary traversal is present in the result.
Screenshots
N/A
Info (please complete the following information):

Additional context
N/A, small database, minimal test does not yield expected behaviour",2021-06-29T13:45:33Z,spl,https://github.com/terminusdb/terminusdb/issues/347#issuecomment-870614080,"Thanks for this, too. We're currently working on revamping a lot of things at the moment. But we'll look into this problem soon.",thank for this too -PRON- be currently work on revamp a lot of thing at the moment but -PRON- will look into this problem soon,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,347,2021-06-28T17:26:04Z,hoijnet,"Path regular expression is inconsistent to expectations for repeated ""any predicate""",https://github.com/terminusdb/terminusdb/issues/347,"Describe the bug
A path regular expression ""()+"" or ""(){1,2} does not identify/traverse more than one level from source to target which would be expected considering that ""(property_name_level_1|property_name_level_2)+"" does traverse more than one level.
To Reproduce
Steps to reproduce the behavior:

Go to TerminusDB Console Query tab
Enter any of the following:

Traverses two levels: WOQL.path(""v:source"", ""(propertyname_firstlevel|propertyname_secondlevel){1,2}"", ""v:target"", ""v:traversed"")
Does not work, expecting same traversals or more than above: WOQL.path(""v:source"", ""(*){1,2}"", ""v:target"", ""v:traversed"")
Traverses two levels: WOQL.path(""v:source"", ""(propertyname_firstlevel|propertyname_secondlevel)+"", ""v:target"", ""v:traversed"")
Does not work, expecting same traversals or more than above: WOQL.path(""v:source"", ""(*)+"", ""v:target"", ""v:traversed"")


Click Run Query

Expected behavior
For the ones that work I got the correct result, traversal from source-->intermediary-->target and the path in v:traversed.
For the ones that does not work, when using ""any predicate, *"", only one level is traversed: source-->intermediary, and only the path for the source-->intermediary traversal is present in the result.
Screenshots
N/A
Info (please complete the following information):

Additional context
N/A, small database, minimal test does not yield expected behaviour",2021-08-26T15:52:05Z,GavinMendelGleason,https://github.com/terminusdb/terminusdb/issues/347#issuecomment-906532797,This is indeed a bug in the processing of wild-card edges. We will be processing this differently in the TerminusDB 5.0 release and will no longer be relevant.,this be indeed a bug in the processing of wild - card edge -PRON- will be process this differently in the terminusdb 50 release and will no longer be relevant,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,347,2021-06-28T17:26:04Z,hoijnet,"Path regular expression is inconsistent to expectations for repeated ""any predicate""",https://github.com/terminusdb/terminusdb/issues/347,"Describe the bug
A path regular expression ""()+"" or ""(){1,2} does not identify/traverse more than one level from source to target which would be expected considering that ""(property_name_level_1|property_name_level_2)+"" does traverse more than one level.
To Reproduce
Steps to reproduce the behavior:

Go to TerminusDB Console Query tab
Enter any of the following:

Traverses two levels: WOQL.path(""v:source"", ""(propertyname_firstlevel|propertyname_secondlevel){1,2}"", ""v:target"", ""v:traversed"")
Does not work, expecting same traversals or more than above: WOQL.path(""v:source"", ""(*){1,2}"", ""v:target"", ""v:traversed"")
Traverses two levels: WOQL.path(""v:source"", ""(propertyname_firstlevel|propertyname_secondlevel)+"", ""v:target"", ""v:traversed"")
Does not work, expecting same traversals or more than above: WOQL.path(""v:source"", ""(*)+"", ""v:target"", ""v:traversed"")


Click Run Query

Expected behavior
For the ones that work I got the correct result, traversal from source-->intermediary-->target and the path in v:traversed.
For the ones that does not work, when using ""any predicate, *"", only one level is traversed: source-->intermediary, and only the path for the source-->intermediary traversal is present in the result.
Screenshots
N/A
Info (please complete the following information):

Additional context
N/A, small database, minimal test does not yield expected behaviour",2021-08-27T05:31:48Z,hoijnet,https://github.com/terminusdb/terminusdb/issues/347#issuecomment-906937209,Looking forward to 5.0 in anticipation! Thanks for confirming the bug.,look forward to 50 in anticipation thank for confirm the bug,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,348,2021-06-29T13:11:30Z,GavinMendelGleason,Topological sorting relies on very complex non-definitional ordering,https://github.com/terminusdb/terminusdb/issues/348,We might want to make the listing of inherited properties an ordered list so that the user can specify a multiple inheritance system which exhibits something which can be topologically sorted.,,,,,,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,349,2021-07-05T11:29:12Z,rrooij,GPL leftovers,https://github.com/terminusdb/terminusdb/issues/349,"Describe the bug
Some files are still GPLv3 even though we moved to the Apache license. I like GPL v3 but we should be consistent.
src/core/util/remote_file.pl
16: *  it under the terms of the GNU General Public License as published by *
23: *  GNU General Public License for more details.                         *
25: *  You should have received a copy of the GNU General Public License    *

src/core/util/xsd_parser.pl
52: *  it under the terms of the GNU General Public License as published by *
59: *  GNU General Public License for more details.                         *
61: *  You should have received a copy of the GNU General Public License    *

src/core/util/speculative_parse.pl
18: *  it under the terms of the GNU General Public License as published by *
25: *  GNU General Public License for more details.                         *
27: *  You should have received a copy of the GNU General Public License    *

src/core/triple/upgrade_db.pl
31: *  it under the terms of the GNU General Public License as published by *
38: *  GNU General Public License for more details.                         *
40: *  You should have received a copy of the GNU General Public License    *

src/core/util/file_utils.pl
30: *  it under the terms of the GNU General Public License as published by *
37: *  GNU General Public License for more details.                         *
39: *  You should have received a copy of the GNU General Public License    *

src/core/triple/iana.pl
12: *  it under the terms of the GNU General Public License as published by *
19: *  GNU General Public License for more details.                         *
21: *  You should have received a copy of the GNU General Public License    *

src/core/transaction/descriptor.pl
129: *  it under the terms of the GNU General Public License as published by *
136: *  GNU General Public License for more details.                         *
138: *  You should have received a copy of the GNU General Public License    *",2021-07-05T16:10:15Z,AstroChelonian,https://github.com/terminusdb/terminusdb/issues/349#issuecomment-874217383,Task completed.,task complete,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,350,2021-07-05T11:53:00Z,spl,Use label deletion,https://github.com/terminusdb/terminusdb/issues/350,See terminusdb/terminus_store_prolog#25,,,,,,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,351,2021-07-05T11:59:34Z,matko,Ensure all unit tests run properly,https://github.com/terminusdb/terminusdb/issues/351,"As part of the document interface refactor, we disabled a bunch of unit tests in the prolog code. We now need to re-enable them and fix the underlying code where necessary.
Some endpoints have become obsolete or cannot be made to work properly in a short timeframe. These should be deleted or temporarily disabled.",2021-07-12T13:25:21Z,spl,https://github.com/terminusdb/terminusdb/issues/351#issuecomment-878277657,Yay! 🚀,yay 🚀,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,352,2021-07-05T13:21:47Z,Francesca-Bit,Schema graph - Create a SubDocument or a Document using the document interface,https://github.com/terminusdb/terminusdb/issues/352,"Describe the bug
Using the document interface
I can not create a new document or subdocument in the schema graph
To Reproduce
Steps to reproduce the behavior:

call the end point : https://127.0.0.1:6363/api/document/admin/_DB_NAME_/local/branch/main?author=admin&message=add%20a%20new%20document

with the follow payload
Payload =
{ ""@id"" : ""ArithmeticExpression"",
""@type"" : ""Class"",
""@subdocument"" : [],
""@abstract"" : [] }
2. See error :
{
""api:message"":""Error: type_not_found('http://terminusdb.com/schema/system#Class%27)%5Cn%5Cn%5CnNote: some frames are missing due to last-call optimization.\nRe-run your program in debug mode (:- debug.) to get more detail.\n\n"",
""api:status"":""api:server_error""
}",2021-07-05T13:31:39Z,Francesca-Bit,https://github.com/terminusdb/terminusdb/issues/352#issuecomment-874117110,I found the problem,i find the problem,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,354,2021-07-05T15:55:07Z,Francesca-Bit,Document Interface - PUT rest call - by web interface,https://github.com/terminusdb/terminusdb/issues/354,"Describe the bug
When you trying to update a document using the PUT call throw the dashboard interface you get an error
To Reproduce
Steps to reproduce the behavior:

Go to dashboard document  test interface
Try to replace an existed document
See error

Access to XMLHttpRequest at 'https://127.0.0.1:6363/api/document/admin/new_test/local/branch/main?graph_type=schema&author=admin&message=update%20document' from origin 'http://localhost:3030' has been blocked by CORS policy: No 'Access-Control-Allow-Origin' header is present on the requested resource.
xhr.js:177 PUT https://127.0.0.1:6363/api/document/admin/new_test/local/branch/main?graph_type=schema&author=admin&message=update%20document net::ERR_FAILED",2021-07-06T13:15:18Z,matko,https://github.com/terminusdb/terminusdb/issues/354#issuecomment-874750055,"If you actually look at the message that came back that results in the CORS error, what is in there?
Generally, you can still see the contents despite the CORS error in your network debug view thingie. It may provide some sort of hint of what is going wrong here.",if -PRON- actually look at the message that come back that result in the cor error what be in there generally -PRON- can still see the content despite the cor error in -PRON- network debug view thingie -PRON- may provide some sort of hint of what be go wrong here,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,354,2021-07-05T15:55:07Z,Francesca-Bit,Document Interface - PUT rest call - by web interface,https://github.com/terminusdb/terminusdb/issues/354,"Describe the bug
When you trying to update a document using the PUT call throw the dashboard interface you get an error
To Reproduce
Steps to reproduce the behavior:

Go to dashboard document  test interface
Try to replace an existed document
See error

Access to XMLHttpRequest at 'https://127.0.0.1:6363/api/document/admin/new_test/local/branch/main?graph_type=schema&author=admin&message=update%20document' from origin 'http://localhost:3030' has been blocked by CORS policy: No 'Access-Control-Allow-Origin' header is present on the requested resource.
xhr.js:177 PUT https://127.0.0.1:6363/api/document/admin/new_test/local/branch/main?graph_type=schema&author=admin&message=update%20document net::ERR_FAILED",2021-09-03T10:37:47Z,matko,https://github.com/terminusdb/terminusdb/issues/354#issuecomment-912440453,I believe this has actually been resolved. @Francesca-Bit confirm?,i believe this have actually be resolve @francesca - bit confirm,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,354,2021-07-05T15:55:07Z,Francesca-Bit,Document Interface - PUT rest call - by web interface,https://github.com/terminusdb/terminusdb/issues/354,"Describe the bug
When you trying to update a document using the PUT call throw the dashboard interface you get an error
To Reproduce
Steps to reproduce the behavior:

Go to dashboard document  test interface
Try to replace an existed document
See error

Access to XMLHttpRequest at 'https://127.0.0.1:6363/api/document/admin/new_test/local/branch/main?graph_type=schema&author=admin&message=update%20document' from origin 'http://localhost:3030' has been blocked by CORS policy: No 'Access-Control-Allow-Origin' header is present on the requested resource.
xhr.js:177 PUT https://127.0.0.1:6363/api/document/admin/new_test/local/branch/main?graph_type=schema&author=admin&message=update%20document net::ERR_FAILED",2021-09-03T15:10:32Z,Francesca-Bit,https://github.com/terminusdb/terminusdb/issues/354#issuecomment-912610582,yes I confirm,yes i confirm,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,356,2021-07-05T16:04:58Z,Francesca-Bit,Document Interface overwrite a schema graph,https://github.com/terminusdb/terminusdb/issues/356,"Describe the bug
try to override a schema graph
To Reproduce
Steps to reproduce the behavior:

POST url https://127.0.0.1:6363/api/document/admin/__DB__NAME__/local/branch/main?graph_type=schema&overwrite=true&author=admin&message=add%20a%20new%20document

Payload:
[
{
""@base"": ""terminusdb:///data/"",
""@Schema"": ""terminusdb:///schema#"",
""@type"": ""@context""
},
{
""@id"": ""Coordinate"",
""@type"": ""Class"",
""x"": ""xsd:decimal"",
""y"": ""xsd:decimal""
}
]
3. See error
_{
""api:message"":""Error: no_id_in_schema_document(json{'@base':""terminusdb:///data/"",'@Schema':""terminusdb:///schema#"",'@type':""@context""})\n\n\nNote: some frames are missing due to last-call optimization.\nRe-run your program in debug mode (:- debug.) to get more detail.\n\n"",
""api:status"":""api:server_error""
}",2021-07-06T12:35:30Z,matko,https://github.com/terminusdb/terminusdb/issues/356#issuecomment-874721169,"Don't use overwrite but full_replace as a query parameter if you want to do a full replacement.
Overwrite is not implemented at the moment.
You cannot use POST to insert the context document at the moment.",do not use overwrite but full_replace as a query parameter if -PRON- want to do a full replacement overwrite be not implement at the moment -PRON- can not use post to insert the context document at the moment,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,357,2021-07-05T18:13:53Z,spl,Document how to use a local terminus_store_prolog for development,https://github.com/terminusdb/terminusdb/issues/357,"Is your feature request related to a problem?
When developing terminusdb, it is useful to use a local build of terminus_store_prolog, because, for example, it has been changed and not yet released. It was not clear to me how to do this.
Describe the solution you'd like
I'd like to add documentation on how to accomplish this. Ask suggested by @rrooij, I used the following:
cd ~/.local/share/swi-prolog/pack
ln -s ~/projects/terminusdb/terminus_store_prolog

As long as I opened swipl after creating the symbolic link, this worked for me.
Describe alternatives you've considered
I tried doing this inside swipl:

Additional context
Add any other context or screenshots about the feature request here.",,,,,,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,358,2021-07-05T18:19:36Z,spl,Document how to use a local terminus_store_prolog for development,https://github.com/terminusdb/terminusdb/issues/358,"Is your feature request related to a problem?
When developing terminusdb, it is useful to use a local build of terminus_store_prolog, because, for example, terminus_store_prolog has been changed and not yet released. It was not clear to me how to do this.
Describe the solution you'd like
I'd like to add documentation on how to accomplish this. Ask suggested by @rrooij, I used the following:
cd /home/spl/.local/share/swi-prolog/pack
ln -s /home/spl/projects/terminusdb/terminus_store_prolog

As long as I opened swipl after creating the symbolic link, this worked for me.
Describe alternatives you've considered
I tried doing this inside swipl:
pack_install('file:///home/spl/projects/terminusdb/terminus_store_prolog').
But it complained with an error when trying to build terminus-store:
% mkdir -p lib/x86_64-darwin
% cd rust; cargo build --release
%    Compiling libc v0.2.94
%    Compiling proc-macro2
%  v1.0.26
%    Compiling memchr v2.4.0
ERROR: error: failed to run custom build command for `libc v0.2.94`
ERROR: 
ERROR: Caused by:
ERROR:   could not execute process `/home/spl/.local/share/swi-prolog/pack/terminus_store_prolog/rust/target/release/build/libc-431a69216e032de1/build-script-build` (never executed)
ERROR: 
ERROR: Caused by:
ERROR:   Permission denied (os error 13)
ERROR: warning: build failed, waiting for other jobs to finish...
ERROR: error: build failed
% make: *** [build] Error 101
ERROR: Process ""process(path(make),[all])"": exit status: 2
ERROR: In:
ERROR:   [20] throw(error(process_error(...,...),_10632))
ERROR:   [18] '$apply':forall('<garbage_collected>',prolog_pack:run_process(...,...,...)) at /usr/local/Cellar/swi-prolog/8.2.4/libexec/lib/swipl/boot/apply.pl:52
ERROR:   [14] prolog_pack:pack_post_install(terminus_store_prolog,'/home/spl/.local/share/swi-prolog/pack/terminus_store_prolog',[pack(terminus_store_prolog),...|...]) at /usr/local/Cellar/swi-prolog/8.2.4/libexec/lib/swipl/library/prolog_pack.pl:1017
ERROR:    [9] <user>

I'm not sure where to add this documentation. So, for now, this issue documents the problem and current solution.",,,,,,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,360,2021-07-07T15:51:11Z,GavinMendelGleason,Lexical Key in subdocument gives instantiation error,https://github.com/terminusdb/terminusdb/issues/360,"Describe the bug
Create a sub-document which makes use of the lexical key strategy.
To Reproduce
Given the following schema:
{ ""@type"" : ""@context"",
  ""@base"" : ""http://i/"",
  ""@schema"" : ""http://s/"" }

{ ""@documentation"": {""@comment"": ""This is address"", ""@properties"": {}},
  ""@id"": ""Address"",
  ""@key"": {""@fields"": [""street"", ""postal_code""],
           ""@type"": ""Lexical""},
  ""@subdocument"": [],
  ""@type"": ""Class"",
  ""country"": ""Country"",
  ""postal_code"": ""xsd:string"",
  ""street"": ""xsd:string""}

{ ""@id"": ""Country"",
  ""@key"": {""@type"": ""ValueHash""},
  ""@type"": ""Class"",
  ""name"": ""xsd:string"",
  ""perimeter"": {""@class"": ""Coordinate"",
                ""@type"": ""List""}}

{ ""@id"": ""Coordinate"",
  ""@key"": {""@type"": ""Random""},
  ""@type"": ""Class"",
  ""x"": ""xsd:decimal"",
  ""y"": ""xsd:decimal""}

{ ""@id"": ""Team"", ""@type"": ""Enum"",
  ""@value"": [""IT"", ""Marketing""]}

{ ""@id"": ""Employee"",
  ""@inherits"": ""Person"",
  ""@key"": {""@type"": ""Random""},
  ""@type"": ""Class"",
  ""address_of"": ""Address"",
  ""contact_number"": {""@class"": ""xsd:string"",
                     ""@type"": ""Optional""},
  ""managed_by"": ""Employee""}

{ ""@documentation"": { ""@comment"": ""This is a person"",
                      ""@properties"": { ""age"": ""Age of the person."",
                                       ""name"": ""Name of the person."" }},
  ""@id"": ""Person"",
  ""@key"": {""@type"": ""Random""},
  ""@type"": ""Class"",
  ""age"": ""xsd:integer"",
  ""friend_of"": {""@class"": ""Person"",
                ""@type"": ""Set""},
  ""name"": ""xsd:string""}
Attempt to add the following documents:
[{'@type': 'Country', 'name': 'United Kingdom'},
 {'@type': 'Address',
  'country': {'@type': 'Country', 'name': 'United Kingdom'},
  'postal_code': 'A12 345',
  'street': '123 Abc Street'},
 {'@id': 'Employee_def2f711f95943378d8b9712b2820a8a',
  '@type': 'Employee',
  'address_of': {'@type': 'Address',
                 'country': {'@type': 'Country', 'name': 'United Kingdom'},
                 'postal_code': 'A12 345',
                 'street': '123 Abc Street'},
  'contact_number': '07777123456',
  'managed_by': {'@id': 'Employee_def2f711f95943378d8b9712b2820a8a',
                 '@type': '@id'}}] 
This will yield an instantiation error due to the failure of the lexical idgen routine to find a definite value for the parent key despite the attempt at co-routining.",,,,,,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,361,2021-07-12T13:27:06Z,spl,Create seshat in new document interface. Test update / retrieval,https://github.com/terminusdb/terminusdb/issues/361,,,,,,,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,362,2021-07-12T14:10:42Z,matko,document insertion and replacement does not properly check for duplicates,https://github.com/terminusdb/terminusdb/issues/362,"When using the document interface with the POST or PUT method to insert new documents or replace existing documents, it is possible to submit multiple documents with the same ID (or the same set of keys yielding the same ID). The result will be that the document interface will attempt to insert a union of all these objects into the database, which in most cases would probably cause a schema failure but in some cases may just silently insert a bunch of data.
This is not what should happen here. Only one document should be inserted as a replacement. We should error if we encounter a document with the same ID again, and fail the transaction.",2021-08-12T09:14:36Z,spl,https://github.com/terminusdb/terminusdb/issues/362#issuecomment-897478284,"Here's how I reproduce this issue with instances that have duplicate @ids:
# Create the database
./terminusdb db create testdb --label label --comment comment \
  --prefixes '{""@base"":""http://base/"",""@schema"":""http://schema/""}'

# Create the schema
echo '{""@type"":""Class"",""@id"":""Test1"",""s"":""xsd:string""}' | \
  xh 'http://localhost:6363/api/document/admin/testdb?author=author&message=message&graph_type=schema'

# Post two instances of the schema in the same request. The instances have the same @id.
echo '{""@type"":""Test1"",""@id"":""id2"",""s"":""blah""} {""@type"":""Test1"",""@id"":""id2"",""s"":""blah2""}' | \
  xh 'http://localhost:6363/api/document/admin/testdb?author=author&message=message'
The response for the last command is:
HTTP/1.1 200 OK
connection: keep-alive
content-length: 40
content-type: application/json; charset=UTF-8
date: Thu, 12 Aug 2021 09:12:20 GMT

[
    ""http://base/id2"",
    ""http://base/id2""
]

For a query on the documents:
xh 'http://localhost:6363/api/document/admin/testdb'
I get this:
HTTP/1.1 200 OK
connection: keep-alive
content-length: 43
content-type: application/json; stream=true; charset=UTF-8
date: Thu, 12 Aug 2021 09:12:52 GMT

{
    ""@id"": ""id2"",
    ""@type"": ""Test1"",
    ""s"": ""blah""
}","here be how i reproduce this issue with instance that have duplicate @id # create the database /terminusdb db create testdb --label label --comment comment \ --prefixe ' { "" @base""""http//base/""""@schema""""http//schema/ "" } ' # create the schema echo ' { "" @type""""class""""@id""""test1""""s""""xsdstring "" } ' | \ xh ' http//localhost6363 / api / document / admin / testdbauthor = author&message = message&graph_type = schema ' # post two instance of the schema in the same request the instance have the same @id echo ' { "" @type""""test1""""@id""""id2""""s""""blah "" } { "" @type""""test1""""@id""""id2""""s""""blah2 "" } ' | \ xh ' http//localhost6363 / api / document / admin / testdbauthor = author&message = message ' the response for the last command be http/11 200 ok connection keep - alive content - length 40 content - type application / json charset = utf-8 date thu 12 aug 2021 091220 gmt [ "" http//base / id2 "" "" http//base / id2 "" ] for a query on the document xh ' http//localhost6363 / api / document / admin / testdb ' i get this http/11 200 ok connection keep - alive content - length 43 content - type application / json stream = true charset = utf-8 date thu 12 aug 2021 091252 gmt { "" @id "" "" id2 "" "" @type "" "" test1 "" "" s "" "" blah "" }",0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,362,2021-07-12T14:10:42Z,matko,document insertion and replacement does not properly check for duplicates,https://github.com/terminusdb/terminusdb/issues/362,"When using the document interface with the POST or PUT method to insert new documents or replace existing documents, it is possible to submit multiple documents with the same ID (or the same set of keys yielding the same ID). The result will be that the document interface will attempt to insert a union of all these objects into the database, which in most cases would probably cause a schema failure but in some cases may just silently insert a bunch of data.
This is not what should happen here. Only one document should be inserted as a replacement. We should error if we encounter a document with the same ID again, and fail the transaction.",2021-08-16T12:01:56Z,spl,https://github.com/terminusdb/terminusdb/issues/362#issuecomment-899453888,"The above reproduces only the POST document insertion issue. The following reproduces a similar issue with the PUT document update:
# Create the database
./terminusdb db create testdb --label label --comment comment \
  --prefixes '{""@base"":""http://base/"",""@schema"":""http://schema/""}'

# Create the schema
echo '{""@type"":""Class"",""@id"":""Test1"",""s"":""xsd:string""}' | \
  xh 'http://localhost:6363/api/document/admin/testdb?author=author&message=message&graph_type=schema'

# Post one instance of the schema
echo '{""@type"":""Test1"",""@id"":""id2"",""s"":""s1""}' | \
  xh 'http://localhost:6363/api/document/admin/testdb?author=author&message=message'

# Put two instances of the same `@id`
echo '{""@type"":""Test1"",""@id"":""id2"",""s"":""s2""} {""@type"":""Test1"",""@id"":""id2"",""s"":""s3""}' | \
  xh PUT 'http://localhost:6363/api/document/admin/testdb?author=author&message=message'
To see what the s field has:
xh 'http://localhost:6363/api/document/admin/testdb?author=author&message=message'
HTTP/1.1 200 OK
connection: keep-alive
content-length: 41
content-type: application/json; stream=true; charset=UTF-8
date: Mon, 16 Aug 2021 12:02:13 GMT

{
    ""@id"": ""id2"",
    ""@type"": ""Test1"",
    ""s"": ""s2""
}

Note that the s3 value was ignored, which is unexpected.","the above reproduce only the post document insertion issue the follow reproduce a similar issue with the put document update # create the database /terminusdb db create testdb --label label --comment comment \ --prefixe ' { "" @base""""http//base/""""@schema""""http//schema/ "" } ' # create the schema echo ' { "" @type""""class""""@id""""test1""""s""""xsdstring "" } ' | \ xh ' http//localhost6363 / api / document / admin / testdbauthor = author&message = message&graph_type = schema ' # post one instance of the schema echo ' { "" @type""""test1""""@id""""id2""""s""""s1 "" } ' | \ xh ' http//localhost6363 / api / document / admin / testdbauthor = author&message = message ' # put two instance of the same ` @id ` echo ' { "" @type""""test1""""@id""""id2""""s""""s2 "" } { "" @type""""test1""""@id""""id2""""s""""s3 "" } ' | \ xh put ' http//localhost6363 / api / document / admin / testdbauthor = author&message = message ' to see what the s field have xh ' http//localhost6363 / api / document / admin / testdbauthor = author&message = message ' http/11 200 ok connection keep - alive content - length 41 content - type application / json stream = true charset = utf-8 date mon 16 aug 2021 120213 gmt { "" @id "" "" id2 "" "" @type "" "" test1 "" "" s "" "" s2 "" } note that the s3 value be ignore which be unexpected",0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,364,2021-07-14T09:40:27Z,rrooij,Initialize DBs/data products with schemas on fresh start,https://github.com/terminusdb/terminusdb/issues/364,"The Docker images of databases like Postgres allow startup scripts to be defined. This makes deployment a lot easier, as you don't have to use separate Python scripts and the like or create new images with the databases/data products already bootstrapped.
See for information on how Postgres does this: https://github.com/docker-library/docs/blob/master/postgres/README.md#initialization-scripts
I think that we could do this by:

Scan for JSON files in /docker-entrypoint-init-documents.d in which we add the documents to the system in lexical order.
We put this logic in the Docker initialization script

Saves a lot of hassle during deployment.",,,,,,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,366,2021-07-19T12:59:01Z,spl,API microbenchmark,https://github.com/terminusdb/terminusdb/issues/366,"Benchmark the operations on the documents interface.
There are different sorts of performance analysis we can do, but, now that we have a new HTTP API (the documents interface), I think the first thing we want to do is analyze the performance of each operation and/or sequence of operations. This will give us visibility into which parts of the operation need to be optimized.
Later, we can work on load testing to see how well we handle simultaneous connections and various operations.",2021-07-19T17:25:22Z,spl,https://github.com/terminusdb/terminusdb/issues/366#issuecomment-882725643,"This is also called response time testing.

Building a Response Timer to Benchmark API Performance - uses curl templates",this be also call response time test build a response timer to benchmark api performance - use curl template,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,366,2021-07-19T12:59:01Z,spl,API microbenchmark,https://github.com/terminusdb/terminusdb/issues/366,"Benchmark the operations on the documents interface.
There are different sorts of performance analysis we can do, but, now that we have a new HTTP API (the documents interface), I think the first thing we want to do is analyze the performance of each operation and/or sequence of operations. This will give us visibility into which parts of the operation need to be optimized.
Later, we can work on load testing to see how well we handle simultaneous connections and various operations.",2021-07-19T17:55:20Z,spl,https://github.com/terminusdb/terminusdb/issues/366#issuecomment-882743637,"This is also called unit load testing at k6, which looks like an attractive option for scripted testing. There's also k6-action.",this be also call unit load testing at k6 which look like an attractive option for script testing there be also k6-action,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,366,2021-07-19T12:59:01Z,spl,API microbenchmark,https://github.com/terminusdb/terminusdb/issues/366,"Benchmark the operations on the documents interface.
There are different sorts of performance analysis we can do, but, now that we have a new HTTP API (the documents interface), I think the first thing we want to do is analyze the performance of each operation and/or sequence of operations. This will give us visibility into which parts of the operation need to be optimized.
Later, we can work on load testing to see how well we handle simultaneous connections and various operations.",2021-07-19T18:31:08Z,spl,https://github.com/terminusdb/terminusdb/issues/366#issuecomment-882767014,"Open source load testing tool review 2020 - a comparison by a k6 developer, found in HN comments.",open source load testing tool review 2020 - a comparison by a k6 developer find in hn comment,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,366,2021-07-19T12:59:01Z,spl,API microbenchmark,https://github.com/terminusdb/terminusdb/issues/366,"Benchmark the operations on the documents interface.
There are different sorts of performance analysis we can do, but, now that we have a new HTTP API (the documents interface), I think the first thing we want to do is analyze the performance of each operation and/or sequence of operations. This will give us visibility into which parts of the operation need to be optimized.
Later, we can work on load testing to see how well we handle simultaneous connections and various operations.",2021-08-23T13:31:11Z,spl,https://github.com/terminusdb/terminusdb/issues/366#issuecomment-903771576,@rrooij Can we talk about getting a dedicated system to run these benchmarks? There's a lot of variability across runs that make it very difficult to determine performance changes over time.,@rrooij can -PRON- talk about get a dedicated system to run these benchmark there be a lot of variability across run that make -PRON- very difficult to determine performance change over time,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,366,2021-07-19T12:59:01Z,spl,API microbenchmark,https://github.com/terminusdb/terminusdb/issues/366,"Benchmark the operations on the documents interface.
There are different sorts of performance analysis we can do, but, now that we have a new HTTP API (the documents interface), I think the first thing we want to do is analyze the performance of each operation and/or sequence of operations. This will give us visibility into which parts of the operation need to be optimized.
Later, we can work on load testing to see how well we handle simultaneous connections and various operations.",2021-08-30T14:52:30Z,spl,https://github.com/terminusdb/terminusdb/issues/366#issuecomment-908409471,Fixed in be6dcdb.,fix in be6dcdb,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,367,2021-07-19T13:22:40Z,AstroChelonian,Authorization should do a pre-scan of WOQL to obtain resources and check once.,https://github.com/terminusdb/terminusdb/issues/367,,,,,,,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,368,2021-07-19T13:23:23Z,AstroChelonian,Schema checking should be made fast and efficient in prolog.,https://github.com/terminusdb/terminusdb/issues/368,"Contexts are retrieved repeatedly!
 Evaluate compilation to prolog possibly based on commit?",,,,,,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,369,2021-07-19T13:43:33Z,AstroChelonian,Test WoQL queries,https://github.com/terminusdb/terminusdb/issues/369,We should be attempting to save WOQL queries into a WOQL schema aware graph so we can test correctness of tests.,2021-07-22T10:34:42Z,GavinMendelGleason,https://github.com/terminusdb/terminusdb/issues/369#issuecomment-884812736,Completed in 9d0b4b7,complete in 9d0b4b7,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,370,2021-07-21T11:13:23Z,spl,Route handler should return useful error for unexpected content-type,https://github.com/terminusdb/terminusdb/issues/370,"Describe the bug
If I don't provide the HTTP header Content-type: application/json when I create a database (e.g. /api/db/admin/database-name), I get the unhelpful error:
HTTP/1.1 500 Internal Server Error
Date: Wed, 21 Jul 2021 11:07:49 GMT
Connection: Keep-Alive
Content-Type: application/json
Content-Length: 202

{
  ""@type"":""api:ErrorResponse"",
  ""api:error"": {""@type"":""api:APIEndpointFailed""},
  ""api:message"":""Failed to run the API endpoint goal db_handler(admin,'database-name')"",
  ""api:status"":""api:failure""
}

To Reproduce
curl -i ""http://127.0.0.1:6363/api/db/admin/database-name"" \
  -u ""admin:root"" \
  -d '{""comment"":""New database"",""label"":""database-name""}' \
  -H 'Content-Type:'

Expected behavior
I would expect an error about a missing or incorrect Content-Type, since that seems to be the first check that should be done.",,,,,,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,371,2021-07-21T13:29:15Z,spl,HTTP client errors should not be 500 Internal Server Error,https://github.com/terminusdb/terminusdb/issues/371,"Currently, it seems like TerminusDB uses the HTTP response status 500 Internal Server Error for most (if not all) errors. Many errors can be attributed to bad requests by clients. Those responses should probably have the status 400 Bad Request. There may be other cases in which the 500 should be a client error response. In general, we should review the errors we return.",,,,,,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,372,2021-07-21T19:00:22Z,Cheukting,[new document API] Enum is not updating,https://github.com/terminusdb/terminusdb/issues/372,"Describe the bug
When updating an Enum class in the schema with replace_document() in the Python client, the backend does not get updated
To Reproduce
updating Team with:
{'@type': 'Enum', '@id': 'Team', '@value': ['IT', 'Marketing']}
but got this when call get_all_documents():
{'@id': 'Team', '@type': 'Enum', '@value': ['Information Technology', 'Amazing Marketing']}
which is the old Team definition
Expected behavior
get back what I send with replace_document()
Additional context
This issue appear in the latest development branch",2021-07-22T12:50:38Z,GavinMendelGleason,https://github.com/terminusdb/terminusdb/issues/372#issuecomment-884886312,Fixed in 6159398,fix in 6159398,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,373,2021-07-21T21:23:06Z,GavinMendelGleason,List carriage in new list parameter to data type in python/js client,https://github.com/terminusdb/terminusdb/issues/373,"Describe the bug
Currently data lists are being generated in a non-savable format that does not conform to the woql.json schema. This can be fixed by placing the list in a DataValue carriage on the 'list' key.",2021-08-12T08:48:25Z,GavinMendelGleason,https://github.com/terminusdb/terminusdb/issues/373#issuecomment-897460753,Fixed in JS and Python client,fix in js and python client,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,374,2021-07-21T22:38:25Z,GavinMendelGleason,Optionals of Enums elaborate incorrectly,https://github.com/terminusdb/terminusdb/issues/374,"Describe the bug
The elaborator does not work properly for optionals of enums in the new document interface.
To Reproduce
Create a schema with an optional of an enum and then attempt to save and retrieve the document. This will yield a failure to recognise the elaborated value.",2021-07-22T10:31:54Z,GavinMendelGleason,https://github.com/terminusdb/terminusdb/issues/374#issuecomment-884811315,Fixed in 883cfcf,fix in 883cfcf,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,375,2021-07-22T09:41:39Z,rrooij,DBs/data products have unique names that can't be used across organizations,https://github.com/terminusdb/terminusdb/issues/375,"If one organization has a database called test, another organization can't create a DB/data product with the same name anymore. This is very problematic as we should allow the same names to be used across multiple organizations.
Tested on latest main branch commit.",2021-07-22T10:33:05Z,GavinMendelGleason,https://github.com/terminusdb/terminusdb/issues/375#issuecomment-884811919,Fixed in 0287ef5,fix in 0287ef5,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,376,2021-07-23T08:56:14Z,GavinMendelGleason,Documentation object in class_frame/3,https://github.com/terminusdb/terminusdb/issues/376,class_frame/3 should return a JSON document with a full documentation string for the class so that applications can report the naming and comment information appropriately.,,,,,,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,378,2021-07-26T13:17:19Z,GavinMendelGleason,Improve document interface documentation.,https://github.com/terminusdb/terminusdb/issues/378,The document interface documentation in notion should be checked for correctness and expanded where necessary with discursive parts removed.,2021-08-26T15:39:57Z,GavinMendelGleason,https://github.com/terminusdb/terminusdb/issues/378#issuecomment-906523150,"It will have to improve forever, but at least a reasonable basis is now in place.",-PRON- will have to improve forever but at least a reasonable basis be now in place,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,379,2021-07-28T09:30:36Z,ghpu,Cannot bundle a database,https://github.com/terminusdb/terminusdb/issues/379,"When trying to bundle a database with the cli : terminusdb bundle admin/my_db ,
I got the error :
Error: schema_check_failure([_17722{'@type':'vio:UntypedInstance','vio:message':_17750{'@type':'xsd:string','@value':'The subject terminusdb:///repository/data/Remote_79b96e8b0b24e00e25cf2adb6602233f has no defined class.'},'vio:subject':_17738{'@type':'xsd:anyURI','@value':'terminusdb:///repository/data/Remote_79b96e8b0b24e00e25cf2adb6602233f'}}])
The problem appears my_db being empty or full.
Infos

OS:  Ubuntu 21.04
terminusdb compiled from v4.2.1 tag,
SWI Prolog v 8.2.4",2021-08-03T07:21:35Z,ghpu,https://github.com/terminusdb/terminusdb/issues/379#issuecomment-891602347,"As a work-around, one can comment out  ""validate_validation_objects(Validations, All_Witnesses, Witnesses)"" in the   run_transactions(Transactions, All_Witnesses, Meta_Data) , line 235 of src/core/transaction/database.pl .","as a work - around one can comment out "" validate_validation_objects(validation all_witnesses witness ) "" in the run_transactions(transactions all_witnesses meta_data ) line 235 of src / core / transaction / databasepl",0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,379,2021-07-28T09:30:36Z,ghpu,Cannot bundle a database,https://github.com/terminusdb/terminusdb/issues/379,"When trying to bundle a database with the cli : terminusdb bundle admin/my_db ,
I got the error :
Error: schema_check_failure([_17722{'@type':'vio:UntypedInstance','vio:message':_17750{'@type':'xsd:string','@value':'The subject terminusdb:///repository/data/Remote_79b96e8b0b24e00e25cf2adb6602233f has no defined class.'},'vio:subject':_17738{'@type':'xsd:anyURI','@value':'terminusdb:///repository/data/Remote_79b96e8b0b24e00e25cf2adb6602233f'}}])
The problem appears my_db being empty or full.
Infos

OS:  Ubuntu 21.04
terminusdb compiled from v4.2.1 tag,
SWI Prolog v 8.2.4",2021-08-11T13:57:59Z,spl,https://github.com/terminusdb/terminusdb/issues/379#issuecomment-896850251,@ghpu Merci beaucoup for your helpful error report! The fix in our main branch and will be in our next major release.,@ghpu merci beaucoup for -PRON- helpful error report the fix in -PRON- main branch and will be in -PRON- next major release,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,380,2021-07-28T11:59:46Z,fidelthomet,WOQL: using «update_triple» with three parameters and «and»,https://github.com/terminusdb/terminusdb/issues/380,"Describe the bug
WOQL.update_triple can be used with either three or four parameters. The fourth one represents the old value (oldObjValue) which should be replaced. If the fourth parameter is set to a new variable (e.g. v:oldValue) all existing values are overwritten since the variable matches all of them. The same happens when the fourth parameter is omitted, in the JSON-LD representation a variable AnyObject is introduced.
So far, so good. But what happens when I want update multiple triples at once?
To Reproduce
in the desktop client query interface enter (but don't run) this query
WOQL
  .update_triple('doc:Card', 'scm:x', 100)
  .update_triple('doc:Card', 'scm:y', 150)
looking at the JSON-LD representation, again the variable AnyObject is introduced. For both. Which essentially turns the above query into:
WOQL
  .update_triple('doc:Card', 'scm:x', 100, 'v:AnyObject')
  .update_triple('doc:Card', 'scm:y', 150, 'v:AnyObject')
assuming that the existing values for scm:x and scm:y differ, v:AnyObject will match all values for scm:x but none for scm:y. Therefore scm:x is updated correctly, while an additional value is inserted for scm:y
Expected behavior
running the query should replace existing values for both properties / v:AnyObject should not be reused.
Info (please complete the following information):
Desktop Client Version: 4.2.3
Workaround
provide a fourth parameter with different variable names:
WOQL
  .update_triple('doc:Card', 'scm:x', 100, 'v:oldX')
  .update_triple('doc:Card', 'scm:y', 150, 'v:oldY')",,,,,,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,381,2021-07-28T13:16:58Z,NeelParihar,WOQL: Issue while executing a WOQL query having a undefined variable having group_by ,https://github.com/terminusdb/terminusdb/issues/381,"Describe the bug
I was executing a WOQL query having a ""group_by"" function, ""length"" function using an undefined variable inside a ""and"" function using WOQL javascript client. While executing it the terminal just got stuck for 5 to 10 mins after this it gave a error of socket hangup.
To Reproduce
Steps to reproduce the behavior:

Write a query having the same structure as the below query and execute it.

WOQL.select(""v:JobInterest"", ""v:Id"", ""v:TheCount"", ""v:JobRoleInterest_group"")
    .and(
WOQL.group_by(""JobInterest"", [""Id""], ""JobRoleInterest_group"")
             .triple(""v:Coder"", ""Id"", ""v:Id"")
             .triple(""v:Coder"", ""JobInfo"", ""v:JobInfo"")
             .triple(""v:JobInfo"", ""JobInterest"", ""v:JobInterest""),
WOQL.length(""v:JobRoleInterestGroup"",""v:TheCount"")
)

Here ""v:JobRoleInterestGroup"" is the undefined variable.

After executing the query the terminal should have stuck in between.
After some time it throws an Exception having socket hangup.

Expected behavior
After executing the query using the WOQL client it should throw a syntax error or a variable undefined error.
Info:

OS: Mac OS Catalina
How did you run terminus-server (terminus-quickstart, using Docker directly, manual installation) : Using terminus-bootstrap (Dev branch)",2021-08-17T15:13:51Z,GavinMendelGleason,https://github.com/terminusdb/terminusdb/issues/381#issuecomment-900386490,Can I get the schema and a couple of documents for this example?,can i get the schema and a couple of document for this example,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,381,2021-07-28T13:16:58Z,NeelParihar,WOQL: Issue while executing a WOQL query having a undefined variable having group_by ,https://github.com/terminusdb/terminusdb/issues/381,"Describe the bug
I was executing a WOQL query having a ""group_by"" function, ""length"" function using an undefined variable inside a ""and"" function using WOQL javascript client. While executing it the terminal just got stuck for 5 to 10 mins after this it gave a error of socket hangup.
To Reproduce
Steps to reproduce the behavior:

Write a query having the same structure as the below query and execute it.

WOQL.select(""v:JobInterest"", ""v:Id"", ""v:TheCount"", ""v:JobRoleInterest_group"")
    .and(
WOQL.group_by(""JobInterest"", [""Id""], ""JobRoleInterest_group"")
             .triple(""v:Coder"", ""Id"", ""v:Id"")
             .triple(""v:Coder"", ""JobInfo"", ""v:JobInfo"")
             .triple(""v:JobInfo"", ""JobInterest"", ""v:JobInterest""),
WOQL.length(""v:JobRoleInterestGroup"",""v:TheCount"")
)

Here ""v:JobRoleInterestGroup"" is the undefined variable.

After executing the query the terminal should have stuck in between.
After some time it throws an Exception having socket hangup.

Expected behavior
After executing the query using the WOQL client it should throw a syntax error or a variable undefined error.
Info:

OS: Mac OS Catalina
How did you run terminus-server (terminus-quickstart, using Docker directly, manual installation) : Using terminus-bootstrap (Dev branch)",2021-08-17T16:14:20Z,NeelParihar,https://github.com/terminusdb/terminusdb/issues/381#issuecomment-900435739,"This was the schema used to insert documents:
[
  {
    '@base': 'terminusdb:///data/',
    '@schema': 'terminusdb:///schema#',
    '@type': '@context'
  },
  {
    '@id': 'BootcampInfo',
    '@key': { '@type': 'ValueHash' },
    '@subdocument': [],
    '@type': 'Class',
    AttendedBootcamp: 'xsd:string',
    BootcampFinish: 'xsd:string',
    BootcampLoanYesNo: 'xsd:string',
    BootcampName: 'xsd:string',
    BootcampRecommend: 'xsd:string'
  },
  {
    '@id': 'Channel',
    '@type': 'Enum',
    '@value': [
      'YouTubeCodeCourse',
      'YouTubeCodingTrain',
      'YouTubeCodingTut360',
      'YouTubeComputerphile',
      'YouTubeDerekBanas',
      'YouTubeDevTips',
      'YouTubeEngineeredTruth',
      'YouTubeFCC',
      'YouTubeFunFunFunction',
      'YouTubeGoogleDev',
      'YouTubeLearnCode',
      'YouTubeLevelUpTuts',
      'YouTubeMIT',
      'YouTubeMozillaHacks',
      'YouTubeOther',
      'YouTubeSimplilearn',
      'YouTubeTheNewBoston',
      'NA'
    ]
  },
  {
    '@base': 'Coder_',
    '@id': 'Coder',
    '@key': { '@fields': [Array], '@type': 'Lexical' },
    '@type': 'Class',
    Age: 'xsd:string',
    BootcampInfo: 'BootcampInfo',
    Channels: { '@class': 'Channel', '@type': 'Set' },
    CityPopulation: 'xsd:string',
    CommuteTime: 'xsd:string',
    CountryCitizen: 'Country',
    CountryLive: 'Country',
    EmploymentField: 'xsd:string',
    EmploymentStatus: 'xsd:string',
    EventsAttended: { '@class': 'Event', '@type': 'Set' },
    ExpectedEarning: 'xsd:string',
    FinanciallySupporting: 'xsd:string',
    FirstDevJob: 'xsd:string',
    HoursLearning: 'xsd:string',
    Id: 'xsd:string',
    JobInfo: 'JobInfo',
    LanguageAtHome: 'xsd:string',
    ListensToPodcast: { '@class': 'Podcast', '@type': 'Set' },
    MoneyForLearning: 'xsd:string',
    MonthsProgramming: 'xsd:string',
    PersonalInfo: 'PersonalInfo',
    SchoolDegree: 'xsd:string',
    SchoolMajor: 'xsd:string',
    StudentDebtOwe: 'xsd:string',
    UsesResource: { '@class': 'Resource', '@type': 'Set' }
  },
  {
    '@id': 'Country',
    '@key': { '@fields': [Array], '@type': 'Lexical' },
    '@type': 'Class',
    Name: 'xsd:string'
  },
  {
    '@id': 'Event',
    '@type': 'Enum',
    '@value': [
      'CodeEventConferences',
      'CodeEventDjangoGirls',
      'CodeEventFCC',
      'CodeEventGameJam',
      'CodeEventGirlDev',
      'CodeEventHackathons',
      'CodeEventMeetup',
      'CodeEventNodeSchool',
      'CodeEventNone',
      'CodeEventOther',
      'CodeEventRailsBridge',
      'CodeEventRailsGirls',
      'CodeEventStartUpWknd',
      'CodeEventWkdBootcamps',
      'CodeEventWomenCode',
      'CodeEventWorkshops',
      'NA'
    ]
  },
  {
    '@id': 'Job',
    '@type': 'Enum',
    '@value': [
      'JobInterestBackEnd',
      'JobInterestDataEngr',
      'JobInterestDataSci',
      'JobInterestDevOps',
      'JobInterestFrontEnd',
      'JobInterestFullStack',
      'JobInterestGameDev',
      'JobInterestInfoSec',
      'JobInterestMobile',
      'JobInterestOther',
      'JobInterestProjMngr',
      'JobInterestQAEngr',
      'JobInterestUX',
      'NA'
    ]
  },
  {
    '@id': 'JobInfo',
    '@key': { '@type': 'ValueHash' },
    '@subdocument': [],
    '@type': 'Class',
    IsSoftwareDev: 'xsd:string',
    IsUnderEmployed: 'xsd:string',
    JobApplyWhen: 'xsd:string',
    JobInterest: { '@class': 'Job', '@type': 'Set' },
    JobPref: 'xsd:string',
    JobRelocateYesNo: 'xsd:string',
    JobRoleInterest: 'xsd:string'
  },
  {
    '@id': 'PersonalInfo',
    '@key': { '@type': 'ValueHash' },
    '@subdocument': [],
    '@type': 'Class',
    Gender: 'xsd:string',
    HasChildren: 'xsd:string',
    HasDebt: 'xsd:string',
    HasFinancialDependents: 'xsd:string',
    HasHomeMortgage: 'xsd:string',
    HasServedInMilitary: 'xsd:string',
    HasStudentDebt: 'xsd:string',
    HomeMortgageOwe: 'xsd:string',
    Income: 'xsd:string',
    IsEthnicMinority: 'xsd:string',
    MaritalStatus: 'xsd:string'
  },
  {
    '@id': 'Podcast',
    '@type': 'Enum',
    '@value': [
      'PodcastChangeLog',     'PodcastCodeNewbie',
      'PodcastCodePen',       'PodcastDevTea',
      'PodcastDotNET',        'PodcastGiantRobots',
      'PodcastJSAir',         'PodcastJSJabber',
      'PodcastNone',          'PodcastOther',
      'PodcastProgThrowdown', 'PodcastRubyRogues',
      'PodcastSEDaily',       'PodcastSERadio',
      'PodcastShopTalk',      'PodcastTalkPython',
      'PodcastTheWebAhead',   'NA'
    ]
  },
  {
    '@id': 'Resource',
    '@type': 'Enum',
    '@value': [
      'ResourceCodecademy', 'ResourceCodeWars',
      'ResourceCoursera',   'ResourceCSS',
      'ResourceEdX',        'ResourceEgghead',
      'ResourceFCC',        'ResourceHackerRank',
      'ResourceKA',         'ResourceLynda',
      'ResourceMDN',        'ResourceOdinProj',
      'ResourceOther',      'ResourcePluralSight',
      'ResourceSkillcrush', 'ResourceSO',
      'ResourceTreehouse',  'ResourceUdacity',
      'ResourceUdemy',      'ResourceW3S',
      'NA'
    ]
  }
]

These are some documents which you can use for testing purpose.
[
{
    '@id': 'Coder_0beaec444f842245092fce0bcced5b7f',
    '@type': 'Coder',
    Age: '22',
    BootcampInfo: {
      '@id': 'BootcampInfo_bcb6bed85df8383270e489604f621a7783b1ec1b',
      '@type': 'BootcampInfo',
      AttendedBootcamp: '0',
      BootcampFinish: 'NA',
      BootcampLoanYesNo: 'NA',
      BootcampName: 'NA',
      BootcampRecommend: 'NA'
    },
    CityPopulation: 'more than 1 million',
    CommuteTime: 'Less than 15 minutes',
    CountryCitizen: 'Country_United%20States%20of%20America',
    CountryLive: 'Country_United%20States%20of%20America',
    EmploymentField: 'software development and IT',
    EmploymentStatus: 'Employed for wages',
    ExpectedEarning: 'NA',
    FinanciallySupporting: 'NA',
    FirstDevJob: '1',
    HoursLearning: '20',
    Id: '0beaec444f842245092fce0bcced5b7f',
    JobInfo: {
      '@id': 'JobInfo_3fcaa80a364d278f79d9cf1184dc5f9fadb5cc94',
      '@type': 'JobInfo',
      IsSoftwareDev: '1',
      IsUnderEmployed: '0',
      JobApplyWhen: 'NA',
      JobPref: 'NA',
      JobRelocateYesNo: 'NA',
      JobRoleInterest: 'NA'
    },
    LanguageAtHome: 'Thai',
    MoneyForLearning: '0',
    MonthsProgramming: '55',
    PersonalInfo: {
      '@id': 'PersonalInfo_639241973c3ed50811ec3fa3e7067c3c3c0d647a',
      '@type': 'PersonalInfo',
      Gender: 'female',
      HasChildren: 'NA',
      HasDebt: '1',
      HasFinancialDependents: '0',
      HasHomeMortgage: '0',
      HasServedInMilitary: '0',
      HasStudentDebt: '1',
      HomeMortgageOwe: 'NA',
      Income: '100000',
      IsEthnicMinority: '1',
      MaritalStatus: 'single, never married'
    },
    SchoolDegree: ""bachelor's degree"",
    SchoolMajor: 'Computer Science',
    StudentDebtOwe: '30000',
    UsesResource: [ 'ResourceMDN', 'ResourceSO', 'ResourceW3S' ]
  },
  {
    '@id': 'Coder_0bf96b5bd925b9ea6ae3975cf8b748f3',
    '@type': 'Coder',
    Age: '22',
    BootcampInfo: {
      '@id': 'BootcampInfo_bcb6bed85df8383270e489604f621a7783b1ec1b',
      '@type': 'BootcampInfo',
      AttendedBootcamp: '0',
      BootcampFinish: 'NA',
      BootcampLoanYesNo: 'NA',
      BootcampName: 'NA',
      BootcampRecommend: 'NA'
    },
    CityPopulation: 'less than 100,000',
    CommuteTime: 'Less than 15 minutes',
    CountryCitizen: 'Country_Canada',
    CountryLive: 'Country_Canada',
    EmploymentField: 'NA',
    EmploymentStatus: 'Employed for wages',
    ExpectedEarning: '50000',
    FinanciallySupporting: 'NA',
    FirstDevJob: 'NA',
    HoursLearning: '20',
    Id: '0bf96b5bd925b9ea6ae3975cf8b748f3',
    JobInfo: {
      '@id': 'JobInfo_e8411098c5cb185de7c86e48d3e96e54ec5b6f7b',
      '@type': 'JobInfo',
      IsSoftwareDev: '0',
      IsUnderEmployed: '1',
      JobApplyWhen: ""I haven't decided"",
      JobInterest: [Array],
      JobPref: 'work for a startup',
      JobRelocateYesNo: 'NA',
      JobRoleInterest: 'Full-Stack Web Developer'
    },
    LanguageAtHome: 'English',
    MoneyForLearning: 'NA',
    MonthsProgramming: '60',
    PersonalInfo: {
      '@id': 'PersonalInfo_435e037d5f6d2874e12932c960f13222af44f28e',
      '@type': 'PersonalInfo',
      Gender: 'male',
      HasChildren: 'NA',
      HasDebt: '1',
      HasFinancialDependents: '0',
      HasHomeMortgage: '0',
      HasServedInMilitary: '0',
      HasStudentDebt: '1',
      HomeMortgageOwe: 'NA',
      Income: '7500',
      IsEthnicMinority: '0',
      MaritalStatus: 'single, never married'
    },
    SchoolDegree: 'some college credit, no degree',
    SchoolMajor: 'NA',
    StudentDebtOwe: '1500',
    UsesResource: [ 'ResourceSO' ]
  },
  {
    '@id': 'Coder_0c126bfa39a884dc8efa907cd71ad351',
    '@type': 'Coder',
    Age: '33',
    BootcampInfo: {
      '@id': 'BootcampInfo_bcb6bed85df8383270e489604f621a7783b1ec1b',
      '@type': 'BootcampInfo',
      AttendedBootcamp: '0',
      BootcampFinish: 'NA',
      BootcampLoanYesNo: 'NA',
      BootcampName: 'NA',
      BootcampRecommend: 'NA'
    },
    Channels: [ 'YouTubeFCC', 'YouTubeGoogleDev' ],
    CityPopulation: 'more than 1 million',
    CommuteTime: '30 to 44 minutes',
    CountryCitizen: 'Country_Germany',
    CountryLive: 'Country_Germany',
    EmploymentField: 'software development and IT',
    EmploymentStatus: 'Employed for wages',
    EventsAttended: [ 'CodeEventHackathons', 'CodeEventMeetup', 'CodeEventWorkshops' ],
    ExpectedEarning: '60000',
    FinanciallySupporting: '1',
    FirstDevJob: 'NA',
    HoursLearning: '5',
    Id: '0c126bfa39a884dc8efa907cd71ad351',
    JobInfo: {
      '@id': 'JobInfo_736dcd42a9776c1cc477a32e1ec1b8514a913dad',
      '@type': 'JobInfo',
      IsSoftwareDev: '0',
      IsUnderEmployed: '0',
      JobApplyWhen: 'more than 12 months from now',
      JobInterest: [Array],
      JobPref: 'work for a medium-sized company',
      JobRelocateYesNo: 'NA',
      JobRoleInterest: '  Front-End Web Developer, Data Engineer, Back-End Web Developer,   Quality Assurance Engineer'
    },
    LanguageAtHome: 'Sindhi',
    MoneyForLearning: '2000',
    MonthsProgramming: '6',
    PersonalInfo: {
      '@id': 'PersonalInfo_946d21b9ea29d1911a082da0757ee3cfa7359dd2',
      '@type': 'PersonalInfo',
      Gender: 'male',
      HasChildren: '0',
      HasDebt: '0',
      HasFinancialDependents: '1',
      HasHomeMortgage: 'NA',
      HasServedInMilitary: '0',
      HasStudentDebt: 'NA',
      HomeMortgageOwe: 'NA',
      Income: '85000',
      IsEthnicMinority: '1',
      MaritalStatus: 'married or domestic partnership'
    },
    SchoolDegree: ""bachelor's degree"",
    SchoolMajor: 'Computer Science',
    StudentDebtOwe: 'NA',
    UsesResource: [
      'ResourceCSS',
      'ResourceCodeWars',
      'ResourceFCC',
      'ResourceMDN',
      'ResourceSO'
    ]
  },
  {
    '@id': 'Coder_0c1f5973b546cd698b0012178493d38c',
    '@type': 'Coder',
    Age: '20',
    BootcampInfo: {
      '@id': 'BootcampInfo_bcb6bed85df8383270e489604f621a7783b1ec1b',
      '@type': 'BootcampInfo',
      AttendedBootcamp: '0',
      BootcampFinish: 'NA',
      BootcampLoanYesNo: 'NA',
      BootcampName: 'NA',
      BootcampRecommend: 'NA'
    },
    Channels: [
      'YouTubeCodingTut360',
      'YouTubeDevTips',
      'YouTubeEngineeredTruth',
      'YouTubeFCC',
      'YouTubeFunFunFunction',
      'YouTubeLearnCode',
      'YouTubeLevelUpTuts',
      'YouTubeMIT'
    ],
    CityPopulation: 'between 100,000 and 1 million',
    CommuteTime: 'NA',
    CountryCitizen: 'Country_United%20States%20of%20America',
    CountryLive: 'Country_United%20States%20of%20America',
    EmploymentField: 'NA',
    EmploymentStatus: 'Not working but looking for work',
    ExpectedEarning: 'NA',
    FinanciallySupporting: 'NA',
    FirstDevJob: 'NA',
    HoursLearning: '30',
    Id: '0c1f5973b546cd698b0012178493d38c',
    JobInfo: {
      '@id': 'JobInfo_61311e87e7cf914b8c7b3ccc50a16cfb09a7a228',
      '@type': 'JobInfo',
      IsSoftwareDev: '0',
      IsUnderEmployed: 'NA',
      JobApplyWhen: ""I haven't decided"",
      JobInterest: [Array],
      JobPref: 'work for a medium-sized company',
      JobRelocateYesNo: '1',
      JobRoleInterest: 'Back-End Web Developer, Full-Stack Web Developer'
    },
    LanguageAtHome: 'English',
    MoneyForLearning: '15',
    MonthsProgramming: '6',
    PersonalInfo: {
      '@id': 'PersonalInfo_20a2abee392bd5db73e77cd2961272daa340c6ec',
      '@type': 'PersonalInfo',
      Gender: 'female',
      HasChildren: 'NA',
      HasDebt: '0',
      HasFinancialDependents: '0',
      HasHomeMortgage: 'NA',
      HasServedInMilitary: '0',
      HasStudentDebt: 'NA',
      HomeMortgageOwe: 'NA',
      Income: 'NA',
      IsEthnicMinority: '0',
      MaritalStatus: 'single, never married'
    },
    SchoolDegree: 'some college credit, no degree',
    SchoolMajor: 'NA',
    StudentDebtOwe: 'NA',
    UsesResource: [
      'ResourceCSS',
      'ResourceCodecademy',
      'ResourceFCC',
      'ResourceHackerRank',
      'ResourceKA',
      'ResourceSO',
      'ResourceUdacity',
      'ResourceUdemy',
      'ResourceW3S'
    ]
  }
]

Let me know if you need something else","this be the schema use to insert document [ { ' @base ' ' terminusdb///data/ ' ' @schema ' ' terminusdb///schema # ' ' @type ' ' @context ' } { ' @id ' ' bootcampinfo ' ' @key ' { ' @type ' ' valuehash ' } ' @subdocument ' [ ] ' @type ' ' class ' attendedbootcamp ' xsdstring ' bootcampfinish ' xsdstring ' bootcamploanyesno ' xsdstring ' bootcampname ' xsdstring ' bootcamprecommend ' xsdstring ' } { ' @id ' ' channel ' ' @type ' ' enum ' ' @value ' [ ' youtubecodecourse ' ' youtubecodingtrain ' ' youtubecodingtut360 ' ' youtubecomputerphile ' ' youtubederekbana ' ' youtubedevtip ' ' youtubeengineeredtruth ' ' youtubefcc ' ' youtubefunfunfunction ' ' youtubegoogledev ' ' youtubelearncode ' ' youtubeleveluptut ' ' youtubemit ' ' youtubemozillahack ' ' youtubeother ' ' youtubesimplilearn ' ' youtubethenewboston ' ' na ' ] } { ' @base ' ' coder _ ' ' @id ' ' coder ' ' @key ' { ' @field ' [ array ] ' @type ' ' lexical ' } ' @type ' ' class ' age ' xsdstring ' bootcampinfo ' bootcampinfo ' channel { ' @class ' ' channel ' ' @type ' ' set ' } citypopulation ' xsdstring ' commutetime ' xsdstring ' countrycitizen ' country ' countrylive ' country ' employmentfield ' xsdstre ' employmentstatus ' xsdstring ' eventsattende { ' @class ' ' event ' ' @type ' ' set ' } expectedearne ' xsdstre ' financiallysupporting ' xsdstring ' firstdevjob ' xsdstre ' hourslearning ' xsdstring ' -PRON- d ' xsdstre ' jobinfo ' jobinfo ' languageathome ' xsdstre ' listenstopodcast { ' @class ' ' podcast ' ' @type ' ' set ' } moneyforlearne ' xsdstre ' monthsprogramme ' xsdstre ' personalinfo ' personalinfo ' schooldegree ' xsdstre ' schoolmajor ' xsdstre ' studentdebtowe ' xsdstring ' usesresource { ' @class ' ' resource ' ' @type ' ' set ' } } { ' @id ' ' country ' ' @key ' { ' @field ' [ array ] ' @type ' ' lexical ' } ' @type ' ' class ' name ' xsdstring ' } { ' @id ' ' event ' ' @type ' ' enum ' ' @value ' [ ' codeeventconference ' ' codeeventdjangogirl ' ' codeeventfcc ' ' codeeventgamejam ' ' codeeventgirldev ' ' codeeventhackathon ' ' codeeventmeetup ' ' codeeventnodeschool ' ' codeeventnone ' ' codeeventother ' ' codeeventrailsbridge ' ' codeeventrailsgirl ' ' codeeventstartupwknd ' ' codeeventwkdbootcamp ' ' codeeventwomencode ' ' codeeventworkshop ' ' na ' ] } { ' @id ' ' job ' ' @type ' ' enum ' ' @value ' [ ' jobinterestbackend ' ' jobinterestdataengr ' ' jobinterestdatasci ' ' jobinterestdevop ' ' jobinterestfrontend ' ' jobinterestfullstack ' ' jobinterestgamedev ' ' jobinterestinfosec ' ' jobinterestmobile ' ' jobinterestother ' ' jobinterestprojmngr ' ' jobinterestqaengr ' ' jobinterestux ' ' na ' ] } { ' @id ' ' jobinfo ' ' @key ' { ' @type ' ' valuehash ' } ' @subdocument ' [ ] ' @type ' ' class ' issoftwaredev ' xsdstring ' isunderemploye ' xsdstring ' jobapplywhen ' xsdstre ' jobinter { ' @class ' ' job ' ' @type ' ' set ' } jobpref ' xsdstre ' jobrelocateyesno ' xsdstring ' jobroleinterest ' xsdstring ' } { ' @id ' ' personalinfo ' ' @key ' { ' @type ' ' valuehash ' } ' @subdocument ' [ ] ' @type ' ' class ' gender ' xsdstring ' haschildren ' xsdstre ' hasdebt ' xsdstring ' hasfinancialdependent ' xsdstre ' hashomemortgage ' xsdstre ' hasservedinmilitary ' xsdstre ' hasstudentdebt ' xsdstre ' homemortgageowe ' xsdstre ' income ' xsdstre ' isethnicminority ' xsdstre ' maritalstatus ' xsdstring ' } { ' @id ' ' podcast ' ' @type ' ' enum ' ' @value ' [ ' podcastchangelog ' ' podcastcodenewbie ' ' podcastcodepen ' ' podcastdevtea ' ' podcastdotnet ' ' podcastgiantrobot ' ' podcastjsair ' ' podcastjsjabber ' ' podcastnone ' ' podcastother ' ' podcastprogthrowdown ' ' podcastrubyrogue ' ' podcastsedaily ' ' podcastseradio ' ' podcastshoptalk ' ' podcasttalkpython ' ' podcastthewebahead ' ' na ' ] } { ' @id ' ' resource ' ' @type ' ' enum ' ' @value ' [ ' resourcecodecademy ' ' resourcecodewar ' ' resourcecoursera ' ' resourcecss ' ' resourceedx ' ' resourceegghead ' ' resourcefcc ' ' resourcehackerrank ' ' resourceka ' ' resourcelynda ' ' resourcemdn ' ' resourceodinproj ' ' resourceoth ' ' resourcepluralsight ' ' resourceskillcrush ' ' resourceso ' ' resourcetreehouse ' ' resourceudacity ' ' resourceudemy ' ' resourcew3s ' ' na ' ] } ] these be some document which -PRON- can use for testing purpose [ { ' @id ' ' coder_0beaec444f842245092fce0bcced5b7f ' ' @type ' ' coder ' age ' 22 ' bootcampinfo { ' @id ' ' bootcampinfo_bcb6bed85df8383270e489604f621a7783b1ec1b ' ' @type ' ' bootcampinfo ' attendedbootcamp ' 0 ' bootcampfinish ' na ' bootcamploanyesno ' na ' bootcampname ' na ' bootcamprecommend ' na ' } citypopulation ' more than 1 million ' commutetime ' less than 15 minute ' countrycitizen ' country_united%20states%20of%20america ' countrylive ' country_united%20states%20of%20america ' employmentfield ' software development and -PRON- ' employmentstatus ' employ for wage ' expectedearning ' na ' financiallysupporting ' na ' firstdevjob ' 1 ' hourslearning ' 20 ' -PRON- d ' 0beaec444f842245092fce0bcced5b7f ' jobinfo { ' @id ' ' jobinfo_3fcaa80a364d278f79d9cf1184dc5f9fadb5cc94 ' ' @type ' ' jobinfo ' issoftwaredev ' 1 ' isunderemploye ' 0 ' jobapplywhen ' na ' jobpref ' na ' jobrelocateyesno ' na ' jobroleinterest ' na ' } languageathome ' thai ' moneyforlearning ' 0 ' monthsprogramme ' 55 ' personalinfo { ' @id ' ' personalinfo_639241973c3ed50811ec3fa3e7067c3c3c0d647a ' ' @type ' ' personalinfo ' gender ' female ' haschildren ' na ' hasdebt ' 1 ' hasfinancialdependent ' 0 ' hashomemortgage ' 0 ' hasservedinmilitary ' 0 ' hasstudentdebt ' 1 ' homemortgageowe ' na ' income ' 100000 ' isethnicminority ' 1 ' maritalstatus ' single never marry ' } schooldegree "" bachelor 's degree "" schoolmajor ' computer science ' studentdebtowe ' 30000 ' usesresource [ ' resourcemdn ' ' resourceso ' ' resourcew3s ' ] } { ' @id ' ' coder_0bf96b5bd925b9ea6ae3975cf8b748f3 ' ' @type ' ' coder ' age ' 22 ' bootcampinfo { ' @id ' ' bootcampinfo_bcb6bed85df8383270e489604f621a7783b1ec1b ' ' @type ' ' bootcampinfo ' attendedbootcamp ' 0 ' bootcampfinish ' na ' bootcamploanyesno ' na ' bootcampname ' na ' bootcamprecommend ' na ' } citypopulation ' less than 100000 ' commutetime ' less than 15 minute ' countrycitizen ' country_canada ' countrylive ' country_canada ' employmentfield ' na ' employmentstatus ' employ for wage ' expectedearne ' 50000 ' financiallysupporting ' na ' firstdevjob ' na ' hourslearning ' 20 ' -PRON- d ' 0bf96b5bd925b9ea6ae3975cf8b748f3 ' jobinfo { ' @id ' ' jobinfo_e8411098c5cb185de7c86e48d3e96e54ec5b6f7b ' ' @type ' ' jobinfo ' issoftwaredev ' 0 ' isunderemploye ' 1 ' jobapplywhen "" i have not decide "" jobinterest [ array ] jobpref ' work for a startup ' jobrelocateyesno ' na ' jobroleinterest ' full - stack web developer ' } languageathome ' english ' moneyforlearning ' na ' monthsprogramme ' 60 ' personalinfo { ' @id ' ' personalinfo_435e037d5f6d2874e12932c960f13222af44f28e ' ' @type ' ' personalinfo ' gender ' male ' haschildren ' na ' hasdebt ' 1 ' hasfinancialdependent ' 0 ' hashomemortgage ' 0 ' hasservedinmilitary ' 0 ' hasstudentdebt ' 1 ' homemortgageowe ' na ' income ' 7500 ' isethnicminority ' 0 ' maritalstatus ' single never marry ' } schooldegree ' some college credit no degree ' schoolmajor ' na ' studentdebtowe ' 1500 ' usesresource [ ' resourceso ' ] } { ' @id ' ' coder_0c126bfa39a884dc8efa907cd71ad351 ' ' @type ' ' coder ' age ' 33 ' bootcampinfo { ' @id ' ' bootcampinfo_bcb6bed85df8383270e489604f621a7783b1ec1b ' ' @type ' ' bootcampinfo ' attendedbootcamp ' 0 ' bootcampfinish ' na ' bootcamploanyesno ' na ' bootcampname ' na ' bootcamprecommend ' na ' } channel [ ' youtubefcc ' ' youtubegoogledev ' ] citypopulation ' more than 1 million ' commutetime ' 30 to 44 minute ' countrycitizen ' country_germany ' countrylive ' country_germany ' employmentfield ' software development and -PRON- ' employmentstatus ' employ for wage ' eventsattende [ ' codeeventhackathon ' ' codeeventmeetup ' ' codeeventworkshop ' ] expectedearne ' 60000 ' financiallysupporting ' 1 ' firstdevjob ' na ' hourslearning ' 5 ' -PRON- d ' 0c126bfa39a884dc8efa907cd71ad351 ' jobinfo { ' @id ' ' jobinfo_736dcd42a9776c1cc477a32e1ec1b8514a913dad ' ' @type ' ' jobinfo ' issoftwaredev ' 0 ' isunderemploye ' 0 ' jobapplywhen ' more than 12 month from now ' jobinter [ array ] jobpref ' work for a medium - sized company ' jobrelocateyesno ' na ' jobroleinterest ' front - end web developer datum engineer back - end web developer quality assurance engineer ' } languageathome ' sindhi ' moneyforlearning ' 2000 ' monthsprogramme ' 6 ' personalinfo { ' @id ' ' personalinfo_946d21b9ea29d1911a082da0757ee3cfa7359dd2 ' ' @type ' ' personalinfo ' gender ' male ' haschildren ' 0 ' hasdebt ' 0 ' hasfinancialdependent ' 1 ' hashomemortgage ' na ' hasservedinmilitary ' 0 ' hasstudentdebt ' na ' homemortgageowe ' na ' income ' 85000 ' isethnicminority ' 1 ' maritalstatus ' married or domestic partnership ' } schooldegree "" bachelor 's degree "" schoolmajor ' computer science ' studentdebtowe ' na ' usesresource [ ' resourcecs ' ' resourcecodewar ' ' resourcefcc ' ' resourcemdn ' ' resourceso ' ] } { ' @id ' ' coder_0c1f5973b546cd698b0012178493d38c ' ' @type ' ' coder ' age ' 20 ' bootcampinfo { ' @id ' ' bootcampinfo_bcb6bed85df8383270e489604f621a7783b1ec1b ' ' @type ' ' bootcampinfo ' attendedbootcamp ' 0 ' bootcampfinish ' na ' bootcamploanyesno ' na ' bootcampname ' na ' bootcamprecommend ' na ' } channel [ ' youtubecodingtut360 ' ' youtubedevtip ' ' youtubeengineeredtruth ' ' youtubefcc ' ' youtubefunfunfunction ' ' youtubelearncode ' ' youtubeleveluptut ' ' youtubemit ' ] citypopulation ' between 100000 and 1 million ' commutetime ' na ' countrycitizen ' country_united%20states%20of%20america ' countrylive ' country_united%20states%20of%20america ' employmentfield ' na ' employmentstatus ' not work but look for work ' expectedearning ' na ' financiallysupporting ' na ' firstdevjob ' na ' hourslearning ' 30 ' -PRON- d ' 0c1f5973b546cd698b0012178493d38c ' jobinfo { ' @id ' ' jobinfo_61311e87e7cf914b8c7b3ccc50a16cfb09a7a228 ' ' @type ' ' jobinfo ' issoftwaredev ' 0 ' isunderemploye ' na ' jobapplywhen "" i have not decide "" jobinterest [ array ] jobpref ' work for a medium - sized company ' jobrelocateyesno ' 1 ' jobroleinterest ' back - end web developer full - stack web developer ' } languageathome ' english ' moneyforlearne ' 15 ' monthsprogramme ' 6 ' personalinfo { ' @id ' ' personalinfo_20a2abee392bd5db73e77cd2961272daa340c6ec ' ' @type ' ' personalinfo ' gender ' female ' haschildren ' na ' hasdebt ' 0 ' hasfinancialdependent ' 0 ' hashomemortgage ' na ' hasservedinmilitary ' 0 ' hasstudentdebt ' na ' homemortgageowe ' na ' income ' na ' isethnicminority ' 0 ' maritalstatus ' single never marry ' } schooldegree ' some college credit no degree ' schoolmajor ' na ' studentdebtowe ' na ' usesresource [ ' resourcecs ' ' resourcecodecademy ' ' resourcefcc ' ' resourcehackerrank ' ' resourceka ' ' resourceso ' ' resourceudacity ' ' resourceudemy ' ' resourcew3s ' ] } ] let -PRON- know if -PRON- need something else",0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,381,2021-07-28T13:16:58Z,NeelParihar,WOQL: Issue while executing a WOQL query having a undefined variable having group_by ,https://github.com/terminusdb/terminusdb/issues/381,"Describe the bug
I was executing a WOQL query having a ""group_by"" function, ""length"" function using an undefined variable inside a ""and"" function using WOQL javascript client. While executing it the terminal just got stuck for 5 to 10 mins after this it gave a error of socket hangup.
To Reproduce
Steps to reproduce the behavior:

Write a query having the same structure as the below query and execute it.

WOQL.select(""v:JobInterest"", ""v:Id"", ""v:TheCount"", ""v:JobRoleInterest_group"")
    .and(
WOQL.group_by(""JobInterest"", [""Id""], ""JobRoleInterest_group"")
             .triple(""v:Coder"", ""Id"", ""v:Id"")
             .triple(""v:Coder"", ""JobInfo"", ""v:JobInfo"")
             .triple(""v:JobInfo"", ""JobInterest"", ""v:JobInterest""),
WOQL.length(""v:JobRoleInterestGroup"",""v:TheCount"")
)

Here ""v:JobRoleInterestGroup"" is the undefined variable.

After executing the query the terminal should have stuck in between.
After some time it throws an Exception having socket hangup.

Expected behavior
After executing the query using the WOQL client it should throw a syntax error or a variable undefined error.
Info:

OS: Mac OS Catalina
How did you run terminus-server (terminus-quickstart, using Docker directly, manual installation) : Using terminus-bootstrap (Dev branch)",2021-08-18T13:02:30Z,GavinMendelGleason,https://github.com/terminusdb/terminusdb/issues/381#issuecomment-901096321,"Ok, the error is still poor, but I can achieve the appropriate query as follows:
WOQL.select(""v:JobInterest"", ""v:TheCount"", ""v:JobRoleInterestGroup"")
    .and(WOQL.group_by(""JobInterest"", ""Id"", ""JobRoleInterestGroup"")
                       .triple(""v:Coder"", ""Id"", ""v:Id"")
                       .triple(""v:Coder"", ""JobInfo"", ""v:JobInfo"")
                       .triple(""v:JobInfo"", ""JobInterest"", ""v:JobInterest""),
            WOQL.length(""v:JobRoleInterestGroup"",""v:TheCount"")
    )","ok the error be still poor but i can achieve the appropriate query as follow woqlselect(""vjobinterest "" "" vthecount "" "" vjobroleinterestgroup "" ) and(woqlgroup_by(""jobinterest "" "" -PRON- d "" "" jobroleinterestgroup "" ) triple(""vcoder "" "" -PRON- d "" "" vid "" ) triple(""vcoder "" "" jobinfo "" "" vjobinfo "" ) triple(""vjobinfo "" "" jobinterest "" "" vjobinter "" ) woqllength(""vjobroleinterestgroup""""vthecount "" ) )",0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,381,2021-07-28T13:16:58Z,NeelParihar,WOQL: Issue while executing a WOQL query having a undefined variable having group_by ,https://github.com/terminusdb/terminusdb/issues/381,"Describe the bug
I was executing a WOQL query having a ""group_by"" function, ""length"" function using an undefined variable inside a ""and"" function using WOQL javascript client. While executing it the terminal just got stuck for 5 to 10 mins after this it gave a error of socket hangup.
To Reproduce
Steps to reproduce the behavior:

Write a query having the same structure as the below query and execute it.

WOQL.select(""v:JobInterest"", ""v:Id"", ""v:TheCount"", ""v:JobRoleInterest_group"")
    .and(
WOQL.group_by(""JobInterest"", [""Id""], ""JobRoleInterest_group"")
             .triple(""v:Coder"", ""Id"", ""v:Id"")
             .triple(""v:Coder"", ""JobInfo"", ""v:JobInfo"")
             .triple(""v:JobInfo"", ""JobInterest"", ""v:JobInterest""),
WOQL.length(""v:JobRoleInterestGroup"",""v:TheCount"")
)

Here ""v:JobRoleInterestGroup"" is the undefined variable.

After executing the query the terminal should have stuck in between.
After some time it throws an Exception having socket hangup.

Expected behavior
After executing the query using the WOQL client it should throw a syntax error or a variable undefined error.
Info:

OS: Mac OS Catalina
How did you run terminus-server (terminus-quickstart, using Docker directly, manual installation) : Using terminus-bootstrap (Dev branch)",2021-08-18T13:03:19Z,GavinMendelGleason,https://github.com/terminusdb/terminusdb/issues/381#issuecomment-901096904,I will close as JobRoleInterestGroup was genuinely unbound...,i will close as jobroleinterestgroup be genuinely unbound,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,382,2021-08-01T00:54:32Z,jryans,Puzzled by empty pages on documentation site,https://github.com/terminusdb/terminusdb/issues/382,"TerminusDB seems like an interesting project, and I'd like to better understand it! 😄
I've run into a few walls on the documentation site at https://terminusdb.github.io/terminusdb/... For example the page at https://terminusdb.github.io/terminusdb/#/Explanation/unification seems to just be empty...?
I was thinking I'd try to fix it if it was a broken link, but then I noticed it seems to be different from the docs folder in this repo, so perhaps it's somewhere else...?
Anyway, happy to help if I can. 🙂",2021-08-23T13:23:17Z,spl,https://github.com/terminusdb/terminusdb/issues/382#issuecomment-903764323,"@jryans Thanks for checking out TerminusDB and letting us know. We are hard at work developing a new release and, along with that, some fantastic new documentation. We'll make sure to have those unification docs filled in. They may have gotten lost in the ether at some point.",@jryan thank for check out terminusdb and let -PRON- know -PRON- be hard at work develop a new release and along with that some fantastic new documentation -PRON- will make sure to have those unification doc fill in -PRON- may have get lose in the ether at some point,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,382,2021-08-01T00:54:32Z,jryans,Puzzled by empty pages on documentation site,https://github.com/terminusdb/terminusdb/issues/382,"TerminusDB seems like an interesting project, and I'd like to better understand it! 😄
I've run into a few walls on the documentation site at https://terminusdb.github.io/terminusdb/... For example the page at https://terminusdb.github.io/terminusdb/#/Explanation/unification seems to just be empty...?
I was thinking I'd try to fix it if it was a broken link, but then I noticed it seems to be different from the docs folder in this repo, so perhaps it's somewhere else...?
Anyway, happy to help if I can. 🙂",2021-08-23T13:41:41Z,spl,https://github.com/terminusdb/terminusdb/issues/382#issuecomment-903780437,See also #341 for the issue tracking the documentation overhaul.,see also # 341 for the issue track the documentation overhaul,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,382,2021-08-01T00:54:32Z,jryans,Puzzled by empty pages on documentation site,https://github.com/terminusdb/terminusdb/issues/382,"TerminusDB seems like an interesting project, and I'd like to better understand it! 😄
I've run into a few walls on the documentation site at https://terminusdb.github.io/terminusdb/... For example the page at https://terminusdb.github.io/terminusdb/#/Explanation/unification seems to just be empty...?
I was thinking I'd try to fix it if it was a broken link, but then I noticed it seems to be different from the docs folder in this repo, so perhaps it's somewhere else...?
Anyway, happy to help if I can. 🙂",2021-08-26T15:50:25Z,GavinMendelGleason,https://github.com/terminusdb/terminusdb/issues/382#issuecomment-906531449,This will be fixed on release.,this will be fix on release,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,383,2021-08-06T07:55:27Z,Francesca-Bit,Document Interface bugs tests report ,https://github.com/terminusdb/terminusdb/issues/383,"Describe the bug
Problem in Insert/Update the database using the document interface
SCHEMA GRAPH PROBLEM WITH ""@documentation""
First test
To Reproduce
Steps to reproduce the behavior:


Add the following json class with the property @documentation in the database schema --ADD DOCUMENT POST CALL--
[{""@type"":""Class"",""@id"":""Doc01"",""@key"":{""@type"":""Random""},""num01"":""xsd:byte"",""@documentation"":{""@properties"":{""num01"":""hello""}}}]


Call The GET method to view the database schema


[{ ""@base"":""terminusdb:///data/"",
  ""@schema"":""terminusdb:///schema#"",
  ""@type"":""@context""
},
{
  ""@id"":""Doc01"",
  ""@key"": {""@type"":""Random""},
  ""@type"":""Class"",
  ""num01"":""xsd:byte""
}
]

PROBLEM : the @documentation does not appear
Second Test
To Reproduce
Steps to reproduce the behavior:


Update the Doc01 using the following json class  --UPDATE DOCUMENT PUT CALL--
[{""@id"":""Doc01"",""@key"":{""@type"":""Random""},""@type"":""Class"",""num01"":""xsd:byte"",""@documentation"":{""@comment"":""Doc description"",""@properties"":{""num01"":""num01 description""}}}]


Call The GET method to view the database schema


[
{
  ""@base"":""terminusdb:///data/"",
  ""@schema"":""terminusdb:///schema#"",
  ""@type"":""@context""
}Access-Control-Allow-Methods: GET, POST, PUT, DELETE, OPTIONS
Access-Control-Allow-Credentials: true
Access-Control-Max-Age: 1728000
Access-Control-Allow-Headers: Authorization, Authorization-Remote, Accept, Accept-Encoding, Accept-Language, Host, Origin, Referer, Content-Type, Content-Length, Content-Range, Content-Disposition, Content-Description
Access-Control-Allow-Origin: http://localhost:3030
Status: 500
Content-type: application/json

{
  ""api:message"":""Error: duplicate_key('terminusdb:///schema#num01')\ncontext(system:dict_pairs/3,_22252)\n"",
  ""api:status"":""api:server_error""
}

PROBLEM the database schema is broken",2021-08-10T19:45:08Z,spl,https://github.com/terminusdb/terminusdb/issues/383#issuecomment-896269403,"To reproduce the first issue (using xh):
$ ./terminusdb db create asdf -l ""asdf"" -c ""asdf"" --schema true --prefixes '{""@base"":""http://blah/base"",""@schema"":""http://blah/schema""}'

$ xh 'http://localhost:6363/api/document/admin/asdf?author=test&message=test&graph_type=schema' << EOF
> {""@type"":""Class"",""@id"":""Doc01"",""@key"":{""@type"":""Random""},""num01"":""xsd:byte"",""@documentation"":{""@properties"":{""num01"":""hello""}}}
> EOF
HTTP/1.1 200 OK
connection: keep-alive
content-length: 11
content-type: application/json; charset=UTF-8
date: Tue, 10 Aug 2021 19:29:59 GMT

[
    ""Doc01""
]

$ xh 'http://localhost:6363/api/document/admin/asdf?author=test&message=test&graph_type=schema'
HTTP/1.1 200 OK
connection: keep-alive
content-length: 244
content-type: application/json; stream=true; charset=UTF-8
date: Tue, 10 Aug 2021 19:30:29 GMT

{
  ""@base"":""http://blah/base"",
  ""@schema"":""http://blah/schema"",
  ""@type"":""@context"",
  ""doc"":""terminusdb:///data/"",
  ""scm"":""terminusdb:///schema#""
}
{
  ""@id"":""Doc01"",
  ""@key"": {""@type"":""Random""},
  ""@type"":""Class"",
  ""num01"":""xsd:byte""
}","to reproduce the first issue ( use xh ) $ /terminusdb db create asdf -l "" asdf "" -c "" asdf "" --schema true --prefixe ' { "" @base""""http//blah / base""""@schema""""http//blah / schema "" } ' $ xh ' http//localhost6363 / api / document / admin / asdfauthor = test&message = test&graph_type = schema ' < < eof > { "" @type""""class""""@id""""doc01""""@key""{""@type""""random""}""num01""""xsdbyte""""@documentation""{""@properties""{""num01""""hello "" } } } > eof http/11 200 ok connection keep - alive content - length 11 content - type application / json charset = utf-8 date tue 10 aug 2021 192959 gmt [ "" doc01 "" ] $ xh ' http//localhost6363 / api / document / admin / asdfauthor = test&message = test&graph_type = schema ' http/11 200 ok connection keep - alive content - length 244 content - type application / json stream = true charset = utf-8 date tue 10 aug 2021 193029 gmt { "" @base""""http//blah / base "" "" @schema""""http//blah / schema "" "" @type""""@context "" "" doc""""terminusdb///data/ "" "" scm""""terminusdb///schema # "" } { "" @id""""doc01 "" "" @key "" { "" @type""""random "" } "" @type""""class "" "" num01""""xsdbyte "" }",0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,383,2021-08-06T07:55:27Z,Francesca-Bit,Document Interface bugs tests report ,https://github.com/terminusdb/terminusdb/issues/383,"Describe the bug
Problem in Insert/Update the database using the document interface
SCHEMA GRAPH PROBLEM WITH ""@documentation""
First test
To Reproduce
Steps to reproduce the behavior:


Add the following json class with the property @documentation in the database schema --ADD DOCUMENT POST CALL--
[{""@type"":""Class"",""@id"":""Doc01"",""@key"":{""@type"":""Random""},""num01"":""xsd:byte"",""@documentation"":{""@properties"":{""num01"":""hello""}}}]


Call The GET method to view the database schema


[{ ""@base"":""terminusdb:///data/"",
  ""@schema"":""terminusdb:///schema#"",
  ""@type"":""@context""
},
{
  ""@id"":""Doc01"",
  ""@key"": {""@type"":""Random""},
  ""@type"":""Class"",
  ""num01"":""xsd:byte""
}
]

PROBLEM : the @documentation does not appear
Second Test
To Reproduce
Steps to reproduce the behavior:


Update the Doc01 using the following json class  --UPDATE DOCUMENT PUT CALL--
[{""@id"":""Doc01"",""@key"":{""@type"":""Random""},""@type"":""Class"",""num01"":""xsd:byte"",""@documentation"":{""@comment"":""Doc description"",""@properties"":{""num01"":""num01 description""}}}]


Call The GET method to view the database schema


[
{
  ""@base"":""terminusdb:///data/"",
  ""@schema"":""terminusdb:///schema#"",
  ""@type"":""@context""
}Access-Control-Allow-Methods: GET, POST, PUT, DELETE, OPTIONS
Access-Control-Allow-Credentials: true
Access-Control-Max-Age: 1728000
Access-Control-Allow-Headers: Authorization, Authorization-Remote, Accept, Accept-Encoding, Accept-Language, Host, Origin, Referer, Content-Type, Content-Length, Content-Range, Content-Disposition, Content-Description
Access-Control-Allow-Origin: http://localhost:3030
Status: 500
Content-type: application/json

{
  ""api:message"":""Error: duplicate_key('terminusdb:///schema#num01')\ncontext(system:dict_pairs/3,_22252)\n"",
  ""api:status"":""api:server_error""
}

PROBLEM the database schema is broken",2021-08-10T19:46:30Z,spl,https://github.com/terminusdb/terminusdb/issues/383#issuecomment-896270367,"Is it somewhere here?
id_schema_json(DB, Prefixes, Id, JSON) :-
    database_schema(DB,Schema),
    (   ground(Id)
    ->  prefix_expand_schema(Id, Prefixes, Id_Ex)
    ;   Id = Id_Ex
    ),

    xrdf(Schema, Id_Ex, rdf:type, Class),

    findall(
        K-V,
        (   distinct([P],xrdf(Schema,Id_Ex,P,O)),
            schema_subject_predicate_object_key_value(DB,Prefixes,Id_Ex,P,O,K,V)
        ),   
        Data),
    !,   
    compress_schema_uri(Id_Ex, Prefixes, Id_Compressed),
    compress_schema_uri(Class, Prefixes, Class_Compressed),
    (   atom_concat('sys:',Small_Class, Class_Compressed)
    ->  true 
    ;   Small_Class = Class_Compressed),
    dict_create(JSON,json,['@id'-Id_Compressed,
                           '@type'-Small_Class
                           |Data]).",be -PRON- somewhere here id_schema_json(db prefix -PRON- d json ) - database_schema(dbschema ) ( ground(id ) - > prefix_expand_schema(id prefix id_ex ) -PRON- d = id_ex ) xrdf(schema id_ex rdftype class ) findall ( k - v ( distinct([p]xrdf(schemaid_expo ) ) schema_subject_predicate_object_key_value(dbprefixesid_expokv ) ) data ) compress_schema_uri(id_ex prefix id_compresse ) compress_schema_uri(class prefix class_compresse ) ( atom_concat('sys'small_class class_compressed ) - > true small_class = class_compressed ) dict_create(jsonjson['@id'-id_compresse ' @type'-small_clas |data ] ),0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,383,2021-08-06T07:55:27Z,Francesca-Bit,Document Interface bugs tests report ,https://github.com/terminusdb/terminusdb/issues/383,"Describe the bug
Problem in Insert/Update the database using the document interface
SCHEMA GRAPH PROBLEM WITH ""@documentation""
First test
To Reproduce
Steps to reproduce the behavior:


Add the following json class with the property @documentation in the database schema --ADD DOCUMENT POST CALL--
[{""@type"":""Class"",""@id"":""Doc01"",""@key"":{""@type"":""Random""},""num01"":""xsd:byte"",""@documentation"":{""@properties"":{""num01"":""hello""}}}]


Call The GET method to view the database schema


[{ ""@base"":""terminusdb:///data/"",
  ""@schema"":""terminusdb:///schema#"",
  ""@type"":""@context""
},
{
  ""@id"":""Doc01"",
  ""@key"": {""@type"":""Random""},
  ""@type"":""Class"",
  ""num01"":""xsd:byte""
}
]

PROBLEM : the @documentation does not appear
Second Test
To Reproduce
Steps to reproduce the behavior:


Update the Doc01 using the following json class  --UPDATE DOCUMENT PUT CALL--
[{""@id"":""Doc01"",""@key"":{""@type"":""Random""},""@type"":""Class"",""num01"":""xsd:byte"",""@documentation"":{""@comment"":""Doc description"",""@properties"":{""num01"":""num01 description""}}}]


Call The GET method to view the database schema


[
{
  ""@base"":""terminusdb:///data/"",
  ""@schema"":""terminusdb:///schema#"",
  ""@type"":""@context""
}Access-Control-Allow-Methods: GET, POST, PUT, DELETE, OPTIONS
Access-Control-Allow-Credentials: true
Access-Control-Max-Age: 1728000
Access-Control-Allow-Headers: Authorization, Authorization-Remote, Accept, Accept-Encoding, Accept-Language, Host, Origin, Referer, Content-Type, Content-Length, Content-Range, Content-Disposition, Content-Description
Access-Control-Allow-Origin: http://localhost:3030
Status: 500
Content-type: application/json

{
  ""api:message"":""Error: duplicate_key('terminusdb:///schema#num01')\ncontext(system:dict_pairs/3,_22252)\n"",
  ""api:status"":""api:server_error""
}

PROBLEM the database schema is broken",2021-08-11T08:16:08Z,spl,https://github.com/terminusdb/terminusdb/issues/383#issuecomment-896609696,First one is fixed in 4be8175.,first one be fix in 4be8175,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,383,2021-08-06T07:55:27Z,Francesca-Bit,Document Interface bugs tests report ,https://github.com/terminusdb/terminusdb/issues/383,"Describe the bug
Problem in Insert/Update the database using the document interface
SCHEMA GRAPH PROBLEM WITH ""@documentation""
First test
To Reproduce
Steps to reproduce the behavior:


Add the following json class with the property @documentation in the database schema --ADD DOCUMENT POST CALL--
[{""@type"":""Class"",""@id"":""Doc01"",""@key"":{""@type"":""Random""},""num01"":""xsd:byte"",""@documentation"":{""@properties"":{""num01"":""hello""}}}]


Call The GET method to view the database schema


[{ ""@base"":""terminusdb:///data/"",
  ""@schema"":""terminusdb:///schema#"",
  ""@type"":""@context""
},
{
  ""@id"":""Doc01"",
  ""@key"": {""@type"":""Random""},
  ""@type"":""Class"",
  ""num01"":""xsd:byte""
}
]

PROBLEM : the @documentation does not appear
Second Test
To Reproduce
Steps to reproduce the behavior:


Update the Doc01 using the following json class  --UPDATE DOCUMENT PUT CALL--
[{""@id"":""Doc01"",""@key"":{""@type"":""Random""},""@type"":""Class"",""num01"":""xsd:byte"",""@documentation"":{""@comment"":""Doc description"",""@properties"":{""num01"":""num01 description""}}}]


Call The GET method to view the database schema


[
{
  ""@base"":""terminusdb:///data/"",
  ""@schema"":""terminusdb:///schema#"",
  ""@type"":""@context""
}Access-Control-Allow-Methods: GET, POST, PUT, DELETE, OPTIONS
Access-Control-Allow-Credentials: true
Access-Control-Max-Age: 1728000
Access-Control-Allow-Headers: Authorization, Authorization-Remote, Accept, Accept-Encoding, Accept-Language, Host, Origin, Referer, Content-Type, Content-Length, Content-Range, Content-Disposition, Content-Description
Access-Control-Allow-Origin: http://localhost:3030
Status: 500
Content-type: application/json

{
  ""api:message"":""Error: duplicate_key('terminusdb:///schema#num01')\ncontext(system:dict_pairs/3,_22252)\n"",
  ""api:status"":""api:server_error""
}

PROBLEM the database schema is broken",2021-08-11T08:19:37Z,spl,https://github.com/terminusdb/terminusdb/issues/383#issuecomment-896612107,"@Francesca-Bit To elaborate, we now throw an error if no @comment is included in the @documentation.",@francesca - bit to elaborate -PRON- now throw an error if no @comment be include in the @documentation,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,383,2021-08-06T07:55:27Z,Francesca-Bit,Document Interface bugs tests report ,https://github.com/terminusdb/terminusdb/issues/383,"Describe the bug
Problem in Insert/Update the database using the document interface
SCHEMA GRAPH PROBLEM WITH ""@documentation""
First test
To Reproduce
Steps to reproduce the behavior:


Add the following json class with the property @documentation in the database schema --ADD DOCUMENT POST CALL--
[{""@type"":""Class"",""@id"":""Doc01"",""@key"":{""@type"":""Random""},""num01"":""xsd:byte"",""@documentation"":{""@properties"":{""num01"":""hello""}}}]


Call The GET method to view the database schema


[{ ""@base"":""terminusdb:///data/"",
  ""@schema"":""terminusdb:///schema#"",
  ""@type"":""@context""
},
{
  ""@id"":""Doc01"",
  ""@key"": {""@type"":""Random""},
  ""@type"":""Class"",
  ""num01"":""xsd:byte""
}
]

PROBLEM : the @documentation does not appear
Second Test
To Reproduce
Steps to reproduce the behavior:


Update the Doc01 using the following json class  --UPDATE DOCUMENT PUT CALL--
[{""@id"":""Doc01"",""@key"":{""@type"":""Random""},""@type"":""Class"",""num01"":""xsd:byte"",""@documentation"":{""@comment"":""Doc description"",""@properties"":{""num01"":""num01 description""}}}]


Call The GET method to view the database schema


[
{
  ""@base"":""terminusdb:///data/"",
  ""@schema"":""terminusdb:///schema#"",
  ""@type"":""@context""
}Access-Control-Allow-Methods: GET, POST, PUT, DELETE, OPTIONS
Access-Control-Allow-Credentials: true
Access-Control-Max-Age: 1728000
Access-Control-Allow-Headers: Authorization, Authorization-Remote, Accept, Accept-Encoding, Accept-Language, Host, Origin, Referer, Content-Type, Content-Length, Content-Range, Content-Disposition, Content-Description
Access-Control-Allow-Origin: http://localhost:3030
Status: 500
Content-type: application/json

{
  ""api:message"":""Error: duplicate_key('terminusdb:///schema#num01')\ncontext(system:dict_pairs/3,_22252)\n"",
  ""api:status"":""api:server_error""
}

PROBLEM the database schema is broken",2021-08-11T08:42:31Z,spl,https://github.com/terminusdb/terminusdb/issues/383#issuecomment-896627053,"To reproduce the second issue:
$ ./terminusdb db create testdb --label label --comment comment --prefixes '{""@base"":""http://base/"",""@schema"":""http://schema/""}'

$ echo '{""@type"":""Class"",""@id"":""Doc01"",""@key"":{""@type"":""Random""},""num01"":""xsd:byte"",""@documentation"":{""@properties"":{""num01"":""hello""},""@comment"":""comment""}}' | \
xh 'http://localhost:6363/api/document/admin/testdb?author=author&message=message&graph_type=schema'
HTTP/1.1 200 OK
connection: keep-alive
content-length: 11
content-type: application/json; charset=UTF-8
date: Wed, 11 Aug 2021 08:28:39 GMT

[
    ""Doc01""
]

$ echo '{""@type"":""Class"",""@id"":""Doc01"",""@key"":{""@type"":""Random""},""num01"":""xsd:byte"",""@documentation"":{""@properties"":{""num01"":""hello""},""@comment"":""comment""}}' | \
xh 'http://localhost:6363/api/document/admin/testdb?author=author&message=message&graph_type=schema'

$ xh 'http://localhost:6363/api/document/admin/testdb?author=author&message=message&graph_type=schema'
HTTP/1.1 200 OK
connection: keep-alive
content-length: 340
content-type: application/json; stream=true; charset=UTF-8
date: Wed, 11 Aug 2021 08:30:38 GMT

{
  ""@base"":""http://base/"",
  ""@schema"":""http://schema/"",
  ""@type"":""@context"",
  ""doc"":""terminusdb:///data/"",
  ""scm"":""terminusdb:///schema#""
}
Status: 500
Content-type: application/json

{
  ""api:message"":""Error: duplicate_key('http://blah/schemanum01')\ncontext(system:dict_pairs/3,_11496)\n"",
  ""api:status"":""api:server_error""
}","to reproduce the second issue $ /terminusdb db create testdb --label label --comment comment --prefixe ' { "" @base""""http//base/""""@schema""""http//schema/ "" } ' $ echo ' { "" @type""""class""""@id""""doc01""""@key""{""@type""""random""}""num01""""xsdbyte""""@documentation""{""@properties""{""num01""""hello""}""@comment""""comment "" } } ' | \ xh ' http//localhost6363 / api / document / admin / testdbauthor = author&message = message&graph_type = schema ' http/11 200 ok connection keep - alive content - length 11 content - type application / json charset = utf-8 date -PRON- d 11 aug 2021 082839 gmt [ "" doc01 "" ] $ echo ' { "" @type""""class""""@id""""doc01""""@key""{""@type""""random""}""num01""""xsdbyte""""@documentation""{""@properties""{""num01""""hello""}""@comment""""comment "" } } ' | \ xh ' http//localhost6363 / api / document / admin / testdbauthor = author&message = message&graph_type = schema ' $ xh ' http//localhost6363 / api / document / admin / testdbauthor = author&message = message&graph_type = schema ' http/11 200 ok connection keep - alive content - length 340 content - type application / json stream = true charset = utf-8 date -PRON- d 11 aug 2021 083038 gmt { "" @base""""http//base/ "" "" @schema""""http//schema/ "" "" @type""""@context "" "" doc""""terminusdb///data/ "" "" scm""""terminusdb///schema # "" } status 500 content - type application / json { "" apimessage""""error duplicate_key('http//blah / schemanum01')\ncontext(systemdict_pairs/3_11496)\n "" "" apistatus""""apiserver_error "" }",0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,383,2021-08-06T07:55:27Z,Francesca-Bit,Document Interface bugs tests report ,https://github.com/terminusdb/terminusdb/issues/383,"Describe the bug
Problem in Insert/Update the database using the document interface
SCHEMA GRAPH PROBLEM WITH ""@documentation""
First test
To Reproduce
Steps to reproduce the behavior:


Add the following json class with the property @documentation in the database schema --ADD DOCUMENT POST CALL--
[{""@type"":""Class"",""@id"":""Doc01"",""@key"":{""@type"":""Random""},""num01"":""xsd:byte"",""@documentation"":{""@properties"":{""num01"":""hello""}}}]


Call The GET method to view the database schema


[{ ""@base"":""terminusdb:///data/"",
  ""@schema"":""terminusdb:///schema#"",
  ""@type"":""@context""
},
{
  ""@id"":""Doc01"",
  ""@key"": {""@type"":""Random""},
  ""@type"":""Class"",
  ""num01"":""xsd:byte""
}
]

PROBLEM : the @documentation does not appear
Second Test
To Reproduce
Steps to reproduce the behavior:


Update the Doc01 using the following json class  --UPDATE DOCUMENT PUT CALL--
[{""@id"":""Doc01"",""@key"":{""@type"":""Random""},""@type"":""Class"",""num01"":""xsd:byte"",""@documentation"":{""@comment"":""Doc description"",""@properties"":{""num01"":""num01 description""}}}]


Call The GET method to view the database schema


[
{
  ""@base"":""terminusdb:///data/"",
  ""@schema"":""terminusdb:///schema#"",
  ""@type"":""@context""
}Access-Control-Allow-Methods: GET, POST, PUT, DELETE, OPTIONS
Access-Control-Allow-Credentials: true
Access-Control-Max-Age: 1728000
Access-Control-Allow-Headers: Authorization, Authorization-Remote, Accept, Accept-Encoding, Accept-Language, Host, Origin, Referer, Content-Type, Content-Length, Content-Range, Content-Disposition, Content-Description
Access-Control-Allow-Origin: http://localhost:3030
Status: 500
Content-type: application/json

{
  ""api:message"":""Error: duplicate_key('terminusdb:///schema#num01')\ncontext(system:dict_pairs/3,_22252)\n"",
  ""api:status"":""api:server_error""
}

PROBLEM the database schema is broken",2021-08-11T08:44:21Z,spl,https://github.com/terminusdb/terminusdb/issues/383#issuecomment-896628197,"Two problems:

We have duplicate keys, which should not happen.
We're returning a server error (status 500) with HTTP headers in the middle of a JSON stream response, which should not happen.",two problem -PRON- have duplicate key which should not happen -PRON- be return a server error ( status 500 ) with http header in the middle of a json stream response which should not happen,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,383,2021-08-06T07:55:27Z,Francesca-Bit,Document Interface bugs tests report ,https://github.com/terminusdb/terminusdb/issues/383,"Describe the bug
Problem in Insert/Update the database using the document interface
SCHEMA GRAPH PROBLEM WITH ""@documentation""
First test
To Reproduce
Steps to reproduce the behavior:


Add the following json class with the property @documentation in the database schema --ADD DOCUMENT POST CALL--
[{""@type"":""Class"",""@id"":""Doc01"",""@key"":{""@type"":""Random""},""num01"":""xsd:byte"",""@documentation"":{""@properties"":{""num01"":""hello""}}}]


Call The GET method to view the database schema


[{ ""@base"":""terminusdb:///data/"",
  ""@schema"":""terminusdb:///schema#"",
  ""@type"":""@context""
},
{
  ""@id"":""Doc01"",
  ""@key"": {""@type"":""Random""},
  ""@type"":""Class"",
  ""num01"":""xsd:byte""
}
]

PROBLEM : the @documentation does not appear
Second Test
To Reproduce
Steps to reproduce the behavior:


Update the Doc01 using the following json class  --UPDATE DOCUMENT PUT CALL--
[{""@id"":""Doc01"",""@key"":{""@type"":""Random""},""@type"":""Class"",""num01"":""xsd:byte"",""@documentation"":{""@comment"":""Doc description"",""@properties"":{""num01"":""num01 description""}}}]


Call The GET method to view the database schema


[
{
  ""@base"":""terminusdb:///data/"",
  ""@schema"":""terminusdb:///schema#"",
  ""@type"":""@context""
}Access-Control-Allow-Methods: GET, POST, PUT, DELETE, OPTIONS
Access-Control-Allow-Credentials: true
Access-Control-Max-Age: 1728000
Access-Control-Allow-Headers: Authorization, Authorization-Remote, Accept, Accept-Encoding, Accept-Language, Host, Origin, Referer, Content-Type, Content-Length, Content-Range, Content-Disposition, Content-Description
Access-Control-Allow-Origin: http://localhost:3030
Status: 500
Content-type: application/json

{
  ""api:message"":""Error: duplicate_key('terminusdb:///schema#num01')\ncontext(system:dict_pairs/3,_22252)\n"",
  ""api:status"":""api:server_error""
}

PROBLEM the database schema is broken",2021-08-11T12:35:36Z,spl,https://github.com/terminusdb/terminusdb/issues/383#issuecomment-896789153,"This appears to be fixed as of f883376.
Since this error is due to a bug, we plan to not have any future server errors in the middle of a JSON stream, because we will not have any bugs!",this appear to be fix as of f883376 since this error be due to a bug -PRON- plan to not have any future server error in the middle of a json stream because -PRON- will not have any bug,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,383,2021-08-06T07:55:27Z,Francesca-Bit,Document Interface bugs tests report ,https://github.com/terminusdb/terminusdb/issues/383,"Describe the bug
Problem in Insert/Update the database using the document interface
SCHEMA GRAPH PROBLEM WITH ""@documentation""
First test
To Reproduce
Steps to reproduce the behavior:


Add the following json class with the property @documentation in the database schema --ADD DOCUMENT POST CALL--
[{""@type"":""Class"",""@id"":""Doc01"",""@key"":{""@type"":""Random""},""num01"":""xsd:byte"",""@documentation"":{""@properties"":{""num01"":""hello""}}}]


Call The GET method to view the database schema


[{ ""@base"":""terminusdb:///data/"",
  ""@schema"":""terminusdb:///schema#"",
  ""@type"":""@context""
},
{
  ""@id"":""Doc01"",
  ""@key"": {""@type"":""Random""},
  ""@type"":""Class"",
  ""num01"":""xsd:byte""
}
]

PROBLEM : the @documentation does not appear
Second Test
To Reproduce
Steps to reproduce the behavior:


Update the Doc01 using the following json class  --UPDATE DOCUMENT PUT CALL--
[{""@id"":""Doc01"",""@key"":{""@type"":""Random""},""@type"":""Class"",""num01"":""xsd:byte"",""@documentation"":{""@comment"":""Doc description"",""@properties"":{""num01"":""num01 description""}}}]


Call The GET method to view the database schema


[
{
  ""@base"":""terminusdb:///data/"",
  ""@schema"":""terminusdb:///schema#"",
  ""@type"":""@context""
}Access-Control-Allow-Methods: GET, POST, PUT, DELETE, OPTIONS
Access-Control-Allow-Credentials: true
Access-Control-Max-Age: 1728000
Access-Control-Allow-Headers: Authorization, Authorization-Remote, Accept, Accept-Encoding, Accept-Language, Host, Origin, Referer, Content-Type, Content-Length, Content-Range, Content-Disposition, Content-Description
Access-Control-Allow-Origin: http://localhost:3030
Status: 500
Content-type: application/json

{
  ""api:message"":""Error: duplicate_key('terminusdb:///schema#num01')\ncontext(system:dict_pairs/3,_22252)\n"",
  ""api:status"":""api:server_error""
}

PROBLEM the database schema is broken",2021-08-11T12:37:02Z,spl,https://github.com/terminusdb/terminusdb/issues/383#issuecomment-896790065,"This is what the second issue response now looks like:
$ xh 'http://localhost:6363/api/document/admin/testdb?author=author&message=message&graph_type=schema'
HTTP/1.1 200 OK
connection: keep-alive
content-length: 314
content-type: application/json; stream=true; charset=UTF-8
date: Wed, 11 Aug 2021 12:34:38 GMT

{
  ""@base"":""http://base/"",
  ""@schema"":""http://schema/"",
  ""@type"":""@context"",
  ""doc"":""terminusdb:///data/"",
  ""scm"":""terminusdb:///schema#""
}
{
  ""@documentation"": {""@comment"":""comment"", ""@properties"": {""num01"":""hello""}},
  ""@id"":""Doc01"",
  ""@key"": {""@type"":""Random""},
  ""@type"":""Class"",
  ""num01"":""xsd:byte""
}","this be what the second issue response now look like $ xh ' http//localhost6363 / api / document / admin / testdbauthor = author&message = message&graph_type = schema ' http/11 200 ok connection keep - alive content - length 314 content - type application / json stream = true charset = utf-8 date -PRON- d 11 aug 2021 123438 gmt { "" @base""""http//base/ "" "" @schema""""http//schema/ "" "" @type""""@context "" "" doc""""terminusdb///data/ "" "" scm""""terminusdb///schema # "" } { "" @documentation "" { "" @comment""""comment "" "" @propertie "" { "" num01""""hello "" } } "" @id""""doc01 "" "" @key "" { "" @type""""random "" } "" @type""""class "" "" num01""""xsdbyte "" }",0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,383,2021-08-06T07:55:27Z,Francesca-Bit,Document Interface bugs tests report ,https://github.com/terminusdb/terminusdb/issues/383,"Describe the bug
Problem in Insert/Update the database using the document interface
SCHEMA GRAPH PROBLEM WITH ""@documentation""
First test
To Reproduce
Steps to reproduce the behavior:


Add the following json class with the property @documentation in the database schema --ADD DOCUMENT POST CALL--
[{""@type"":""Class"",""@id"":""Doc01"",""@key"":{""@type"":""Random""},""num01"":""xsd:byte"",""@documentation"":{""@properties"":{""num01"":""hello""}}}]


Call The GET method to view the database schema


[{ ""@base"":""terminusdb:///data/"",
  ""@schema"":""terminusdb:///schema#"",
  ""@type"":""@context""
},
{
  ""@id"":""Doc01"",
  ""@key"": {""@type"":""Random""},
  ""@type"":""Class"",
  ""num01"":""xsd:byte""
}
]

PROBLEM : the @documentation does not appear
Second Test
To Reproduce
Steps to reproduce the behavior:


Update the Doc01 using the following json class  --UPDATE DOCUMENT PUT CALL--
[{""@id"":""Doc01"",""@key"":{""@type"":""Random""},""@type"":""Class"",""num01"":""xsd:byte"",""@documentation"":{""@comment"":""Doc description"",""@properties"":{""num01"":""num01 description""}}}]


Call The GET method to view the database schema


[
{
  ""@base"":""terminusdb:///data/"",
  ""@schema"":""terminusdb:///schema#"",
  ""@type"":""@context""
}Access-Control-Allow-Methods: GET, POST, PUT, DELETE, OPTIONS
Access-Control-Allow-Credentials: true
Access-Control-Max-Age: 1728000
Access-Control-Allow-Headers: Authorization, Authorization-Remote, Accept, Accept-Encoding, Accept-Language, Host, Origin, Referer, Content-Type, Content-Length, Content-Range, Content-Disposition, Content-Description
Access-Control-Allow-Origin: http://localhost:3030
Status: 500
Content-type: application/json

{
  ""api:message"":""Error: duplicate_key('terminusdb:///schema#num01')\ncontext(system:dict_pairs/3,_22252)\n"",
  ""api:status"":""api:server_error""
}

PROBLEM the database schema is broken",2021-09-01T13:24:02Z,Francesca-Bit,https://github.com/terminusdb/terminusdb/issues/383#issuecomment-910282689,"using Terminusdb branch dev (server)
If I try to update (PUT call) a schema graph class with a @documentation field
Here the step to reproduce it :

create a database
add a new document

{
  ""@type"": ""Class"",
  ""@id"": ""Doc001"",
  ""@key"": {
    ""@type"": ""Random""
  },
  ""@documentation"": {
    ""@comment"": ""comment 01""
  }
}


update the new document (change the @documentation value)

{
  ""@type"": ""Class"",
  ""@id"": ""Doc001"",
  ""@key"": {
    ""@type"": ""Random""
  },
  ""@documentation"": {
    ""@comment"": ""comment 01 02""
  }
}


the update call is ok (I got back the [Doc001])
The error happen when you try to get the schema back (GET CALL)

http://127.0.0.1:6363/api/document/admin/mytest/local/branch/main?as_list=true&graph_type=schema
if you try to get the database schema, you get the following error instead of the json schema
[
{
  ""@base"":""terminusdb:///data/"",
  ""@schema"":""terminusdb:///schema#"",
  ""@type"":""@context""
}Access-Control-Allow-Methods: GET, POST, PUT, DELETE, OPTIONS
Access-Control-Allow-Credentials: true
Access-Control-Max-Age: 1728000
Access-Control-Allow-Headers: Authorization, Authorization-Remote, Accept, Accept-Encoding, Accept-Language, Host, Origin, Referer, Content-Type, Content-Length, Content-Range, Content-Disposition, Content-Description
Access-Control-Allow-Origin: http://localhost:3030
Status: 500
Content-type: application/json

{
  ""api:message"":""Error: duplicate_key('@documentation')\n  [36] dict_create(_6448,json,['@id'-'Doc002',...|...])\n  [33] api_document:api_generate_documents_(schema,transaction_object{commit_info:commit_info{},descriptor:branch_descriptor{branch_name:\""main\"",repository_descriptor: ...},inference_objects:[],instance_objects:[...],parent:transaction_object{descriptor: ...,inference_objects:[],instance_objects: ...,parent: ...,schema_objects: ...},schema_objects:[...]},true,true,0,unlimited,_6518) at /app/terminusdb/src/core/api/api_document.pl:77\n  [31] '$apply':forall('<garbage_collected>',routes:json_write_with_header(...,_6648,...,true,[])) at /usr/lib/swipl/boot/apply.pl:52\n  [30] '<meta-call>'('<garbage_collected>') <foreign>\n  [29] catch(routes:(...,...),error(duplicate_key('@documentation'),context(...,_6750)),routes:do_or_die(...,...)) at /usr/lib/swipl/boot/init.pl:532\n  [28] catch_with_backtrace('<garbage_collected>','<garbage_collected>','<garbage_collected>') at /usr/lib/swipl/boot/init.pl:582\n\nNote: some frames are missing due to last-call optimization.\nRe-run your program in debug mode (:- debug.) to get more detail.\n\n"",
  ""api:status"":""api:server_error""
}```","use terminusdb branch dev ( server ) if i try to update ( put call ) a schema graph class with a @documentation field here the step to reproduce -PRON- create a database add a new document { "" @type "" "" class "" "" @id "" "" doc001 "" "" @key "" { "" @type "" "" random "" } "" @documentation "" { "" @comment "" "" comment 01 "" } } update the new document ( change the @documentation value ) { "" @type "" "" class "" "" @id "" "" doc001 "" "" @key "" { "" @type "" "" random "" } "" @documentation "" { "" @comment "" "" comment 01 02 "" } } the update call be ok ( i get back the [ doc001 ] ) the error happen when -PRON- try to get the schema back ( get call ) http//1270016363 / api / document / admin / mytest / local / branch / mainas_list = true&graph_type = schema if -PRON- try to get the database schema -PRON- get the follow error instead of the json schema [ { "" @base""""terminusdb///data/ "" "" @schema""""terminusdb///schema # "" "" @type""""@context "" } access - control - allow - method get post put delete option access - control - allow - credential true access - control - max - age 1728000 access - control - allow - header authorization authorization - remote accept accept - encode accept - language host origin referer content - type content - length content - range content - disposition content - description access - control - allow - origin http//localhost3030 status 500 content - type application / json { "" apimessage""""error duplicate_key('@documentation')\n [ 36 ] dict_create(_6448json['@id'-'doc002'|])\n [ 33 ] api_documentapi_generate_documents_(schematransaction_object{commit_infocommit_info{}descriptorbranch_descriptor{branch_name\""main\""repository_descriptor } inference_objects[]instance_objects[]parenttransaction_object{descriptor inference_objects[]instance_objects parent schema_object } schema_objects[]}truetrue0unlimited_6518 ) at /app / terminusdb / src / core / api / api_documentpl77\n [ 31 ] ' $ apply'forall('<garbage_collected>'routesjson_write_with_header(_6648true [ ] ) ) at /usr / lib / swipl / boot / applypl52\n [ 30 ] ' < meta - call>'('<garbage_collecte > ' ) < foreign>\n [ 29 ] catch(routes()error(duplicate_key('@documentation')context(_6750))routesdo_or_die ( ) ) at /usr / lib / swipl / boot / initpl532\n [ 28 ] catch_with_backtrace('<garbage_collected>''<garbage_collected>''<garbage_collecte > ' ) at /usr / lib / swipl / boot / initpl582\n\nnote some frame be miss due to last - call optimization\nre - run -PRON- program in debug mode ( - debug ) to get more detail\n\n "" "" apistatus""""apiserver_error "" } ` ` `",0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,383,2021-08-06T07:55:27Z,Francesca-Bit,Document Interface bugs tests report ,https://github.com/terminusdb/terminusdb/issues/383,"Describe the bug
Problem in Insert/Update the database using the document interface
SCHEMA GRAPH PROBLEM WITH ""@documentation""
First test
To Reproduce
Steps to reproduce the behavior:


Add the following json class with the property @documentation in the database schema --ADD DOCUMENT POST CALL--
[{""@type"":""Class"",""@id"":""Doc01"",""@key"":{""@type"":""Random""},""num01"":""xsd:byte"",""@documentation"":{""@properties"":{""num01"":""hello""}}}]


Call The GET method to view the database schema


[{ ""@base"":""terminusdb:///data/"",
  ""@schema"":""terminusdb:///schema#"",
  ""@type"":""@context""
},
{
  ""@id"":""Doc01"",
  ""@key"": {""@type"":""Random""},
  ""@type"":""Class"",
  ""num01"":""xsd:byte""
}
]

PROBLEM : the @documentation does not appear
Second Test
To Reproduce
Steps to reproduce the behavior:


Update the Doc01 using the following json class  --UPDATE DOCUMENT PUT CALL--
[{""@id"":""Doc01"",""@key"":{""@type"":""Random""},""@type"":""Class"",""num01"":""xsd:byte"",""@documentation"":{""@comment"":""Doc description"",""@properties"":{""num01"":""num01 description""}}}]


Call The GET method to view the database schema


[
{
  ""@base"":""terminusdb:///data/"",
  ""@schema"":""terminusdb:///schema#"",
  ""@type"":""@context""
}Access-Control-Allow-Methods: GET, POST, PUT, DELETE, OPTIONS
Access-Control-Allow-Credentials: true
Access-Control-Max-Age: 1728000
Access-Control-Allow-Headers: Authorization, Authorization-Remote, Accept, Accept-Encoding, Accept-Language, Host, Origin, Referer, Content-Type, Content-Length, Content-Range, Content-Disposition, Content-Description
Access-Control-Allow-Origin: http://localhost:3030
Status: 500
Content-type: application/json

{
  ""api:message"":""Error: duplicate_key('terminusdb:///schema#num01')\ncontext(system:dict_pairs/3,_22252)\n"",
  ""api:status"":""api:server_error""
}

PROBLEM the database schema is broken",2021-09-03T08:23:13Z,GavinMendelGleason,https://github.com/terminusdb/terminusdb/issues/383#issuecomment-912356604,Fixed in 2905d0e,fix in 2905d0e,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,384,2021-08-06T08:11:45Z,KittyJose,Unable to update a sub document,https://github.com/terminusdb/terminusdb/issues/384,"Describe the bug
Unable to update a sub document
To Reproduce
Steps to reproduce the behavior:


Create a document with sub document
{""@id"":""Organization_somewhere"",""@type"":""Organization"",""invitations"":[{""@id"":""Invitation_5797a4718bf61f25899ba0dfb49ec355d1b437ff"",""@type"":""Invitation"",""email_to"":""something"",""note"":""whjgasdj"",""status"":""pending""}],""organization_name"":""somewhere"",""status"":""inactive""}


Try to update email_to to something else
{""@id"":""Organization_somewhere"",""@type"":""Organization"",""invitations"":[{""@id"":""Invitation_5797a4718bf61f25899ba0dfb49ec355d1b437ff"",""@type"":""Invitation"",""email_to"":""**somethingmaybe**"",""note"":""whjgasdj"",""status"":""pending""}],""organization_name"":""somewhere"",""status"":""inactive""}


I dont get any response from the database and next time when I view the document I dont see my update to email field",2021-08-16T18:40:54Z,GavinMendelGleason,https://github.com/terminusdb/terminusdb/issues/384#issuecomment-899733720,"""Invitation"" is type ""ValueHash"" so you cannot use the same ID with different content. If you remove the id it will work, or if you change ""Invitation"" to ""Random"" it will work.",""" invitation "" be type "" valuehash "" so -PRON- can not use the same -PRON- d with different content if -PRON- remove the -PRON- d -PRON- will work or if -PRON- change "" invitation "" to "" random "" -PRON- will work",0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,385,2021-08-06T09:47:20Z,trialblaze-bt,"Identity, Authentication & Authorisation (Community Discussion)",https://github.com/terminusdb/terminusdb/issues/385,"Identity, Authentication & Authorisation
This is a placeholder for community discussion about Authn & Authz in TerminusDB
Identity and Authentication
Authentication - Most common approach/solution
Lots of vendor solutions exist using OAuth 2.0/OIDC or SAML (SAML usually for enterprise SSO).
Identity
An identity is usually an email address in the scope of the above Authn approach. Using Identity Providers (IdPs) such as Google, Facebook (et al.) gives a user a social identity. This does not support the concept of an agent acting on behalf of an entity (person, organisation, device).
WebID is a W3C draft spec that Tim Berners-Lee and co. have tried to get off the ground to support decentralised, discoverable identities for entities (using RDF).
The folks at Solid are using the WebID spec along with OIDC to impl. decentralised IDs.
DIDs Decentralised ID is a new recommended spec from W3C - A globally unique persistent identifier that does not require a centralized registration authority and is often generated and/or registered cryptographically.
IPFS- Interplanetary File System, for example, uses DIDs. The Solid guys mentioned above are also talking about support for DIDs as well as WebIDs.
Authentication - Emerging
Passwordless

FIDO Alliance - Many IdPs have joined the alliance. Allows for standard approaches to MFA and uses Public Key Cryptography.
FIDO2 - Using W3C WebAuthn spec and FIDO Alliance CTAP protocol.
Blockchain based solutions are also joining the alliance.
No sign of Auth0, Okta and co. joining the alliance yet, but there is some online material suggesting Auth0 supports WebAuthn, for instance.

Authorisation
This is the real beast!
Generally, ppl impl. Authz themselves based on business requirements. Some will use claims/roles in JWT and keep it simple.
At my company we have impl'd a custom fine grained authz system (API based) and simply use JWTs for authn. We have an interceptor in the API call chain that checks the API being called against the user's permissions and based on the data being queried/modified - multi-tenant solution where tenants can collaborate on data based on the owner of the data giving permissions to do so.
I don't know if TerminusDB was designed for the distributed web as the ppl at Solid or Tim Berners-Lee maybe think of that concept. However, RDF and OWL (a flavour of) are core to the architecture, as well as collaborative authoring and strict versioning (append only). Strict versioning reminds me of IPFS, though that's a global distributed file system with permanently addressable immutable data.
The ppl at Solid are extending an original spec from W3C called Web Access Control - WAC to define authz requirements for Solid servers & clients. However, that is based on HTTP resources and so it's different from TerminusDB triples.
I know this blurb hasn't helped in terms of suggesting solutions to Authz. However, I just wanted to put something out there to see what ppl are thinking about in this space. I'm really interested in data that can be shared across organisations/entities. I've looked a bit at the WAC stuff above, as well as the basic acl ontology from W3C.
The guys at Fluree have done a good job of securing and making data shareable. However, as they don't use OWL/2 I don't see a way to use their solution for reasoning - e.g. OWL 2 RL.
Maybe @GavinMendelGleason will list a number of problems with OWL 2 RL that I don't know about :) and honestly, I don't profess to be an expert when it comes to this stuff!
Thanks for reading.
Brian T.",2021-08-06T11:18:58Z,trialblaze-bt,https://github.com/terminusdb/terminusdb/issues/385#issuecomment-894191817,"Should permissions (maybe using  the roles concept of grouped permissions) be applied at the schema graph level or the instance graph level.
Would schema level mean that updated permissions grant/revoke a users access to all the graph data derived from a class?

i.e. instance data of a class that you used to have read access to are no longer visible when read access on the class is revoked.
And instance level mean that updated permissions grant/revoke a users access to all future graph data derived from a class?
i.e. The user can still read older instance data that existed before the permissions were updated.",should permission ( maybe use the role concept of group permission ) be apply at the schema graph level or the instance graph level would schema level mean that update permission grant / revoke a user access to all the graph datum derive from a class ie instance datum of a class that -PRON- use to have read access to be no long visible when read access on the class be revoke and instance level mean that update permission grant / revoke a user access to all future graph datum derive from a class ie the user can still read old instance datum that exist before the permission be update,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,385,2021-08-06T09:47:20Z,trialblaze-bt,"Identity, Authentication & Authorisation (Community Discussion)",https://github.com/terminusdb/terminusdb/issues/385,"Identity, Authentication & Authorisation
This is a placeholder for community discussion about Authn & Authz in TerminusDB
Identity and Authentication
Authentication - Most common approach/solution
Lots of vendor solutions exist using OAuth 2.0/OIDC or SAML (SAML usually for enterprise SSO).
Identity
An identity is usually an email address in the scope of the above Authn approach. Using Identity Providers (IdPs) such as Google, Facebook (et al.) gives a user a social identity. This does not support the concept of an agent acting on behalf of an entity (person, organisation, device).
WebID is a W3C draft spec that Tim Berners-Lee and co. have tried to get off the ground to support decentralised, discoverable identities for entities (using RDF).
The folks at Solid are using the WebID spec along with OIDC to impl. decentralised IDs.
DIDs Decentralised ID is a new recommended spec from W3C - A globally unique persistent identifier that does not require a centralized registration authority and is often generated and/or registered cryptographically.
IPFS- Interplanetary File System, for example, uses DIDs. The Solid guys mentioned above are also talking about support for DIDs as well as WebIDs.
Authentication - Emerging
Passwordless

FIDO Alliance - Many IdPs have joined the alliance. Allows for standard approaches to MFA and uses Public Key Cryptography.
FIDO2 - Using W3C WebAuthn spec and FIDO Alliance CTAP protocol.
Blockchain based solutions are also joining the alliance.
No sign of Auth0, Okta and co. joining the alliance yet, but there is some online material suggesting Auth0 supports WebAuthn, for instance.

Authorisation
This is the real beast!
Generally, ppl impl. Authz themselves based on business requirements. Some will use claims/roles in JWT and keep it simple.
At my company we have impl'd a custom fine grained authz system (API based) and simply use JWTs for authn. We have an interceptor in the API call chain that checks the API being called against the user's permissions and based on the data being queried/modified - multi-tenant solution where tenants can collaborate on data based on the owner of the data giving permissions to do so.
I don't know if TerminusDB was designed for the distributed web as the ppl at Solid or Tim Berners-Lee maybe think of that concept. However, RDF and OWL (a flavour of) are core to the architecture, as well as collaborative authoring and strict versioning (append only). Strict versioning reminds me of IPFS, though that's a global distributed file system with permanently addressable immutable data.
The ppl at Solid are extending an original spec from W3C called Web Access Control - WAC to define authz requirements for Solid servers & clients. However, that is based on HTTP resources and so it's different from TerminusDB triples.
I know this blurb hasn't helped in terms of suggesting solutions to Authz. However, I just wanted to put something out there to see what ppl are thinking about in this space. I'm really interested in data that can be shared across organisations/entities. I've looked a bit at the WAC stuff above, as well as the basic acl ontology from W3C.
The guys at Fluree have done a good job of securing and making data shareable. However, as they don't use OWL/2 I don't see a way to use their solution for reasoning - e.g. OWL 2 RL.
Maybe @GavinMendelGleason will list a number of problems with OWL 2 RL that I don't know about :) and honestly, I don't profess to be an expert when it comes to this stuff!
Thanks for reading.
Brian T.",2021-08-26T15:49:36Z,GavinMendelGleason,https://github.com/terminusdb/terminusdb/issues/385#issuecomment-906530791,"Authentication
Identity is established by our authentication. Currently this is either basic auth or JWT but this could (and should) be extended with other approaches as required.
Authorization
Currently we have ""Actions"" which are collected into roles. These roles are in turn coupled with a scope to give us a capability. The capabilities are available to the user as collections of capabilities in the system graph.
We can add new actions as they are required. Currently the granularity is rather coarse, but this isn't required by the capability model.
In terms of reasoning - inference is possible using query. We previously supported reasoning in the schema, but found this was not a very useful approach. If you want to actually do materialised reasoning this is easily done with a query and given that we have a branch model, you can reason from a base into an expanded database thereby averting the problems inherent in simply materialising everything into the same graph.
We're happy to review pull requests extending the model - but for the moment we're going to stick with our current capability model.","authentication identity be establish by -PRON- authentication currently this be either basic auth or jwt but this could ( and should ) be extend with other approach as require authorization currently -PRON- have "" action "" which be collect into role these role be in turn couple with a scope to give -PRON- a capability the capability be available to the user as collection of capability in the system graph -PRON- can add new action as -PRON- be require currently the granularity be rather coarse but this be not require by the capability model in term of reasoning - inference be possible use query -PRON- previously support reason in the schema but find this be not a very useful approach if -PRON- want to actually do materialised reason this be easily do with a query and give that -PRON- have a branch model -PRON- can reason from a base into an expand database thereby avert the problem inherent in simply materialise everything into the same graph -PRON- be happy to review pull request extend the model - but for the moment -PRON- be go to stick with -PRON- current capability model",0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,386,2021-08-06T14:12:05Z,KittyJose,When I add properties with cardinality messes up all the other document classes,https://github.com/terminusdb/terminusdb/issues/386,"Describe the bug
When I add properties with cardinality messes up all the other document classes
To Reproduce
 //cardinality  [{ ""@type"" : ""Class"", ""@id"" : ""Person"", ""name"" : ""xsd:string"", ""age"": { ""@type"" : ""Optional"",  ""@class"" : ""xsd:decimal""}, ""friend_of"" : { ""@type"" : ""Cardinality"",  ""@class"" : ""Person"",  ""@cardinality"" : 3 } }]
after this update you loose other document classes as well of the db",2021-08-09T07:55:03Z,GavinMendelGleason,https://github.com/terminusdb/terminusdb/issues/386#issuecomment-895022615,Currently cardinality is unimplemented - we should throw an error instead and implement cardinality later.,currently cardinality be unimplemente - -PRON- should throw an error instead and implement cardinality later,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,386,2021-08-06T14:12:05Z,KittyJose,When I add properties with cardinality messes up all the other document classes,https://github.com/terminusdb/terminusdb/issues/386,"Describe the bug
When I add properties with cardinality messes up all the other document classes
To Reproduce
 //cardinality  [{ ""@type"" : ""Class"", ""@id"" : ""Person"", ""name"" : ""xsd:string"", ""age"": { ""@type"" : ""Optional"",  ""@class"" : ""xsd:decimal""}, ""friend_of"" : { ""@type"" : ""Cardinality"",  ""@class"" : ""Person"",  ""@cardinality"" : 3 } }]
after this update you loose other document classes as well of the db",2021-08-09T08:11:42Z,KittyJose,https://github.com/terminusdb/terminusdb/issues/386#issuecomment-895031947,"Why throwing an error? if its un imlemented, is it okay to nuke it from the UI for now?",why throw an error if -PRON- un imlemente be -PRON- okay to nuke -PRON- from the ui for now,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,386,2021-08-06T14:12:05Z,KittyJose,When I add properties with cardinality messes up all the other document classes,https://github.com/terminusdb/terminusdb/issues/386,"Describe the bug
When I add properties with cardinality messes up all the other document classes
To Reproduce
 //cardinality  [{ ""@type"" : ""Class"", ""@id"" : ""Person"", ""name"" : ""xsd:string"", ""age"": { ""@type"" : ""Optional"",  ""@class"" : ""xsd:decimal""}, ""friend_of"" : { ""@type"" : ""Cardinality"",  ""@class"" : ""Person"",  ""@cardinality"" : 3 } }]
after this update you loose other document classes as well of the db",2021-08-09T08:13:14Z,GavinMendelGleason,https://github.com/terminusdb/terminusdb/issues/386#issuecomment-895032897,"Well we shouldn't silently allow them to specify cardinalities if we don't treat them. Yes, it can be safely nuked from UI for now.",well -PRON- should not silently allow -PRON- to specify cardinality if -PRON- do not treat -PRON- yes -PRON- can be safely nuke from ui for now,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,386,2021-08-06T14:12:05Z,KittyJose,When I add properties with cardinality messes up all the other document classes,https://github.com/terminusdb/terminusdb/issues/386,"Describe the bug
When I add properties with cardinality messes up all the other document classes
To Reproduce
 //cardinality  [{ ""@type"" : ""Class"", ""@id"" : ""Person"", ""name"" : ""xsd:string"", ""age"": { ""@type"" : ""Optional"",  ""@class"" : ""xsd:decimal""}, ""friend_of"" : { ""@type"" : ""Cardinality"",  ""@class"" : ""Person"",  ""@cardinality"" : 3 } }]
after this update you loose other document classes as well of the db",2021-08-09T08:22:02Z,KittyJose,https://github.com/terminusdb/terminusdb/issues/386#issuecomment-895038146,"Great, closing this issue for now :)",great closing this issue for now ),0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,387,2021-08-06T14:16:33Z,Francesca-Bit,Document Interface DELETE multi classes ,https://github.com/terminusdb/terminusdb/issues/387,"Describe the bug
Using DELETE Action with payload to delete multi class from schema
To Reproduce
Steps to reproduce the behavior:
DELETE CALL URL EXAMPLE
http://127.0.0.1:6363/api/document/admin/test001/local/branch/main?graph_type=schema&author=admin&message=delete%20document
Payload = [""Doc01"",""Doc002""]
got the follow error
{
""api:message"":""Error: not_a_proper_id([""Doc01"",""Doc002""])"",
""api:status"":""api:server_error""
}",,,,,,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,388,2021-08-09T09:01:45Z,AstroChelonian,Map and update WoQL compiler,https://github.com/terminusdb/terminusdb/issues/388,,,,,,,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,389,2021-08-09T11:22:40Z,GavinMendelGleason,Subdocuments should require Random key,https://github.com/terminusdb/terminusdb/issues/389,"Describe the bug
Currently the only key that works is Random so this should be required or throw an error.
To Reproduce
Create a document with a subdocument having no key.
Info (please complete the following information):
Occurs in main branch.",2021-08-12T08:46:28Z,GavinMendelGleason,https://github.com/terminusdb/terminusdb/issues/389#issuecomment-897459519,Fixed in 07157c8,fix in 07157c8,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,390,2021-08-09T15:00:41Z,Cheukting,Error Raised when try to insert schema,https://github.com/terminusdb/terminusdb/issues/390,"Input:
[{'@type': 'Class', '@id': 'my_ip', '@key': {'@type': 'Lexical', '@fields': ['timestamp']}, 'ip': 'xsd:string', 'timestamp': 'xsd:dataTime'}]

Error:
{'code': 500, 'message': ""dict_create/3: Type error: `key-value' expected, found `witness{'@type':not_a_class_or_base_type,class:'http://www.w3.org/2001/XMLSchema#dataTime'}' (a dict)""}",2021-08-09T15:04:06Z,Cheukting,https://github.com/terminusdb/terminusdb/issues/390#issuecomment-895300783,"I think it's a problem of the xsd:dateTime, can someone tell me what it should be like to use datatime?",i think -PRON- be a problem of the xsddatetime can someone tell -PRON- what -PRON- should be like to use datatime,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,390,2021-08-09T15:00:41Z,Cheukting,Error Raised when try to insert schema,https://github.com/terminusdb/terminusdb/issues/390,"Input:
[{'@type': 'Class', '@id': 'my_ip', '@key': {'@type': 'Lexical', '@fields': ['timestamp']}, 'ip': 'xsd:string', 'timestamp': 'xsd:dataTime'}]

Error:
{'code': 500, 'message': ""dict_create/3: Type error: `key-value' expected, found `witness{'@type':not_a_class_or_base_type,class:'http://www.w3.org/2001/XMLSchema#dataTime'}' (a dict)""}",2021-08-09T15:04:46Z,Cheukting,https://github.com/terminusdb/terminusdb/issues/390#issuecomment-895301431,It may not be a bug but I want an answer XD,-PRON- may not be a bug but i want an answer xd,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,390,2021-08-09T15:00:41Z,Cheukting,Error Raised when try to insert schema,https://github.com/terminusdb/terminusdb/issues/390,"Input:
[{'@type': 'Class', '@id': 'my_ip', '@key': {'@type': 'Lexical', '@fields': ['timestamp']}, 'ip': 'xsd:string', 'timestamp': 'xsd:dataTime'}]

Error:
{'code': 500, 'message': ""dict_create/3: Type error: `key-value' expected, found `witness{'@type':not_a_class_or_base_type,class:'http://www.w3.org/2001/XMLSchema#dataTime'}' (a dict)""}",2021-08-09T15:06:05Z,Cheukting,https://github.com/terminusdb/terminusdb/issues/390#issuecomment-895302731,"ah typo, I see it now",ah typo i see -PRON- now,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,391,2021-08-09T15:10:04Z,Cheukting,Casting error in timestamp,https://github.com/terminusdb/terminusdb/issues/391,"Schema:
[{'@type': 'Class', '@id': 'my_ip', '@key': {'@type': 'Lexical', '@fields': ['timestamp']}, 'ip': 'xsd:string', 'timestamp': 'xsd:dateTime'}]
Data:
[{'@type': 'my_ip', 'timestamp': '2021-08-09T15:08:02.538620+00:00', 'ip': '0:0:0:0:0:0:0:1'}]
Error:
terminusdb_client.errors.DatabaseError: Error: casting_error(""2021-08-09T15:08:02.538620+00:00"",'http://www.w3.org/2001/XMLSchema#dateTime')",2021-08-09T15:11:56Z,Cheukting,https://github.com/terminusdb/terminusdb/issues/391#issuecomment-895307558,Can someone tell me more about how timestamp is handled in the backend? I may have to do some meshing before giving it to the backend (if backend is not handling it that is) What are the standards? It's definitely a tricky issue,can someone tell -PRON- more about how timestamp be handle in the backend i may have to do some meshing before give -PRON- to the backend ( if backend be not handle -PRON- that be ) what be the standard -PRON- be definitely a tricky issue,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,391,2021-08-09T15:10:04Z,Cheukting,Casting error in timestamp,https://github.com/terminusdb/terminusdb/issues/391,"Schema:
[{'@type': 'Class', '@id': 'my_ip', '@key': {'@type': 'Lexical', '@fields': ['timestamp']}, 'ip': 'xsd:string', 'timestamp': 'xsd:dateTime'}]
Data:
[{'@type': 'my_ip', 'timestamp': '2021-08-09T15:08:02.538620+00:00', 'ip': '0:0:0:0:0:0:0:1'}]
Error:
terminusdb_client.errors.DatabaseError: Error: casting_error(""2021-08-09T15:08:02.538620+00:00"",'http://www.w3.org/2001/XMLSchema#dateTime')",2021-08-09T15:31:52Z,Cheukting,https://github.com/terminusdb/terminusdb/issues/391#issuecomment-895322759,timestamp is generate form here: https://github.com/singer-io/getting-started/blob/master/docs/RUNNING_AND_DEVELOPING.md#a-python-tap,timestamp be generate form here https//githubcom / singer - io / get - start / blob / master / doc / running_and_developingmd#a - python - tap,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,391,2021-08-09T15:10:04Z,Cheukting,Casting error in timestamp,https://github.com/terminusdb/terminusdb/issues/391,"Schema:
[{'@type': 'Class', '@id': 'my_ip', '@key': {'@type': 'Lexical', '@fields': ['timestamp']}, 'ip': 'xsd:string', 'timestamp': 'xsd:dateTime'}]
Data:
[{'@type': 'my_ip', 'timestamp': '2021-08-09T15:08:02.538620+00:00', 'ip': '0:0:0:0:0:0:0:1'}]
Error:
terminusdb_client.errors.DatabaseError: Error: casting_error(""2021-08-09T15:08:02.538620+00:00"",'http://www.w3.org/2001/XMLSchema#dateTime')",2021-08-10T10:54:47Z,GavinMendelGleason,https://github.com/terminusdb/terminusdb/issues/391#issuecomment-895930829,Fixed in 7c0999c,fix in 7c0999c,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,392,2021-08-10T12:01:09Z,Cheukting,Error: type_not_found('terminusdb:///schema#Class'),https://github.com/terminusdb/terminusdb/issues/392,"I run my test:
   def test_insert_cheuk(docker_url):
        uk = Country()
        uk.name = ""United Kingdom""
    
        home = Address()
        home.street = ""123 Abc Street""
        home.country = uk
        home.postal_code = ""A12 345""
    
        cheuk = Employee()
        cheuk.address_of = home
        cheuk.contact_number = ""07777123456""
        cheuk.age = 21
        cheuk.name = ""Cheuk""
        cheuk.managed_by = cheuk
        cheuk.member_of = Team.IT
    
        client = WOQLClient(docker_url, insecure=True)
        client.connect(db=""test_docapi"")
        # client.create_database(""test_docapi"")
        # my_schema.commit(client)
        result = client.get_all_documents(graph_type=""schema"")
        for item in result:
            print(item)
>       client.insert_document([uk, home, cheuk], commit_msg=""Adding cheuk"")

And got
Error: type_not_found('terminusdb:///schema#Class')
But my schema is:
{'@base': 'terminusdb:///data/', '@schema': 'terminusdb:///schema#', '@type': '@context'}
{'@documentation': {'@comment': 'This is address'}, '@id': 'Address', '@key': {'@type': 'Random'}, '@subdocument': [], '@type': 'Class', 'country': 'Country', 'postal_code': 'xsd:string', 'street': 'xsd:string'}
{'@id': 'Coordinate', '@key': {'@type': 'Random'}, '@type': 'Class', 'x': 'xsd:decimal', 'y': 'xsd:decimal'}
{'@id': 'Country', '@key': {'@type': 'ValueHash'}, '@type': 'Class', 'name': 'xsd:string', 'perimeter': {'@class': 'Coordinate', '@type': 'List'}}
{'@id': 'Employee', '@inherits': 'Person', '@key': {'@type': 'Random'}, '@type': 'Class', 'address_of': 'Address', 'age': 'xsd:integer', 'contact_number': {'@class': 'xsd:string', '@type': 'Optional'}, 'friend_of': {'@class': 'Person', '@type': 'Set'}, 'managed_by': 'Employee', 'member_of': 'Team', 'name': 'xsd:string'}
{'@documentation': {'@comment': 'This is a person', '@properties': {'age': 'Age of the person.', 'name': 'Name of the person.'}}, '@id': 'Person', '@key': {'@type': 'Random'}, '@type': 'Class', 'age': 'xsd:integer', 'friend_of': {'@class': 'Person', '@type': 'Set'}, 'name': 'xsd:string'}
{'@id': 'Team', '@type': 'Enum', '@value': ['Information Technology', 'Marketing']}

so why?",2021-08-10T15:34:16Z,Cheukting,https://github.com/terminusdb/terminusdb/issues/392#issuecomment-896121939,"{'@type': 'Employee', '@id': 'Employee_5cc8e01826684b0b8e3b9785cd683613', 'address_of': {'@type': 'Address', 'street': '123 Abc Street', 'postal_code': 'A12 345', 'country': {'@id': 'Country_564fc4da302745c78c345dcbc0f697be', '@type': '@id'}}, 'contact_number': '07777123456', 'managed_by': {'@id': 'Employee_5cc8e01826684b0b8e3b9785cd683613', '@type': '@id'}, 'member_of': 'Information Technology', 'name': 'Cheuk', 'age': 21}
This is the son of the object cheuk I have insert uk and country in the same query",{ ' @type ' ' employee ' ' @id ' ' employee_5cc8e01826684b0b8e3b9785cd683613 ' ' address_of ' { ' @type ' ' address ' ' street ' ' 123 abc street ' ' postal_code ' ' a12 345 ' ' country ' { ' @id ' ' country_564fc4da302745c78c345dcbc0f697be ' ' @type ' ' @id ' } } ' contact_number ' ' 07777123456 ' ' managed_by ' { ' @id ' ' employee_5cc8e01826684b0b8e3b9785cd683613 ' ' @type ' ' @id ' } ' member_of ' ' information technology ' ' name ' ' cheuk ' ' age ' 21 } this be the son of the object cheuk i have insert uk and country in the same query,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,392,2021-08-10T12:01:09Z,Cheukting,Error: type_not_found('terminusdb:///schema#Class'),https://github.com/terminusdb/terminusdb/issues/392,"I run my test:
   def test_insert_cheuk(docker_url):
        uk = Country()
        uk.name = ""United Kingdom""
    
        home = Address()
        home.street = ""123 Abc Street""
        home.country = uk
        home.postal_code = ""A12 345""
    
        cheuk = Employee()
        cheuk.address_of = home
        cheuk.contact_number = ""07777123456""
        cheuk.age = 21
        cheuk.name = ""Cheuk""
        cheuk.managed_by = cheuk
        cheuk.member_of = Team.IT
    
        client = WOQLClient(docker_url, insecure=True)
        client.connect(db=""test_docapi"")
        # client.create_database(""test_docapi"")
        # my_schema.commit(client)
        result = client.get_all_documents(graph_type=""schema"")
        for item in result:
            print(item)
>       client.insert_document([uk, home, cheuk], commit_msg=""Adding cheuk"")

And got
Error: type_not_found('terminusdb:///schema#Class')
But my schema is:
{'@base': 'terminusdb:///data/', '@schema': 'terminusdb:///schema#', '@type': '@context'}
{'@documentation': {'@comment': 'This is address'}, '@id': 'Address', '@key': {'@type': 'Random'}, '@subdocument': [], '@type': 'Class', 'country': 'Country', 'postal_code': 'xsd:string', 'street': 'xsd:string'}
{'@id': 'Coordinate', '@key': {'@type': 'Random'}, '@type': 'Class', 'x': 'xsd:decimal', 'y': 'xsd:decimal'}
{'@id': 'Country', '@key': {'@type': 'ValueHash'}, '@type': 'Class', 'name': 'xsd:string', 'perimeter': {'@class': 'Coordinate', '@type': 'List'}}
{'@id': 'Employee', '@inherits': 'Person', '@key': {'@type': 'Random'}, '@type': 'Class', 'address_of': 'Address', 'age': 'xsd:integer', 'contact_number': {'@class': 'xsd:string', '@type': 'Optional'}, 'friend_of': {'@class': 'Person', '@type': 'Set'}, 'managed_by': 'Employee', 'member_of': 'Team', 'name': 'xsd:string'}
{'@documentation': {'@comment': 'This is a person', '@properties': {'age': 'Age of the person.', 'name': 'Name of the person.'}}, '@id': 'Person', '@key': {'@type': 'Random'}, '@type': 'Class', 'age': 'xsd:integer', 'friend_of': {'@class': 'Person', '@type': 'Set'}, 'name': 'xsd:string'}
{'@id': 'Team', '@type': 'Enum', '@value': ['Information Technology', 'Marketing']}

so why?",2021-08-10T21:46:12Z,Cheukting,https://github.com/terminusdb/terminusdb/issues/392#issuecomment-896335199,Problem solved!,problem solve,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,393,2021-08-10T12:15:13Z,Cheukting,fields_not_a_valid_list_in_key,https://github.com/terminusdb/terminusdb/issues/393,"I got this:
{
   ""code"": 500,
   ""message"": ""dict_create/3: Type error: `key-value' expected, found `witness{'@type':fields_not_a_valid_list_in_key,class:'terminusdb:///schema#my_ip',key:'terminusdb:///schema#my_ip_Lexical_timestamp',type:'http://terminusdb.com/schema/sys#Lexical'}' (a dict)""
}

When I try to send this:
[
   {
      ""@type"": ""Class"",
      ""@id"": ""my_ip"",
      ""@key"": {
         ""@type"": ""Lexical"",
         ""@fields"": [
            ""timestamp""
         ]
      },
      ""ip"": ""xsd:string"",
      ""timestamp"": ""xsd:dateTime""
   }
]

What does the error means?",2021-08-11T05:48:30Z,GavinMendelGleason,https://github.com/terminusdb/terminusdb/issues/393#issuecomment-896520397,I can't seem to reproduce this - creating this schema passes in main.,i can not seem to reproduce this - create this schema pass in main,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,393,2021-08-10T12:15:13Z,Cheukting,fields_not_a_valid_list_in_key,https://github.com/terminusdb/terminusdb/issues/393,"I got this:
{
   ""code"": 500,
   ""message"": ""dict_create/3: Type error: `key-value' expected, found `witness{'@type':fields_not_a_valid_list_in_key,class:'terminusdb:///schema#my_ip',key:'terminusdb:///schema#my_ip_Lexical_timestamp',type:'http://terminusdb.com/schema/sys#Lexical'}' (a dict)""
}

When I try to send this:
[
   {
      ""@type"": ""Class"",
      ""@id"": ""my_ip"",
      ""@key"": {
         ""@type"": ""Lexical"",
         ""@fields"": [
            ""timestamp""
         ]
      },
      ""ip"": ""xsd:string"",
      ""timestamp"": ""xsd:dateTime""
   }
]

What does the error means?",2021-08-11T12:02:29Z,spl,https://github.com/terminusdb/terminusdb/issues/393#issuecomment-896767246,@Cheukting We think this is fixed in main. Can you try again?,@cheukte -PRON- think this be fix in main can -PRON- try again,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,393,2021-08-10T12:15:13Z,Cheukting,fields_not_a_valid_list_in_key,https://github.com/terminusdb/terminusdb/issues/393,"I got this:
{
   ""code"": 500,
   ""message"": ""dict_create/3: Type error: `key-value' expected, found `witness{'@type':fields_not_a_valid_list_in_key,class:'terminusdb:///schema#my_ip',key:'terminusdb:///schema#my_ip_Lexical_timestamp',type:'http://terminusdb.com/schema/sys#Lexical'}' (a dict)""
}

When I try to send this:
[
   {
      ""@type"": ""Class"",
      ""@id"": ""my_ip"",
      ""@key"": {
         ""@type"": ""Lexical"",
         ""@fields"": [
            ""timestamp""
         ]
      },
      ""ip"": ""xsd:string"",
      ""timestamp"": ""xsd:dateTime""
   }
]

What does the error means?",2021-08-16T13:17:13Z,Cheukting,https://github.com/terminusdb/terminusdb/issues/393#issuecomment-899502582,@GavinMendelGleason fixed it,@gavinmendelgleason fix -PRON-,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,394,2021-08-10T12:20:12Z,KittyJose,Error in creating a normal subdocument which is not List/Set,https://github.com/terminusdb/terminusdb/issues/394,"Describe the bug
Error in creating a normal subdocument which is not List/Set
To Reproduce
Schema used is
[
    {
        ""@abstract"": [],
        ""@id"": ""Entity"",
        ""@type"": ""Class"",
        ""status"": ""Status""
    },
    {
        ""@id"": ""Invitation"",
        ""@inherits"": ""Entity"",
        ""@key"": {
            ""@type"": ""ValueHash""
        },
        ""@subdocument"": [],
        ""@type"": ""Class"",
        ""email_to"": ""xsd:string"",
        ""invited_by"": ""User"",
        ""note"": {
            ""@class"": ""xsd:string"",
            ""@type"": ""Optional""
        },
        ""sent_date"": {
            ""@class"": ""xsd:dateTime"",
            ""@type"": ""Optional""
        }
    },
    {
        ""@id"": ""Organization"",
        ""@inherits"": ""Entity"",
        ""@key"": {
            ""@fields"": [
                ""organization_name""
            ],
            ""@type"": ""Lexical""
        },
        ""@type"": ""Class"",
        ""child"": {
            ""@class"": ""Organization"",
            ""@type"": ""Set""
        },
        ""collaborators"": {
            ""@class"": ""User"",
            ""@type"": ""Set""
        },
        ""creation_date"": ""xsd:dateTime"",
        ""expiration_data"": {
            ""@class"": ""xsd:dateTime"",
            ""@type"": ""Optional""
        },
        ""invitations"": {
            ""@class"": ""Invitation"",
            ""@type"": ""Set""
        },
        ""organization_name"": ""xsd:string"",
        ""owned_by"": ""User"",
        ""stripe_subscription"": ""StripeSubscription""
    },
    {
        ""@id"": ""Personal"",
        ""@inherits"": ""Organization"",
        ""@type"": ""Class""
    },
    {
        ""@id"": ""Status"",
        ""@type"": ""Enum"",
        ""@value"": [
            ""pending"",
            ""inactive"",
            ""active"",
            ""needs_invite"",
            ""invite_sent"",
            ""accepted"",
            ""rejected""
        ]
    },
    {
        ""@id"": ""StripeSubscription"",
        ""@inherits"": ""Entity"",
        ""@subdocument"": [],
        ""@type"": ""Class"",
        ""billing_email"": ""xsd:string"",
        ""stripe_id"": ""xsd:string"",
        ""stripe_quantity"": ""xsd:decimal"",
        ""stripe_user"": ""User"",
        ""subscription_id"": ""xsd:string""
    },
    {
        ""@id"": ""Team"",
        ""@inherits"": ""Organization"",
        ""@type"": ""Class""
    },
    {
        ""@id"": ""User"",
        ""@inherits"": ""Entity"",
        ""@key"": {
            ""@fields"": [
                ""user_id""
            ],
            ""@type"": ""Lexical""
        },
        ""@type"": ""Class"",
        ""company"": ""xsd:string"",
        ""email"": ""xsd:string"",
        ""first_name"": ""xsd:string"",
        ""last_name"": ""xsd:string"",
        ""picture"": ""xsd:string"",
        ""registration_date"": {
            ""@class"": ""xsd:dateTime"",
            ""@type"": ""Optional""
        },
        ""user_id"": ""xsd:string""
    }
  ]
I get 400 BAD REQUEST on creating Organization
{
   ""@type"":""Organization"",
   ""child"":[
      ""Organization_kitty"",
      ""Organization_DUNDRUM""
   ],
   ""collaborators"":[
      ""User_MASS""
   ],
   ""expiration_data"":"""",
   ""invitations"":[
      {
         ""email_to"":""hello"",
         ""invited_by"":""User_Mocking"",
         ""status"":""needs_invite""
      },
      {
         ""email_to"":""monkey"",
         ""invited_by"":""User_Mocking"",
         ""status"":""needs_invite""
      }
   ],
   ""organization_name"":""withsubscription"",
   ""status"":""invite_sent"",
   ""stripe_subscription"":[
      {
         ""billing_email"":""somewkjf"",
         ""status"":""active"",
         ""stripe_id"":""KItty"",
         ""stripe_quantity"":""32"",
         ""stripe_user"":""User_Mocking"",
         ""subscription_id"":""932438238429384ASBJDA""
      }
   ],
   ""creation_date"":""2011-01-01 01:00:37""
}
If I sent stripe_subscription as a json and not in an array (since its not a list or set) then I get 500 Internal error
{
   ""@type"":""Organization"",
   ""child"":[
      ""Organization_kitty"",
      ""Organization_DUNDRUM""
   ],
   ""collaborators"":[
      ""User_MASS""
   ],
   ""expiration_data"":"""",
   ""invitations"":[
      {
         ""email_to"":""hello"",
         ""invited_by"":""User_Mocking"",
         ""status"":""needs_invite""
      },
      {
         ""email_to"":""monkey"",
         ""invited_by"":""User_Mocking"",
         ""status"":""needs_invite""
      }
   ],
   ""organization_name"":""withsubscription"",
   ""status"":""invite_sent"",
   ""stripe_subscription"": {
         ""billing_email"":""somewkjf"",
         ""status"":""active"",
         ""stripe_id"":""KItty"",
         ""stripe_quantity"":""32"",
         ""stripe_user"":""User_Mocking"",
         ""subscription_id"":""932438238429384ASBJDA""
      },
   ""creation_date"":""2011-01-01 01:00:37""
}
500 error is
{
  ""api:message"":""Error: document_has_no_type(json{billing_email:\""somewkjf\"",status:\""active\"",stripe_id:\""KItty\"",stripe_quantity:\""32\"",stripe_user:\""User_Mocking\"",subscription_id:\""932438238429384ASBJDA\""})\n\n\nNote: some frames are missing due to last-call optimization.\nRe-run your program in debug mode (:- debug.) to get more detail.\n\n"",
  ""api:status"":""api:server_error""
}
Note: this maybe an error which I am doing, but let me know the correct way of sending a stripe_subscription which is a simple sub document with no type",2021-08-13T13:53:33Z,spl,https://github.com/terminusdb/terminusdb/issues/394#issuecomment-898473456,"Here's what I did to reproduce:


In one terminal:
rm -rf ./storage
./terminusdb store init
./terminusdb db create testdb --label label --comment comment --prefixes '{""@base"":""http://base/"",""@schema"":""http://schema/""}'
./terminusdb serve



In another terminal:
cat schema.json | xh 'http://localhost:6363/api/document/admin/testdb?author=author&message=message&graph_type=schema'

HTTP/1.1 200 OK
connection: keep-alive
content-length: 119
content-type: application/json; charset=UTF-8
date: Fri, 13 Aug 2021 13:43:09 GMT
[
    ""Entity"",
    ""Invitation"",
    ""Organization"",
    ""Personal"",
    ""Status"",
    ""StripeSubscription"",
    ""Team"",
    ""User""
]

cat create-org-1.json | xh 'http://localhost:6363/api/document/admin/testdb?author=author&message=message'

HTTP/1.1 500 Internal Server Error
connection: keep-alive
content-length: 387
content-type: application/json
date: Fri, 13 Aug 2021 13:45:22 GMT
{
    ""api:message"": ""Error: document_has_no_type(json{billing_email:\""somewkjf\"",status:\""active\"",stripe_id:\""KItty\"",stripe_quantity:\""32\"",stripe_user:\""User_Mocking\"",subscription_id:\""932438238429384ASBJDA\""})\n\n\nNote: some frames are missing due to last-call optimization.\nRe-run your program in debug mode (:- debug.) to get more detail.\n\n"",
    ""api:status"": ""api:server_error""
}

cat create-org-2.json | xh 'http://localhost:6363/api/document/admin/testdb?author=author&message=message'

HTTP/1.1 500 Internal Server Error
connection: keep-alive
content-length: 387
content-type: application/json
date: Fri, 13 Aug 2021 13:52:47 GMT
{
    ""api:message"": ""Error: document_has_no_type(json{billing_email:\""somewkjf\"",status:\""active\"",stripe_id:\""KItty\"",stripe_quantity:\""32\"",stripe_user:\""User_Mocking\"",subscription_id:\""932438238429384ASBJDA\""})\n\n\nNote: some frames are missing due to last-call optimization.\nRe-run your program in debug mode (:- debug.) to get more detail.\n\n"",
    ""api:status"": ""api:server_error""
}

diff -uw create-org-{1,2}.json

--- create-org-1.json	2021-08-13 15:44:56.572414104 +0200
+++ create-org-2.json	2021-08-13 15:52:32.435367846 +0200
@@ -22,15 +22,13 @@
   ],
   ""organization_name"": ""withsubscription"",
   ""status"": ""invite_sent"",
-  ""stripe_subscription"": [
-    {
+  ""stripe_subscription"": {
       ""billing_email"": ""somewkjf"",
       ""status"": ""active"",
       ""stripe_id"": ""KItty"",
       ""stripe_quantity"": ""32"",
       ""stripe_user"": ""User_Mocking"",
       ""subscription_id"": ""932438238429384ASBJDA""
-    }
-  ],
+  },
   ""creation_date"": ""2011-01-01 01:00:37""
 }","here be what i do to reproduce in one terminal rm -rf /storage /terminusdb store init /terminusdb db create testdb --label label --comment comment --prefixe ' { "" @base""""http//base/""""@schema""""http//schema/ "" } ' /terminusdb serve in another terminal cat schemajson | xh ' http//localhost6363 / api / document / admin / testdbauthor = author&message = message&graph_type = schema ' http/11 200 ok connection keep - alive content - length 119 content - type application / json charset = utf-8 date fri 13 aug 2021 134309 gmt [ "" entity "" "" invitation "" "" organization "" "" personal "" "" status "" "" stripesubscription "" "" team "" "" user "" ] cat create - org-1json | xh ' http//localhost6363 / api / document / admin / testdbauthor = author&message = message ' http/11 500 internal server error connection keep - alive content - length 387 content - type application / json date fri 13 aug 2021 134522 gmt { "" apimessage "" "" error document_has_no_type(json{billing_email\""somewkjf\""status\""active\""stripe_id\""kitty\""stripe_quantity\""32\""stripe_user\""user_mocking\""subscription_id\""932438238429384asbjda\""})\n\n\nnote some frame be miss due to last - call optimization\nre - run -PRON- program in debug mode ( - debug ) to get more detail\n\n "" "" apistatus "" "" apiserver_error "" } cat create - org-2json | xh ' http//localhost6363 / api / document / admin / testdbauthor = author&message = message ' http/11 500 internal server error connection keep - alive content - length 387 content - type application / json date fri 13 aug 2021 135247 gmt { "" apimessage "" "" error document_has_no_type(json{billing_email\""somewkjf\""status\""active\""stripe_id\""kitty\""stripe_quantity\""32\""stripe_user\""user_mocking\""subscription_id\""932438238429384asbjda\""})\n\n\nnote some frame be miss due to last - call optimization\nre - run -PRON- program in debug mode ( - debug ) to get more detail\n\n "" "" apistatus "" "" apiserver_error "" } diff -uw create - org-{12}json --- create - org-1json 2021 - 08 - 13 154456572414104 +0200 + + + create - org-2json 2021 - 08 - 13 155232435367846 +0200 @@ -2215 +2213 @@ ] "" organization_name "" "" withsubscription "" "" status "" "" invite_sent "" - "" stripe_subscription "" [ - { + "" stripe_subscription "" { "" billing_email "" "" somewkjf "" "" status "" "" active "" "" stripe_id "" "" kitty "" "" stripe_quantity "" "" 32 "" "" stripe_user "" "" user_mocke "" "" subscription_id "" "" 932438238429384asbjda "" - } - ] + } "" creation_date "" "" 2011 - 01 - 01 010037 "" }",0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,394,2021-08-10T12:20:12Z,KittyJose,Error in creating a normal subdocument which is not List/Set,https://github.com/terminusdb/terminusdb/issues/394,"Describe the bug
Error in creating a normal subdocument which is not List/Set
To Reproduce
Schema used is
[
    {
        ""@abstract"": [],
        ""@id"": ""Entity"",
        ""@type"": ""Class"",
        ""status"": ""Status""
    },
    {
        ""@id"": ""Invitation"",
        ""@inherits"": ""Entity"",
        ""@key"": {
            ""@type"": ""ValueHash""
        },
        ""@subdocument"": [],
        ""@type"": ""Class"",
        ""email_to"": ""xsd:string"",
        ""invited_by"": ""User"",
        ""note"": {
            ""@class"": ""xsd:string"",
            ""@type"": ""Optional""
        },
        ""sent_date"": {
            ""@class"": ""xsd:dateTime"",
            ""@type"": ""Optional""
        }
    },
    {
        ""@id"": ""Organization"",
        ""@inherits"": ""Entity"",
        ""@key"": {
            ""@fields"": [
                ""organization_name""
            ],
            ""@type"": ""Lexical""
        },
        ""@type"": ""Class"",
        ""child"": {
            ""@class"": ""Organization"",
            ""@type"": ""Set""
        },
        ""collaborators"": {
            ""@class"": ""User"",
            ""@type"": ""Set""
        },
        ""creation_date"": ""xsd:dateTime"",
        ""expiration_data"": {
            ""@class"": ""xsd:dateTime"",
            ""@type"": ""Optional""
        },
        ""invitations"": {
            ""@class"": ""Invitation"",
            ""@type"": ""Set""
        },
        ""organization_name"": ""xsd:string"",
        ""owned_by"": ""User"",
        ""stripe_subscription"": ""StripeSubscription""
    },
    {
        ""@id"": ""Personal"",
        ""@inherits"": ""Organization"",
        ""@type"": ""Class""
    },
    {
        ""@id"": ""Status"",
        ""@type"": ""Enum"",
        ""@value"": [
            ""pending"",
            ""inactive"",
            ""active"",
            ""needs_invite"",
            ""invite_sent"",
            ""accepted"",
            ""rejected""
        ]
    },
    {
        ""@id"": ""StripeSubscription"",
        ""@inherits"": ""Entity"",
        ""@subdocument"": [],
        ""@type"": ""Class"",
        ""billing_email"": ""xsd:string"",
        ""stripe_id"": ""xsd:string"",
        ""stripe_quantity"": ""xsd:decimal"",
        ""stripe_user"": ""User"",
        ""subscription_id"": ""xsd:string""
    },
    {
        ""@id"": ""Team"",
        ""@inherits"": ""Organization"",
        ""@type"": ""Class""
    },
    {
        ""@id"": ""User"",
        ""@inherits"": ""Entity"",
        ""@key"": {
            ""@fields"": [
                ""user_id""
            ],
            ""@type"": ""Lexical""
        },
        ""@type"": ""Class"",
        ""company"": ""xsd:string"",
        ""email"": ""xsd:string"",
        ""first_name"": ""xsd:string"",
        ""last_name"": ""xsd:string"",
        ""picture"": ""xsd:string"",
        ""registration_date"": {
            ""@class"": ""xsd:dateTime"",
            ""@type"": ""Optional""
        },
        ""user_id"": ""xsd:string""
    }
  ]
I get 400 BAD REQUEST on creating Organization
{
   ""@type"":""Organization"",
   ""child"":[
      ""Organization_kitty"",
      ""Organization_DUNDRUM""
   ],
   ""collaborators"":[
      ""User_MASS""
   ],
   ""expiration_data"":"""",
   ""invitations"":[
      {
         ""email_to"":""hello"",
         ""invited_by"":""User_Mocking"",
         ""status"":""needs_invite""
      },
      {
         ""email_to"":""monkey"",
         ""invited_by"":""User_Mocking"",
         ""status"":""needs_invite""
      }
   ],
   ""organization_name"":""withsubscription"",
   ""status"":""invite_sent"",
   ""stripe_subscription"":[
      {
         ""billing_email"":""somewkjf"",
         ""status"":""active"",
         ""stripe_id"":""KItty"",
         ""stripe_quantity"":""32"",
         ""stripe_user"":""User_Mocking"",
         ""subscription_id"":""932438238429384ASBJDA""
      }
   ],
   ""creation_date"":""2011-01-01 01:00:37""
}
If I sent stripe_subscription as a json and not in an array (since its not a list or set) then I get 500 Internal error
{
   ""@type"":""Organization"",
   ""child"":[
      ""Organization_kitty"",
      ""Organization_DUNDRUM""
   ],
   ""collaborators"":[
      ""User_MASS""
   ],
   ""expiration_data"":"""",
   ""invitations"":[
      {
         ""email_to"":""hello"",
         ""invited_by"":""User_Mocking"",
         ""status"":""needs_invite""
      },
      {
         ""email_to"":""monkey"",
         ""invited_by"":""User_Mocking"",
         ""status"":""needs_invite""
      }
   ],
   ""organization_name"":""withsubscription"",
   ""status"":""invite_sent"",
   ""stripe_subscription"": {
         ""billing_email"":""somewkjf"",
         ""status"":""active"",
         ""stripe_id"":""KItty"",
         ""stripe_quantity"":""32"",
         ""stripe_user"":""User_Mocking"",
         ""subscription_id"":""932438238429384ASBJDA""
      },
   ""creation_date"":""2011-01-01 01:00:37""
}
500 error is
{
  ""api:message"":""Error: document_has_no_type(json{billing_email:\""somewkjf\"",status:\""active\"",stripe_id:\""KItty\"",stripe_quantity:\""32\"",stripe_user:\""User_Mocking\"",subscription_id:\""932438238429384ASBJDA\""})\n\n\nNote: some frames are missing due to last-call optimization.\nRe-run your program in debug mode (:- debug.) to get more detail.\n\n"",
  ""api:status"":""api:server_error""
}
Note: this maybe an error which I am doing, but let me know the correct way of sending a stripe_subscription which is a simple sub document with no type",2021-08-13T13:54:10Z,spl,https://github.com/terminusdb/terminusdb/issues/394#issuecomment-898473846,I got a 500 Internal Server Error in both cases.,i get a 500 internal server error in both case,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,394,2021-08-10T12:20:12Z,KittyJose,Error in creating a normal subdocument which is not List/Set,https://github.com/terminusdb/terminusdb/issues/394,"Describe the bug
Error in creating a normal subdocument which is not List/Set
To Reproduce
Schema used is
[
    {
        ""@abstract"": [],
        ""@id"": ""Entity"",
        ""@type"": ""Class"",
        ""status"": ""Status""
    },
    {
        ""@id"": ""Invitation"",
        ""@inherits"": ""Entity"",
        ""@key"": {
            ""@type"": ""ValueHash""
        },
        ""@subdocument"": [],
        ""@type"": ""Class"",
        ""email_to"": ""xsd:string"",
        ""invited_by"": ""User"",
        ""note"": {
            ""@class"": ""xsd:string"",
            ""@type"": ""Optional""
        },
        ""sent_date"": {
            ""@class"": ""xsd:dateTime"",
            ""@type"": ""Optional""
        }
    },
    {
        ""@id"": ""Organization"",
        ""@inherits"": ""Entity"",
        ""@key"": {
            ""@fields"": [
                ""organization_name""
            ],
            ""@type"": ""Lexical""
        },
        ""@type"": ""Class"",
        ""child"": {
            ""@class"": ""Organization"",
            ""@type"": ""Set""
        },
        ""collaborators"": {
            ""@class"": ""User"",
            ""@type"": ""Set""
        },
        ""creation_date"": ""xsd:dateTime"",
        ""expiration_data"": {
            ""@class"": ""xsd:dateTime"",
            ""@type"": ""Optional""
        },
        ""invitations"": {
            ""@class"": ""Invitation"",
            ""@type"": ""Set""
        },
        ""organization_name"": ""xsd:string"",
        ""owned_by"": ""User"",
        ""stripe_subscription"": ""StripeSubscription""
    },
    {
        ""@id"": ""Personal"",
        ""@inherits"": ""Organization"",
        ""@type"": ""Class""
    },
    {
        ""@id"": ""Status"",
        ""@type"": ""Enum"",
        ""@value"": [
            ""pending"",
            ""inactive"",
            ""active"",
            ""needs_invite"",
            ""invite_sent"",
            ""accepted"",
            ""rejected""
        ]
    },
    {
        ""@id"": ""StripeSubscription"",
        ""@inherits"": ""Entity"",
        ""@subdocument"": [],
        ""@type"": ""Class"",
        ""billing_email"": ""xsd:string"",
        ""stripe_id"": ""xsd:string"",
        ""stripe_quantity"": ""xsd:decimal"",
        ""stripe_user"": ""User"",
        ""subscription_id"": ""xsd:string""
    },
    {
        ""@id"": ""Team"",
        ""@inherits"": ""Organization"",
        ""@type"": ""Class""
    },
    {
        ""@id"": ""User"",
        ""@inherits"": ""Entity"",
        ""@key"": {
            ""@fields"": [
                ""user_id""
            ],
            ""@type"": ""Lexical""
        },
        ""@type"": ""Class"",
        ""company"": ""xsd:string"",
        ""email"": ""xsd:string"",
        ""first_name"": ""xsd:string"",
        ""last_name"": ""xsd:string"",
        ""picture"": ""xsd:string"",
        ""registration_date"": {
            ""@class"": ""xsd:dateTime"",
            ""@type"": ""Optional""
        },
        ""user_id"": ""xsd:string""
    }
  ]
I get 400 BAD REQUEST on creating Organization
{
   ""@type"":""Organization"",
   ""child"":[
      ""Organization_kitty"",
      ""Organization_DUNDRUM""
   ],
   ""collaborators"":[
      ""User_MASS""
   ],
   ""expiration_data"":"""",
   ""invitations"":[
      {
         ""email_to"":""hello"",
         ""invited_by"":""User_Mocking"",
         ""status"":""needs_invite""
      },
      {
         ""email_to"":""monkey"",
         ""invited_by"":""User_Mocking"",
         ""status"":""needs_invite""
      }
   ],
   ""organization_name"":""withsubscription"",
   ""status"":""invite_sent"",
   ""stripe_subscription"":[
      {
         ""billing_email"":""somewkjf"",
         ""status"":""active"",
         ""stripe_id"":""KItty"",
         ""stripe_quantity"":""32"",
         ""stripe_user"":""User_Mocking"",
         ""subscription_id"":""932438238429384ASBJDA""
      }
   ],
   ""creation_date"":""2011-01-01 01:00:37""
}
If I sent stripe_subscription as a json and not in an array (since its not a list or set) then I get 500 Internal error
{
   ""@type"":""Organization"",
   ""child"":[
      ""Organization_kitty"",
      ""Organization_DUNDRUM""
   ],
   ""collaborators"":[
      ""User_MASS""
   ],
   ""expiration_data"":"""",
   ""invitations"":[
      {
         ""email_to"":""hello"",
         ""invited_by"":""User_Mocking"",
         ""status"":""needs_invite""
      },
      {
         ""email_to"":""monkey"",
         ""invited_by"":""User_Mocking"",
         ""status"":""needs_invite""
      }
   ],
   ""organization_name"":""withsubscription"",
   ""status"":""invite_sent"",
   ""stripe_subscription"": {
         ""billing_email"":""somewkjf"",
         ""status"":""active"",
         ""stripe_id"":""KItty"",
         ""stripe_quantity"":""32"",
         ""stripe_user"":""User_Mocking"",
         ""subscription_id"":""932438238429384ASBJDA""
      },
   ""creation_date"":""2011-01-01 01:00:37""
}
500 error is
{
  ""api:message"":""Error: document_has_no_type(json{billing_email:\""somewkjf\"",status:\""active\"",stripe_id:\""KItty\"",stripe_quantity:\""32\"",stripe_user:\""User_Mocking\"",subscription_id:\""932438238429384ASBJDA\""})\n\n\nNote: some frames are missing due to last-call optimization.\nRe-run your program in debug mode (:- debug.) to get more detail.\n\n"",
  ""api:status"":""api:server_error""
}
Note: this maybe an error which I am doing, but let me know the correct way of sending a stripe_subscription which is a simple sub document with no type",2021-08-13T14:52:13Z,spl,https://github.com/terminusdb/terminusdb/issues/394#issuecomment-898512347,"I think the document_has_no_type error means that the stripe_subscription subdocument does not have a @type, so let's see what happens if I add it.
diff -uw create-org-{2,3}.json

--- create-org-2.json	2021-08-13 15:52:32.435367846 +0200
+++ create-org-3.json	2021-08-13 16:07:10.094210627 +0200
@@ -23,6 +23,7 @@
   ""organization_name"": ""withsubscription"",
   ""status"": ""invite_sent"",
   ""stripe_subscription"": {
+    ""@type"": ""StripeSubscription"",
     ""billing_email"": ""somewkjf"",
     ""status"": ""active"",
     ""stripe_id"": ""KItty"",
cat create-org-3.json | xh 'http://localhost:6363/api/document/admin/testdb?author=author&message=message'

HTTP/1.1 500 Internal Server Error
connection: keep-alive
content-length: 857
content-type: application/json
date: Fri, 13 Aug 2021 14:07:35 GMT
{
    ""api:message"": ""Error: no_id(json{'@type':'http://schema/StripeSubscription','http://schema/billing_email':json{'@type':'http://www.w3.org/2001/XMLSchema#string','@value':\""somewkjf\""},'http://schema/status':json{'@id':'http://schema/Status_active','@type':\""@id\""},'http://schema/stripe_id':json{'@type':'http://www.w3.org/2001/XMLSchema#string','@value':\""KItty\""},'http://schema/stripe_quantity':json{'@type':'http://www.w3.org/2001/XMLSchema#decimal','@value':\""32\""},'http://schema/stripe_user':json{'@id':'http://base/User_Mocking','@type':\""@id\""},'http://schema/subscription_id':json{'@type':'http://www.w3.org/2001/XMLSchema#string','@value':\""932438238429384ASBJDA\""}})\n\n\nNote: some frames are missing due to last-call optimization.\nRe-run your program in debug mode (:- debug.) to get more detail.\n\n"",
    ""api:status"": ""api:server_error""
}

Then again, maybe I'm wrong.","i think the document_has_no_type error mean that the stripe_subscription subdocument do not have a @type so let -PRON- see what happen if i add -PRON- diff -uw create - org-{23}json --- create - org-2json 2021 - 08 - 13 155232435367846 +0200 + + + create - org-3json 2021 - 08 - 13 160710094210627 +0200 @@ -236 +237 @@ "" organization_name "" "" withsubscription "" "" status "" "" invite_sent "" "" stripe_subscription "" { + "" @type "" "" stripesubscription "" "" billing_email "" "" somewkjf "" "" status "" "" active "" "" stripe_id "" "" kitty "" cat create - org-3json | xh ' http//localhost6363 / api / document / admin / testdbauthor = author&message = message ' http/11 500 internal server error connection keep - alive content - length 857 content - type application / json date fri 13 aug 2021 140735 gmt { "" apimessage "" "" error no_id(json{'@type''http//schema / stripesubscription''http//schema / billing_email'json{'@type''http//wwww3org/2001 / xmlschema#string''@value'\""somewkjf\""}'http//schema / status'json{'@id''http//schema / status_active''@type'\""@id\""}'http//schema / stripe_id'json{'@type''http//wwww3org/2001 / xmlschema#string''@value'\""kitty\""}'http//schema / stripe_quantity'json{'@type''http//wwww3org/2001 / xmlschema#decimal''@value'\""32\""}'http//schema / stripe_user'json{'@id''http//base / user_mocking''@type'\""@id\""}'http//schema / subscription_id'json{'@type''http//wwww3org/2001 / xmlschema#string''@value'\""932438238429384asbjda\""}})\n\n\nnote some frame be miss due to last - call optimization\nre - run -PRON- program in debug mode ( - debug ) to get more detail\n\n "" "" apistatus "" "" apiserver_error "" } then again maybe -PRON- be wrong",0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,394,2021-08-10T12:20:12Z,KittyJose,Error in creating a normal subdocument which is not List/Set,https://github.com/terminusdb/terminusdb/issues/394,"Describe the bug
Error in creating a normal subdocument which is not List/Set
To Reproduce
Schema used is
[
    {
        ""@abstract"": [],
        ""@id"": ""Entity"",
        ""@type"": ""Class"",
        ""status"": ""Status""
    },
    {
        ""@id"": ""Invitation"",
        ""@inherits"": ""Entity"",
        ""@key"": {
            ""@type"": ""ValueHash""
        },
        ""@subdocument"": [],
        ""@type"": ""Class"",
        ""email_to"": ""xsd:string"",
        ""invited_by"": ""User"",
        ""note"": {
            ""@class"": ""xsd:string"",
            ""@type"": ""Optional""
        },
        ""sent_date"": {
            ""@class"": ""xsd:dateTime"",
            ""@type"": ""Optional""
        }
    },
    {
        ""@id"": ""Organization"",
        ""@inherits"": ""Entity"",
        ""@key"": {
            ""@fields"": [
                ""organization_name""
            ],
            ""@type"": ""Lexical""
        },
        ""@type"": ""Class"",
        ""child"": {
            ""@class"": ""Organization"",
            ""@type"": ""Set""
        },
        ""collaborators"": {
            ""@class"": ""User"",
            ""@type"": ""Set""
        },
        ""creation_date"": ""xsd:dateTime"",
        ""expiration_data"": {
            ""@class"": ""xsd:dateTime"",
            ""@type"": ""Optional""
        },
        ""invitations"": {
            ""@class"": ""Invitation"",
            ""@type"": ""Set""
        },
        ""organization_name"": ""xsd:string"",
        ""owned_by"": ""User"",
        ""stripe_subscription"": ""StripeSubscription""
    },
    {
        ""@id"": ""Personal"",
        ""@inherits"": ""Organization"",
        ""@type"": ""Class""
    },
    {
        ""@id"": ""Status"",
        ""@type"": ""Enum"",
        ""@value"": [
            ""pending"",
            ""inactive"",
            ""active"",
            ""needs_invite"",
            ""invite_sent"",
            ""accepted"",
            ""rejected""
        ]
    },
    {
        ""@id"": ""StripeSubscription"",
        ""@inherits"": ""Entity"",
        ""@subdocument"": [],
        ""@type"": ""Class"",
        ""billing_email"": ""xsd:string"",
        ""stripe_id"": ""xsd:string"",
        ""stripe_quantity"": ""xsd:decimal"",
        ""stripe_user"": ""User"",
        ""subscription_id"": ""xsd:string""
    },
    {
        ""@id"": ""Team"",
        ""@inherits"": ""Organization"",
        ""@type"": ""Class""
    },
    {
        ""@id"": ""User"",
        ""@inherits"": ""Entity"",
        ""@key"": {
            ""@fields"": [
                ""user_id""
            ],
            ""@type"": ""Lexical""
        },
        ""@type"": ""Class"",
        ""company"": ""xsd:string"",
        ""email"": ""xsd:string"",
        ""first_name"": ""xsd:string"",
        ""last_name"": ""xsd:string"",
        ""picture"": ""xsd:string"",
        ""registration_date"": {
            ""@class"": ""xsd:dateTime"",
            ""@type"": ""Optional""
        },
        ""user_id"": ""xsd:string""
    }
  ]
I get 400 BAD REQUEST on creating Organization
{
   ""@type"":""Organization"",
   ""child"":[
      ""Organization_kitty"",
      ""Organization_DUNDRUM""
   ],
   ""collaborators"":[
      ""User_MASS""
   ],
   ""expiration_data"":"""",
   ""invitations"":[
      {
         ""email_to"":""hello"",
         ""invited_by"":""User_Mocking"",
         ""status"":""needs_invite""
      },
      {
         ""email_to"":""monkey"",
         ""invited_by"":""User_Mocking"",
         ""status"":""needs_invite""
      }
   ],
   ""organization_name"":""withsubscription"",
   ""status"":""invite_sent"",
   ""stripe_subscription"":[
      {
         ""billing_email"":""somewkjf"",
         ""status"":""active"",
         ""stripe_id"":""KItty"",
         ""stripe_quantity"":""32"",
         ""stripe_user"":""User_Mocking"",
         ""subscription_id"":""932438238429384ASBJDA""
      }
   ],
   ""creation_date"":""2011-01-01 01:00:37""
}
If I sent stripe_subscription as a json and not in an array (since its not a list or set) then I get 500 Internal error
{
   ""@type"":""Organization"",
   ""child"":[
      ""Organization_kitty"",
      ""Organization_DUNDRUM""
   ],
   ""collaborators"":[
      ""User_MASS""
   ],
   ""expiration_data"":"""",
   ""invitations"":[
      {
         ""email_to"":""hello"",
         ""invited_by"":""User_Mocking"",
         ""status"":""needs_invite""
      },
      {
         ""email_to"":""monkey"",
         ""invited_by"":""User_Mocking"",
         ""status"":""needs_invite""
      }
   ],
   ""organization_name"":""withsubscription"",
   ""status"":""invite_sent"",
   ""stripe_subscription"": {
         ""billing_email"":""somewkjf"",
         ""status"":""active"",
         ""stripe_id"":""KItty"",
         ""stripe_quantity"":""32"",
         ""stripe_user"":""User_Mocking"",
         ""subscription_id"":""932438238429384ASBJDA""
      },
   ""creation_date"":""2011-01-01 01:00:37""
}
500 error is
{
  ""api:message"":""Error: document_has_no_type(json{billing_email:\""somewkjf\"",status:\""active\"",stripe_id:\""KItty\"",stripe_quantity:\""32\"",stripe_user:\""User_Mocking\"",subscription_id:\""932438238429384ASBJDA\""})\n\n\nNote: some frames are missing due to last-call optimization.\nRe-run your program in debug mode (:- debug.) to get more detail.\n\n"",
  ""api:status"":""api:server_error""
}
Note: this maybe an error which I am doing, but let me know the correct way of sending a stripe_subscription which is a simple sub document with no type",2021-08-13T15:10:00Z,spl,https://github.com/terminusdb/terminusdb/issues/394#issuecomment-898531268,"Here's the working JSON:
{
  ""@type"": ""Organization"",
  ""child"": [
    ""Organization_kitty"",
    ""Organization_DUNDRUM""
  ],
  ""collaborators"": [
    ""User_MASS""
  ],
  ""invitations"": [
    {
      ""email_to"": ""hello"",
      ""invited_by"": ""User_Mocking"",
      ""status"": ""needs_invite""
    },
    {
      ""email_to"": ""monkey"",
      ""invited_by"": ""User_Mocking"",
      ""status"": ""needs_invite""
    }
  ],
  ""organization_name"": ""withsubscription"",
  ""status"": ""invite_sent"",
  ""stripe_subscription"": {
    ""@type"": ""StripeSubscription"",
    ""billing_email"": ""somewkjf"",
    ""status"": ""active"",
    ""stripe_id"": ""KItty"",
    ""stripe_quantity"": ""32"",
    ""stripe_user"": ""User_Mocking"",
    ""subscription_id"": ""932438238429384ASBJDA""
  },
  ""creation_date"": ""2011-01-01T01:00:37Z""
}","here be the work json { "" @type "" "" organization "" "" child "" [ "" organization_kitty "" "" organization_dundrum "" ] "" collaborator "" [ "" user_mass "" ] "" invitation "" [ { "" email_to "" "" hello "" "" invited_by "" "" user_mocke "" "" status "" "" needs_invite "" } { "" email_to "" "" monkey "" "" invited_by "" "" user_mocke "" "" status "" "" needs_invite "" } ] "" organization_name "" "" withsubscription "" "" status "" "" invite_sent "" "" stripe_subscription "" { "" @type "" "" stripesubscription "" "" billing_email "" "" somewkjf "" "" status "" "" active "" "" stripe_id "" "" kitty "" "" stripe_quantity "" "" 32 "" "" stripe_user "" "" user_mocke "" "" subscription_id "" "" 932438238429384asbjda "" } "" creation_date "" "" 2011 - 01 - 01t010037z "" }",0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,394,2021-08-10T12:20:12Z,KittyJose,Error in creating a normal subdocument which is not List/Set,https://github.com/terminusdb/terminusdb/issues/394,"Describe the bug
Error in creating a normal subdocument which is not List/Set
To Reproduce
Schema used is
[
    {
        ""@abstract"": [],
        ""@id"": ""Entity"",
        ""@type"": ""Class"",
        ""status"": ""Status""
    },
    {
        ""@id"": ""Invitation"",
        ""@inherits"": ""Entity"",
        ""@key"": {
            ""@type"": ""ValueHash""
        },
        ""@subdocument"": [],
        ""@type"": ""Class"",
        ""email_to"": ""xsd:string"",
        ""invited_by"": ""User"",
        ""note"": {
            ""@class"": ""xsd:string"",
            ""@type"": ""Optional""
        },
        ""sent_date"": {
            ""@class"": ""xsd:dateTime"",
            ""@type"": ""Optional""
        }
    },
    {
        ""@id"": ""Organization"",
        ""@inherits"": ""Entity"",
        ""@key"": {
            ""@fields"": [
                ""organization_name""
            ],
            ""@type"": ""Lexical""
        },
        ""@type"": ""Class"",
        ""child"": {
            ""@class"": ""Organization"",
            ""@type"": ""Set""
        },
        ""collaborators"": {
            ""@class"": ""User"",
            ""@type"": ""Set""
        },
        ""creation_date"": ""xsd:dateTime"",
        ""expiration_data"": {
            ""@class"": ""xsd:dateTime"",
            ""@type"": ""Optional""
        },
        ""invitations"": {
            ""@class"": ""Invitation"",
            ""@type"": ""Set""
        },
        ""organization_name"": ""xsd:string"",
        ""owned_by"": ""User"",
        ""stripe_subscription"": ""StripeSubscription""
    },
    {
        ""@id"": ""Personal"",
        ""@inherits"": ""Organization"",
        ""@type"": ""Class""
    },
    {
        ""@id"": ""Status"",
        ""@type"": ""Enum"",
        ""@value"": [
            ""pending"",
            ""inactive"",
            ""active"",
            ""needs_invite"",
            ""invite_sent"",
            ""accepted"",
            ""rejected""
        ]
    },
    {
        ""@id"": ""StripeSubscription"",
        ""@inherits"": ""Entity"",
        ""@subdocument"": [],
        ""@type"": ""Class"",
        ""billing_email"": ""xsd:string"",
        ""stripe_id"": ""xsd:string"",
        ""stripe_quantity"": ""xsd:decimal"",
        ""stripe_user"": ""User"",
        ""subscription_id"": ""xsd:string""
    },
    {
        ""@id"": ""Team"",
        ""@inherits"": ""Organization"",
        ""@type"": ""Class""
    },
    {
        ""@id"": ""User"",
        ""@inherits"": ""Entity"",
        ""@key"": {
            ""@fields"": [
                ""user_id""
            ],
            ""@type"": ""Lexical""
        },
        ""@type"": ""Class"",
        ""company"": ""xsd:string"",
        ""email"": ""xsd:string"",
        ""first_name"": ""xsd:string"",
        ""last_name"": ""xsd:string"",
        ""picture"": ""xsd:string"",
        ""registration_date"": {
            ""@class"": ""xsd:dateTime"",
            ""@type"": ""Optional""
        },
        ""user_id"": ""xsd:string""
    }
  ]
I get 400 BAD REQUEST on creating Organization
{
   ""@type"":""Organization"",
   ""child"":[
      ""Organization_kitty"",
      ""Organization_DUNDRUM""
   ],
   ""collaborators"":[
      ""User_MASS""
   ],
   ""expiration_data"":"""",
   ""invitations"":[
      {
         ""email_to"":""hello"",
         ""invited_by"":""User_Mocking"",
         ""status"":""needs_invite""
      },
      {
         ""email_to"":""monkey"",
         ""invited_by"":""User_Mocking"",
         ""status"":""needs_invite""
      }
   ],
   ""organization_name"":""withsubscription"",
   ""status"":""invite_sent"",
   ""stripe_subscription"":[
      {
         ""billing_email"":""somewkjf"",
         ""status"":""active"",
         ""stripe_id"":""KItty"",
         ""stripe_quantity"":""32"",
         ""stripe_user"":""User_Mocking"",
         ""subscription_id"":""932438238429384ASBJDA""
      }
   ],
   ""creation_date"":""2011-01-01 01:00:37""
}
If I sent stripe_subscription as a json and not in an array (since its not a list or set) then I get 500 Internal error
{
   ""@type"":""Organization"",
   ""child"":[
      ""Organization_kitty"",
      ""Organization_DUNDRUM""
   ],
   ""collaborators"":[
      ""User_MASS""
   ],
   ""expiration_data"":"""",
   ""invitations"":[
      {
         ""email_to"":""hello"",
         ""invited_by"":""User_Mocking"",
         ""status"":""needs_invite""
      },
      {
         ""email_to"":""monkey"",
         ""invited_by"":""User_Mocking"",
         ""status"":""needs_invite""
      }
   ],
   ""organization_name"":""withsubscription"",
   ""status"":""invite_sent"",
   ""stripe_subscription"": {
         ""billing_email"":""somewkjf"",
         ""status"":""active"",
         ""stripe_id"":""KItty"",
         ""stripe_quantity"":""32"",
         ""stripe_user"":""User_Mocking"",
         ""subscription_id"":""932438238429384ASBJDA""
      },
   ""creation_date"":""2011-01-01 01:00:37""
}
500 error is
{
  ""api:message"":""Error: document_has_no_type(json{billing_email:\""somewkjf\"",status:\""active\"",stripe_id:\""KItty\"",stripe_quantity:\""32\"",stripe_user:\""User_Mocking\"",subscription_id:\""932438238429384ASBJDA\""})\n\n\nNote: some frames are missing due to last-call optimization.\nRe-run your program in debug mode (:- debug.) to get more detail.\n\n"",
  ""api:status"":""api:server_error""
}
Note: this maybe an error which I am doing, but let me know the correct way of sending a stripe_subscription which is a simple sub document with no type",2021-08-13T15:11:58Z,spl,https://github.com/terminusdb/terminusdb/issues/394#issuecomment-898532544,"And the working schema:
[
  {
    ""@abstract"": [],
    ""@id"": ""Entity"",
    ""@type"": ""Class"",
    ""status"": ""Status""
  },
  {
    ""@id"": ""Invitation"",
    ""@inherits"": ""Entity"",
    ""@key"": {
      ""@type"": ""ValueHash""
    },
    ""@subdocument"": [],
    ""@type"": ""Class"",
    ""email_to"": ""xsd:string"",
    ""invited_by"": ""User"",
    ""note"": {
      ""@class"": ""xsd:string"",
      ""@type"": ""Optional""
    },
    ""sent_date"": {
      ""@class"": ""xsd:dateTime"",
      ""@type"": ""Optional""
    }
  },
  {
    ""@id"": ""Organization"",
    ""@inherits"": ""Entity"",
    ""@key"": {
      ""@fields"": [
        ""organization_name""
      ],
      ""@type"": ""Lexical""
    },
    ""@type"": ""Class"",
    ""child"": {
      ""@class"": ""Organization"",
      ""@type"": ""Set""
    },
    ""collaborators"": {
      ""@class"": ""User"",
      ""@type"": ""Set""
    },
    ""creation_date"": ""xsd:dateTime"",
    ""expiration_data"": {
      ""@class"": ""xsd:dateTime"",
      ""@type"": ""Optional""
    },
    ""invitations"": {
      ""@class"": ""Invitation"",
      ""@type"": ""Set""
    },
    ""organization_name"": ""xsd:string"",
    ""owned_by"": ""User"",
    ""stripe_subscription"": ""StripeSubscription""
  },
  {
    ""@id"": ""Personal"",
    ""@inherits"": ""Organization"",
    ""@type"": ""Class""
  },
  {
    ""@id"": ""Status"",
    ""@type"": ""Enum"",
    ""@value"": [
      ""pending"",
      ""inactive"",
      ""active"",
      ""needs_invite"",
      ""invite_sent"",
      ""accepted"",
      ""rejected""
    ]
  },
  {
    ""@id"": ""StripeSubscription"",
    ""@inherits"": ""Entity"",
    ""@subdocument"": [],
    ""@type"": ""Class"",
    ""@key"" : {
      ""@type"": ""Random""
    },
    ""billing_email"": ""xsd:string"",
    ""stripe_id"": ""xsd:string"",
    ""stripe_quantity"": ""xsd:decimal"",
    ""stripe_user"": ""User"",
    ""subscription_id"": ""xsd:string""
  },
  {
    ""@id"": ""Team"",
    ""@inherits"": ""Organization"",
    ""@type"": ""Class""
  },
  {
    ""@id"": ""User"",
    ""@inherits"": ""Entity"",
    ""@key"": {
      ""@fields"": [
        ""user_id""
      ],
      ""@type"": ""Lexical""
    },
    ""@type"": ""Class"",
    ""company"": ""xsd:string"",
    ""email"": ""xsd:string"",
    ""first_name"": ""xsd:string"",
    ""last_name"": ""xsd:string"",
    ""picture"": ""xsd:string"",
    ""registration_date"": {
      ""@class"": ""xsd:dateTime"",
      ""@type"": ""Optional""
    },
    ""user_id"": ""xsd:string""
  }
]","and the working schema [ { "" @abstract "" [ ] "" @id "" "" entity "" "" @type "" "" class "" "" status "" "" status "" } { "" @id "" "" invitation "" "" @inherit "" "" entity "" "" @key "" { "" @type "" "" valuehash "" } "" @subdocument "" [ ] "" @type "" "" class "" "" email_to "" "" xsdstre "" "" invited_by "" "" user "" "" note "" { "" @class "" "" xsdstring "" "" @type "" "" optional "" } "" sent_date "" { "" @class "" "" xsddatetime "" "" @type "" "" optional "" } } { "" @id "" "" organization "" "" @inherit "" "" entity "" "" @key "" { "" @field "" [ "" organization_name "" ] "" @type "" "" lexical "" } "" @type "" "" class "" "" child "" { "" @class "" "" organization "" "" @type "" "" set "" } "" collaborator "" { "" @class "" "" user "" "" @type "" "" set "" } "" creation_date "" "" xsddatetime "" "" expiration_data "" { "" @class "" "" xsddatetime "" "" @type "" "" optional "" } "" invitation "" { "" @class "" "" invitation "" "" @type "" "" set "" } "" organization_name "" "" xsdstring "" "" owned_by "" "" user "" "" stripe_subscription "" "" stripesubscription "" } { "" @id "" "" personal "" "" @inherit "" "" organization "" "" @type "" "" class "" } { "" @id "" "" status "" "" @type "" "" enum "" "" @value "" [ "" pende "" "" inactive "" "" active "" "" needs_invite "" "" invite_sent "" "" accept "" "" reject "" ] } { "" @id "" "" stripesubscription "" "" @inherit "" "" entity "" "" @subdocument "" [ ] "" @type "" "" class "" "" @key "" { "" @type "" "" random "" } "" billing_email "" "" xsdstre "" "" stripe_id "" "" xsdstre "" "" stripe_quantity "" "" xsddecimal "" "" stripe_user "" "" user "" "" subscription_id "" "" xsdstring "" } { "" @id "" "" team "" "" @inherit "" "" organization "" "" @type "" "" class "" } { "" @id "" "" user "" "" @inherit "" "" entity "" "" @key "" { "" @field "" [ "" user_id "" ] "" @type "" "" lexical "" } "" @type "" "" class "" "" company "" "" xsdstre "" "" email "" "" xsdstre "" "" first_name "" "" xsdstring "" "" last_name "" "" xsdstre "" "" picture "" "" xsdstring "" "" registration_date "" { "" @class "" "" xsddatetime "" "" @type "" "" optional "" } "" user_id "" "" xsdstring "" } ]",0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,394,2021-08-10T12:20:12Z,KittyJose,Error in creating a normal subdocument which is not List/Set,https://github.com/terminusdb/terminusdb/issues/394,"Describe the bug
Error in creating a normal subdocument which is not List/Set
To Reproduce
Schema used is
[
    {
        ""@abstract"": [],
        ""@id"": ""Entity"",
        ""@type"": ""Class"",
        ""status"": ""Status""
    },
    {
        ""@id"": ""Invitation"",
        ""@inherits"": ""Entity"",
        ""@key"": {
            ""@type"": ""ValueHash""
        },
        ""@subdocument"": [],
        ""@type"": ""Class"",
        ""email_to"": ""xsd:string"",
        ""invited_by"": ""User"",
        ""note"": {
            ""@class"": ""xsd:string"",
            ""@type"": ""Optional""
        },
        ""sent_date"": {
            ""@class"": ""xsd:dateTime"",
            ""@type"": ""Optional""
        }
    },
    {
        ""@id"": ""Organization"",
        ""@inherits"": ""Entity"",
        ""@key"": {
            ""@fields"": [
                ""organization_name""
            ],
            ""@type"": ""Lexical""
        },
        ""@type"": ""Class"",
        ""child"": {
            ""@class"": ""Organization"",
            ""@type"": ""Set""
        },
        ""collaborators"": {
            ""@class"": ""User"",
            ""@type"": ""Set""
        },
        ""creation_date"": ""xsd:dateTime"",
        ""expiration_data"": {
            ""@class"": ""xsd:dateTime"",
            ""@type"": ""Optional""
        },
        ""invitations"": {
            ""@class"": ""Invitation"",
            ""@type"": ""Set""
        },
        ""organization_name"": ""xsd:string"",
        ""owned_by"": ""User"",
        ""stripe_subscription"": ""StripeSubscription""
    },
    {
        ""@id"": ""Personal"",
        ""@inherits"": ""Organization"",
        ""@type"": ""Class""
    },
    {
        ""@id"": ""Status"",
        ""@type"": ""Enum"",
        ""@value"": [
            ""pending"",
            ""inactive"",
            ""active"",
            ""needs_invite"",
            ""invite_sent"",
            ""accepted"",
            ""rejected""
        ]
    },
    {
        ""@id"": ""StripeSubscription"",
        ""@inherits"": ""Entity"",
        ""@subdocument"": [],
        ""@type"": ""Class"",
        ""billing_email"": ""xsd:string"",
        ""stripe_id"": ""xsd:string"",
        ""stripe_quantity"": ""xsd:decimal"",
        ""stripe_user"": ""User"",
        ""subscription_id"": ""xsd:string""
    },
    {
        ""@id"": ""Team"",
        ""@inherits"": ""Organization"",
        ""@type"": ""Class""
    },
    {
        ""@id"": ""User"",
        ""@inherits"": ""Entity"",
        ""@key"": {
            ""@fields"": [
                ""user_id""
            ],
            ""@type"": ""Lexical""
        },
        ""@type"": ""Class"",
        ""company"": ""xsd:string"",
        ""email"": ""xsd:string"",
        ""first_name"": ""xsd:string"",
        ""last_name"": ""xsd:string"",
        ""picture"": ""xsd:string"",
        ""registration_date"": {
            ""@class"": ""xsd:dateTime"",
            ""@type"": ""Optional""
        },
        ""user_id"": ""xsd:string""
    }
  ]
I get 400 BAD REQUEST on creating Organization
{
   ""@type"":""Organization"",
   ""child"":[
      ""Organization_kitty"",
      ""Organization_DUNDRUM""
   ],
   ""collaborators"":[
      ""User_MASS""
   ],
   ""expiration_data"":"""",
   ""invitations"":[
      {
         ""email_to"":""hello"",
         ""invited_by"":""User_Mocking"",
         ""status"":""needs_invite""
      },
      {
         ""email_to"":""monkey"",
         ""invited_by"":""User_Mocking"",
         ""status"":""needs_invite""
      }
   ],
   ""organization_name"":""withsubscription"",
   ""status"":""invite_sent"",
   ""stripe_subscription"":[
      {
         ""billing_email"":""somewkjf"",
         ""status"":""active"",
         ""stripe_id"":""KItty"",
         ""stripe_quantity"":""32"",
         ""stripe_user"":""User_Mocking"",
         ""subscription_id"":""932438238429384ASBJDA""
      }
   ],
   ""creation_date"":""2011-01-01 01:00:37""
}
If I sent stripe_subscription as a json and not in an array (since its not a list or set) then I get 500 Internal error
{
   ""@type"":""Organization"",
   ""child"":[
      ""Organization_kitty"",
      ""Organization_DUNDRUM""
   ],
   ""collaborators"":[
      ""User_MASS""
   ],
   ""expiration_data"":"""",
   ""invitations"":[
      {
         ""email_to"":""hello"",
         ""invited_by"":""User_Mocking"",
         ""status"":""needs_invite""
      },
      {
         ""email_to"":""monkey"",
         ""invited_by"":""User_Mocking"",
         ""status"":""needs_invite""
      }
   ],
   ""organization_name"":""withsubscription"",
   ""status"":""invite_sent"",
   ""stripe_subscription"": {
         ""billing_email"":""somewkjf"",
         ""status"":""active"",
         ""stripe_id"":""KItty"",
         ""stripe_quantity"":""32"",
         ""stripe_user"":""User_Mocking"",
         ""subscription_id"":""932438238429384ASBJDA""
      },
   ""creation_date"":""2011-01-01 01:00:37""
}
500 error is
{
  ""api:message"":""Error: document_has_no_type(json{billing_email:\""somewkjf\"",status:\""active\"",stripe_id:\""KItty\"",stripe_quantity:\""32\"",stripe_user:\""User_Mocking\"",subscription_id:\""932438238429384ASBJDA\""})\n\n\nNote: some frames are missing due to last-call optimization.\nRe-run your program in debug mode (:- debug.) to get more detail.\n\n"",
  ""api:status"":""api:server_error""
}
Note: this maybe an error which I am doing, but let me know the correct way of sending a stripe_subscription which is a simple sub document with no type",2021-08-13T15:13:01Z,spl,https://github.com/terminusdb/terminusdb/issues/394#issuecomment-898533281,"There were a couple of issues:

StripeSubscription needed an explicit key type: ""@key"" : { ""@type"": ""Random"" }
expiration_data should not be included if it's optional and not required.","there be a couple of issue stripesubscription need an explicit key type "" @key "" { "" @type "" "" random "" } expiration_data should not be include if -PRON- be optional and not require",0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,394,2021-08-10T12:20:12Z,KittyJose,Error in creating a normal subdocument which is not List/Set,https://github.com/terminusdb/terminusdb/issues/394,"Describe the bug
Error in creating a normal subdocument which is not List/Set
To Reproduce
Schema used is
[
    {
        ""@abstract"": [],
        ""@id"": ""Entity"",
        ""@type"": ""Class"",
        ""status"": ""Status""
    },
    {
        ""@id"": ""Invitation"",
        ""@inherits"": ""Entity"",
        ""@key"": {
            ""@type"": ""ValueHash""
        },
        ""@subdocument"": [],
        ""@type"": ""Class"",
        ""email_to"": ""xsd:string"",
        ""invited_by"": ""User"",
        ""note"": {
            ""@class"": ""xsd:string"",
            ""@type"": ""Optional""
        },
        ""sent_date"": {
            ""@class"": ""xsd:dateTime"",
            ""@type"": ""Optional""
        }
    },
    {
        ""@id"": ""Organization"",
        ""@inherits"": ""Entity"",
        ""@key"": {
            ""@fields"": [
                ""organization_name""
            ],
            ""@type"": ""Lexical""
        },
        ""@type"": ""Class"",
        ""child"": {
            ""@class"": ""Organization"",
            ""@type"": ""Set""
        },
        ""collaborators"": {
            ""@class"": ""User"",
            ""@type"": ""Set""
        },
        ""creation_date"": ""xsd:dateTime"",
        ""expiration_data"": {
            ""@class"": ""xsd:dateTime"",
            ""@type"": ""Optional""
        },
        ""invitations"": {
            ""@class"": ""Invitation"",
            ""@type"": ""Set""
        },
        ""organization_name"": ""xsd:string"",
        ""owned_by"": ""User"",
        ""stripe_subscription"": ""StripeSubscription""
    },
    {
        ""@id"": ""Personal"",
        ""@inherits"": ""Organization"",
        ""@type"": ""Class""
    },
    {
        ""@id"": ""Status"",
        ""@type"": ""Enum"",
        ""@value"": [
            ""pending"",
            ""inactive"",
            ""active"",
            ""needs_invite"",
            ""invite_sent"",
            ""accepted"",
            ""rejected""
        ]
    },
    {
        ""@id"": ""StripeSubscription"",
        ""@inherits"": ""Entity"",
        ""@subdocument"": [],
        ""@type"": ""Class"",
        ""billing_email"": ""xsd:string"",
        ""stripe_id"": ""xsd:string"",
        ""stripe_quantity"": ""xsd:decimal"",
        ""stripe_user"": ""User"",
        ""subscription_id"": ""xsd:string""
    },
    {
        ""@id"": ""Team"",
        ""@inherits"": ""Organization"",
        ""@type"": ""Class""
    },
    {
        ""@id"": ""User"",
        ""@inherits"": ""Entity"",
        ""@key"": {
            ""@fields"": [
                ""user_id""
            ],
            ""@type"": ""Lexical""
        },
        ""@type"": ""Class"",
        ""company"": ""xsd:string"",
        ""email"": ""xsd:string"",
        ""first_name"": ""xsd:string"",
        ""last_name"": ""xsd:string"",
        ""picture"": ""xsd:string"",
        ""registration_date"": {
            ""@class"": ""xsd:dateTime"",
            ""@type"": ""Optional""
        },
        ""user_id"": ""xsd:string""
    }
  ]
I get 400 BAD REQUEST on creating Organization
{
   ""@type"":""Organization"",
   ""child"":[
      ""Organization_kitty"",
      ""Organization_DUNDRUM""
   ],
   ""collaborators"":[
      ""User_MASS""
   ],
   ""expiration_data"":"""",
   ""invitations"":[
      {
         ""email_to"":""hello"",
         ""invited_by"":""User_Mocking"",
         ""status"":""needs_invite""
      },
      {
         ""email_to"":""monkey"",
         ""invited_by"":""User_Mocking"",
         ""status"":""needs_invite""
      }
   ],
   ""organization_name"":""withsubscription"",
   ""status"":""invite_sent"",
   ""stripe_subscription"":[
      {
         ""billing_email"":""somewkjf"",
         ""status"":""active"",
         ""stripe_id"":""KItty"",
         ""stripe_quantity"":""32"",
         ""stripe_user"":""User_Mocking"",
         ""subscription_id"":""932438238429384ASBJDA""
      }
   ],
   ""creation_date"":""2011-01-01 01:00:37""
}
If I sent stripe_subscription as a json and not in an array (since its not a list or set) then I get 500 Internal error
{
   ""@type"":""Organization"",
   ""child"":[
      ""Organization_kitty"",
      ""Organization_DUNDRUM""
   ],
   ""collaborators"":[
      ""User_MASS""
   ],
   ""expiration_data"":"""",
   ""invitations"":[
      {
         ""email_to"":""hello"",
         ""invited_by"":""User_Mocking"",
         ""status"":""needs_invite""
      },
      {
         ""email_to"":""monkey"",
         ""invited_by"":""User_Mocking"",
         ""status"":""needs_invite""
      }
   ],
   ""organization_name"":""withsubscription"",
   ""status"":""invite_sent"",
   ""stripe_subscription"": {
         ""billing_email"":""somewkjf"",
         ""status"":""active"",
         ""stripe_id"":""KItty"",
         ""stripe_quantity"":""32"",
         ""stripe_user"":""User_Mocking"",
         ""subscription_id"":""932438238429384ASBJDA""
      },
   ""creation_date"":""2011-01-01 01:00:37""
}
500 error is
{
  ""api:message"":""Error: document_has_no_type(json{billing_email:\""somewkjf\"",status:\""active\"",stripe_id:\""KItty\"",stripe_quantity:\""32\"",stripe_user:\""User_Mocking\"",subscription_id:\""932438238429384ASBJDA\""})\n\n\nNote: some frames are missing due to last-call optimization.\nRe-run your program in debug mode (:- debug.) to get more detail.\n\n"",
  ""api:status"":""api:server_error""
}
Note: this maybe an error which I am doing, but let me know the correct way of sending a stripe_subscription which is a simple sub document with no type",2021-08-17T08:51:23Z,KittyJose,https://github.com/terminusdb/terminusdb/issues/394#issuecomment-900115154,"OK, this works for me when I provide ""@type"": ""StripeSubscription"" and ""@key"" : { ""@type"": ""ValueHash"" }
I have set key of a subdocument to point to ValueHash/ Random always from the Model building tool.
My only comment would be to error out from backend as well (while creating schema) when attempting to set key of subdocument to something other than ValueHash/ Random. This is important because we can create a schema via JSON or  query.
Other than this I am good to close this issue :)","ok this work for -PRON- when i provide "" @type "" "" stripesubscription "" and "" @key "" { "" @type "" "" valuehash "" } i have set key of a subdocument to point to valuehash/ random always from the model building tool -PRON- only comment would be to error out from backend as well ( while create schema ) when attempt to set key of subdocument to something other than valuehash/ random this be important because -PRON- can create a schema via json or query other than this i be good to close this issue )",0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,394,2021-08-10T12:20:12Z,KittyJose,Error in creating a normal subdocument which is not List/Set,https://github.com/terminusdb/terminusdb/issues/394,"Describe the bug
Error in creating a normal subdocument which is not List/Set
To Reproduce
Schema used is
[
    {
        ""@abstract"": [],
        ""@id"": ""Entity"",
        ""@type"": ""Class"",
        ""status"": ""Status""
    },
    {
        ""@id"": ""Invitation"",
        ""@inherits"": ""Entity"",
        ""@key"": {
            ""@type"": ""ValueHash""
        },
        ""@subdocument"": [],
        ""@type"": ""Class"",
        ""email_to"": ""xsd:string"",
        ""invited_by"": ""User"",
        ""note"": {
            ""@class"": ""xsd:string"",
            ""@type"": ""Optional""
        },
        ""sent_date"": {
            ""@class"": ""xsd:dateTime"",
            ""@type"": ""Optional""
        }
    },
    {
        ""@id"": ""Organization"",
        ""@inherits"": ""Entity"",
        ""@key"": {
            ""@fields"": [
                ""organization_name""
            ],
            ""@type"": ""Lexical""
        },
        ""@type"": ""Class"",
        ""child"": {
            ""@class"": ""Organization"",
            ""@type"": ""Set""
        },
        ""collaborators"": {
            ""@class"": ""User"",
            ""@type"": ""Set""
        },
        ""creation_date"": ""xsd:dateTime"",
        ""expiration_data"": {
            ""@class"": ""xsd:dateTime"",
            ""@type"": ""Optional""
        },
        ""invitations"": {
            ""@class"": ""Invitation"",
            ""@type"": ""Set""
        },
        ""organization_name"": ""xsd:string"",
        ""owned_by"": ""User"",
        ""stripe_subscription"": ""StripeSubscription""
    },
    {
        ""@id"": ""Personal"",
        ""@inherits"": ""Organization"",
        ""@type"": ""Class""
    },
    {
        ""@id"": ""Status"",
        ""@type"": ""Enum"",
        ""@value"": [
            ""pending"",
            ""inactive"",
            ""active"",
            ""needs_invite"",
            ""invite_sent"",
            ""accepted"",
            ""rejected""
        ]
    },
    {
        ""@id"": ""StripeSubscription"",
        ""@inherits"": ""Entity"",
        ""@subdocument"": [],
        ""@type"": ""Class"",
        ""billing_email"": ""xsd:string"",
        ""stripe_id"": ""xsd:string"",
        ""stripe_quantity"": ""xsd:decimal"",
        ""stripe_user"": ""User"",
        ""subscription_id"": ""xsd:string""
    },
    {
        ""@id"": ""Team"",
        ""@inherits"": ""Organization"",
        ""@type"": ""Class""
    },
    {
        ""@id"": ""User"",
        ""@inherits"": ""Entity"",
        ""@key"": {
            ""@fields"": [
                ""user_id""
            ],
            ""@type"": ""Lexical""
        },
        ""@type"": ""Class"",
        ""company"": ""xsd:string"",
        ""email"": ""xsd:string"",
        ""first_name"": ""xsd:string"",
        ""last_name"": ""xsd:string"",
        ""picture"": ""xsd:string"",
        ""registration_date"": {
            ""@class"": ""xsd:dateTime"",
            ""@type"": ""Optional""
        },
        ""user_id"": ""xsd:string""
    }
  ]
I get 400 BAD REQUEST on creating Organization
{
   ""@type"":""Organization"",
   ""child"":[
      ""Organization_kitty"",
      ""Organization_DUNDRUM""
   ],
   ""collaborators"":[
      ""User_MASS""
   ],
   ""expiration_data"":"""",
   ""invitations"":[
      {
         ""email_to"":""hello"",
         ""invited_by"":""User_Mocking"",
         ""status"":""needs_invite""
      },
      {
         ""email_to"":""monkey"",
         ""invited_by"":""User_Mocking"",
         ""status"":""needs_invite""
      }
   ],
   ""organization_name"":""withsubscription"",
   ""status"":""invite_sent"",
   ""stripe_subscription"":[
      {
         ""billing_email"":""somewkjf"",
         ""status"":""active"",
         ""stripe_id"":""KItty"",
         ""stripe_quantity"":""32"",
         ""stripe_user"":""User_Mocking"",
         ""subscription_id"":""932438238429384ASBJDA""
      }
   ],
   ""creation_date"":""2011-01-01 01:00:37""
}
If I sent stripe_subscription as a json and not in an array (since its not a list or set) then I get 500 Internal error
{
   ""@type"":""Organization"",
   ""child"":[
      ""Organization_kitty"",
      ""Organization_DUNDRUM""
   ],
   ""collaborators"":[
      ""User_MASS""
   ],
   ""expiration_data"":"""",
   ""invitations"":[
      {
         ""email_to"":""hello"",
         ""invited_by"":""User_Mocking"",
         ""status"":""needs_invite""
      },
      {
         ""email_to"":""monkey"",
         ""invited_by"":""User_Mocking"",
         ""status"":""needs_invite""
      }
   ],
   ""organization_name"":""withsubscription"",
   ""status"":""invite_sent"",
   ""stripe_subscription"": {
         ""billing_email"":""somewkjf"",
         ""status"":""active"",
         ""stripe_id"":""KItty"",
         ""stripe_quantity"":""32"",
         ""stripe_user"":""User_Mocking"",
         ""subscription_id"":""932438238429384ASBJDA""
      },
   ""creation_date"":""2011-01-01 01:00:37""
}
500 error is
{
  ""api:message"":""Error: document_has_no_type(json{billing_email:\""somewkjf\"",status:\""active\"",stripe_id:\""KItty\"",stripe_quantity:\""32\"",stripe_user:\""User_Mocking\"",subscription_id:\""932438238429384ASBJDA\""})\n\n\nNote: some frames are missing due to last-call optimization.\nRe-run your program in debug mode (:- debug.) to get more detail.\n\n"",
  ""api:status"":""api:server_error""
}
Note: this maybe an error which I am doing, but let me know the correct way of sending a stripe_subscription which is a simple sub document with no type",2021-08-17T09:01:18Z,spl,https://github.com/terminusdb/terminusdb/issues/394#issuecomment-900121675,"My only comment would be to error out from backend as well (while creating schema) when attempting to set key of subdocument to something other than ValueHash/ Random. This is important because we can create a schema via JSON or query.

I believe this is happening now:
#!/bin/bash

./terminusdb db create t --label label --comment comment --prefixes '{""@base"":""http://b/"",""@schema"":""http://s/""}'

xh 'http://localhost:6363/api/document/admin/t?author=a&message=m&graph_type=schema' <<EOF
{""@id"":""Child"",""@type"":""Class"",""@key"":{""@type"":""Lexical"",""@fields"":[""name""]},""@subdocument"":[],""name"":""xsd:string""}
{""@id"":""Parent"",""@type"":""Class"",""@key"":{""@type"":""Random""},""children"":{""@type"":""Set"",""@class"":""Child""},""name"":""xsd:string""}
EOF

HTTP/1.1 400 Bad Request
connection: keep-alive
content-length: 246
content-type: application/json
date: Tue, 17 Aug 2021 09:01:09 GMT

{
    ""api:message"": ""Schema check failure"",
    ""api:status"": ""api:failure"",
    ""system:witnesses"": [
        {
            ""@type"": ""subdocuments_must_be_random_or_value_hash"",
            ""class"": ""http://s/Child"",
            ""key"": ""http://s/Child_Lexical_name""
        }
    ]
}","-PRON- only comment would be to error out from backend as well ( while create schema ) when attempt to set key of subdocument to something other than valuehash/ random this be important because -PRON- can create a schema via json or query i believe this be happen now # /bin / bash /terminusdb db create t --label label --comment comment --prefixe ' { "" @base""""http//b/""""@schema""""http//s/ "" } ' xh ' http//localhost6363 / api / document / admin / tauthor = a&message = m&graph_type = schema ' < < eof { "" @id""""child""""@type""""class""""@key""{""@type""""lexical""""@fields""[""name""]}""@subdocument""[]""name""""xsdstre "" } { "" @id""""parent""""@type""""class""""@key""{""@type""""random""}""children""{""@type""""set""""@class""""child""}""name""""xsdstre "" } eof http/11 400 bad request connection keep - alive content - length 246 content - type application / json date tue 17 aug 2021 090109 gmt { "" apimessage "" "" schema check failure "" "" apistatus "" "" apifailure "" "" systemwitnesse "" [ { "" @type "" "" subdocuments_must_be_random_or_value_hash "" "" class "" "" http//s / child "" "" key "" "" http//s / child_lexical_name "" } ] }",0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,394,2021-08-10T12:20:12Z,KittyJose,Error in creating a normal subdocument which is not List/Set,https://github.com/terminusdb/terminusdb/issues/394,"Describe the bug
Error in creating a normal subdocument which is not List/Set
To Reproduce
Schema used is
[
    {
        ""@abstract"": [],
        ""@id"": ""Entity"",
        ""@type"": ""Class"",
        ""status"": ""Status""
    },
    {
        ""@id"": ""Invitation"",
        ""@inherits"": ""Entity"",
        ""@key"": {
            ""@type"": ""ValueHash""
        },
        ""@subdocument"": [],
        ""@type"": ""Class"",
        ""email_to"": ""xsd:string"",
        ""invited_by"": ""User"",
        ""note"": {
            ""@class"": ""xsd:string"",
            ""@type"": ""Optional""
        },
        ""sent_date"": {
            ""@class"": ""xsd:dateTime"",
            ""@type"": ""Optional""
        }
    },
    {
        ""@id"": ""Organization"",
        ""@inherits"": ""Entity"",
        ""@key"": {
            ""@fields"": [
                ""organization_name""
            ],
            ""@type"": ""Lexical""
        },
        ""@type"": ""Class"",
        ""child"": {
            ""@class"": ""Organization"",
            ""@type"": ""Set""
        },
        ""collaborators"": {
            ""@class"": ""User"",
            ""@type"": ""Set""
        },
        ""creation_date"": ""xsd:dateTime"",
        ""expiration_data"": {
            ""@class"": ""xsd:dateTime"",
            ""@type"": ""Optional""
        },
        ""invitations"": {
            ""@class"": ""Invitation"",
            ""@type"": ""Set""
        },
        ""organization_name"": ""xsd:string"",
        ""owned_by"": ""User"",
        ""stripe_subscription"": ""StripeSubscription""
    },
    {
        ""@id"": ""Personal"",
        ""@inherits"": ""Organization"",
        ""@type"": ""Class""
    },
    {
        ""@id"": ""Status"",
        ""@type"": ""Enum"",
        ""@value"": [
            ""pending"",
            ""inactive"",
            ""active"",
            ""needs_invite"",
            ""invite_sent"",
            ""accepted"",
            ""rejected""
        ]
    },
    {
        ""@id"": ""StripeSubscription"",
        ""@inherits"": ""Entity"",
        ""@subdocument"": [],
        ""@type"": ""Class"",
        ""billing_email"": ""xsd:string"",
        ""stripe_id"": ""xsd:string"",
        ""stripe_quantity"": ""xsd:decimal"",
        ""stripe_user"": ""User"",
        ""subscription_id"": ""xsd:string""
    },
    {
        ""@id"": ""Team"",
        ""@inherits"": ""Organization"",
        ""@type"": ""Class""
    },
    {
        ""@id"": ""User"",
        ""@inherits"": ""Entity"",
        ""@key"": {
            ""@fields"": [
                ""user_id""
            ],
            ""@type"": ""Lexical""
        },
        ""@type"": ""Class"",
        ""company"": ""xsd:string"",
        ""email"": ""xsd:string"",
        ""first_name"": ""xsd:string"",
        ""last_name"": ""xsd:string"",
        ""picture"": ""xsd:string"",
        ""registration_date"": {
            ""@class"": ""xsd:dateTime"",
            ""@type"": ""Optional""
        },
        ""user_id"": ""xsd:string""
    }
  ]
I get 400 BAD REQUEST on creating Organization
{
   ""@type"":""Organization"",
   ""child"":[
      ""Organization_kitty"",
      ""Organization_DUNDRUM""
   ],
   ""collaborators"":[
      ""User_MASS""
   ],
   ""expiration_data"":"""",
   ""invitations"":[
      {
         ""email_to"":""hello"",
         ""invited_by"":""User_Mocking"",
         ""status"":""needs_invite""
      },
      {
         ""email_to"":""monkey"",
         ""invited_by"":""User_Mocking"",
         ""status"":""needs_invite""
      }
   ],
   ""organization_name"":""withsubscription"",
   ""status"":""invite_sent"",
   ""stripe_subscription"":[
      {
         ""billing_email"":""somewkjf"",
         ""status"":""active"",
         ""stripe_id"":""KItty"",
         ""stripe_quantity"":""32"",
         ""stripe_user"":""User_Mocking"",
         ""subscription_id"":""932438238429384ASBJDA""
      }
   ],
   ""creation_date"":""2011-01-01 01:00:37""
}
If I sent stripe_subscription as a json and not in an array (since its not a list or set) then I get 500 Internal error
{
   ""@type"":""Organization"",
   ""child"":[
      ""Organization_kitty"",
      ""Organization_DUNDRUM""
   ],
   ""collaborators"":[
      ""User_MASS""
   ],
   ""expiration_data"":"""",
   ""invitations"":[
      {
         ""email_to"":""hello"",
         ""invited_by"":""User_Mocking"",
         ""status"":""needs_invite""
      },
      {
         ""email_to"":""monkey"",
         ""invited_by"":""User_Mocking"",
         ""status"":""needs_invite""
      }
   ],
   ""organization_name"":""withsubscription"",
   ""status"":""invite_sent"",
   ""stripe_subscription"": {
         ""billing_email"":""somewkjf"",
         ""status"":""active"",
         ""stripe_id"":""KItty"",
         ""stripe_quantity"":""32"",
         ""stripe_user"":""User_Mocking"",
         ""subscription_id"":""932438238429384ASBJDA""
      },
   ""creation_date"":""2011-01-01 01:00:37""
}
500 error is
{
  ""api:message"":""Error: document_has_no_type(json{billing_email:\""somewkjf\"",status:\""active\"",stripe_id:\""KItty\"",stripe_quantity:\""32\"",stripe_user:\""User_Mocking\"",subscription_id:\""932438238429384ASBJDA\""})\n\n\nNote: some frames are missing due to last-call optimization.\nRe-run your program in debug mode (:- debug.) to get more detail.\n\n"",
  ""api:status"":""api:server_error""
}
Note: this maybe an error which I am doing, but let me know the correct way of sending a stripe_subscription which is a simple sub document with no type",2021-08-17T09:02:45Z,KittyJose,https://github.com/terminusdb/terminusdb/issues/394#issuecomment-900122793,OH nice great ! Ok so I am going to close this issue :),oh nice great ok so i be go to close this issue ),0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,395,2021-08-10T12:55:37Z,Cheukting,Hash key is lost in a round trip,https://github.com/terminusdb/terminusdb/issues/395,"what goes in:
{'@type': 'Class', '@id': 'Grades', 'last_name': 'xsd:string', 'first_name': 'xsd:string', 'ssn': 'xsd:string', 'test1': 'xsd:decimal', 'test2': 'xsd:decimal', 'test3': 'xsd:decimal', 'test4': 'xsd:decimal', 'final': 'xsd:decimal', 'grade': 'xsd:string', '@key': {'@type': 'Hash', '@field': ['last_name', 'first_name', 'ssn', 'test1', 'test2', 'test3', 'test4', 'final', 'grade']}}
What comes out (via get documents):
{'@id': 'Grades', '@type': 'Class', 'final': 'xsd:decimal', 'first_name': 'xsd:string', 'grade': 'xsd:string', 'last_name': 'xsd:string', 'ssn': 'xsd:string', 'test1': 'xsd:decimal', 'test2': 'xsd:decimal', 'test3': 'xsd:decimal', 'test4': 'xsd:decimal'}",2021-08-10T16:30:50Z,GavinMendelGleason,https://github.com/terminusdb/terminusdb/issues/395#issuecomment-896131124,Fixed in 013a606 by reporting an error about the form of the key.,fix in 013a606 by report an error about the form of the key,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,397,2021-08-10T18:53:25Z,spl,Erratic error on db_delete and db_force_delete_unfinalized_system_only,https://github.com/terminusdb/terminusdb/issues/397,"I've seen this sort of problem happen several times on the Publish action:
% PL-Unit: db_endpoint ......
ERROR: /app/terminusdb/src/server/routes.pl:354:
	test db_delete: received error: url `'http://127.0.0.1:64273/api/db/admin/TEST_DB'' does not exist (status(500,Internal Server Error))
..... done
% PL-Unit: triples_endpoint ..... done

% PL-Unit: db_endpoint .......
ERROR: /home/runner/work/terminusdb/terminusdb/src/server/routes.pl:366:
	test db_force_delete_unfinalized_system_only: received error: url `'http://127.0.0.1:55106/api/db/admin/foo'' does not exist (status(400,Bad Request))
.... done

The most recent occurrence is here. It's happened at least with the tests named db_delete and db_force_delete_unfinalized_system_only. Here is a log from a previous run.
The error sometimes goes away after re-running the action. I can't reproduce it locally. I'm guessing it's a timing issue: perhaps the database creation has somehow not completed before the HTTP request (http_get or http_delete)?",2021-08-13T08:32:09Z,spl,https://github.com/terminusdb/terminusdb/issues/397#issuecomment-898284801,"This may actually be due to #406, which was fixed by #408, because the problem in #406 generally appeared when deleting a database.",this may actually be due to # 406 which be fix by # 408 because the problem in # 406 generally appear when delete a database,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,397,2021-08-10T18:53:25Z,spl,Erratic error on db_delete and db_force_delete_unfinalized_system_only,https://github.com/terminusdb/terminusdb/issues/397,"I've seen this sort of problem happen several times on the Publish action:
% PL-Unit: db_endpoint ......
ERROR: /app/terminusdb/src/server/routes.pl:354:
	test db_delete: received error: url `'http://127.0.0.1:64273/api/db/admin/TEST_DB'' does not exist (status(500,Internal Server Error))
..... done
% PL-Unit: triples_endpoint ..... done

% PL-Unit: db_endpoint .......
ERROR: /home/runner/work/terminusdb/terminusdb/src/server/routes.pl:366:
	test db_force_delete_unfinalized_system_only: received error: url `'http://127.0.0.1:55106/api/db/admin/foo'' does not exist (status(400,Bad Request))
.... done

The most recent occurrence is here. It's happened at least with the tests named db_delete and db_force_delete_unfinalized_system_only. Here is a log from a previous run.
The error sometimes goes away after re-running the action. I can't reproduce it locally. I'm guessing it's a timing issue: perhaps the database creation has somehow not completed before the HTTP request (http_get or http_delete)?",2021-08-13T08:38:28Z,spl,https://github.com/terminusdb/terminusdb/issues/397#issuecomment-898288232,"Let's revisit this in a week. If it doesn't appear again, we'll assume it's fixed.",let -PRON- revisit this in a week if -PRON- do not appear again -PRON- will assume -PRON- be fix,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,397,2021-08-10T18:53:25Z,spl,Erratic error on db_delete and db_force_delete_unfinalized_system_only,https://github.com/terminusdb/terminusdb/issues/397,"I've seen this sort of problem happen several times on the Publish action:
% PL-Unit: db_endpoint ......
ERROR: /app/terminusdb/src/server/routes.pl:354:
	test db_delete: received error: url `'http://127.0.0.1:64273/api/db/admin/TEST_DB'' does not exist (status(500,Internal Server Error))
..... done
% PL-Unit: triples_endpoint ..... done

% PL-Unit: db_endpoint .......
ERROR: /home/runner/work/terminusdb/terminusdb/src/server/routes.pl:366:
	test db_force_delete_unfinalized_system_only: received error: url `'http://127.0.0.1:55106/api/db/admin/foo'' does not exist (status(400,Bad Request))
.... done

The most recent occurrence is here. It's happened at least with the tests named db_delete and db_force_delete_unfinalized_system_only. Here is a log from a previous run.
The error sometimes goes away after re-running the action. I can't reproduce it locally. I'm guessing it's a timing issue: perhaps the database creation has somehow not completed before the HTTP request (http_get or http_delete)?",2021-08-16T13:23:35Z,spl,https://github.com/terminusdb/terminusdb/issues/397#issuecomment-899507432,"We haven't seen this repeat, so it looks good.",-PRON- have not see this repeat so -PRON- look good,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,398,2021-08-11T10:18:09Z,matko,schema error on deleting a database,https://github.com/terminusdb/terminusdb/issues/398,@KittyJose is reporting a bug where she occasionally gets a CORS error when she tries to delete a database.,2021-08-11T10:29:46Z,KittyJose,https://github.com/terminusdb/terminusdb/issues/398#issuecomment-896706623,"Issue in deleting some database, receive CORS error. I suspect this occurs for only some databases coz its gone into a weird state.
If CORS is resolved i can see the response back coz as of now it fails to reload :)",issue in delete some database receive cor error i suspect this occur for only some database coz -PRON- go into a weird state if cor be resolve i can see the response back coz as of now -PRON- fail to reload ),0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,398,2021-08-11T10:18:09Z,matko,schema error on deleting a database,https://github.com/terminusdb/terminusdb/issues/398,@KittyJose is reporting a bug where she occasionally gets a CORS error when she tries to delete a database.,2021-08-12T08:33:42Z,KittyJose,https://github.com/terminusdb/terminusdb/issues/398#issuecomment-897451554,"Update - After @matko resolving CORS error at server side I can see the error now on deleting some dataproducts.
Error
{ ""api:message"":""Schema check failure"", ""api:status"":""api:failure"", ""system:witnesses"": [ { ""@type"":""subject_has_no_type"", ""subject"":""terminusdb://system/data/UserDatabase_b7207c2d8d5d7bcd2e277010eb9f9aba"" } ] }","update - after @matko resolve cor error at server side i can see the error now on delete some dataproduct error { "" apimessage""""schema check failure "" "" apistatus""""apifailure "" "" systemwitnesse "" [ { "" @type""""subject_has_no_type "" "" subject""""terminusdb//system / datum / userdatabase_b7207c2d8d5d7bcd2e277010eb9f9aba "" } ] }",0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,398,2021-08-11T10:18:09Z,matko,schema error on deleting a database,https://github.com/terminusdb/terminusdb/issues/398,@KittyJose is reporting a bug where she occasionally gets a CORS error when she tries to delete a database.,2021-08-12T09:39:28Z,matko,https://github.com/terminusdb/terminusdb/issues/398#issuecomment-897494424,"I did not resolve the CORS error :) That was Gavin.
Looks like we're trying to delete something in the system database and are leaving behind orphan triples. Interesting.",i do not resolve the cor error ) that be gavin look like -PRON- be try to delete something in the system database and be leave behind orphan triple interesting,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,398,2021-08-11T10:18:09Z,matko,schema error on deleting a database,https://github.com/terminusdb/terminusdb/issues/398,@KittyJose is reporting a bug where she occasionally gets a CORS error when she tries to delete a database.,2021-08-12T09:59:59Z,AstroChelonian,https://github.com/terminusdb/terminusdb/issues/398#issuecomment-897507263,it is sth similar to what we resolved on Monday. I will try to find where the orphan triples are.,-PRON- be sth similar to what -PRON- resolve on monday i will try to find where the orphan triple be,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,398,2021-08-11T10:18:09Z,matko,schema error on deleting a database,https://github.com/terminusdb/terminusdb/issues/398,@KittyJose is reporting a bug where she occasionally gets a CORS error when she tries to delete a database.,2021-08-12T14:15:48Z,spl,https://github.com/terminusdb/terminusdb/issues/398#issuecomment-897676904,I believe we've found the deletion issue. I opened up #406 to track it with more details.,i believe -PRON- have find the deletion issue i open up # 406 to track -PRON- with more detail,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,399,2021-08-11T15:29:41Z,Cheukting,"get doucments by type won't take {'count': ""unlimited""}",https://github.com/terminusdb/terminusdb/issues/399,What should it be?,2021-08-11T15:32:55Z,matko,https://github.com/terminusdb/terminusdb/issues/399#issuecomment-896928328,"We do not have 'unlimited' as an argument you can specify. If you do not want a limit, you simply don't specify the count argument and it'll return everything.",-PRON- do not have ' unlimited ' as an argument -PRON- can specify if -PRON- do not want a limit -PRON- simply do not specify the count argument and -PRON- will return everything,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,400,2021-08-12T09:47:48Z,GavinMendelGleason,HOWTO update data needs update,https://github.com/terminusdb/terminusdb/issues/400,https://terminusdb.github.io/terminusdb/#/How_To/update_data,,,,,,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,401,2021-08-12T09:48:31Z,spl,Subdocument key requirement change broke schema6 tests,https://github.com/terminusdb/terminusdb/issues/401,"The change in 07157c8 broke two tests in src/core/document/json.pl that use schema6:

employee_documents:doc_frame
employee_documents:insert_employee

Here are the error reports, which, strangely, don't cause CI to fail:
ERROR: /app/terminusdb/src/core/document/json.pl:5359:
	error in setup: Unknown error term: schema_check_failure([witness{'@type':subdocuments_must_be_random_or_value_hash,class:'http://s/Address',key:'http://s/Address_Lexical_street_postal_code'}])
ERROR: /app/terminusdb/src/core/document/json.pl:5422:
	error in setup: Unknown error term: schema_check_failure([witness{'@type':subdocuments_must_be_random_or_value_hash,class:'http://s/Address',key:'http://s/Address_Lexical_street_postal_code'}])
 done

I believe it's because schema6 contains the @key shown below:
{ ""@documentation"": {""@comment"": ""This is address"", ""@properties"": {}},
  ""@id"": ""Address"",
  ""@key"": {""@fields"": [""street"", ""postal_code""], ""@type"": ""Lexical""},
  ""@subdocument"": [],
  ""@type"": ""Class"",
  ""country"": ""Country"",
  ""postal_code"": ""xsd:string"",
  ""street"": ""xsd:string""}",2021-08-12T09:50:21Z,spl,https://github.com/terminusdb/terminusdb/issues/401#issuecomment-897501091,Is the solution to change the tests?,be the solution to change the test,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,402,2021-08-12T09:49:05Z,GavinMendelGleason,Graph query documentation needs slight syntax update to nodes,https://github.com/terminusdb/terminusdb/issues/402,https://terminusdb.github.io/terminusdb/#/How_To/graph_query,2021-08-12T11:21:05Z,GavinMendelGleason,https://github.com/terminusdb/terminusdb/issues/402#issuecomment-897556749,Fixed in 388dab7,fix in 388dab7,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,403,2021-08-12T09:53:19Z,GavinMendelGleason,API reference needs to be updated to reflect TerminusX,https://github.com/terminusdb/terminusdb/issues/403,"This document needs to reflect the actual API in routes.pl
https://terminusdb.github.io/terminusdb/#/reference/API",2021-08-23T13:16:51Z,spl,https://github.com/terminusdb/terminusdb/issues/403#issuecomment-903758920,@GavinMendelGleason will work on this for now.,@gavinmendelgleason will work on this for now,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,403,2021-08-12T09:53:19Z,GavinMendelGleason,API reference needs to be updated to reflect TerminusX,https://github.com/terminusdb/terminusdb/issues/403,"This document needs to reflect the actual API in routes.pl
https://terminusdb.github.io/terminusdb/#/reference/API",2021-08-31T20:56:09Z,AstroChelonian,https://github.com/terminusdb/terminusdb/issues/403#issuecomment-909630123,Document updated as per instructions at review and saved in the new repository for documents.,document update as per instruction at review and save in the new repository for document,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,404,2021-08-12T10:02:39Z,GavinMendelGleason,Create documentation from Schema - specifically necessary for WOQL,https://github.com/terminusdb/terminusdb/issues/404,"This needs to be generated automatically from terminusdb/terminusdb/src/terminus-schema/WOQL.json
This can be done either in python or javascript
https://terminusdb.github.io/terminusdb/#/reference/WOQL_JSON",2021-08-12T10:04:03Z,Cheukting,https://github.com/terminusdb/terminusdb/issues/404#issuecomment-897509959,"To put it another way, it is to generate a md file for documentation from the json file above",to put -PRON- another way -PRON- be to generate a md file for documentation from the json file above,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,405,2021-08-12T10:04:58Z,GavinMendelGleason,Regenerate CLI documentation,https://github.com/terminusdb/terminusdb/issues/405,"I believe this may be done via makefile in terminusdb
https://github.com/terminusdb/terminusdb/blob/dev/docs/CLI.md",2021-08-12T11:18:47Z,GavinMendelGleason,https://github.com/terminusdb/terminusdb/issues/405#issuecomment-897555428,fixed in 74b4732,fix in 74b4732,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,406,2021-08-12T14:14:00Z,spl,Creation date time changes between create and delete of database,https://github.com/terminusdb/terminusdb/issues/406,"This script eventually fails with a schema check failure:
#!/usr/bin/env bash
set -e -x
rm -rf ./storage
./terminusdb store init
while true; do
  ./terminusdb db create testdb --label label --comment comment --prefixes '{""@base"":""http://base/"",""@schema"":""http://schema/""}'
  ./terminusdb db delete testdb
done
@matko found that there is a difference with the creation_date, which leads to it not being deleted:

This is what is there:
value(""\""2021-08-12T13:20:29.846Z\""^^'http://www.w3.org/2001/XMLSchema#dateTime'"")

This is what we delete:
value(""\""2021-08-12T13:20:29.845Z\""^^'http://www.w3.org/2001/XMLSchema#dateTime'"")


It looks like 7d8b3bf led to this problem.",2021-08-13T08:30:45Z,spl,https://github.com/terminusdb/terminusdb/issues/406#issuecomment-898284081,Closed by #408,close by # 408,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,407,2021-08-12T14:59:36Z,matko,Nodes in the object position of triples are not type checked,https://github.com/terminusdb/terminusdb/issues/407,"It is possible for a triple to have a node in the object position, even though this node is otherwise unused. This means it is possible to point at an object that has no type, or doesn't exist.
This may be intended behavior (maybe we want to point to objects in third party graphs?) but currently it's caused us to accidentally leave behind garbage in our own internal graphs. An example is when deleting a database from the system graph. This currently leaves behind a reference to this database from an organization, and we're never notified by the schema checker that this is the case.",2021-08-26T14:24:15Z,matko,https://github.com/terminusdb/terminusdb/issues/407#issuecomment-906460043,Will be fixed in #475.,will be fix in # 475,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,411,2021-08-13T15:15:33Z,spl,Should a subdocument key have the type Random by default?,https://github.com/terminusdb/terminusdb/issues/411,Is this the expected thing? Does any other key type alternative make sense?,2021-08-13T20:49:27Z,GavinMendelGleason,https://github.com/terminusdb/terminusdb/issues/411#issuecomment-898713250,It's possible for lexical/hash for subdocuments to have meaning but they have to acquire the id from the path from the parent. This turned out to be a little intricate to programme and so I punted.,-PRON- be possible for lexical / hash for subdocument to have meaning but -PRON- have to acquire the -PRON- d from the path from the parent this turn out to be a little intricate to programme and so i punt,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,411,2021-08-13T15:15:33Z,spl,Should a subdocument key have the type Random by default?,https://github.com/terminusdb/terminusdb/issues/411,Is this the expected thing? Does any other key type alternative make sense?,2021-08-14T11:45:59Z,spl,https://github.com/terminusdb/terminusdb/issues/411#issuecomment-898883841,"When thinking about this, we might consider what conceptually makes the most sense to — or, alternatively, is the simplest to understand for — the schema creator who did not think it was necessary to add a @key to the subdocument.",when think about this -PRON- may consider what conceptually make the most sense to — or alternatively be the simple to understand for — the schema creator who do not think -PRON- be necessary to add a @key to the subdocument,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,411,2021-08-13T15:15:33Z,spl,Should a subdocument key have the type Random by default?,https://github.com/terminusdb/terminusdb/issues/411,Is this the expected thing? Does any other key type alternative make sense?,2021-08-16T13:11:01Z,Cheukting,https://github.com/terminusdb/terminusdb/issues/411#issuecomment-899497794,Yes,yes,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,411,2021-08-13T15:15:33Z,spl,Should a subdocument key have the type Random by default?,https://github.com/terminusdb/terminusdb/issues/411,Is this the expected thing? Does any other key type alternative make sense?,2021-08-17T09:04:26Z,GavinMendelGleason,https://github.com/terminusdb/terminusdb/issues/411#issuecomment-900123996,Now enforced in c49f9f2,now enforce in c49f9f2,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,412,2021-08-13T15:27:25Z,KittyJose,Update sub document doesn't work ,https://github.com/terminusdb/terminusdb/issues/412,"Describe the bug
Update sub document doesn't work
To Reproduce
{
   ""@id"":""Organization_kitty"",
   ""@type"":""Organization"",
   ""creation_date"":""2011-01-01T01:00:37Z"",
   ""invitations"":[
      {
         ""@id"":""Invitation_86b7ca4ee3bf6da3cb4a27d56005b862e207f653"",
         ""@type"":""Invitation"",
         ""email_to"":""kitzkan@gmail.net"",
         ""invited_by"":""User_MASS"",
         ""note"":""sure"",
         ""status"":""pending""
      }
   ],
   ""organization_name"":""kitty"",
   ""owned_by"":""User_MASS"",
   ""status"":""active""
}
For above invitation (subdocument) if you alter any property email or invited_by. No response comes back from database and hence the update mever happens
Expected behavior
A clear and concise description of what you expected to happen.
Screenshots",2021-08-16T13:47:07Z,spl,https://github.com/terminusdb/terminusdb/issues/412#issuecomment-899524935,"Okay, so a simple subdocument update does work:
schema.json
{""@id"":""Child"",""@type"":""Class"",""@key"":{""@type"":""Random""},""@subdocument"":[],""name"":""xsd:string""}
{""@id"":""Parent"",""@type"":""Class"",""@key"":{""@type"":""Random""},""child"":""Child"",""name"":""xsd:string""}
instance1.json
{""@id"":""Mother"",""@type"":""Parent"",""name"":""Gaia"",""child"":{""@type"":""Child"",""name"":""Uranus""}}
instance2.json
{""@id"":""Mother"",""@type"":""Parent"",""name"":""Gaia"",""child"":{""@type"":""Child"",""name"":""Oceanus""}}
#!/bin/sh
./terminusdb db create testdb --label label --comment comment --prefixes '{""@base"":""http://base/"",""@schema"":""http://schema/""}'
cat schema.json | xh 'http://localhost:6363/api/document/admin/testdb?author=a&message=m&graph_type=schema'
cat instance1.json | xh 'http://localhost:6363/api/document/admin/testdb?author=a&message=m'
xh 'http://localhost:6363/api/document/admin/testdb?author=a&message=m' # show 1
cat instance2.json | xh PUT 'http://localhost:6363/api/document/admin/testdb?author=a&message=m'
xh 'http://localhost:6363/api/document/admin/testdb?author=a&message=m' # show 2
These are the documents at show 1 and show 2:
{""@id"":""Mother"",""@type"":""Parent"",""child"":{""@id"":""Child_2fc0f5e12d4bd32d60d80ffdeeeef0f5"",""@type"":""Child"",""name"":""Uranus""},""name"":""Gaia""}
{""@id"":""Mother"",""@type"":""Parent"",""child"":{""@id"":""Child_93ebeb2be60629071db93e860c8e11f5"",""@type"":""Child"",""name"":""Oceanus""},""name"":""Gaia""}","okay so a simple subdocument update do work schemajson { "" @id""""child""""@type""""class""""@key""{""@type""""random""}""@subdocument""[]""name""""xsdstre "" } { "" @id""""parent""""@type""""class""""@key""{""@type""""random""}""child""""child""""name""""xsdstring "" } instance1json { "" @id""""mother""""@type""""parent""""name""""gaia""""child""{""@type""""child""""name""""uranus "" } } instance2json { "" @id""""mother""""@type""""parent""""name""""gaia""""child""{""@type""""child""""name""""oceanus "" } } # /bin / sh /terminusdb db create testdb --label label --comment comment --prefixe ' { "" @base""""http//base/""""@schema""""http//schema/ "" } ' cat schemajson | xh ' http//localhost6363 / api / document / admin / testdbauthor = a&message = m&graph_type = schema ' cat instance1json | xh ' http//localhost6363 / api / document / admin / testdbauthor = a&message = m ' xh ' http//localhost6363 / api / document / admin / testdbauthor = a&message = m ' # show 1 cat instance2json | xh put ' http//localhost6363 / api / document / admin / testdbauthor = a&message = m ' xh ' http//localhost6363 / api / document / admin / testdbauthor = a&message = m ' # show 2 these be the document at show 1 and show 2 { "" @id""""mother""""@type""""parent""""child""{""@id""""child_2fc0f5e12d4bd32d60d80ffdeeeef0f5""""@type""""child""""name""""uranus""}""name""""gaia "" } { "" @id""""mother""""@type""""parent""""child""{""@id""""child_93ebeb2be60629071db93e860c8e11f5""""@type""""child""""name""""oceanus""}""name""""gaia "" }",0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,412,2021-08-13T15:27:25Z,KittyJose,Update sub document doesn't work ,https://github.com/terminusdb/terminusdb/issues/412,"Describe the bug
Update sub document doesn't work
To Reproduce
{
   ""@id"":""Organization_kitty"",
   ""@type"":""Organization"",
   ""creation_date"":""2011-01-01T01:00:37Z"",
   ""invitations"":[
      {
         ""@id"":""Invitation_86b7ca4ee3bf6da3cb4a27d56005b862e207f653"",
         ""@type"":""Invitation"",
         ""email_to"":""kitzkan@gmail.net"",
         ""invited_by"":""User_MASS"",
         ""note"":""sure"",
         ""status"":""pending""
      }
   ],
   ""organization_name"":""kitty"",
   ""owned_by"":""User_MASS"",
   ""status"":""active""
}
For above invitation (subdocument) if you alter any property email or invited_by. No response comes back from database and hence the update mever happens
Expected behavior
A clear and concise description of what you expected to happen.
Screenshots",2021-08-16T15:00:08Z,GavinMendelGleason,https://github.com/terminusdb/terminusdb/issues/412#issuecomment-899580965,"I have a rather elaborate test using the Profile schema that demonstrates that it does work. I think it is best however if ids are not used in updating of subdocuments, but rather they are automatically hashed or made random.",i have a rather elaborate test use the profile schema that demonstrate that -PRON- do work i think -PRON- be good however if id be not use in updating of subdocument but rather -PRON- be automatically hash or make random,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,412,2021-08-13T15:27:25Z,KittyJose,Update sub document doesn't work ,https://github.com/terminusdb/terminusdb/issues/412,"Describe the bug
Update sub document doesn't work
To Reproduce
{
   ""@id"":""Organization_kitty"",
   ""@type"":""Organization"",
   ""creation_date"":""2011-01-01T01:00:37Z"",
   ""invitations"":[
      {
         ""@id"":""Invitation_86b7ca4ee3bf6da3cb4a27d56005b862e207f653"",
         ""@type"":""Invitation"",
         ""email_to"":""kitzkan@gmail.net"",
         ""invited_by"":""User_MASS"",
         ""note"":""sure"",
         ""status"":""pending""
      }
   ],
   ""organization_name"":""kitty"",
   ""owned_by"":""User_MASS"",
   ""status"":""active""
}
For above invitation (subdocument) if you alter any property email or invited_by. No response comes back from database and hence the update mever happens
Expected behavior
A clear and concise description of what you expected to happen.
Screenshots",2021-08-16T15:00:55Z,GavinMendelGleason,https://github.com/terminusdb/terminusdb/issues/412#issuecomment-899581557,See: #418,see # 418,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,412,2021-08-13T15:27:25Z,KittyJose,Update sub document doesn't work ,https://github.com/terminusdb/terminusdb/issues/412,"Describe the bug
Update sub document doesn't work
To Reproduce
{
   ""@id"":""Organization_kitty"",
   ""@type"":""Organization"",
   ""creation_date"":""2011-01-01T01:00:37Z"",
   ""invitations"":[
      {
         ""@id"":""Invitation_86b7ca4ee3bf6da3cb4a27d56005b862e207f653"",
         ""@type"":""Invitation"",
         ""email_to"":""kitzkan@gmail.net"",
         ""invited_by"":""User_MASS"",
         ""note"":""sure"",
         ""status"":""pending""
      }
   ],
   ""organization_name"":""kitty"",
   ""owned_by"":""User_MASS"",
   ""status"":""active""
}
For above invitation (subdocument) if you alter any property email or invited_by. No response comes back from database and hence the update mever happens
Expected behavior
A clear and concise description of what you expected to happen.
Screenshots",2021-08-16T17:32:07Z,spl,https://github.com/terminusdb/terminusdb/issues/412#issuecomment-899688805,"I was working on a test with Set before I got distracted by #416. The following also worked for me:
schema.json
{""@id"":""Child"",""@type"":""Class"",""@key"":{""@type"":""Random""},""@subdocument"":[],""name"":""xsd:string""}
{""@id"":""Parent"",""@type"":""Class"",""@key"":{""@type"":""Random""},""children"":{""@type"":""Set"",""@class"":""Child""},""name"":""xsd:string""}
instance1.json
{""@id"":""Mother"",""@type"":""Parent"",""name"":""Gaia"",""children"":[{""@type"":""Child"",""name"":""Uranus""}]}
instance2.json
{""@id"":""Mother"",""@type"":""Parent"",""name"":""Gaia"",""children"":[{""@type"":""Child"",""name"":""Uranus""},{""@type"":""Child"",""name"":""Phoebe""},{""@type"":""Child"",""name"":""Tethys""}]}
#!/bin/sh
./terminusdb db create t --label label --comment comment --prefixes '{""@base"":""http://b/"",""@schema"":""http://s/""}'
cat schema.json | xh 'http://localhost:6363/api/document/admin/t?author=a&message=m&graph_type=schema'
cat instance1.json | xh 'http://localhost:6363/api/document/admin/t?author=a&message=m'
cat instance2.json | xh PUT 'http://localhost:6363/api/document/admin/t?author=a&message=m'
xh 'http://localhost:6363/api/document/admin/t'","i be work on a test with set before i get distract by # 416 the following also work for -PRON- schemajson { "" @id""""child""""@type""""class""""@key""{""@type""""random""}""@subdocument""[]""name""""xsdstre "" } { "" @id""""parent""""@type""""class""""@key""{""@type""""random""}""children""{""@type""""set""""@class""""child""}""name""""xsdstring "" } instance1json { "" @id""""mother""""@type""""parent""""name""""gaia""""children""[{""@type""""child""""name""""uranus "" } ] } instance2json { "" @id""""mother""""@type""""parent""""name""""gaia""""children""[{""@type""""child""""name""""uranus""}{""@type""""child""""name""""phoebe""}{""@type""""child""""name""""tethys "" } ] } # /bin / sh /terminusdb db create t --label label --comment comment --prefixe ' { "" @base""""http//b/""""@schema""""http//s/ "" } ' cat schemajson | xh ' http//localhost6363 / api / document / admin / tauthor = a&message = m&graph_type = schema ' cat instance1json | xh ' http//localhost6363 / api / document / admin / tauthor = a&message = m ' cat instance2json | xh put ' http//localhost6363 / api / document / admin / tauthor = a&message = m ' xh ' http//localhost6363 / api / document / admin / t '",0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,412,2021-08-13T15:27:25Z,KittyJose,Update sub document doesn't work ,https://github.com/terminusdb/terminusdb/issues/412,"Describe the bug
Update sub document doesn't work
To Reproduce
{
   ""@id"":""Organization_kitty"",
   ""@type"":""Organization"",
   ""creation_date"":""2011-01-01T01:00:37Z"",
   ""invitations"":[
      {
         ""@id"":""Invitation_86b7ca4ee3bf6da3cb4a27d56005b862e207f653"",
         ""@type"":""Invitation"",
         ""email_to"":""kitzkan@gmail.net"",
         ""invited_by"":""User_MASS"",
         ""note"":""sure"",
         ""status"":""pending""
      }
   ],
   ""organization_name"":""kitty"",
   ""owned_by"":""User_MASS"",
   ""status"":""active""
}
For above invitation (subdocument) if you alter any property email or invited_by. No response comes back from database and hence the update mever happens
Expected behavior
A clear and concise description of what you expected to happen.
Screenshots",2021-08-16T17:48:45Z,spl,https://github.com/terminusdb/terminusdb/issues/412#issuecomment-899699437,"@GavinMendelGleason What should be the expected result if the subdocument id is used? I suppose this seems correct:

Run this:

#!/bin/bash

schema=$(cat <<EOF
{""@id"":""Child"",""@type"":""Class"",""@key"":{""@type"":""Random""},""@subdocument"":[],""name"":""xsd:string""}
{""@id"":""Parent"",""@type"":""Class"",""@key"":{""@type"":""Random""},""children"":{""@type"":""Set"",""@class"":""Child""},""name"":""xsd:string""}
EOF
)

instance1=$(cat <<EOF
{""@id"":""Mother"",""@type"":""Parent"",""name"":""Gaia"",""children"":[{""@type"":""Child"",""name"":""Uranus""}]}
EOF
)

./terminusdb db create t --label label --comment comment --prefixes '{""@base"":""http://b/"",""@schema"":""http://s/""}'
echo ""$schema"" | xh 'http://localhost:6363/api/document/admin/t?author=a&message=m&graph_type=schema'
echo ""$instance1"" | xh 'http://localhost:6363/api/document/admin/t?author=a&message=m'
xh 'http://localhost:6363/api/document/admin/t'

Collect the subdocument id:

{""@id"":""Mother"",""@type"":""Parent"",""children"":[{""@id"":""Child_4e6a0ba4a19826aa989b9bfe4d64500d"",""@type"":""Child"",""name"":""Uranus""}],""name"":""Gaia""}

Run this using the above id:

#!/bin/bash

instance2=$(cat <<EOF
{""@id"":""Mother"",""@type"":""Parent"",""name"":""Gaia"",""children"":[{""@id"":""Child_4e6a0ba4a19826aa989b9bfe4d64500d"",""@type"":""Child"",""name"":""Tethys""}]}
EOF
)

echo ""$instance2"" | xh PUT 'http://localhost:6363/api/document/admin/t?author=a&message=m'
xh 'http://localhost:6363/api/document/admin/t'
The result is:
{""@id"":""Mother"",""@type"":""Parent"",""children"":[{""@id"":""Child_4e6a0ba4a19826aa989b9bfe4d64500d"",""@type"":""Child"",""name"":""Tethys""}],""name"":""Gaia""}","@gavinmendelgleason what should be the expected result if the subdocument -PRON- d be use i suppose this seem correct run this # /bin / bash schema=$(cat < < eof { "" @id""""child""""@type""""class""""@key""{""@type""""random""}""@subdocument""[]""name""""xsdstre "" } { "" @id""""parent""""@type""""class""""@key""{""@type""""random""}""children""{""@type""""set""""@class""""child""}""name""""xsdstring "" } eof ) instance1=$(cat < < eof { "" @id""""mother""""@type""""parent""""name""""gaia""""children""[{""@type""""child""""name""""uranus "" } ] } eof ) /terminusdb db create t --label label --comment comment --prefixe ' { "" @base""""http//b/""""@schema""""http//s/ "" } ' echo "" $ schema "" | xh ' http//localhost6363 / api / document / admin / tauthor = a&message = m&graph_type = schema ' echo "" $ instance1 "" | xh ' http//localhost6363 / api / document / admin / tauthor = a&message = m ' xh ' http//localhost6363 / api / document / admin / t ' collect the subdocument -PRON- d { "" @id""""mother""""@type""""parent""""children""[{""@id""""child_4e6a0ba4a19826aa989b9bfe4d64500d""""@type""""child""""name""""uranus""}]""name""""gaia "" } run this use the above -PRON- d # /bin / bash instance2=$(cat < < eof { "" @id""""mother""""@type""""parent""""name""""gaia""""children""[{""@id""""child_4e6a0ba4a19826aa989b9bfe4d64500d""""@type""""child""""name""""tethys "" } ] } eof ) echo "" $ instance2 "" | xh put ' http//localhost6363 / api / document / admin / tauthor = a&message = m ' xh ' http//localhost6363 / api / document / admin / t ' the result be { "" @id""""mother""""@type""""parent""""children""[{""@id""""child_4e6a0ba4a19826aa989b9bfe4d64500d""""@type""""child""""name""""tethys""}]""name""""gaia "" }",0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,412,2021-08-13T15:27:25Z,KittyJose,Update sub document doesn't work ,https://github.com/terminusdb/terminusdb/issues/412,"Describe the bug
Update sub document doesn't work
To Reproduce
{
   ""@id"":""Organization_kitty"",
   ""@type"":""Organization"",
   ""creation_date"":""2011-01-01T01:00:37Z"",
   ""invitations"":[
      {
         ""@id"":""Invitation_86b7ca4ee3bf6da3cb4a27d56005b862e207f653"",
         ""@type"":""Invitation"",
         ""email_to"":""kitzkan@gmail.net"",
         ""invited_by"":""User_MASS"",
         ""note"":""sure"",
         ""status"":""pending""
      }
   ],
   ""organization_name"":""kitty"",
   ""owned_by"":""User_MASS"",
   ""status"":""active""
}
For above invitation (subdocument) if you alter any property email or invited_by. No response comes back from database and hence the update mever happens
Expected behavior
A clear and concise description of what you expected to happen.
Screenshots",2021-08-16T18:47:25Z,GavinMendelGleason,https://github.com/terminusdb/terminusdb/issues/412#issuecomment-899737622,"Yes that appears correct. As long as the user is careful to supply ids or they use ValueHash or Random we should be ok.
I don't think there is a bug here.",yes that appear correct as long as the user be careful to supply ids or -PRON- use valuehash or random -PRON- should be ok i do not think there be a bug here,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,412,2021-08-13T15:27:25Z,KittyJose,Update sub document doesn't work ,https://github.com/terminusdb/terminusdb/issues/412,"Describe the bug
Update sub document doesn't work
To Reproduce
{
   ""@id"":""Organization_kitty"",
   ""@type"":""Organization"",
   ""creation_date"":""2011-01-01T01:00:37Z"",
   ""invitations"":[
      {
         ""@id"":""Invitation_86b7ca4ee3bf6da3cb4a27d56005b862e207f653"",
         ""@type"":""Invitation"",
         ""email_to"":""kitzkan@gmail.net"",
         ""invited_by"":""User_MASS"",
         ""note"":""sure"",
         ""status"":""pending""
      }
   ],
   ""organization_name"":""kitty"",
   ""owned_by"":""User_MASS"",
   ""status"":""active""
}
For above invitation (subdocument) if you alter any property email or invited_by. No response comes back from database and hence the update mever happens
Expected behavior
A clear and concise description of what you expected to happen.
Screenshots",2021-08-17T08:16:15Z,spl,https://github.com/terminusdb/terminusdb/issues/412#issuecomment-900091751,"Closing for now, since we can't reproduce it.",close for now since -PRON- can not reproduce -PRON-,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,412,2021-08-13T15:27:25Z,KittyJose,Update sub document doesn't work ,https://github.com/terminusdb/terminusdb/issues/412,"Describe the bug
Update sub document doesn't work
To Reproduce
{
   ""@id"":""Organization_kitty"",
   ""@type"":""Organization"",
   ""creation_date"":""2011-01-01T01:00:37Z"",
   ""invitations"":[
      {
         ""@id"":""Invitation_86b7ca4ee3bf6da3cb4a27d56005b862e207f653"",
         ""@type"":""Invitation"",
         ""email_to"":""kitzkan@gmail.net"",
         ""invited_by"":""User_MASS"",
         ""note"":""sure"",
         ""status"":""pending""
      }
   ],
   ""organization_name"":""kitty"",
   ""owned_by"":""User_MASS"",
   ""status"":""active""
}
For above invitation (subdocument) if you alter any property email or invited_by. No response comes back from database and hence the update mever happens
Expected behavior
A clear and concise description of what you expected to happen.
Screenshots",2021-08-17T09:54:18Z,KittyJose,https://github.com/terminusdb/terminusdb/issues/412#issuecomment-900157102,"I pass id as well as type to update this invitation and I get blank response from the server.
Initiation is of ValueHash key in this case

  ""@id"": ""Organization_easy"",
  ""@type"": ""Organization"",
  ""creation_date"": ""2011-01-01T01:00:37Z"",
  ""invitations"": [
    {
      ""@id"": ""Invitation_194a47ac692422015511fe438c8d36366162c39a"",
      ""@type"": ""Invitation"",
      ""email_to"": ""easypeasy"",
      ""invited_by"": ""User_Nancy"",
      ""status"": ""needs_invite""
    }
  ],
  ""organization_name"": ""easy"",
  ""owned_by"": ""User_Nancy""
}","i pass -PRON- d as well as type to update this invitation and i get blank response from the server initiation be of valuehash key in this case "" @id "" "" organization_easy "" "" @type "" "" organization "" "" creation_date "" "" 2011 - 01 - 01t010037z "" "" invitation "" [ { "" @id "" "" invitation_194a47ac692422015511fe438c8d36366162c39a "" "" @type "" "" invitation "" "" email_to "" "" easypeasy "" "" invited_by "" "" user_nancy "" "" status "" "" needs_invite "" } ] "" organization_name "" "" easy "" "" owned_by "" "" user_nancy "" }",0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,412,2021-08-13T15:27:25Z,KittyJose,Update sub document doesn't work ,https://github.com/terminusdb/terminusdb/issues/412,"Describe the bug
Update sub document doesn't work
To Reproduce
{
   ""@id"":""Organization_kitty"",
   ""@type"":""Organization"",
   ""creation_date"":""2011-01-01T01:00:37Z"",
   ""invitations"":[
      {
         ""@id"":""Invitation_86b7ca4ee3bf6da3cb4a27d56005b862e207f653"",
         ""@type"":""Invitation"",
         ""email_to"":""kitzkan@gmail.net"",
         ""invited_by"":""User_MASS"",
         ""note"":""sure"",
         ""status"":""pending""
      }
   ],
   ""organization_name"":""kitty"",
   ""owned_by"":""User_MASS"",
   ""status"":""active""
}
For above invitation (subdocument) if you alter any property email or invited_by. No response comes back from database and hence the update mever happens
Expected behavior
A clear and concise description of what you expected to happen.
Screenshots",2021-08-17T10:45:50Z,GavinMendelGleason,https://github.com/terminusdb/terminusdb/issues/412#issuecomment-900189238,"Well, it definitely should not give an empty response, this is surprising, but you should also not be supplying the id given that Invitation is a value hash.
Could the empty response be a CORS issue?",well -PRON- definitely should not give an empty response this be surprising but -PRON- should also not be supply the -PRON- d give that invitation be a value hash could the empty response be a cor issue,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,412,2021-08-13T15:27:25Z,KittyJose,Update sub document doesn't work ,https://github.com/terminusdb/terminusdb/issues/412,"Describe the bug
Update sub document doesn't work
To Reproduce
{
   ""@id"":""Organization_kitty"",
   ""@type"":""Organization"",
   ""creation_date"":""2011-01-01T01:00:37Z"",
   ""invitations"":[
      {
         ""@id"":""Invitation_86b7ca4ee3bf6da3cb4a27d56005b862e207f653"",
         ""@type"":""Invitation"",
         ""email_to"":""kitzkan@gmail.net"",
         ""invited_by"":""User_MASS"",
         ""note"":""sure"",
         ""status"":""pending""
      }
   ],
   ""organization_name"":""kitty"",
   ""owned_by"":""User_MASS"",
   ""status"":""active""
}
For above invitation (subdocument) if you alter any property email or invited_by. No response comes back from database and hence the update mever happens
Expected behavior
A clear and concise description of what you expected to happen.
Screenshots",2021-08-17T11:17:06Z,KittyJose,https://github.com/terminusdb/terminusdb/issues/412#issuecomment-900209116,"So, I removed the Id from Invitation on an update and it worked, I got the updated value. But I still dont get a response from database, all though its a 200 OK (no CORS).
Also @GavinMendelGleason am I right to not send id for a subdocument at all irrespective to key being a value hash or random?
{""@id"":""Organization_easy"",""@type"":""Organization"",""creation_date"":""2011-01-01T01:00:37Z"",""invitations"":[{""@type"":""Invitation"",""email_to"":""easypeasy"",""invited_by"":""User_Nancy""}],""organization_name"":""easy"",""owned_by"":""User_Nancy""}","so i remove the -PRON- d from invitation on an update and -PRON- work i get the update value but i still do not get a response from database all though -PRON- a 200 ok ( no cor ) also @gavinmendelgleason be i right to not send -PRON- d for a subdocument at all irrespective to key be a value hash or random { "" @id""""organization_easy""""@type""""organization""""creation_date""""2011 - 01 - 01t010037z""""invitations""[{""@type""""invitation""""email_to""""easypeasy""""invited_by""""user_nancy""}]""organization_name""""easy""""owned_by""""user_nancy "" }",0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,412,2021-08-13T15:27:25Z,KittyJose,Update sub document doesn't work ,https://github.com/terminusdb/terminusdb/issues/412,"Describe the bug
Update sub document doesn't work
To Reproduce
{
   ""@id"":""Organization_kitty"",
   ""@type"":""Organization"",
   ""creation_date"":""2011-01-01T01:00:37Z"",
   ""invitations"":[
      {
         ""@id"":""Invitation_86b7ca4ee3bf6da3cb4a27d56005b862e207f653"",
         ""@type"":""Invitation"",
         ""email_to"":""kitzkan@gmail.net"",
         ""invited_by"":""User_MASS"",
         ""note"":""sure"",
         ""status"":""pending""
      }
   ],
   ""organization_name"":""kitty"",
   ""owned_by"":""User_MASS"",
   ""status"":""active""
}
For above invitation (subdocument) if you alter any property email or invited_by. No response comes back from database and hence the update mever happens
Expected behavior
A clear and concise description of what you expected to happen.
Screenshots",2021-08-17T13:56:35Z,GavinMendelGleason,https://github.com/terminusdb/terminusdb/issues/412#issuecomment-900321011,"That is correct, in the case of ValueHash it needs to actually be the hash of values so will not be the same hash after changes. In the case of Random it will be generated automatically each time.
Not sure regards the CORS - we might have to ask @matko if he knows...",that be correct in the case of valuehash -PRON- need to actually be the hash of value so will not be the same hash after change in the case of random -PRON- will be generate automatically each time not sure regard the cor - -PRON- may have to ask @matko if -PRON- know,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,412,2021-08-13T15:27:25Z,KittyJose,Update sub document doesn't work ,https://github.com/terminusdb/terminusdb/issues/412,"Describe the bug
Update sub document doesn't work
To Reproduce
{
   ""@id"":""Organization_kitty"",
   ""@type"":""Organization"",
   ""creation_date"":""2011-01-01T01:00:37Z"",
   ""invitations"":[
      {
         ""@id"":""Invitation_86b7ca4ee3bf6da3cb4a27d56005b862e207f653"",
         ""@type"":""Invitation"",
         ""email_to"":""kitzkan@gmail.net"",
         ""invited_by"":""User_MASS"",
         ""note"":""sure"",
         ""status"":""pending""
      }
   ],
   ""organization_name"":""kitty"",
   ""owned_by"":""User_MASS"",
   ""status"":""active""
}
For above invitation (subdocument) if you alter any property email or invited_by. No response comes back from database and hence the update mever happens
Expected behavior
A clear and concise description of what you expected to happen.
Screenshots",2021-08-19T06:50:07Z,spl,https://github.com/terminusdb/terminusdb/issues/412#issuecomment-901655824,"@KittyJose @GavinMendelGleason I believe I've reproduced below what has been discussed so far. Is the problem that the PUT does not return the same response as the POST? That is, in my script, the POST responds with [""http://b/Organization_easy""], but the PUT response is empty but shouldn't be. Are there any other issues?
set -ex

./terminusdb db create t --label l --comment c --prefixes '{""@base"":""http://b/"",""@schema"":""http://s/""}'

xh 'http://localhost:6363/api/document/admin/t?author=a&message=m&graph_type=schema' <<EOF
[
  {
    ""@abstract"": [],
    ""@id"": ""Entity"",
    ""@type"": ""Class"",
    ""status"": ""Status""
  },
  {
    ""@id"": ""Invitation"",
    ""@inherits"": ""Entity"",
    ""@key"": {
      ""@type"": ""ValueHash""
    },
    ""@subdocument"": [],
    ""@type"": ""Class"",
    ""email_to"": ""xsd:string"",
    ""invited_by"": ""User"",
    ""note"": {
      ""@class"": ""xsd:string"",
      ""@type"": ""Optional""
    },
    ""sent_date"": {
      ""@class"": ""xsd:dateTime"",
      ""@type"": ""Optional""
    }
  },
  {
    ""@id"": ""Organization"",
    ""@inherits"": ""Entity"",
    ""@key"": {
      ""@fields"": [
        ""organization_name""
      ],
      ""@type"": ""Lexical""
    },
    ""@type"": ""Class"",
    ""child"": {
      ""@class"": ""Organization"",
      ""@type"": ""Set""
    },
    ""collaborators"": {
      ""@class"": ""User"",
      ""@type"": ""Set""
    },
    ""creation_date"": ""xsd:dateTime"",
    ""expiration_data"": {
      ""@class"": ""xsd:dateTime"",
      ""@type"": ""Optional""
    },
    ""invitations"": {
      ""@class"": ""Invitation"",
      ""@type"": ""Set""
    },
    ""organization_name"": ""xsd:string"",
    ""owned_by"": ""User"",
    ""stripe_subscription"": ""StripeSubscription""
  },
  {
    ""@id"": ""Personal"",
    ""@inherits"": ""Organization"",
    ""@type"": ""Class""
  },
  {
    ""@id"": ""Status"",
    ""@type"": ""Enum"",
    ""@value"": [
      ""pending"",
      ""inactive"",
      ""active"",
      ""needs_invite"",
      ""invite_sent"",
      ""accepted"",
      ""rejected""
    ]
  },
  {
    ""@id"": ""StripeSubscription"",
    ""@inherits"": ""Entity"",
    ""@subdocument"": [],
    ""@type"": ""Class"",
    ""@key"" : {
      ""@type"": ""Random""
    },
    ""billing_email"": ""xsd:string"",
    ""stripe_id"": ""xsd:string"",
    ""stripe_quantity"": ""xsd:decimal"",
    ""stripe_user"": ""User"",
    ""subscription_id"": ""xsd:string""
  },
  {
    ""@id"": ""Team"",
    ""@inherits"": ""Organization"",
    ""@type"": ""Class""
  },
  {
    ""@id"": ""User"",
    ""@inherits"": ""Entity"",
    ""@key"": {
      ""@fields"": [
        ""user_id""
      ],
      ""@type"": ""Lexical""
    },
    ""@type"": ""Class"",
    ""company"": ""xsd:string"",
    ""email"": ""xsd:string"",
    ""first_name"": ""xsd:string"",
    ""last_name"": ""xsd:string"",
    ""picture"": ""xsd:string"",
    ""registration_date"": {
      ""@class"": ""xsd:dateTime"",
      ""@type"": ""Optional""
    },
    ""user_id"": ""xsd:string""
  }
]
EOF

xh 'http://localhost:6363/api/document/admin/t?author=a&message=m' <<EOF
{
  ""@id"": ""Organization_easy"",
  ""@type"": ""Organization"",
  ""creation_date"": ""2011-01-01T01:00:37Z"",
  ""invitations"": [
    {
      ""@type"": ""Invitation"",
      ""email_to"": ""easypeasy"",
      ""invited_by"": ""User_Nancy""
    }
  ],
  ""organization_name"": ""easy"",
  ""owned_by"": ""User_Nancy""
}
EOF

xh PUT 'http://localhost:6363/api/document/admin/t?author=a&message=m' <<EOF
{
  ""@id"": ""Organization_easy"",
  ""@type"": ""Organization"",
  ""creation_date"": ""2011-01-01T01:00:37Z"",
  ""invitations"": [
    {
      ""@type"": ""Invitation"",
      ""email_to"": ""easypeasy"",
      ""invited_by"": ""User_Nancy"",
      ""status"": ""needs_invite""
    }
  ],
  ""organization_name"": ""easy"",
  ""owned_by"": ""User_Nancy""
}
EOF

xh 'http://localhost:6363/api/document/admin/t?graph_type=schema'
xh 'http://localhost:6363/api/document/admin/t'","@kittyjose @gavinmendelgleason i believe -PRON- have reproduce below what have be discuss so far be the problem that the put do not return the same response as the post that be in -PRON- script the post respond with [ "" http//b / organization_easy "" ] but the put response be empty but should not be be there any other issue set -ex /terminusdb db create t --label l --comment c --prefixe ' { "" @base""""http//b/""""@schema""""http//s/ "" } ' xh ' http//localhost6363 / api / document / admin / tauthor = a&message = m&graph_type = schema ' < < eof [ { "" @abstract "" [ ] "" @id "" "" entity "" "" @type "" "" class "" "" status "" "" status "" } { "" @id "" "" invitation "" "" @inherit "" "" entity "" "" @key "" { "" @type "" "" valuehash "" } "" @subdocument "" [ ] "" @type "" "" class "" "" email_to "" "" xsdstre "" "" invited_by "" "" user "" "" note "" { "" @class "" "" xsdstring "" "" @type "" "" optional "" } "" sent_date "" { "" @class "" "" xsddatetime "" "" @type "" "" optional "" } } { "" @id "" "" organization "" "" @inherit "" "" entity "" "" @key "" { "" @field "" [ "" organization_name "" ] "" @type "" "" lexical "" } "" @type "" "" class "" "" child "" { "" @class "" "" organization "" "" @type "" "" set "" } "" collaborator "" { "" @class "" "" user "" "" @type "" "" set "" } "" creation_date "" "" xsddatetime "" "" expiration_data "" { "" @class "" "" xsddatetime "" "" @type "" "" optional "" } "" invitation "" { "" @class "" "" invitation "" "" @type "" "" set "" } "" organization_name "" "" xsdstring "" "" owned_by "" "" user "" "" stripe_subscription "" "" stripesubscription "" } { "" @id "" "" personal "" "" @inherit "" "" organization "" "" @type "" "" class "" } { "" @id "" "" status "" "" @type "" "" enum "" "" @value "" [ "" pende "" "" inactive "" "" active "" "" needs_invite "" "" invite_sent "" "" accept "" "" reject "" ] } { "" @id "" "" stripesubscription "" "" @inherit "" "" entity "" "" @subdocument "" [ ] "" @type "" "" class "" "" @key "" { "" @type "" "" random "" } "" billing_email "" "" xsdstre "" "" stripe_id "" "" xsdstre "" "" stripe_quantity "" "" xsddecimal "" "" stripe_user "" "" user "" "" subscription_id "" "" xsdstring "" } { "" @id "" "" team "" "" @inherit "" "" organization "" "" @type "" "" class "" } { "" @id "" "" user "" "" @inherit "" "" entity "" "" @key "" { "" @field "" [ "" user_id "" ] "" @type "" "" lexical "" } "" @type "" "" class "" "" company "" "" xsdstre "" "" email "" "" xsdstre "" "" first_name "" "" xsdstring "" "" last_name "" "" xsdstre "" "" picture "" "" xsdstring "" "" registration_date "" { "" @class "" "" xsddatetime "" "" @type "" "" optional "" } "" user_id "" "" xsdstring "" } ] eof xh ' http//localhost6363 / api / document / admin / tauthor = a&message = m ' < < eof { "" @id "" "" organization_easy "" "" @type "" "" organization "" "" creation_date "" "" 2011 - 01 - 01t010037z "" "" invitation "" [ { "" @type "" "" invitation "" "" email_to "" "" easypeasy "" "" invited_by "" "" user_nancy "" } ] "" organization_name "" "" easy "" "" owned_by "" "" user_nancy "" } eof xh put ' http//localhost6363 / api / document / admin / tauthor = a&message = m ' < < eof { "" @id "" "" organization_easy "" "" @type "" "" organization "" "" creation_date "" "" 2011 - 01 - 01t010037z "" "" invitation "" [ { "" @type "" "" invitation "" "" email_to "" "" easypeasy "" "" invited_by "" "" user_nancy "" "" status "" "" needs_invite "" } ] "" organization_name "" "" easy "" "" owned_by "" "" user_nancy "" } eof xh ' http//localhost6363 / api / document / admin / tgraph_type = schema ' xh ' http//localhost6363 / api / document / admin / t '",0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,413,2021-08-16T12:21:33Z,spl,Make instance and schema predicate names consistent throughout,https://github.com/terminusdb/terminusdb/issues/413,"There are a number of places where there are similar predicates for instances and for schemas. Often, the schema predicate has “schema” in the name while the instance predicate doesn't have “schema.” Here is an example:
api_replace_document_(schema, Transaction, Document) :-
    replace_schema_document(Transaction, Document).
api_replace_document_(instance, Transaction, Document) :-
    replace_document(Transaction, Document).
These are confusing and will likely lead to misuse and bugs in the future. We should rename these.",,,,,,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,414,2021-08-16T13:29:10Z,Cheukting,Hash key becomes RandomKey,https://github.com/terminusdb/terminusdb/issues/414,"What I sent in:
{'@type': 'Class', '@id': 'Grades', 'last_name': 'xsd:string', 'first_name': 'xsd:string', 'ssn': 'xsd:string', 'test1': 'xsd:decimal', 'test2': 'xsd:decimal', 'test3': 'xsd:decimal', 'test4': 'xsd:decimal', 'final': 'xsd:decimal', 'grade': 'xsd:string', '@key': {'@type': 'Hash', '@field': ['last_name', 'first_name', 'ssn', 'test1', 'test2', 'test3', 'test4', 'final', 'grade']}}

What I get back:
[{'@base': 'terminusdb:///data/', '@schema': 'terminusdb:///schema#', '@type': '@context'}, {'@id': 'Grades', '@key': {'@type': 'Random'}, '@type': 'Class', 'final': 'xsd:decimal', 'first_name': 'xsd:string', 'grade': 'xsd:string', 'last_name': 'xsd:string', 'ssn': 'xsd:string', 'test1': 'xsd:decimal', 'test2': 'xsd:decimal', 'test3': 'xsd:decimal', 'test4': 'xsd:decimal'}]",2021-08-16T13:45:30Z,GavinMendelGleason,https://github.com/terminusdb/terminusdb/issues/414#issuecomment-899523777,"Ok, this throws an error because the ""@fields"" is spelled ""@field"" and so doesn't actually update. Somehow the client is not marshalling the error correctly.","ok this throw an error because the "" @field "" be spell "" @field "" and so do not actually update somehow the client be not marshal the error correctly",0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,414,2021-08-16T13:29:10Z,Cheukting,Hash key becomes RandomKey,https://github.com/terminusdb/terminusdb/issues/414,"What I sent in:
{'@type': 'Class', '@id': 'Grades', 'last_name': 'xsd:string', 'first_name': 'xsd:string', 'ssn': 'xsd:string', 'test1': 'xsd:decimal', 'test2': 'xsd:decimal', 'test3': 'xsd:decimal', 'test4': 'xsd:decimal', 'final': 'xsd:decimal', 'grade': 'xsd:string', '@key': {'@type': 'Hash', '@field': ['last_name', 'first_name', 'ssn', 'test1', 'test2', 'test3', 'test4', 'final', 'grade']}}

What I get back:
[{'@base': 'terminusdb:///data/', '@schema': 'terminusdb:///schema#', '@type': '@context'}, {'@id': 'Grades', '@key': {'@type': 'Random'}, '@type': 'Class', 'final': 'xsd:decimal', 'first_name': 'xsd:string', 'grade': 'xsd:string', 'last_name': 'xsd:string', 'ssn': 'xsd:string', 'test1': 'xsd:decimal', 'test2': 'xsd:decimal', 'test3': 'xsd:decimal', 'test4': 'xsd:decimal'}]",2021-08-16T13:46:15Z,GavinMendelGleason,https://github.com/terminusdb/terminusdb/issues/414#issuecomment-899524281,I think this is a bug in the client code and will close (I have reproduced correct behaviour on the server).,i think this be a bug in the client code and will close ( i have reproduce correct behaviour on the server ),0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,414,2021-08-16T13:29:10Z,Cheukting,Hash key becomes RandomKey,https://github.com/terminusdb/terminusdb/issues/414,"What I sent in:
{'@type': 'Class', '@id': 'Grades', 'last_name': 'xsd:string', 'first_name': 'xsd:string', 'ssn': 'xsd:string', 'test1': 'xsd:decimal', 'test2': 'xsd:decimal', 'test3': 'xsd:decimal', 'test4': 'xsd:decimal', 'final': 'xsd:decimal', 'grade': 'xsd:string', '@key': {'@type': 'Hash', '@field': ['last_name', 'first_name', 'ssn', 'test1', 'test2', 'test3', 'test4', 'final', 'grade']}}

What I get back:
[{'@base': 'terminusdb:///data/', '@schema': 'terminusdb:///schema#', '@type': '@context'}, {'@id': 'Grades', '@key': {'@type': 'Random'}, '@type': 'Class', 'final': 'xsd:decimal', 'first_name': 'xsd:string', 'grade': 'xsd:string', 'last_name': 'xsd:string', 'ssn': 'xsd:string', 'test1': 'xsd:decimal', 'test2': 'xsd:decimal', 'test3': 'xsd:decimal', 'test4': 'xsd:decimal'}]",2021-08-16T14:59:45Z,Cheukting,https://github.com/terminusdb/terminusdb/issues/414#issuecomment-899580680,"it will be great if you give me a shout if that's the case, will fix it on my side now",-PRON- will be great if -PRON- give -PRON- a shout if that be the case will fix -PRON- on -PRON- side now,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,416,2021-08-16T14:36:18Z,spl,Invalid URI prefixes cause schemas to be dropped.,https://github.com/terminusdb/terminusdb/issues/416,"From a clean storage:
# Create a database using `b/` and `s/` for the prefixes. Don't use `http://` here.
./terminusdb db create t --label label --comment comment --prefixes '{""@base"":""b/"",""@schema"":""s/""}'

# Create a schema. 
echo '{""@id"":""Parent"",""@type"":""Class"",""@key"":{""@type"":""Random""},""name"":""xsd:string""}' | \
  xh 'http://localhost:6363/api/document/admin/t?author=a&message=m&graph_type=schema'

# Observe that the above schema is not found in a query
xh 'http://localhost:6363/api/document/admin/t?graph_type=schema'
No errors are reported.",,,,,,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,417,2021-08-16T14:56:49Z,Cheukting,_get_current_commit looking for commit?!,https://github.com/terminusdb/terminusdb/issues/417,"what goes in:
{'@type': 'Using', 'collection': '_commits', 'query': {'@type': 'And', 'and': [{'@type': 'Triple', 'subject': {'@type': 'NodeValue', 'variable': 'branch'}, 'predicate': {'@type': 'NodeValue', 'node': 'name'}, 'object': {'@type': 'Value', 'data': {'@type': 'xsd:string', '@value': 'main'}}}, {'@type': 'And', 'and': [{'@type': 'Triple', 'subject': {'@type': 'NodeValue', 'variable': 'branch'}, 'predicate': {'@type': 'NodeValue', 'node': 'head'}, 'object': {'@type': 'Value', 'variable': 'commit'}}, {'@type': 'Triple', 'subject': {'@type': 'NodeValue', 'variable': 'commit'}, 'predicate': {'@type': 'NodeValue', 'node': 'identifier'}, 'object': {'@type': 'Value', 'variable': 'cid'}}]}]}}

what comes out:
E           terminusdb_client.errors.DatabaseError: Error: existence_error(key,message,_5332{})
E             [42] '$get_dict_ex'(message,_5384{},_5380)
E             [41] '$dicts':'.'(_5384{},message,_5424) at /usr/lib/swipl/boot/dicts.pl:46
E             [40] ref_entity:insert_base_commit_object(transaction_object{descriptor:repository_descriptor{database_descriptor: ...,repository_name:""local""},inference_objects:[],instance_objects:[...],parent:transaction_object{descriptor: ...,inference_objects:[],instance_objects: ...,schema_objects: ...},schema_objects:[...]},_5384{},1629124588.784361,""242qbhs84m8so114y2tw1k3qserd84t"",_5472) at /app/terminusdb/src/core/transaction/ref_entity.pl:136
E             [38] validate:replace_initial_commit_on_branch(transaction_object{descriptor:repository_descriptor{database_descriptor: ...,repository_name:""local""},inference_objects:[],instance_objects:[...],parent:transaction_object{descriptor: ...,inference_objects:[],instance_objects: ...,schema_objects: ...},schema_objects:[...]},'<garbage_collected>',""main"",graph_validation_obj{changed:false,descriptor:branch_graph{branch_name:""main"",commit_type:'http://terminusdb.com/schema/ref#InitialCommit',database_name:""test_happy_path"",organization_name:""admin"",repository_name:""local"",type:instance},read:_5662},graph_validation_obj{changed:false,descriptor:branch_graph{branch_name:""main"",commit_type:'http://terminusdb.com/schema/ref#InitialCommit',database_name:""test_happy_path"",organization_name:""admin"",repository_name:""local"",type:schema},read:<layer 478f1a018d7dd0f0b278b8e652d56798e61bf841>}) at /app/terminusdb/src/core/transaction/validate.pl:324
E             [36] validate:commit_validation_objects_('<garbage_collected>') at /app/terminusdb/src/core/transaction/validate.pl:423
E             [34] database:run_transactions('<garbage_collected>',false,_5820) at /app/terminusdb/src/core/transaction/database.pl:259
E             [33] database:with_transaction_(query_context{all_witnesses:false,authorization:'terminusdb://system/data/admin',bindings:[...|...],commit_info:_5384{},default_collection:branch_descriptor{branch_name:""main"",repository_descriptor: ...},files:[],filter:type_filter{types: ...},prefixes:_5956{'@base':""terminusdb:///data/"",'@schema':""terminusdb:///schema#"",'@type':'Context',api:'http://terminusdb.com/schema/api#',doc:""foo://"",owl:'http://www.w3.org/2002/07/owl#',rdf:'http://www.w3.org/1999/02/22-rdf-syntax-ns#',rdfs:'http://www.w3.org/2000/01/rdf-schema#',sys:'http://terminusdb.com/schema/sys#',vio:'http://terminusdb.com/schema/vio#',woql:'http://terminusdb.com/schema/woql#',xdd:'http://terminusdb.com/schema/xdd#',xsd:'http://www.w3.org/2001/XMLSchema#'},selected:[],system:transaction_object{descriptor: ...,inference_objects:[],instance_objects: ...,schema_objects: ...},transaction_objects:[...|...],update_guard:_5906,write_graph:branch_graph{branch_name:""main"",database_name:""test_happy_path"",organization_name:""admin"",repository_name:""local"",type:instance}},query_response:(...,...),_5860) at /app/terminusdb/src/core/transaction/database.pl:222
E             [32] setup_call_catcher_cleanup(database:true,database:with_transaction_(...,...,_6120),_6098,database:true) at /usr/lib/swipl/boot/init.pl:614
E             [29] query_response:run_context_ast_jsonld_response('<garbage_collected>','<garbage_collected>',_6166) at /app/terminusdb/src/core/query/query_response.pl:21
E             [27] '<meta-call>'('<garbage_collected>') <foreign>
E             [26] catch(routes:(...,...),error(existence_error(key,message,...),context(...,_6266)),routes:do_or_die(...,...)) at /usr/lib/swipl/boot/init.pl:532
E             [25] catch_with_backtrace('<garbage_collected>','<garbage_collected>','<garbage_collected>') at /usr/lib/swipl/boot/init.pl:582",2021-08-16T18:33:40Z,GavinMendelGleason,https://github.com/terminusdb/terminusdb/issues/417#issuecomment-899729485,It somehow seems impossible that we'd be conducting a commit on the back of this query. Does something else happen potentially?,-PRON- somehow seem impossible that -PRON- 'd be conduct a commit on the back of this query do something else happen potentially,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,417,2021-08-16T14:56:49Z,Cheukting,_get_current_commit looking for commit?!,https://github.com/terminusdb/terminusdb/issues/417,"what goes in:
{'@type': 'Using', 'collection': '_commits', 'query': {'@type': 'And', 'and': [{'@type': 'Triple', 'subject': {'@type': 'NodeValue', 'variable': 'branch'}, 'predicate': {'@type': 'NodeValue', 'node': 'name'}, 'object': {'@type': 'Value', 'data': {'@type': 'xsd:string', '@value': 'main'}}}, {'@type': 'And', 'and': [{'@type': 'Triple', 'subject': {'@type': 'NodeValue', 'variable': 'branch'}, 'predicate': {'@type': 'NodeValue', 'node': 'head'}, 'object': {'@type': 'Value', 'variable': 'commit'}}, {'@type': 'Triple', 'subject': {'@type': 'NodeValue', 'variable': 'commit'}, 'predicate': {'@type': 'NodeValue', 'node': 'identifier'}, 'object': {'@type': 'Value', 'variable': 'cid'}}]}]}}

what comes out:
E           terminusdb_client.errors.DatabaseError: Error: existence_error(key,message,_5332{})
E             [42] '$get_dict_ex'(message,_5384{},_5380)
E             [41] '$dicts':'.'(_5384{},message,_5424) at /usr/lib/swipl/boot/dicts.pl:46
E             [40] ref_entity:insert_base_commit_object(transaction_object{descriptor:repository_descriptor{database_descriptor: ...,repository_name:""local""},inference_objects:[],instance_objects:[...],parent:transaction_object{descriptor: ...,inference_objects:[],instance_objects: ...,schema_objects: ...},schema_objects:[...]},_5384{},1629124588.784361,""242qbhs84m8so114y2tw1k3qserd84t"",_5472) at /app/terminusdb/src/core/transaction/ref_entity.pl:136
E             [38] validate:replace_initial_commit_on_branch(transaction_object{descriptor:repository_descriptor{database_descriptor: ...,repository_name:""local""},inference_objects:[],instance_objects:[...],parent:transaction_object{descriptor: ...,inference_objects:[],instance_objects: ...,schema_objects: ...},schema_objects:[...]},'<garbage_collected>',""main"",graph_validation_obj{changed:false,descriptor:branch_graph{branch_name:""main"",commit_type:'http://terminusdb.com/schema/ref#InitialCommit',database_name:""test_happy_path"",organization_name:""admin"",repository_name:""local"",type:instance},read:_5662},graph_validation_obj{changed:false,descriptor:branch_graph{branch_name:""main"",commit_type:'http://terminusdb.com/schema/ref#InitialCommit',database_name:""test_happy_path"",organization_name:""admin"",repository_name:""local"",type:schema},read:<layer 478f1a018d7dd0f0b278b8e652d56798e61bf841>}) at /app/terminusdb/src/core/transaction/validate.pl:324
E             [36] validate:commit_validation_objects_('<garbage_collected>') at /app/terminusdb/src/core/transaction/validate.pl:423
E             [34] database:run_transactions('<garbage_collected>',false,_5820) at /app/terminusdb/src/core/transaction/database.pl:259
E             [33] database:with_transaction_(query_context{all_witnesses:false,authorization:'terminusdb://system/data/admin',bindings:[...|...],commit_info:_5384{},default_collection:branch_descriptor{branch_name:""main"",repository_descriptor: ...},files:[],filter:type_filter{types: ...},prefixes:_5956{'@base':""terminusdb:///data/"",'@schema':""terminusdb:///schema#"",'@type':'Context',api:'http://terminusdb.com/schema/api#',doc:""foo://"",owl:'http://www.w3.org/2002/07/owl#',rdf:'http://www.w3.org/1999/02/22-rdf-syntax-ns#',rdfs:'http://www.w3.org/2000/01/rdf-schema#',sys:'http://terminusdb.com/schema/sys#',vio:'http://terminusdb.com/schema/vio#',woql:'http://terminusdb.com/schema/woql#',xdd:'http://terminusdb.com/schema/xdd#',xsd:'http://www.w3.org/2001/XMLSchema#'},selected:[],system:transaction_object{descriptor: ...,inference_objects:[],instance_objects: ...,schema_objects: ...},transaction_objects:[...|...],update_guard:_5906,write_graph:branch_graph{branch_name:""main"",database_name:""test_happy_path"",organization_name:""admin"",repository_name:""local"",type:instance}},query_response:(...,...),_5860) at /app/terminusdb/src/core/transaction/database.pl:222
E             [32] setup_call_catcher_cleanup(database:true,database:with_transaction_(...,...,_6120),_6098,database:true) at /usr/lib/swipl/boot/init.pl:614
E             [29] query_response:run_context_ast_jsonld_response('<garbage_collected>','<garbage_collected>',_6166) at /app/terminusdb/src/core/query/query_response.pl:21
E             [27] '<meta-call>'('<garbage_collected>') <foreign>
E             [26] catch(routes:(...,...),error(existence_error(key,message,...),context(...,_6266)),routes:do_or_die(...,...)) at /usr/lib/swipl/boot/init.pl:532
E             [25] catch_with_backtrace('<garbage_collected>','<garbage_collected>','<garbage_collected>') at /usr/lib/swipl/boot/init.pl:582",2021-08-23T13:13:35Z,spl,https://github.com/terminusdb/terminusdb/issues/417#issuecomment-903755722,Fixed!,fix,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,420,2021-08-17T09:51:14Z,AstroChelonian,api_replace_document_ should never fail,https://github.com/terminusdb/terminusdb/issues/420,"In api_document.pl, api_replace_document_ should not fail.",,,,,,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,423,2021-08-17T14:51:35Z,Cheukting,Getting data from a centain date,https://github.com/terminusdb/terminusdb/issues/423,"So while developing the Singer tap, there is a common setting that someone would like to get all the data starting from a specific date (the date of the last sync etc) and it would be nice to have an easy to do that in the document interface. Ideally an extra parameter in the get_document or get_all_documents.",2021-08-17T14:55:26Z,Cheukting,https://github.com/terminusdb/terminusdb/issues/423#issuecomment-900370913,Also it will be great if the data get back can have an option to stream in chronological order.,also -PRON- will be great if the datum get back can have an option to stream in chronological order,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,431,2021-08-19T10:52:02Z,matko,"In the document API, duplicate IDs error should tell us which ids are duplicates",https://github.com/terminusdb/terminusdb/issues/431,"Currently, when inserting or updating documents resulting in duplicate documents, the system will error. The error however does not say which ids were double, it just returns all the ids that it attempted to insert.
Change this to just the ids that matter for the error.",,,,,,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,433,2021-08-19T11:00:24Z,spl,Tests for push/tus,https://github.com/terminusdb/terminusdb/issues/433,"We can run the unit tests without installing tus, which is used for push. Do we have/need tests for this?",,,,,,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,434,2021-08-19T12:38:37Z,Cheukting,Get a 500 message in a 200 response obj,https://github.com/terminusdb/terminusdb/issues/434,"When calling get_all_documents()
Pdb) result
<Response [200]>
(Pdb) result.text
'{\n  ""@base"":""terminusdb:///data/"",\n  ""@schema"":""terminusdb:///schema#"",\n  ""@type"":""@context""\n}\n{\n  ""@documentation"": {""@comment"":""This is address""},\n  ""@id"":""Address"",\n  ""@key"": {""@type"":""Random""},\n  ""@subdocument"": [],\n  ""@type"":""Class"",\n  ""country"":""Country"",\n  ""postal_code"":""xsd:string"",\n  ""street"":""xsd:string""\n}\n{\n  ""@documentation"": {""@comment"":""TaggedUnion allow options for types""},\n  ""@id"":""Contact"",\n  ""@inherits"":""TaggedUnion"",\n  ""@key"": {""@type"":""Random""},\n  ""@type"":""Class"",\n  ""international"":""xsd:string"",\n  ""local_number"":""xsd:integer""\n}\n{\n  ""@abstract"": [],\n  ""@id"":""Coordinate"",\n  ""@key"": {""@type"":""Random""},\n  ""@type"":""Class"",\n  ""x"":""xsd:decimal"",\n  ""y"":""xsd:decimal""\n}\n{\n  ""@documentation"": {\n    ""@comment"":""This is Country.\\nCountry is a class object in the schema. It\'s class attributes will be the properties of the object. Therefore, a Country object will have a name which is string and a list of alias names that is called \'also_know_as\'""\n  },\n  ""@id"":""Country"",\n  ""@key"": {""@fields"": [""name"" ], ""@type"":""Hash""},\n  ""@type"":""Class"",\n  ""also_know_as"": {""@class"":""xsd:string"", ""@type"":""List""},\n  ""name"":""xsd:string""\n}\n{\n  ""@documentation"": {""@comment"":""Employee will inherits the attributes from Person""},\n  ""@id"":""Employee"",\n  ""@inherits"":""Person"",\n  ""@key"": {""@type"":""Random""},\n  ""@type"":""Class"",\n  ""address_of"":""Address"",\n  ""age"":""xsd:integer"",\n  ""contact_number"": {""@class"":""xsd:string"", ""@type"":""Optional""},\n  ""friend_of"": {""@class"":""Person"", ""@type"":""Set""},\n  ""managed_by"":""Employee"",\n  ""name"":""xsd:string""\n}\n{\n  ""@abstract"": [],\n  ""@documentation"": {\n    ""@comment"":""Location is inherits from Address and Coordinate\\nClass can have multiple inheritance. It will inherits both the attibutes fromAddress and Coordinate.""\n  },\n  ""@id"":""Location"",\n  ""@inherits"": [""Address"", ""Coordinate"" ],\n  ""@key"": {""@type"":""Random""},\n  ""@subdocument"": [],\n  ""@type"":""Class"",\n  ""country"":""Country"",\n  ""name"":""xsd:string"",\n  ""postal_code"":""xsd:string"",\n  ""street"":""xsd:string"",\n  ""x"":""xsd:decimal"",\n  ""y"":""xsd:decimal""\n}\nStatus: 500\nContent-type: application/json\n\n{\n  ""api:message"":""Error: duplicate_key(\'terminusdb:///schema#age\')\\ncontext(system:dict_pairs/3,_6098)\\n"",\n  ""api:status"":""api:server_error""\n}'
(Pdb) 

Status is 200 but there's an error message inside",2021-08-19T12:51:02Z,Cheukting,https://github.com/terminusdb/terminusdb/issues/434#issuecomment-901888294,"{'@type': 'Class', '@id': 'Address', '@documentation': {'@comment': 'This is address', '@properties': {}}, '@subdocument': [], '@key': {'@type': 'Random'}, 'country': 'Country', 'postal_code': 'xsd:string', 'street': 'xsd:string'}
{'@type': 'Class', '@id': 'Contact', '@inherits': ['TaggedUnion'], '@documentation': {'@comment': 'TaggedUnion allow options for types', '@properties': {}}, '@key': {'@type': 'Random'}, 'international': 'xsd:string', 'local_number': 'xsd:integer'}
{'@type': 'Class', '@id': 'Coordinate', '@abstract': [], '@key': {'@type': 'Random'}, 'x': 'xsd:decimal', 'y': 'xsd:decimal'}
{'@type': 'Class', '@id': 'Country', '@documentation': {'@comment': ""This is Country.\nCountry is a class object in the schema. It's class attributes will be the properties of the object. Therefore, a Country object will have a name which is string and a list of alias names that is called 'also_know_as'"", '@properties': {}}, '@key': {'@type': 'Hash', '@fields': ['name']}, 'also_know_as': {'@type': 'List', '@class': 'xsd:string'}, 'name': 'xsd:string'}
{'@type': 'Class', '@id': 'Employee', '@inherits': ['Person'], '@documentation': {'@comment': 'Employee will inherits the attributes from Person', '@properties': {}}, '@key': {'@type': 'Random'}, 'address_of': 'Address', 'age': 'xsd:integer', 'contact_number': {'@type': 'Optional', '@class': 'xsd:string'}, 'friend_of': {'@type': 'Set', '@class': 'Person'}, 'managed_by': 'Employee', 'name': 'xsd:string'}
{'@type': 'Class', '@id': 'Location', '@inherits': ['Address', 'Coordinate'], '@documentation': {'@comment': 'Location is inherits from Address and Coordinate\nClass can have multiple inheritance. It will inherits both the attibutes from Address and Coordinate.', '@properties': {}}, '@subdocument': [], '@key': {'@type': 'Random'}, '@abstract': [], 'country': 'Country', 'name': 'xsd:string', 'postal_code': 'xsd:string', 'street': 'xsd:string', 'x': 'xsd:decimal', 'y': 'xsd:decimal'}
{'@type': 'Class', '@id': 'Person', '@documentation': {'@comment': 'This is a person\nCan store the explanation to the attributes in the docstring. Docstrings needs to be in numpydoc format.', '@properties': {'age': '    Age of the person.\nname : str\n    Name of the person.'}}, '@key': {'@type': 'Random'}, 'age': 'xsd:integer', 'friend_of': {'@type': 'Set', '@class': 'Person'}, 'name': 'xsd:string'}
{'@type': 'Enum', '@id': 'Team', '@value': ['Information Technology', 'Marketing']}

@GavinMendelGleason that's what you ask for","{ ' @type ' ' class ' ' @id ' ' address ' ' @documentation ' { ' @comment ' ' this be address ' ' @propertie ' { } } ' @subdocument ' [ ] ' @key ' { ' @type ' ' random ' } ' country ' ' country ' ' postal_code ' ' xsdstre ' ' street ' ' xsdstring ' } { ' @type ' ' class ' ' @id ' ' contact ' ' @inherits ' [ ' taggedunion ' ] ' @documentation ' { ' @comment ' ' taggedunion allow option for type ' ' @propertie ' { } } ' @key ' { ' @type ' ' random ' } ' international ' ' xsdstre ' ' local_number ' ' xsdinteg ' } { ' @type ' ' class ' ' @id ' ' coordinate ' ' @abstract ' [ ] ' @key ' { ' @type ' ' random ' } ' x ' ' xsddecimal ' ' y ' ' xsddecimal ' } { ' @type ' ' class ' ' @id ' ' country ' ' @documentation ' { ' @comment ' "" this be country\ncountry be a class object in the schema -PRON- be class attribute will be the property of the object therefore a country object will have a name which be string and a list of alias name that be call ' also_know_as ' "" ' @propertie ' { } } ' @key ' { ' @type ' ' hash ' ' @fields ' [ ' name ' ] } ' also_know_as ' { ' @type ' ' list ' ' @class ' ' xsdstre ' } ' name ' ' xsdstring ' } { ' @type ' ' class ' ' @id ' ' employee ' ' @inherits ' [ ' person ' ] ' @documentation ' { ' @comment ' ' employee will inherit the attribute from person ' ' @propertie ' { } } ' @key ' { ' @type ' ' random ' } ' address_of ' ' address ' ' age ' ' xsdinteg ' ' contact_number ' { ' @type ' ' optional ' ' @class ' ' xsdstre ' } ' friend_of ' { ' @type ' ' set ' ' @class ' ' person ' } ' managed_by ' ' employee ' ' name ' ' xsdstring ' } { ' @type ' ' class ' ' @id ' ' location ' ' @inherits ' [ ' address ' ' coordinate ' ] ' @documentation ' { ' @comment ' ' location be inherit from address and coordinate\nclass can have multiple inheritance -PRON- will inherit both the attibute from address and coordinate ' ' @propertie ' { } } ' @subdocument ' [ ] ' @key ' { ' @type ' ' random ' } ' @abstract ' [ ] ' country ' ' country ' ' name ' ' xsdstre ' ' postal_code ' ' xsdstre ' ' street ' ' xsdstre ' ' x ' ' xsddecimal ' ' y ' ' xsddecimal ' } { ' @type ' ' class ' ' @id ' ' person ' ' @documentation ' { ' @comment ' ' this be a person\ncan store the explanation to the attribute in the docstring docstring need to be in numpydoc format ' ' @propertie ' { ' age ' ' age of the person\nname str\n name of the person ' } } ' @key ' { ' @type ' ' random ' } ' age ' ' xsdinteg ' ' friend_of ' { ' @type ' ' set ' ' @class ' ' person ' } ' name ' ' xsdstring ' } { ' @type ' ' enum ' ' @id ' ' team ' ' @value ' [ ' information technology ' ' marketing ' ] } @gavinmendelgleason that be what -PRON- ask for",0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,434,2021-08-19T12:38:37Z,Cheukting,Get a 500 message in a 200 response obj,https://github.com/terminusdb/terminusdb/issues/434,"When calling get_all_documents()
Pdb) result
<Response [200]>
(Pdb) result.text
'{\n  ""@base"":""terminusdb:///data/"",\n  ""@schema"":""terminusdb:///schema#"",\n  ""@type"":""@context""\n}\n{\n  ""@documentation"": {""@comment"":""This is address""},\n  ""@id"":""Address"",\n  ""@key"": {""@type"":""Random""},\n  ""@subdocument"": [],\n  ""@type"":""Class"",\n  ""country"":""Country"",\n  ""postal_code"":""xsd:string"",\n  ""street"":""xsd:string""\n}\n{\n  ""@documentation"": {""@comment"":""TaggedUnion allow options for types""},\n  ""@id"":""Contact"",\n  ""@inherits"":""TaggedUnion"",\n  ""@key"": {""@type"":""Random""},\n  ""@type"":""Class"",\n  ""international"":""xsd:string"",\n  ""local_number"":""xsd:integer""\n}\n{\n  ""@abstract"": [],\n  ""@id"":""Coordinate"",\n  ""@key"": {""@type"":""Random""},\n  ""@type"":""Class"",\n  ""x"":""xsd:decimal"",\n  ""y"":""xsd:decimal""\n}\n{\n  ""@documentation"": {\n    ""@comment"":""This is Country.\\nCountry is a class object in the schema. It\'s class attributes will be the properties of the object. Therefore, a Country object will have a name which is string and a list of alias names that is called \'also_know_as\'""\n  },\n  ""@id"":""Country"",\n  ""@key"": {""@fields"": [""name"" ], ""@type"":""Hash""},\n  ""@type"":""Class"",\n  ""also_know_as"": {""@class"":""xsd:string"", ""@type"":""List""},\n  ""name"":""xsd:string""\n}\n{\n  ""@documentation"": {""@comment"":""Employee will inherits the attributes from Person""},\n  ""@id"":""Employee"",\n  ""@inherits"":""Person"",\n  ""@key"": {""@type"":""Random""},\n  ""@type"":""Class"",\n  ""address_of"":""Address"",\n  ""age"":""xsd:integer"",\n  ""contact_number"": {""@class"":""xsd:string"", ""@type"":""Optional""},\n  ""friend_of"": {""@class"":""Person"", ""@type"":""Set""},\n  ""managed_by"":""Employee"",\n  ""name"":""xsd:string""\n}\n{\n  ""@abstract"": [],\n  ""@documentation"": {\n    ""@comment"":""Location is inherits from Address and Coordinate\\nClass can have multiple inheritance. It will inherits both the attibutes fromAddress and Coordinate.""\n  },\n  ""@id"":""Location"",\n  ""@inherits"": [""Address"", ""Coordinate"" ],\n  ""@key"": {""@type"":""Random""},\n  ""@subdocument"": [],\n  ""@type"":""Class"",\n  ""country"":""Country"",\n  ""name"":""xsd:string"",\n  ""postal_code"":""xsd:string"",\n  ""street"":""xsd:string"",\n  ""x"":""xsd:decimal"",\n  ""y"":""xsd:decimal""\n}\nStatus: 500\nContent-type: application/json\n\n{\n  ""api:message"":""Error: duplicate_key(\'terminusdb:///schema#age\')\\ncontext(system:dict_pairs/3,_6098)\\n"",\n  ""api:status"":""api:server_error""\n}'
(Pdb) 

Status is 200 but there's an error message inside",2021-08-19T12:55:45Z,Cheukting,https://github.com/terminusdb/terminusdb/issues/434#issuecomment-901891455,"I delete the db and start again and it works, the error disappears. But I have a feeling that it is not only happening for the 1s time and there are some steps that I did to break it and I don't know-how.",i delete the db and start again and -PRON- work the error disappear but i have a feeling that -PRON- be not only happen for the 1 time and there be some step that i do to break -PRON- and i do not know - how,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,434,2021-08-19T12:38:37Z,Cheukting,Get a 500 message in a 200 response obj,https://github.com/terminusdb/terminusdb/issues/434,"When calling get_all_documents()
Pdb) result
<Response [200]>
(Pdb) result.text
'{\n  ""@base"":""terminusdb:///data/"",\n  ""@schema"":""terminusdb:///schema#"",\n  ""@type"":""@context""\n}\n{\n  ""@documentation"": {""@comment"":""This is address""},\n  ""@id"":""Address"",\n  ""@key"": {""@type"":""Random""},\n  ""@subdocument"": [],\n  ""@type"":""Class"",\n  ""country"":""Country"",\n  ""postal_code"":""xsd:string"",\n  ""street"":""xsd:string""\n}\n{\n  ""@documentation"": {""@comment"":""TaggedUnion allow options for types""},\n  ""@id"":""Contact"",\n  ""@inherits"":""TaggedUnion"",\n  ""@key"": {""@type"":""Random""},\n  ""@type"":""Class"",\n  ""international"":""xsd:string"",\n  ""local_number"":""xsd:integer""\n}\n{\n  ""@abstract"": [],\n  ""@id"":""Coordinate"",\n  ""@key"": {""@type"":""Random""},\n  ""@type"":""Class"",\n  ""x"":""xsd:decimal"",\n  ""y"":""xsd:decimal""\n}\n{\n  ""@documentation"": {\n    ""@comment"":""This is Country.\\nCountry is a class object in the schema. It\'s class attributes will be the properties of the object. Therefore, a Country object will have a name which is string and a list of alias names that is called \'also_know_as\'""\n  },\n  ""@id"":""Country"",\n  ""@key"": {""@fields"": [""name"" ], ""@type"":""Hash""},\n  ""@type"":""Class"",\n  ""also_know_as"": {""@class"":""xsd:string"", ""@type"":""List""},\n  ""name"":""xsd:string""\n}\n{\n  ""@documentation"": {""@comment"":""Employee will inherits the attributes from Person""},\n  ""@id"":""Employee"",\n  ""@inherits"":""Person"",\n  ""@key"": {""@type"":""Random""},\n  ""@type"":""Class"",\n  ""address_of"":""Address"",\n  ""age"":""xsd:integer"",\n  ""contact_number"": {""@class"":""xsd:string"", ""@type"":""Optional""},\n  ""friend_of"": {""@class"":""Person"", ""@type"":""Set""},\n  ""managed_by"":""Employee"",\n  ""name"":""xsd:string""\n}\n{\n  ""@abstract"": [],\n  ""@documentation"": {\n    ""@comment"":""Location is inherits from Address and Coordinate\\nClass can have multiple inheritance. It will inherits both the attibutes fromAddress and Coordinate.""\n  },\n  ""@id"":""Location"",\n  ""@inherits"": [""Address"", ""Coordinate"" ],\n  ""@key"": {""@type"":""Random""},\n  ""@subdocument"": [],\n  ""@type"":""Class"",\n  ""country"":""Country"",\n  ""name"":""xsd:string"",\n  ""postal_code"":""xsd:string"",\n  ""street"":""xsd:string"",\n  ""x"":""xsd:decimal"",\n  ""y"":""xsd:decimal""\n}\nStatus: 500\nContent-type: application/json\n\n{\n  ""api:message"":""Error: duplicate_key(\'terminusdb:///schema#age\')\\ncontext(system:dict_pairs/3,_6098)\\n"",\n  ""api:status"":""api:server_error""\n}'
(Pdb) 

Status is 200 but there's an error message inside",2021-08-19T12:56:48Z,Cheukting,https://github.com/terminusdb/terminusdb/issues/434#issuecomment-901892209,I guess the bug gat away this time again.,i guess the bug gat away this time again,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,434,2021-08-19T12:38:37Z,Cheukting,Get a 500 message in a 200 response obj,https://github.com/terminusdb/terminusdb/issues/434,"When calling get_all_documents()
Pdb) result
<Response [200]>
(Pdb) result.text
'{\n  ""@base"":""terminusdb:///data/"",\n  ""@schema"":""terminusdb:///schema#"",\n  ""@type"":""@context""\n}\n{\n  ""@documentation"": {""@comment"":""This is address""},\n  ""@id"":""Address"",\n  ""@key"": {""@type"":""Random""},\n  ""@subdocument"": [],\n  ""@type"":""Class"",\n  ""country"":""Country"",\n  ""postal_code"":""xsd:string"",\n  ""street"":""xsd:string""\n}\n{\n  ""@documentation"": {""@comment"":""TaggedUnion allow options for types""},\n  ""@id"":""Contact"",\n  ""@inherits"":""TaggedUnion"",\n  ""@key"": {""@type"":""Random""},\n  ""@type"":""Class"",\n  ""international"":""xsd:string"",\n  ""local_number"":""xsd:integer""\n}\n{\n  ""@abstract"": [],\n  ""@id"":""Coordinate"",\n  ""@key"": {""@type"":""Random""},\n  ""@type"":""Class"",\n  ""x"":""xsd:decimal"",\n  ""y"":""xsd:decimal""\n}\n{\n  ""@documentation"": {\n    ""@comment"":""This is Country.\\nCountry is a class object in the schema. It\'s class attributes will be the properties of the object. Therefore, a Country object will have a name which is string and a list of alias names that is called \'also_know_as\'""\n  },\n  ""@id"":""Country"",\n  ""@key"": {""@fields"": [""name"" ], ""@type"":""Hash""},\n  ""@type"":""Class"",\n  ""also_know_as"": {""@class"":""xsd:string"", ""@type"":""List""},\n  ""name"":""xsd:string""\n}\n{\n  ""@documentation"": {""@comment"":""Employee will inherits the attributes from Person""},\n  ""@id"":""Employee"",\n  ""@inherits"":""Person"",\n  ""@key"": {""@type"":""Random""},\n  ""@type"":""Class"",\n  ""address_of"":""Address"",\n  ""age"":""xsd:integer"",\n  ""contact_number"": {""@class"":""xsd:string"", ""@type"":""Optional""},\n  ""friend_of"": {""@class"":""Person"", ""@type"":""Set""},\n  ""managed_by"":""Employee"",\n  ""name"":""xsd:string""\n}\n{\n  ""@abstract"": [],\n  ""@documentation"": {\n    ""@comment"":""Location is inherits from Address and Coordinate\\nClass can have multiple inheritance. It will inherits both the attibutes fromAddress and Coordinate.""\n  },\n  ""@id"":""Location"",\n  ""@inherits"": [""Address"", ""Coordinate"" ],\n  ""@key"": {""@type"":""Random""},\n  ""@subdocument"": [],\n  ""@type"":""Class"",\n  ""country"":""Country"",\n  ""name"":""xsd:string"",\n  ""postal_code"":""xsd:string"",\n  ""street"":""xsd:string"",\n  ""x"":""xsd:decimal"",\n  ""y"":""xsd:decimal""\n}\nStatus: 500\nContent-type: application/json\n\n{\n  ""api:message"":""Error: duplicate_key(\'terminusdb:///schema#age\')\\ncontext(system:dict_pairs/3,_6098)\\n"",\n  ""api:status"":""api:server_error""\n}'
(Pdb) 

Status is 200 but there's an error message inside",2021-08-26T08:17:51Z,matko,https://github.com/terminusdb/terminusdb/issues/434#issuecomment-906196433,"closing, as the bug appears to not be reproducible.",close as the bug appear to not be reproducible,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,437,2021-08-20T06:19:26Z,matko,Inserting classes of unknown type into the schema graph is not checked properly,https://github.com/terminusdb/terminusdb/issues/437,"It is possible to insert a type definition into a schema like the following:
{""@id"":""Moooo"",
 ""@type"":""This_Type_Has_No_Meaning"",
 ""name"":""xsd:string""}

Even though the given type is unknown in our schema language this will still be accepted. For the above example, it will insert a type triple, as well as a triple for the name property definition, so it appears this is treated like a Class. Subsequent document queries by id can retrieve this definition, but retrieving the full schema will skip over it, since this only retrieves classes of known types.
We should make sure that everything in the schema has a type known to the checker, so we don't end up with garbage in our schemas. This is especially important to catch typos like 'Claass' instead of 'Class'.",2021-08-20T06:31:00Z,matko,https://github.com/terminusdb/terminusdb/issues/437#issuecomment-902465175,Omitting the type is also apparently allowed.,omit the type be also apparently allow,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,437,2021-08-20T06:19:26Z,matko,Inserting classes of unknown type into the schema graph is not checked properly,https://github.com/terminusdb/terminusdb/issues/437,"It is possible to insert a type definition into a schema like the following:
{""@id"":""Moooo"",
 ""@type"":""This_Type_Has_No_Meaning"",
 ""name"":""xsd:string""}

Even though the given type is unknown in our schema language this will still be accepted. For the above example, it will insert a type triple, as well as a triple for the name property definition, so it appears this is treated like a Class. Subsequent document queries by id can retrieve this definition, but retrieving the full schema will skip over it, since this only retrieves classes of known types.
We should make sure that everything in the schema has a type known to the checker, so we don't end up with garbage in our schemas. This is especially important to catch typos like 'Claass' instead of 'Class'.",2021-09-15T12:09:21Z,matko,https://github.com/terminusdb/terminusdb/issues/437#issuecomment-919962057,This has been resolved in pull request #564.,this have be resolve in pull request # 564,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,438,2021-08-20T07:30:17Z,matko,Required fields are not actually required,https://github.com/terminusdb/terminusdb/issues/438,"I've inserted the following class definition:
{""@id"":""Moo"",
 ""@type"":""Class"",
 ""name"":""xsd:string""}

When I then insert the following object:
{""@type"": ""Moo"",
 ""@id"": ""an_absolutely_humongous_moo"",
}

The document is inserted without problems, even though the name field is missing.
This document should have been rejected because it has a missing field.",2021-08-20T09:30:40Z,GavinMendelGleason,https://github.com/terminusdb/terminusdb/issues/438#issuecomment-902564886,Fixed in 2922823,fix in 2922823,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,441,2021-08-21T15:25:04Z,AstroChelonian,Add authentication in api_document.pl,https://github.com/terminusdb/terminusdb/issues/441,Authentication parameters are missing from api_document.pl,2021-08-23T13:12:37Z,matko,https://github.com/terminusdb/terminusdb/issues/441#issuecomment-903754813,"All endpoints that I know of do authorization in the latest version, since last week. Perhaps you don't have it. Could you give more details?",all endpoint that i know of do authorization in the late version since last week perhaps -PRON- do not have -PRON- could -PRON- give more detail,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,441,2021-08-21T15:25:04Z,AstroChelonian,Add authentication in api_document.pl,https://github.com/terminusdb/terminusdb/issues/441,Authentication parameters are missing from api_document.pl,2021-08-30T14:01:11Z,spl,https://github.com/terminusdb/terminusdb/issues/441#issuecomment-908367854,Fixed! 🎉,fix 🎉,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,448,2021-08-23T21:31:28Z,GavinMendelGleason,Schema check failure not caught in DB create for API,https://github.com/terminusdb/terminusdb/issues/448,"This should return a pretty error. Also it would be nice if base and schema prefixes could be specified separately on the command line and with some sensible defaults.
gavin@titan:~/dev/terminusdb$ terminusdb db create test
  [15] throw(error(schema_check_failure(...),_18264))
  [13] db_create:create_db(query_context{all_witnesses:false,authorization:'terminusdb://system/data/admin',bindings:[],default_collection:system_descriptor{},files:[],filter:type_filter{types: ...},prefixes:_18380{'@base':""terminusdb://system/data/"",'@schema':""http://terminusdb.com/schema/system#"",'@type':'Context',api:'http://terminusdb.com/schema/api#',owl:'http://www.w3.org/2002/07/owl#',rdf:'http://www.w3.org/1999/02/22-rdf-syntax-ns#',rdfs:'http://www.w3.org/2000/01/rdf-schema#',sys:'http://terminusdb.com/schema/sys#',vio:'http://terminusdb.com/schema/vio#',woql:'http://terminusdb.com/schema/woql#',xdd:'http://terminusdb.com/schema/xdd#',xsd:""http://www.w3.org/2001/XMLSchema#""},selected:[],system:system_descriptor{},transaction_objects:[...],update_guard:_18358,write_graph:system_graph{type:instance}},'terminusdb://system/data/admin',admin,test,'','',true,false,_18458{doc:'terminusdb:///data/',scm:'terminusdb:///schema#'}) at /home/gavin/dev/terminusdb/src/core/api/db_create.pl:156
  [12] catch(cli:create_db(...,'terminusdb://system/data/admin',admin,test,'','',true,false,...),error(schema_check_failure(...),context(_18540,_18542)),cli:do_or_die(...,...)) at /home/gavin/.swivm/versions/v8.2.2/lib/swipl/boot/init.pl:530
  [11] catch_with_backtrace(cli:create_db(...,'terminusdb://system/data/admin',admin,test,'','',true,false,...),error(schema_check_failure(...),context(_18628,_18630)),cli:do_or_die(...,...)) at /home/gavin/.swivm/versions/v8.2.2/lib/swipl/boot/init.pl:580

Note: some frames are missing due to last-call optimization.
Re-run your program in debug mode (:- debug.) to get more detail.
Error: schema_check_failure([witness{'@type':context_missing_system_prefix,prefix_name:'http://terminusdb.com/schema/sys#base'}])```",,,,,,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,449,2021-08-24T11:55:59Z,GavinMendelGleason,Nonsense data value gives stacktrace rather than JSON error,https://github.com/terminusdb/terminusdb/issues/449,"When I submit a null atom instead of a decimal, I get a stack trace:
Traceback (most recent call last):
  File ""<stdin>"", line 21, in <module>
  File ""/home/gavin/dev/terminusdb-client-python/terminusdb_client/woqlclient/woqlClient.py"", line 817, in insert_document
    return json.loads(_finish_reponse(result))
  File ""/home/gavin/dev/terminusdb-client-python/terminusdb_client/woql_utils.py"", line 146, in _finish_reponse
    raise DatabaseError(request_response)
terminusdb_client.errors.DatabaseError: Error: casting_error(""null"",'http://www.w3.org/2001/XMLSchema#decimal')
  [46] throw(error(casting_error(""null"",'http://www.w3.org/2001/XMLSchema#decimal'),_116566))
  [44] catch('<garbage_collected>','<garbage_collected>','<garbage_collected>') at /usr/lib/swipl/boot/init.pl:533
  [42] api_document:api_insert_document_(instance,transaction_object{commit_info:commit_info{},descriptor:branch_descriptor{branch_name:""main"",repository_descriptor: ...},inference_objects:[],instance_objects:[...],parent:transaction_object{descriptor: ...,inference_objects:[],instance_objects: ...,parent: ...,schema_objects: ...},schema_objects:[...]},<stream>(0x556b3c37d700),_116648) at /app/terminusdb/src/core/api/api_document.pl:180
  [41] '$bags':findall_loop(_116764,'<garbage_collected>',_116768,[]) at /usr/lib/swipl/boot/bags.pl:99
  [40] setup_call_catcher_cleanup('$bags':'$new_findall_bag','$bags':findall_loop(_116822,...,_116826,[]),_116804,'$bags':'$destroy_findall_bag') at /usr/lib/swipl/boot/init.pl:614
  [36] '<meta-call>'('<garbage_collected>') <foreign>
  [35] database:with_transaction_(query_context{all_witnesses:false,authorization:'terminusdb://system/data/admin',bindings:[],commit_info:commit_info{author:admin,message:'Inserting stock exchange ticker chunk 0'},default_collection:branch_descriptor{branch_name:""main"",repository_descriptor: ...},files:[],filter:type_filter{types: ...},prefixes:_116972{'@base':""terminusdb:///stock_exchange/"",'@schema':""terminusdb:///stock_exchange/schema#"",'@type':'Context',api:'http://terminusdb.com/schema/api#',owl:'http://www.w3.org/2002/07/owl#',rdf:'http://www.w3.org/1999/02/22-rdf-syntax-ns#',rdfs:'http://www.w3.org/2000/01/rdf-schema#',sys:'http://terminusdb.com/schema/sys#',vio:'http://terminusdb.com/schema/vio#',woql:'http://terminusdb.com/schema/woql#',xdd:'http://terminusdb.com/schema/xdd#',xsd:'http://www.w3.org/2001/XMLSchema#'},selected:[],system:system_descriptor{},transaction_objects:[...],update_guard:_116958,write_graph:branch_graph{branch_name:""main"",database_name:""sed"",organization_name:""ubf40420team"",repository_name:""local"",type:instance}},api_document:(...;...),_116908) at /app/terminusdb/src/core/transaction/database.pl:220
  [34] setup_call_catcher_cleanup(database:true,database:with_transaction_(...,...,_117150),_117128,database:true) at /usr/lib/swipl/boot/init.pl:614
  [32] database:with_transaction('<garbage_collected>','<garbage_collected>','<garbage_collected>') at /app/terminusdb/src/core/transaction/database.pl:209
  [30] '<meta-call>'('<garbage_collected>') <foreign>
  [29] catch(routes:(...,...),error(casting_error(""null"",'http://www.w3.org/2001/XMLSchema#decimal'),context(_117292,_117294)),routes:do_or_die(...,...)) at /usr/lib/swipl/boot/init.pl:532
  [28] catch_with_backtrace('<garbage_collected>','<garbage_collected>','<garbage_collected>') at /usr/lib/swipl/boot/init.pl:582
This happens in current 'main' branch.",2021-08-24T12:47:03Z,matko,https://github.com/terminusdb/terminusdb/issues/449#issuecomment-904609216,This has been solved in merge request #451,this have be solve in merge request # 451,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,450,2021-08-24T12:42:42Z,GavinMendelGleason,Null cast as string silently,https://github.com/terminusdb/terminusdb/issues/450,"JSON has a null type which should not be automatically cast to a string.
If you have a schema with a property having an xsd:string type, and you attempt to insert a null, the database will silently cast it. This should not happen at all, but rather the property should be treated as not present.",2021-09-02T13:32:13Z,spl,https://github.com/terminusdb/terminusdb/issues/450#issuecomment-911687462,"Reproduced:
#!/bin/bash
set -ex

# Create the database.
xh 'http://admin:root@localhost:6363/api/db/admin/t' <<EOF
{""label"":""l"",""comment"":""c""}
EOF

# Create the schema.
xh 'http://admin:root@localhost:6363/api/document/admin/t?author=a&message=m&graph_type=schema' <<EOF
{
  ""@type"": ""Class"",
  ""@id"": ""String"",
  ""s"": ""xsd:string""
}
EOF

# Post the instance.
xh 'http://admin:root@localhost:6363/api/document/admin/t?author=a&message=m' <<EOF
{
  ""@id"": ""Test1"",
  ""@type"": ""String"",
  ""s"": null
}
EOF

# View the instance.
xh 'http://admin:root@localhost:6363/api/document/admin/t'
HTTP/1.1 200 OK
connection: keep-alive
content-length: 46
content-type: application/json; stream=true; charset=UTF-8
date: Thu, 02 Sep 2021 13:32:06 GMT

{
    ""@id"": ""Test1"",
    ""@type"": ""String"",
    ""s"": ""null""
}","reproduced # /bin / bash set -ex # create the database xh ' http//adminroot@localhost6363 / api / db / admin / t ' < < eof { "" label""""l""""comment""""c "" } eof # create the schema xh ' http//adminroot@localhost6363 / api / document / admin / tauthor = a&message = m&graph_type = schema ' < < eof { "" @type "" "" class "" "" @id "" "" string "" "" s "" "" xsdstring "" } eof # post the instance xh ' http//adminroot@localhost6363 / api / document / admin / tauthor = a&message = m ' < < eof { "" @id "" "" test1 "" "" @type "" "" string "" "" s "" null } eof # view the instance xh ' http//adminroot@localhost6363 / api / document / admin / t ' http/11 200 ok connection keep - alive content - length 46 content - type application / json stream = true charset = utf-8 date thu 02 sep 2021 133206 gmt { "" @id "" "" test1 "" "" @type "" "" string "" "" s "" "" null "" }",0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,450,2021-08-24T12:42:42Z,GavinMendelGleason,Null cast as string silently,https://github.com/terminusdb/terminusdb/issues/450,"JSON has a null type which should not be automatically cast to a string.
If you have a schema with a property having an xsd:string type, and you attempt to insert a null, the database will silently cast it. This should not happen at all, but rather the property should be treated as not present.",2021-09-02T13:49:26Z,spl,https://github.com/terminusdb/terminusdb/issues/450#issuecomment-911705255,"@GavinMendelGleason says it's probably something that needs to be carefully treated in json_woql.pl, and then you'll probably need to elaborate pairs of predicate : null as nothing, and that would be in json.pl.",@gavinmendelgleason say -PRON- be probably something that need to be carefully treat in json_woqlpl and then -PRON- will probably need to elaborate pair of predicate null as nothing and that would be in jsonpl,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,450,2021-08-24T12:42:42Z,GavinMendelGleason,Null cast as string silently,https://github.com/terminusdb/terminusdb/issues/450,"JSON has a null type which should not be automatically cast to a string.
If you have a schema with a property having an xsd:string type, and you attempt to insert a null, the database will silently cast it. This should not happen at all, but rather the property should be treated as not present.",2021-09-02T18:06:49Z,spl,https://github.com/terminusdb/terminusdb/issues/450#issuecomment-911933098,This was not fixed in the big change as of f58d7a0.,this be not fix in the big change as of f58d7a0,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,453,2021-08-24T13:20:01Z,matko,expand create_db prefix precheck and throw a different error,https://github.com/terminusdb/terminusdb/issues/453,"currently, the create_db api method does a pre-check for @base and @schema being present, and being a proper url. if not, a schema error is thrown.
For error reporting purposes, it would be more useful if it threw something else, which we can detect as an api input error. A schema error could have many reasons, some of them due to our own mistakes in our internal code, but we actually know the specific reason here, and it would be good to have a proper error term for it which we could transform to a nice json message.
Secondly, currently the pre-check only checks @base and @schema. It would be good if it actually checked all the prefixes in the prefix document.",2021-08-31T09:48:43Z,spl,https://github.com/terminusdb/terminusdb/issues/453#issuecomment-909079096,"Secondly, currently the pre-check only checks @base and @schema. It would be good if it actually checked all the prefixes in the prefix document.

What should we be checking for all the prefixes? Just that they are all URI's?",secondly currently the pre - check only check @base and @schema -PRON- would be good if -PRON- actually check all the prefix in the prefix document what should -PRON- be check for all the prefix just that -PRON- be all uri 's,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,453,2021-08-24T13:20:01Z,matko,expand create_db prefix precheck and throw a different error,https://github.com/terminusdb/terminusdb/issues/453,"currently, the create_db api method does a pre-check for @base and @schema being present, and being a proper url. if not, a schema error is thrown.
For error reporting purposes, it would be more useful if it threw something else, which we can detect as an api input error. A schema error could have many reasons, some of them due to our own mistakes in our internal code, but we actually know the specific reason here, and it would be good to have a proper error term for it which we could transform to a nice json message.
Secondly, currently the pre-check only checks @base and @schema. It would be good if it actually checked all the prefixes in the prefix document.",2021-09-01T10:22:59Z,matko,https://github.com/terminusdb/terminusdb/issues/453#issuecomment-910147595,"Secondly, currently the pre-check only checks @base and @schema. It would be good if it actually checked all the prefixes in the prefix document.

What should we be checking for all the prefixes? Just that they are all URI's?

Anything that'd otherwise cause the schema to fail on this URI, which as far as I can tell is just checking that it is a string and that it has a protocol.",secondly currently the pre - check only check @base and @schema -PRON- would be good if -PRON- actually check all the prefix in the prefix document what should -PRON- be check for all the prefix just that -PRON- be all uri 's anything that 'd otherwise cause the schema to fail on this uri which as far as i can tell be just check that -PRON- be a string and that -PRON- have a protocol,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,454,2021-08-24T13:47:22Z,KittyJose,Get a long error message when queried against an empty database,https://github.com/terminusdb/terminusdb/issues/454,"Describe the bug
Get a long error message when queried against an empty database
To Reproduce
Create an empty database
Run all()
Error Response
{
  ""api:message"":""Error: existence_error(key,message,_6626{})\n  [43] '$get_dict_ex'(message,_6678{},_6674)\n  [42] '$dicts':'.'(_6678{},message,_6718) at /usr/lib/swipl/boot/dicts.pl:46\n  [41] ref_entity:insert_base_commit_object(transaction_object{descriptor:repository_descriptor{database_descriptor: ...,repository_name:\""local\""},inference_objects:[],instance_objects:[...],parent:transaction_object{descriptor: ...,inference_objects:[],instance_objects: ...,schema_objects: ...},schema_objects:[...]},_6678{},1629812552.7840009,\""5c6pflmdxzpkrvy9hhzmigg181xtwym\"",_6766) at /app/terminusdb/src/core/transaction/ref_entity.pl:136\n  [39] validate:replace_initial_commit_on_branch(transaction_object{descriptor:repository_descriptor{database_descriptor: ...,repository_name:\""local\""},inference_objects:[],instance_objects:[...],parent:transaction_object{descriptor: ...,inference_objects:[],instance_objects: ...,schema_objects: ...},schema_objects:[...]},_6678{},\""main\"",graph_validation_obj{changed:false,descriptor:branch_graph{branch_name:\""main\"",commit_type:'http://terminusdb.com/schema/ref#InitialCommit',database_name:\""emptydb\"",organization_name:\""admin\"",repository_name:\""local\"",type:instance},read:_6960},graph_validation_obj{changed:false,descriptor:branch_graph{branch_name:\""main\"",commit_type:'http://terminusdb.com/schema/ref#InitialCommit',database_name:\""emptydb\"",organization_name:\""admin\"",repository_name:\""local\"",type:schema},read:<layer 6a2eee202676f0fc0b78f81bdca4b169da295efe>}) at /app/terminusdb/src/core/transaction/validate.pl:324\n  [37] validate:commit_validation_objects_([validation_object{commit_info: ...,descriptor: ...,inference_objects:[],instance_objects: ...,parent: ...,schema_objects: ...}]) at /app/terminusdb/src/core/transaction/validate.pl:423\n  [35] database:run_transactions([transaction_object{commit_info: ...,descriptor: ...,inference_objects:[],instance_objects: ...,parent: ...,schema_objects: ...}],false,_7152) at /app/terminusdb/src/core/transaction/database.pl:259\n  [34] database:with_transaction_(query_context{all_witnesses:false,authorization:'terminusdb://system/data/admin',bindings:[...|...],commit_info:_6678{},default_collection:branch_descriptor{branch_name:\""main\"",repository_descriptor: ...},files:[],filter:type_filter{types: ...},prefixes:_7306{'@base':\""terminusdb:///data/\"",'@schema':\""terminusdb:///schema#\"",'@type':'Context',api:'http://terminusdb.com/schema/api#',owl:'http://www.w3.org/2002/07/owl#',rdf:'http://www.w3.org/1999/02/22-rdf-syntax-ns#',rdfs:'http://www.w3.org/2000/01/rdf-schema#',sys:'http://terminusdb.com/schema/sys#',vio:'http://terminusdb.com/schema/vio#',woql:'http://terminusdb.com/schema/woql#',xdd:'http://terminusdb.com/schema/xdd#',xsd:'http://www.w3.org/2001/XMLSchema#'},selected:[],system:transaction_object{descriptor: ...,inference_objects:[],instance_objects: ...,schema_objects: ...},transaction_objects:[...],update_guard:_7276,write_graph:branch_graph{branch_name:\""main\"",database_name:\""emptydb\"",organization_name:\""admin\"",repository_name:\""local\"",type:instance}},query_response:(...,...),_7226) at /app/terminusdb/src/core/transaction/database.pl:222\n  [33] setup_call_catcher_cleanup(database:true,database:with_transaction_(...,...,_7482),_7460,database:true) at /usr/lib/swipl/boot/init.pl:614\n  [30] query_response:run_context_ast_jsonld_response(query_context{all_witnesses:false,authorization:'terminusdb://system/data/admin',bindings:[],commit_info:_6678{},default_collection:branch_descriptor{branch_name:\""main\"",repository_descriptor: ...},files:[],filter:type_filter{types: ...},prefixes:_7306{'@base':\""terminusdb:///data/\"",'@schema':\""terminusdb:///schema#\"",'@type':'Context',api:'http://terminusdb.com/schema/api#',owl:'http://www.w3.org/2002/07/owl#',rdf:'http://www.w3.org/1999/02/22-rdf-syntax-ns#',rdfs:'http://www.w3.org/2000/01/rdf-schema#',sys:'http://terminusdb.com/schema/sys#',vio:'http://terminusdb.com/schema/vio#',woql:'http://terminusdb.com/schema/woql#',xdd:'http://terminusdb.com/schema/xdd#',xsd:'http://www.w3.org/2001/XMLSchema#'},selected:[],system:transaction_object{descriptor: ...,inference_objects:[],instance_objects: ...,schema_objects: ...},transaction_objects:[...],update_guard:_7578,write_graph:branch_graph{branch_name:\""main\"",database_name:\""emptydb\"",organization_name:\""admin\"",repository_name:\""local\"",type:instance}},limit(10^^ ...,t(...,...,...)),_7528) at /app/terminusdb/src/core/query/query_response.pl:21\n  [28] '<meta-call>'('<garbage_collected>') <foreign>\n  [27] catch(routes:(...,...),error(existence_error(key,message,...),context(...,_7830)),routes:do_or_die(...,...)) at /usr/lib/swipl/boot/init.pl:532\n  [26] catch_with_backtrace('<garbage_collected>','<garbage_collected>','<garbage_collected>') at /usr/lib/swipl/boot/init.pl:582\n\nNote: some frames are missing due to last-call optimization.\nRe-run your program in debug mode (:- debug.) to get more detail.\n\n"",
  ""api:status"":""api:server_error""
}


Expected behavior
A very small cute message saying nothing exists to be queried against? 👯‍♀️",2021-08-24T18:26:08Z,GavinMendelGleason,https://github.com/terminusdb/terminusdb/issues/454#issuecomment-904874301,That error looks like it's caused by a missing commit message.,that error look like -PRON- be cause by a miss commit message,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,457,2021-08-25T11:57:06Z,GavinMendelGleason,Authorization checked incorrectly for prefixes,https://github.com/terminusdb/terminusdb/issues/457,Authorization checks still making reference to system:. This may be wrong elsewhere as well.,2021-08-25T12:06:43Z,spl,https://github.com/terminusdb/terminusdb/issues/457#issuecomment-905441460,"Just to be clear, this is referring to the /api/prefixes route for querying the prefixes.",just to be clear this be refer to the /api / prefix route for query the prefix,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,457,2021-08-25T11:57:06Z,GavinMendelGleason,Authorization checked incorrectly for prefixes,https://github.com/terminusdb/terminusdb/issues/457,Authorization checks still making reference to system:. This may be wrong elsewhere as well.,2021-08-25T12:14:23Z,spl,https://github.com/terminusdb/terminusdb/issues/457#issuecomment-905446789,"curl 'https://cloud.terminusdb.com/Integrations/api/prefixes/Integrations/stock_exchange?graph_type=schema' \
   -i \
   -X GET \
   -H ""Content-Type: application/json"" \
   -H ""Authorization: Bearer ...""

HTTP/2 500 
x-powered-by: Express
access-control-allow-origin: *
content-type: application/json
x-cloud-trace-context: f667b8ce077bce4548c51390d0f56011
date: Wed, 25 Aug 2021 12:13:49 GMT
server: Google Frontend
content-length: 6018

{
  ""api:message"":""Error: woql_syntax_error(unresolvable_prefix(system,instance_read_access))\n  [51] throw(error(woql_syntax_error(...),_27458))\n  [50] woql_compile:resolve_prefix(system,instance_read_access,_27502,query_context{all_witnesses:false,authorization:'terminusdb://system/data/admin',bindings:[...|...],default_collection:system_descriptor{},files:[],filter:type_filter{types: ...},prefixes:_27580{'@base':\""terminusdb://system/data/\"",'@schema':\""http://terminusdb.com/schema/system#\"",'@type':'Context',api:'http://terminusdb.com/schema/api#',owl:'http://www.w3.org/2002/07/owl#',rdf:'http://www.w3.org/1999/02/22-rdf-syntax-ns#',rdfs:'http://www.w3.org/2000/01/rdf-schema#',sys:'http://terminusdb.com/schema/sys#',vio:'http://terminusdb.com/schema/vio#',woql:'http://terminusdb.com/schema/woql#',xdd:'http://terminusdb.com/schema/xdd#',xsd:\""http://www.w3.org/2001/XMLSchema#\""},selected:[],system:system_descriptor{},transaction_objects:[...],update_guard:_27540,write_graph:system_graph{type:instance}},_27506) at /app/terminusdb/src/core/query/woql_compile.pl:220\n  [48] woql_compile:compile_wf(t(v('Var3811'),action,system:instance_read_access),_27684,query_context{all_witnesses:false,authorization:'terminusdb://system/data/admin',bindings:[...|...],default_collection:system_descriptor{},files:[],filter:type_filter{types: ...},prefixes:_27580{'@base':\""terminusdb://system/data/\"",'@schema':\""http://terminusdb.com/schema/system#\"",'@type':'Context',api:'http://terminusdb.com/schema/api#',owl:'http://www.w3.org/2002/07/owl#',rdf:'http://www.w3.org/1999/02/22-rdf-syntax-ns#',rdfs:'http://www.w3.org/2000/01/rdf-schema#',sys:'http://terminusdb.com/schema/sys#',vio:'http://terminusdb.com/schema/vio#',woql:'http://terminusdb.com/schema/woql#',xdd:'http://terminusdb.com/schema/xdd#',xsd:\""http://www.w3.org/2001/XMLSchema#\""},selected:[],system:system_descriptor{},transaction_objects:[...],update_guard:_27740,write_graph:system_graph{type:instance}},_27688) at /app/terminusdb/src/core/query/woql_compile.pl:1215\n  [47] woql_compile:compile_wf((path(...,...,'UserDatabase_7657301d4e74533c9749489f9abddd26',...),t(...,action,...)),((...,...),_27918),query_context{all_witnesses:false,authorization:'terminusdb://system/data/admin',bindings:[...|...],default_collection:system_descriptor{},files:[],filter:type_filter{types: ...},prefixes:_27580{'@base':\""terminusdb://system/data/\"",'@schema':\""http://terminusdb.com/schema/system#\"",'@type':'Context',api:'http://terminusdb.com/schema/api#',owl:'http://www.w3.org/2002/07/owl#',rdf:'http://www.w3.org/1999/02/22-rdf-syntax-ns#',rdfs:'http://www.w3.org/2000/01/rdf-schema#',sys:'http://terminusdb.com/schema/sys#',vio:'http://terminusdb.com/schema/vio#',woql:'http://terminusdb.com/schema/woql#',xdd:'http://terminusdb.com/schema/xdd#',xsd:\""http://www.w3.org/2001/XMLSchema#\""},selected:[],system:system_descriptor{},transaction_objects:[...],update_guard:_27958,write_graph:system_graph{type:instance}},_27888) at /app/terminusdb/src/core/query/woql_compile.pl:1268\n  [46] woql_compile:compile_wf((t(...,scope,...),...,...),((...,...),...,_28144),query_context{all_witnesses:false,authorization:'terminusdb://system/data/admin',bindings:[...|...],default_collection:system_descriptor{},files:[],filter:type_filter{types: ...},prefixes:_27580{'@base':\""terminusdb://system/data/\"",'@schema':\""http://terminusdb.com/schema/system#\"",'@type':'Context',api:'http://terminusdb.com/schema/api#',owl:'http://www.w3.org/2002/07/owl#',rdf:'http://www.w3.org/1999/02/22-rdf-syntax-ns#',rdfs:'http://www.w3.org/2000/01/rdf-schema#',sys:'http://terminusdb.com/schema/sys#',vio:'http://terminusdb.com/schema/vio#',woql:'http://terminusdb.com/schema/woql#',xdd:'http://terminusdb.com/schema/xdd#',xsd:\""http://www.w3.org/2001/XMLSchema#\""},selected:[],system:system_descriptor{},transaction_objects:[...],update_guard:_28178,write_graph:system_graph{type:instance}},_28106) at /app/terminusdb/src/core/query/woql_compile.pl:1268\n  [45] woql_compile:compile_wf('<garbage_collected>','<garbage_collected>','<garbage_collected>',_28326) at /app/terminusdb/src/core/query/woql_compile.pl:1268\n  [44] woql_compile:compile_wf('<garbage_collected>','<garbage_collected>','<garbage_collected>',_28368) at /app/terminusdb/src/core/query/woql_compile.pl:1268\n  [43] utils:do_or_die('<garbage_collected>',error(woql_syntax_error(...),_28412)) at /app/terminusdb/src/core/util/utils.pl:106\n  [42] woql_compile:compile_query((t('User_auth0%7C6125fca378b38c006ac1782f',capability,...),...,...),_28454,'<garbage_collected>',_28458) at /app/terminusdb/src/core/query/woql_compile.pl:617\n  [41] ask:ask_ast('<garbage_collected>','<garbage_collected>',_28518) at /app/terminusdb/src/core/query/ask.pl:329\n  [38] capabilities:auth_action_scope(transaction_object{descriptor:system_descriptor{},inference_objects:[],instance_objects:[...],schema_objects:[...]},'User_auth0%7C6125fca378b38c006ac1782f',system:instance_read_access,'UserDatabase_7657301d4e74533c9749489f9abddd26') at /app/terminusdb/src/core/account/capabilities.pl:116\n  [37] capabilities:assert_auth_action_scope('<garbage_collected>','User_auth0%7C6125fca378b38c006ac1782f',system:instance_read_access,'UserDatabase_7657301d4e74533c9749489f9abddd26') at /app/terminusdb/src/core/account/capabilities.pl:455\n  [32] api_prefixes:get_prefixes('Integrations/stock_exchange','<garbage_collected>','User_auth0%7C6125fca378b38c006ac1782f',_28692) at /app/terminusdb/src/core/api/api_prefixes.pl:16\n  [31] '<meta-call>'('<garbage_collected>') <foreign>\n  [30] catch(routes:(...,...),error(woql_syntax_error(...),context(_28786,_28788)),routes:do_or_die(...,...)) at /usr/lib/swipl/boot/init.pl:532\n  [29] catch_with_backtrace('<garbage_collected>','<garbage_collected>','<garbage_collected>') at /usr/lib/swipl/boot/init.pl:582\n\nNote: some frames are missing due to last-call optimization.\nRe-run your program in debug mode (:- debug.) to get more detail.\n\n"",
  ""api:status"":""api:server_error""
}","curl ' https//cloudterminusdbcom / integration / api / prefix / integration / stock_exchangegraph_type = schema ' \ -i \ -x get \ -h "" content - type application / json "" \ -h "" authorization bearer "" http/2 500 x - power - by express access - control - allow - origin * content - type application / json x - cloud - trace - context f667b8ce077bce4548c51390d0f56011 date -PRON- d 25 aug 2021 121349 gmt server google frontend content - length 6018 { "" apimessage""""error woql_syntax_error(unresolvable_prefix(systeminstance_read_access))\n [ 51 ] throw(error(woql_syntax_error()_27458))\n [ 50 ] woql_compileresolve_prefix(systeminstance_read_access_27502query_context{all_witnessesfalseauthorization'terminusdb//system / datum / admin'bindings[|]default_collectionsystem_descriptor{}files[]filtertype_filter{types } prefixes_27580{'@base'\""terminusdb//system / data/\""'@schema'\""http//terminusdbcom / schema / system#\""'@type''context'api'http//terminusdbcom / schema / api#'owl'http//wwww3org/2002/07 / owl#'rdf'http//wwww3org/1999/02/22-rdf - syntax - ns#'rdfs'http//wwww3org/2000/01 / rdf - schema#'sys'http//terminusdbcom / schema / sys#'vio'http//terminusdbcom / schema / vio#'woql'http//terminusdbcom / schema / woql#'xdd'http//terminusdbcom / schema / xdd#'xsd\""http//wwww3org/2001 / xmlschema#\""}selected[]systemsystem_descriptor{}transaction_objects[]update_guard_27540write_graphsystem_graph{typeinstance}}_27506 ) at /app / terminusdb / src / core / query / woql_compilepl220\n [ 48 ] woql_compilecompile_wf(t(v('var3811')actionsysteminstance_read_access)_27684query_context{all_witnessesfalseauthorization'terminusdb//system / datum / admin'bindings[|]default_collectionsystem_descriptor{}files[]filtertype_filter{types } prefixes_27580{'@base'\""terminusdb//system / data/\""'@schema'\""http//terminusdbcom / schema / system#\""'@type''context'api'http//terminusdbcom / schema / api#'owl'http//wwww3org/2002/07 / owl#'rdf'http//wwww3org/1999/02/22-rdf - syntax - ns#'rdfs'http//wwww3org/2000/01 / rdf - schema#'sys'http//terminusdbcom / schema / sys#'vio'http//terminusdbcom / schema / vio#'woql'http//terminusdbcom / schema / woql#'xdd'http//terminusdbcom / schema / xdd#'xsd\""http//wwww3org/2001 / xmlschema#\""}selected[]systemsystem_descriptor{}transaction_objects[]update_guard_27740write_graphsystem_graph{typeinstance}}_27688 ) at /app / terminusdb / src / core / query / woql_compilepl1215\n [ 47 ] woql_compilecompile_wf((path('userdatabase_7657301d4e74533c9749489f9abddd26')t(action))(()_27918)query_context{all_witnessesfalseauthorization'terminusdb//system / datum / admin'bindings[|]default_collectionsystem_descriptor{}files[]filtertype_filter{types } prefixes_27580{'@base'\""terminusdb//system / data/\""'@schema'\""http//terminusdbcom / schema / system#\""'@type''context'api'http//terminusdbcom / schema / api#'owl'http//wwww3org/2002/07 / owl#'rdf'http//wwww3org/1999/02/22-rdf - syntax - ns#'rdfs'http//wwww3org/2000/01 / rdf - schema#'sys'http//terminusdbcom / schema / sys#'vio'http//terminusdbcom / schema / vio#'woql'http//terminusdbcom / schema / woql#'xdd'http//terminusdbcom / schema / xdd#'xsd\""http//wwww3org/2001 / xmlschema#\""}selected[]systemsystem_descriptor{}transaction_objects[]update_guard_27958write_graphsystem_graph{typeinstance}}_27888 ) at /app / terminusdb / src / core / query / woql_compilepl1268\n [ 46 ] woql_compilecompile_wf((t(scope))(()_28144)query_context{all_witnessesfalseauthorization'terminusdb//system / datum / admin'bindings[|]default_collectionsystem_descriptor{}files[]filtertype_filter{types } prefixes_27580{'@base'\""terminusdb//system / data/\""'@schema'\""http//terminusdbcom / schema / system#\""'@type''context'api'http//terminusdbcom / schema / api#'owl'http//wwww3org/2002/07 / owl#'rdf'http//wwww3org/1999/02/22-rdf - syntax - ns#'rdfs'http//wwww3org/2000/01 / rdf - schema#'sys'http//terminusdbcom / schema / sys#'vio'http//terminusdbcom / schema / vio#'woql'http//terminusdbcom / schema / woql#'xdd'http//terminusdbcom / schema / xdd#'xsd\""http//wwww3org/2001 / xmlschema#\""}selected[]systemsystem_descriptor{}transaction_objects[]update_guard_28178write_graphsystem_graph{typeinstance}}_28106 ) at /app / terminusdb / src / core / query / woql_compilepl1268\n [ 45 ] woql_compilecompile_wf('<garbage_collected>''<garbage_collected>''<garbage_collected>'_28326 ) at /app / terminusdb / src / core / query / woql_compilepl1268\n [ 44 ] woql_compilecompile_wf('<garbage_collected>''<garbage_collected>''<garbage_collected>'_28368 ) at /app / terminusdb / src / core / query / woql_compilepl1268\n [ 43 ] utilsdo_or_die('<garbage_collected>'error(woql_syntax_error()_28412 ) ) at /app / terminusdb / src / core / util / utilspl106\n [ 42 ] woql_compilecompile_query((t('user_auth0%7c6125fca378b38c006ac1782f'capability))_28454'<garbage_collected>'_28458 ) at /app / terminusdb / src / core / query / woql_compilepl617\n [ 41 ] askask_ast('<garbage_collected>''<garbage_collected>'_28518 ) at /app / terminusdb / src / core / query / askpl329\n [ 38 ] capabilitiesauth_action_scope(transaction_object{descriptorsystem_descriptor{}inference_objects[]instance_objects[]schema_objects[]}'user_auth0%7c6125fca378b38c006ac1782f'systeminstance_read_access'userdatabase_7657301d4e74533c9749489f9abddd26 ' ) at /app / terminusdb / src / core / account / capabilitiespl116\n [ 37 ] capabilitiesassert_auth_action_scope('<garbage_collected>''user_auth0%7c6125fca378b38c006ac1782f'systeminstance_read_access'userdatabase_7657301d4e74533c9749489f9abddd26 ' ) at /app / terminusdb / src / core / account / capabilitiespl455\n [ 32 ] api_prefixesget_prefixes('integrations / stock_exchange''<garbage_collected>''user_auth0%7c6125fca378b38c006ac1782f'_28692 ) at /app / terminusdb / src / core / api / api_prefixespl16\n [ 31 ] ' < meta - call>'('<garbage_collecte > ' ) < foreign>\n [ 30 ] catch(routes()error(woql_syntax_error()context(_28786_28788))routesdo_or_die ( ) ) at /usr / lib / swipl / boot / initpl532\n [ 29 ] catch_with_backtrace('<garbage_collected>''<garbage_collected>''<garbage_collecte > ' ) at /usr / lib / swipl / boot / initpl582\n\nnote some frame be miss due to last - call optimization\nre - run -PRON- program in debug mode ( - debug ) to get more detail\n\n "" "" apistatus""""apiserver_error "" }",0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,460,2021-08-25T14:02:32Z,spl,Rebase with unknown branch (not main) gives bad error,https://github.com/terminusdb/terminusdb/issues/460,Error message: Unexpected failure in request handler,2021-09-07T14:01:11Z,AstroChelonian,https://github.com/terminusdb/terminusdb/issues/460#issuecomment-914332114,"To replicate the error:
#!/bin/bash

set -ex

# Create the database.
xh 'http://admin:root@localhost:6363/api/db/admin/t' <<EOF
{""label"":""l"",""comment"":""c""}
EOF

# Create the schema.
xh 'http://admin:root@localhost:6363/api/document/admin/t?author=a&message=m&graph_type=schema' <<EOF
{""@type"": ""Class"",""@id"": ""String""}
EOF

# Create a branch.
xh 'http://admin:root@localhost:6363/api/branch/admin/t/local/branch/stars' << EOF
{""origin"" : ""admin/t""}
EOF

# Rebase a known branch.
xh 'http://admin:root@localhost:6363/api/rebase/admin/t/local/branch/main' << EOF
{""rebase_from"" : ""admin/t/local/branch/stars"", ""author"":""me""}
EOF

# Rebase an unknown branch.
xh 'http://admin:root@localhost:6363/api/rebase/admin/t/local/branch/main' << EOF
{""rebase_from"" : ""admin/t/local/branch/unknown"", ""author"":""me""}
EOF
This gives:
{
    ""api:message"": {
        ""@type"": ""xsd:string"",
        ""@value"": ""Unexpected failure in request handler""
    },
    ""api:status"": ""api:failure""
}","to replicate the error # /bin / bash set -ex # create the database xh ' http//adminroot@localhost6363 / api / db / admin / t ' < < eof { "" label""""l""""comment""""c "" } eof # create the schema xh ' http//adminroot@localhost6363 / api / document / admin / tauthor = a&message = m&graph_type = schema ' < < eof { "" @type "" "" class""""@id "" "" string "" } eof # create a branch xh ' http//adminroot@localhost6363 / api / branch / admin / t / local / branch / star ' < < eof { "" origin "" "" admin / t "" } eof # rebase a know branch xh ' http//adminroot@localhost6363 / api / rebase / admin / t / local / branch / main ' < < eof { "" rebase_from "" "" admin / t / local / branch / star "" "" author""""me "" } eof # rebase an unknown branch xh ' http//adminroot@localhost6363 / api / rebase / admin / t / local / branch / main ' < < eof { "" rebase_from "" "" admin / t / local / branch / unknown "" "" author""""me "" } eof this give { "" apimessage "" { "" @type "" "" xsdstring "" "" @value "" "" unexpected failure in request handler "" } "" apistatus "" "" apifailure "" }",0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,461,2021-08-25T14:08:47Z,spl,Merge conflict expected during rebase but not found,https://github.com/terminusdb/terminusdb/issues/461,This came up during our meeting today. @GavinMendelGleason will look into this.,2021-08-25T19:36:55Z,GavinMendelGleason,https://github.com/terminusdb/terminusdb/issues/461#issuecomment-905817983,"This is a genuine bug in the new schema checking mechanism, and not actually in rebase (which seems to work fine). I have a minimal example test, and I'm working on a fix.",this be a genuine bug in the new schema check mechanism and not actually in rebase ( which seem to work fine ) i have a minimal example test and -PRON- be work on a fix,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,461,2021-08-25T14:08:47Z,spl,Merge conflict expected during rebase but not found,https://github.com/terminusdb/terminusdb/issues/461,This came up during our meeting today. @GavinMendelGleason will look into this.,2021-08-26T01:50:53Z,spl,https://github.com/terminusdb/terminusdb/issues/461#issuecomment-906011397,Fixed by #465.,fix by # 465,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,461,2021-08-25T14:08:47Z,spl,Merge conflict expected during rebase but not found,https://github.com/terminusdb/terminusdb/issues/461,This came up during our meeting today. @GavinMendelGleason will look into this.,2021-08-26T08:38:16Z,matko,https://github.com/terminusdb/terminusdb/issues/461#issuecomment-906210356,Reopening. We discovered the main underlying cause was that databases were being created in schemaless mode due to an accidental swapping of the parameters 'schema' and 'public'.,reopen -PRON- discover the main underlying cause be that database be be create in schemaless mode due to an accidental swapping of the parameter ' schema ' and ' public ',0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,462,2021-08-25T14:15:22Z,spl,Long wait for first document when querying documents,https://github.com/terminusdb/terminusdb/issues/462,"When using the /api/document/<org>/<db> query, since we're using a stream, I'd expect the documents to come as soon as they are found.",2021-08-26T08:15:50Z,matko,https://github.com/terminusdb/terminusdb/issues/462#issuecomment-906195086,"Hmm, as far as I know, we are actually writing to the output stream as soon as a document is found. We're not collecting them and sending them out at once.
That said, I don't know exactly how the prolog http server works. It may be queueing things up on its end.",hmm as far as i know -PRON- be actually write to the output stream as soon as a document be find -PRON- be not collect -PRON- and send -PRON- out at once that say i do not know exactly how the prolog http server work -PRON- may be queue thing up on -PRON- end,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,464,2021-08-25T15:52:55Z,Cheukting,Strange thing happend when update schema to a type that violate the existing data,https://github.com/terminusdb/terminusdb/issues/464,"To Reproduce
Load in 'indexData_sample.csv' with cli, schema will have 'volume' as float (which will be xsd:decimal)
Load in 'other.csv' with cli, schema will update 'volume as int (which will be xsd:integer), this is because all volume in this csv are 0 so pandas thinks it's int
No Error message till now
when I get the data entry form indesData_sample I get something like:
{'@id': 'IndexData_25ec6cb430695494e4b16b5a473660a60206d4ab3ff363b786bad0526dca69d2', '@type': 'IndexData', 'adj_close': 506.899994, 'close': 506.899994, 'date': '1970-01-29', 'high': 506.899994, 'index': 'NYA', 'low': 506.899994, 'open': 506.899994, 'volume': {'@type': 'http://www.w3.org/2001/XMLSchema#decimal', '@value': 0.0}}
Obviously volume is now confused as it is now a full path.
Expected behavior
Give a warning when schema update will cause something weird I guess? I can bypass the problem on the client-side but don't know if there are any though about handling it at the backend. What will be the solution?",2021-09-02T13:25:47Z,spl,https://github.com/terminusdb/terminusdb/issues/464#issuecomment-911681006,I'm not sure how this can/should be fixed on the backend. It seems like the client can try to verify that the data matches the schema.,-PRON- be not sure how this can / should be fix on the backend -PRON- seem like the client can try to verify that the data match the schema,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,464,2021-08-25T15:52:55Z,Cheukting,Strange thing happend when update schema to a type that violate the existing data,https://github.com/terminusdb/terminusdb/issues/464,"To Reproduce
Load in 'indexData_sample.csv' with cli, schema will have 'volume' as float (which will be xsd:decimal)
Load in 'other.csv' with cli, schema will update 'volume as int (which will be xsd:integer), this is because all volume in this csv are 0 so pandas thinks it's int
No Error message till now
when I get the data entry form indesData_sample I get something like:
{'@id': 'IndexData_25ec6cb430695494e4b16b5a473660a60206d4ab3ff363b786bad0526dca69d2', '@type': 'IndexData', 'adj_close': 506.899994, 'close': 506.899994, 'date': '1970-01-29', 'high': 506.899994, 'index': 'NYA', 'low': 506.899994, 'open': 506.899994, 'volume': {'@type': 'http://www.w3.org/2001/XMLSchema#decimal', '@value': 0.0}}
Obviously volume is now confused as it is now a full path.
Expected behavior
Give a warning when schema update will cause something weird I guess? I can bypass the problem on the client-side but don't know if there are any though about handling it at the backend. What will be the solution?",2021-09-20T09:06:47Z,spl,https://github.com/terminusdb/terminusdb/issues/464#issuecomment-922750268,"I didn't understand at first, but it seems like the input is 0.0 for an xsd:integer type and the output is {""@type"": ""http://www.w3.org/2001/XMLSchema#decimal"", ""@value"": 0.0}.","i do not understand at first but -PRON- seem like the input be 00 for an xsdinteger type and the output be { "" @type "" "" http//wwww3org/2001 / xmlschema#decimal "" "" @value "" 00 }",0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,464,2021-08-25T15:52:55Z,Cheukting,Strange thing happend when update schema to a type that violate the existing data,https://github.com/terminusdb/terminusdb/issues/464,"To Reproduce
Load in 'indexData_sample.csv' with cli, schema will have 'volume' as float (which will be xsd:decimal)
Load in 'other.csv' with cli, schema will update 'volume as int (which will be xsd:integer), this is because all volume in this csv are 0 so pandas thinks it's int
No Error message till now
when I get the data entry form indesData_sample I get something like:
{'@id': 'IndexData_25ec6cb430695494e4b16b5a473660a60206d4ab3ff363b786bad0526dca69d2', '@type': 'IndexData', 'adj_close': 506.899994, 'close': 506.899994, 'date': '1970-01-29', 'high': 506.899994, 'index': 'NYA', 'low': 506.899994, 'open': 506.899994, 'volume': {'@type': 'http://www.w3.org/2001/XMLSchema#decimal', '@value': 0.0}}
Obviously volume is now confused as it is now a full path.
Expected behavior
Give a warning when schema update will cause something weird I guess? I can bypass the problem on the client-side but don't know if there are any though about handling it at the backend. What will be the solution?",2021-09-20T09:22:53Z,spl,https://github.com/terminusdb/terminusdb/issues/464#issuecomment-922761658,"The script below creates a schema with a value of type xsd:integer and submits a value of 0.0. The response is a BadCast error.
#!/bin/bash
set -ex

xh 'http://admin:root@localhost:6363/api/db/admin/tdb' label=l comment=c

xh 'http://admin:root@localhost:6363/api/document/admin/tdb' graph_type==schema author==a message==m <<EOF
{""@id"":""J"",""@type"":""Class"",""value"":""xsd:integer""}
EOF

xh 'http://admin:root@localhost:6363/api/document/admin/tdb' author==a message==m <<EOF
{""@type"":""J"",""value"":0.0}
EOF
The output from the last command:
HTTP/1.1 400 Bad Request
Connection: keep-alive
Content-Length: 350
Content-Type: application/json
Date: Mon, 20 Sep 2021 09:14:24 GMT

{
    ""@type"": ""api:InsertDocumentErrorResponse"",
    ""api:error"": {
        ""@type"": ""api:BadCast"",
        ""api:document"": {
            ""@type"": ""J"",
            ""value"": 0.0
        },
        ""api:type"": ""http://www.w3.org/2001/XMLSchema#integer"",
        ""api:value"": 0.0
    },
    ""api:message"": ""value 0.0 could not be casted to a 'http://www.w3.org/2001/XMLSchema#integer'"",
    ""api:status"": ""api:failure""
}","the script below create a schema with a value of type xsdinteger and submit a value of 00 the response be a badcast error # /bin / bash set -ex xh ' http//adminroot@localhost6363 / api / db / admin / tdb ' label = l comment = c xh ' http//adminroot@localhost6363 / api / document / admin / tdb ' graph_type==schema author==a message==m < < eof { "" @id""""j""""@type""""class""""value""""xsdinteger "" } eof xh ' http//adminroot@localhost6363 / api / document / admin / tdb ' author==a message==m < < eof { "" @type""""j""""value""00 } eof the output from the last command http/11 400 bad request connection keep - alive content - length 350 content - type application / json date mon 20 sep 2021 091424 gmt { "" @type "" "" apiinsertdocumenterrorresponse "" "" apierror "" { "" @type "" "" apibadcast "" "" apidocument "" { "" @type "" "" j "" "" value "" 00 } "" apitype "" "" http//wwww3org/2001 / xmlschema#integer "" "" apivalue "" 00 } "" apimessage "" "" value 00 could not be cast to a ' http//wwww3org/2001 / xmlschema#integer ' "" "" apistatus "" "" apifailure "" }",0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,464,2021-08-25T15:52:55Z,Cheukting,Strange thing happend when update schema to a type that violate the existing data,https://github.com/terminusdb/terminusdb/issues/464,"To Reproduce
Load in 'indexData_sample.csv' with cli, schema will have 'volume' as float (which will be xsd:decimal)
Load in 'other.csv' with cli, schema will update 'volume as int (which will be xsd:integer), this is because all volume in this csv are 0 so pandas thinks it's int
No Error message till now
when I get the data entry form indesData_sample I get something like:
{'@id': 'IndexData_25ec6cb430695494e4b16b5a473660a60206d4ab3ff363b786bad0526dca69d2', '@type': 'IndexData', 'adj_close': 506.899994, 'close': 506.899994, 'date': '1970-01-29', 'high': 506.899994, 'index': 'NYA', 'low': 506.899994, 'open': 506.899994, 'volume': {'@type': 'http://www.w3.org/2001/XMLSchema#decimal', '@value': 0.0}}
Obviously volume is now confused as it is now a full path.
Expected behavior
Give a warning when schema update will cause something weird I guess? I can bypass the problem on the client-side but don't know if there are any though about handling it at the backend. What will be the solution?",2021-09-20T09:32:37Z,spl,https://github.com/terminusdb/terminusdb/issues/464#issuecomment-922767599,This is already fixed!,this be already fix,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,466,2021-08-26T06:17:56Z,GavinMendelGleason,Concurrency problems,https://github.com/terminusdb/terminusdb/issues/466,Concurrent updates seem to sometimes result in success but not an update. Sometimes we get a transaction retry count exhaustion. We need a strategy to remove both of these problems.,2021-08-26T06:19:18Z,GavinMendelGleason,https://github.com/terminusdb/terminusdb/issues/466#issuecomment-906127947,"Simplest approach to the retry problem is to detect updates and queue the queries rather than allow them run concurrently. Since we do not have any write object model, or the ability to perform subsequent ""rebase"" style application of commits, at the moment this will be faster.","simplest approach to the retry problem be to detect update and queue the query rather than allow -PRON- run concurrently since -PRON- do not have any write object model or the ability to perform subsequent "" rebase "" style application of commit at the moment this will be fast",0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,466,2021-08-26T06:17:56Z,GavinMendelGleason,Concurrency problems,https://github.com/terminusdb/terminusdb/issues/466,Concurrent updates seem to sometimes result in success but not an update. Sometimes we get a transaction retry count exhaustion. We need a strategy to remove both of these problems.,2021-08-26T08:11:45Z,matko,https://github.com/terminusdb/terminusdb/issues/466#issuecomment-906192178,"That simplest approach works as long as there's one server instance, but it won't work with a cluster as the different servers do not know what updates may be running.",that simple approach work as long as there be one server instance but -PRON- will not work with a cluster as the different server do not know what update may be run,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,466,2021-08-26T06:17:56Z,GavinMendelGleason,Concurrency problems,https://github.com/terminusdb/terminusdb/issues/466,Concurrent updates seem to sometimes result in success but not an update. Sometimes we get a transaction retry count exhaustion. We need a strategy to remove both of these problems.,2021-08-26T18:07:52Z,GavinMendelGleason,https://github.com/terminusdb/terminusdb/issues/466#issuecomment-906628843,Yes - good point. At the moment though with sharding on organisation it should work though?  We can look at a write object model later.,yes - good point at the moment though with sharde on organisation -PRON- should work though -PRON- can look at a write object model later,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,466,2021-08-26T06:17:56Z,GavinMendelGleason,Concurrency problems,https://github.com/terminusdb/terminusdb/issues/466,Concurrent updates seem to sometimes result in success but not an update. Sometimes we get a transaction retry count exhaustion. We need a strategy to remove both of these problems.,2021-08-30T13:42:20Z,spl,https://github.com/terminusdb/terminusdb/issues/466#issuecomment-908353868,"Increase transaction retry count
 Implement write transaction locking
 Look into alternative concurrent write transaction approaches",increase transaction retry count implement write transaction lock look into alternative concurrent write transaction approach,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,469,2021-08-26T08:50:07Z,spl,"Strange ""301 Moved Permanently"" response to /api/db",https://github.com/terminusdb/terminusdb/issues/469,"echo '' | xh 'http://localhost:6363/api/db'
HTTP/1.1 301 Moved Permanently
connection: close
content-length: 67
content-type: application/json; charset=UTF-8
date: Thu, 26 Aug 2021 08:47:30 GMT
location: /api/db/

{
    ""code"": 301,
    ""location"": ""/api/db/"",
    ""message"": ""Moved to: /api/db/""
}

Note that there is no authentication here either.",2021-09-06T12:25:33Z,spl,https://github.com/terminusdb/terminusdb/issues/469#issuecomment-913608608,"This is due to find_handler/3:

find_handler(+Path, -Action, -Options) is det
Find the handler to call from Path. Rules:

If there is a matching handler, use this.
If there are multiple prefix(Path) handlers, use the longest.

If there is a handler for /dir/ and the requested path is /dir, find_handler/3 throws a http_reply exception, causing the wrapper to generate a 301 (Moved Permanently) reply.",this be due to find_handler/3 find_handler(+path -action -options ) be det find the handler to call from path rule if there be a match handler use this if there be multiple prefix(path ) handler use the long if there be a handler for /dir/ and the requested path be /dir find_handler/3 throw a http_reply exception cause the wrapper to generate a 301 ( move permanently ) reply,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,469,2021-08-26T08:50:07Z,spl,"Strange ""301 Moved Permanently"" response to /api/db",https://github.com/terminusdb/terminusdb/issues/469,"echo '' | xh 'http://localhost:6363/api/db'
HTTP/1.1 301 Moved Permanently
connection: close
content-length: 67
content-type: application/json; charset=UTF-8
date: Thu, 26 Aug 2021 08:47:30 GMT
location: /api/db/

{
    ""code"": 301,
    ""location"": ""/api/db/"",
    ""message"": ""Moved to: /api/db/""
}

Note that there is no authentication here either.",2021-09-20T12:35:56Z,spl,https://github.com/terminusdb/terminusdb/issues/469#issuecomment-922888288,"The same 301 happens whenever there is a route handler declared as /atom/Var (where atom is an atom, and Var is a variable in Prolog). The route handling in find_handler redirects /atom to /atom/. I reported this at SWI-Prolog/packages-http#148.",the same 301 happen whenever there be a route handler declare as /atom / var ( where atom be an atom and var be a variable in prolog ) the route handling in find_handler redirect /atom to /atom/ i report this at swi - prolog / package - http#148,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,470,2021-08-26T08:54:29Z,spl,Authorization checks happen after request parsing,https://github.com/terminusdb/terminusdb/issues/470,"I expected to see authentication failures instead of these responses.
xh 'http://localhost:6363/api/db/unknown' # GET
HTTP/1.1 405 Method Not Allowed
connection: close
content-length: 105
content-type: application/json; charset=UTF-8
date: Thu, 26 Aug 2021 08:52:56 GMT

{
    ""code"": 405,
    ""location"": ""/api/db/unknown"",
    ""message"": ""Method not allowed: GET"",
    ""method"": ""GET""
}

echo '' | xh 'http://localhost:6363/api/db/unknown' # POST
HTTP/1.1 400 Bad Request
connection: keep-alive
content-length: 173
content-type: application/json
date: Thu, 26 Aug 2021 08:51:43 GMT

{
    ""api:status"": ""api:failure"",
    ""system:witnesses"": [
        {
            ""@type"": ""vio:ViolationWithDatatypeObject"",
            ""vio:literal"": ""json(unexpected_end_of_file)""
        }
    ]
}",2021-08-26T08:55:36Z,spl,https://github.com/terminusdb/terminusdb/issues/470#issuecomment-906222361,Same as above for http://localhost:6363/api/db/unknown/unknown,same as above for http//localhost6363 / api / db / unknown / unknown,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,470,2021-08-26T08:54:29Z,spl,Authorization checks happen after request parsing,https://github.com/terminusdb/terminusdb/issues/470,"I expected to see authentication failures instead of these responses.
xh 'http://localhost:6363/api/db/unknown' # GET
HTTP/1.1 405 Method Not Allowed
connection: close
content-length: 105
content-type: application/json; charset=UTF-8
date: Thu, 26 Aug 2021 08:52:56 GMT

{
    ""code"": 405,
    ""location"": ""/api/db/unknown"",
    ""message"": ""Method not allowed: GET"",
    ""method"": ""GET""
}

echo '' | xh 'http://localhost:6363/api/db/unknown' # POST
HTTP/1.1 400 Bad Request
connection: keep-alive
content-length: 173
content-type: application/json
date: Thu, 26 Aug 2021 08:51:43 GMT

{
    ""api:status"": ""api:failure"",
    ""system:witnesses"": [
        {
            ""@type"": ""vio:ViolationWithDatatypeObject"",
            ""vio:literal"": ""json(unexpected_end_of_file)""
        }
    ]
}",2021-08-26T09:29:17Z,spl,https://github.com/terminusdb/terminusdb/issues/470#issuecomment-906245753,"If I understand, check_descriptor_auth is where the authentication takes place. I don't see any usage of check_descriptor_auth in:

src/server/routes.pl: db_handler
src/core/api/db_create.pl: create_db, create_schema",if i understand check_descriptor_auth be where the authentication take place i do not see any usage of check_descriptor_auth in src / server / routespl db_handler src / core / api / db_createpl create_db create_schema,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,470,2021-08-26T08:54:29Z,spl,Authorization checks happen after request parsing,https://github.com/terminusdb/terminusdb/issues/470,"I expected to see authentication failures instead of these responses.
xh 'http://localhost:6363/api/db/unknown' # GET
HTTP/1.1 405 Method Not Allowed
connection: close
content-length: 105
content-type: application/json; charset=UTF-8
date: Thu, 26 Aug 2021 08:52:56 GMT

{
    ""code"": 405,
    ""location"": ""/api/db/unknown"",
    ""message"": ""Method not allowed: GET"",
    ""method"": ""GET""
}

echo '' | xh 'http://localhost:6363/api/db/unknown' # POST
HTTP/1.1 400 Bad Request
connection: keep-alive
content-length: 173
content-type: application/json
date: Thu, 26 Aug 2021 08:51:43 GMT

{
    ""api:status"": ""api:failure"",
    ""system:witnesses"": [
        {
            ""@type"": ""vio:ViolationWithDatatypeObject"",
            ""vio:literal"": ""json(unexpected_end_of_file)""
        }
    ]
}",2021-08-26T10:06:49Z,spl,https://github.com/terminusdb/terminusdb/issues/470#issuecomment-906270356,"Now I see that auth is handled by assert_auth_action_scope in src/core/api/db_create.pl (create_db_unfinalized). It is strange that other things, such as request body parsing, happen before auth.",now i see that auth be handle by assert_auth_action_scope in src / core / api / db_createpl ( create_db_unfinalized ) -PRON- be strange that other thing such as request body parse happen before auth,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,470,2021-08-26T08:54:29Z,spl,Authorization checks happen after request parsing,https://github.com/terminusdb/terminusdb/issues/470,"I expected to see authentication failures instead of these responses.
xh 'http://localhost:6363/api/db/unknown' # GET
HTTP/1.1 405 Method Not Allowed
connection: close
content-length: 105
content-type: application/json; charset=UTF-8
date: Thu, 26 Aug 2021 08:52:56 GMT

{
    ""code"": 405,
    ""location"": ""/api/db/unknown"",
    ""message"": ""Method not allowed: GET"",
    ""method"": ""GET""
}

echo '' | xh 'http://localhost:6363/api/db/unknown' # POST
HTTP/1.1 400 Bad Request
connection: keep-alive
content-length: 173
content-type: application/json
date: Thu, 26 Aug 2021 08:51:43 GMT

{
    ""api:status"": ""api:failure"",
    ""system:witnesses"": [
        {
            ""@type"": ""vio:ViolationWithDatatypeObject"",
            ""vio:literal"": ""json(unexpected_end_of_file)""
        }
    ]
}",2021-08-26T10:32:24Z,matko,https://github.com/terminusdb/terminusdb/issues/470#issuecomment-906286767,"It may be a little strange, but in general, we only know the parameters to our api calls after figuring out what request method as used and what sort of request document they submitted, so we need to do that parsing work before we have the information required to make a determination on authorization.
This also allows us to have different paths into the same API method, which may get their input arguments in a different way than an HTTP request but nevertheless need to do some form of authorization.
I suppose in this particular case, the submitted document does not actually influence authorization, but it follows the same pattern everywhere. Erroring on an unsupported HTTP method or malformed input data is not a breach, it doesn't leak information or give access to something an unauthorized user should not be allowed to access.",-PRON- may be a little strange but in general -PRON- only know the parameter to -PRON- api call after figure out what request method as use and what sort of request document -PRON- submit so -PRON- need to do that parse work before -PRON- have the information require to make a determination on authorization this also allow -PRON- to have different path into the same api method which may get -PRON- input argument in a different way than an http request but nevertheless need to do some form of authorization i suppose in this particular case the submitted document do not actually influence authorization but -PRON- follow the same pattern everywhere errore on an unsupported http method or malforme input datum be not a breach -PRON- do not leak information or give access to something an unauthorized user should not be allow to access,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,470,2021-08-26T08:54:29Z,spl,Authorization checks happen after request parsing,https://github.com/terminusdb/terminusdb/issues/470,"I expected to see authentication failures instead of these responses.
xh 'http://localhost:6363/api/db/unknown' # GET
HTTP/1.1 405 Method Not Allowed
connection: close
content-length: 105
content-type: application/json; charset=UTF-8
date: Thu, 26 Aug 2021 08:52:56 GMT

{
    ""code"": 405,
    ""location"": ""/api/db/unknown"",
    ""message"": ""Method not allowed: GET"",
    ""method"": ""GET""
}

echo '' | xh 'http://localhost:6363/api/db/unknown' # POST
HTTP/1.1 400 Bad Request
connection: keep-alive
content-length: 173
content-type: application/json
date: Thu, 26 Aug 2021 08:51:43 GMT

{
    ""api:status"": ""api:failure"",
    ""system:witnesses"": [
        {
            ""@type"": ""vio:ViolationWithDatatypeObject"",
            ""vio:literal"": ""json(unexpected_end_of_file)""
        }
    ]
}",2021-08-26T10:54:55Z,spl,https://github.com/terminusdb/terminusdb/issues/470#issuecomment-906301150,"I think this isn't mainly an issue of parsing but a consequence of supporting authorization for both the command-line interface and HTTP interface, which has resulted in making the internal API the smallest set of Prolog predicates shared between the two interfaces and localizing the auth to those predicates. If we focused instead on serving only the HTTP interface, then I think we could simplify the auth story by having it in one place (where the routes and inputs are handled) instead of spreading it around.
That said, I don't think we have any plans to drop the command-line interface at the moment, so I guess we should consider this a won't-fix?",i think this be not mainly an issue of parse but a consequence of support authorization for both the command - line interface and http interface which have result in make the internal api the small set of prolog predicate share between the two interface and localize the auth to those predicate if -PRON- focus instead on serve only the http interface then i think -PRON- could simplify the auth story by have -PRON- in one place ( where the route and input be handle ) instead of spread -PRON- around that say i do not think -PRON- have any plan to drop the command - line interface at the moment so i guess -PRON- should consider this a won't - fix,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,470,2021-08-26T08:54:29Z,spl,Authorization checks happen after request parsing,https://github.com/terminusdb/terminusdb/issues/470,"I expected to see authentication failures instead of these responses.
xh 'http://localhost:6363/api/db/unknown' # GET
HTTP/1.1 405 Method Not Allowed
connection: close
content-length: 105
content-type: application/json; charset=UTF-8
date: Thu, 26 Aug 2021 08:52:56 GMT

{
    ""code"": 405,
    ""location"": ""/api/db/unknown"",
    ""message"": ""Method not allowed: GET"",
    ""method"": ""GET""
}

echo '' | xh 'http://localhost:6363/api/db/unknown' # POST
HTTP/1.1 400 Bad Request
connection: keep-alive
content-length: 173
content-type: application/json
date: Thu, 26 Aug 2021 08:51:43 GMT

{
    ""api:status"": ""api:failure"",
    ""system:witnesses"": [
        {
            ""@type"": ""vio:ViolationWithDatatypeObject"",
            ""vio:literal"": ""json(unexpected_end_of_file)""
        }
    ]
}",2021-09-02T20:34:30Z,GavinMendelGleason,https://github.com/terminusdb/terminusdb/issues/470#issuecomment-912035617,This is a wont-fix.,this be a wont - fix,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,472,2021-08-26T10:39:15Z,Cheukting,Git a stack trace as an error,https://github.com/terminusdb/terminusdb/issues/472,"I got this stack trace:
terminusdb_client.errors.DatabaseError: Error: casting_error(0.0,'http://www.w3.org/2001/XMLSchema#integer')
  [46] throw(error(casting_error(0.0,'http://www.w3.org/2001/XMLSchema#integer'),_69862))
  [44] catch(api_document:do_or_die(...,...),error(casting_error(0.0,'http://www.w3.org/2001/XMLSchema#integer'),_69920),api_document:(...;...)) at /usr/lib/swipl/boot/init.pl:533
  [42] api_document:api_insert_document_(instance,transaction_object{commit_info:commit_info{},descriptor:branch_descriptor{branch_name:""main"",repository_descriptor: ...},inference_objects:[],instance_objects:[...],parent:transaction_object{descriptor: ...,inference_objects:[],instance_objects: ...,parent: ...,schema_objects: ...},schema_objects:[...]},<stream>(0x5614c9102b00),_69980) at /app/terminusdb/src/core/api/api_document.pl:185
  [41] '$bags':findall_loop(_70096,'<garbage_collected>',_70100,[]) at /usr/lib/swipl/boot/bags.pl:99
  [40] setup_call_catcher_cleanup('$bags':'$new_findall_bag','$bags':findall_loop(_70154,...,_70158,[]),_70136,'$bags':'$destroy_findall_bag') at /usr/lib/swipl/boot/init.pl:614
  [36] '<meta-call>'('<garbage_collected>') <foreign>
  [35] database:with_transaction_(query_context{all_witnesses:false,authorization:'terminusdb://system/data/admin',bindings:[],commit_info:commit_info{author:admin,message:'Document object inserted by Python client.'},default_collection:branch_descriptor{branch_name:""main"",repository_descriptor: ...},files:[],filter:type_filter{types: ...},prefixes:_70322{'@base':""terminusdb:///data/"",'@schema':""terminusdb:///schema#"",'@type':'Context',api:'http://terminusdb.com/schema/api#',owl:'http://www.w3.org/2002/07/owl#',rdf:'http://www.w3.org/1999/02/22-rdf-syntax-ns#',rdfs:'http://www.w3.org/2000/01/rdf-schema#',sys:'http://terminusdb.com/schema/sys#',vio:'http://terminusdb.com/schema/vio#',woql:'http://terminusdb.com/schema/woql#',xdd:'http://terminusdb.com/schema/xdd#',xsd:'http://www.w3.org/2001/XMLSchema#'},selected:[],system:system_descriptor{},transaction_objects:[...],update_guard:_70278,write_graph:branch_graph{branch_name:""main"",database_name:""stock_exchange"",organization_name:""Community"",repository_name:""local"",type:instance}},api_document:(...;...),_70240) at /app/terminusdb/src/core/transaction/database.pl:220
  [34] setup_call_catcher_cleanup(database:true,database:with_transaction_(...,...,_70482),_70460,database:true) at /usr/lib/swipl/boot/init.pl:614
  [30] '<meta-call>'('<garbage_collected>') <foreign>
  [29] catch(routes:(...,...),error(casting_error(0.0,'http://www.w3.org/2001/XMLSchema#integer'),context(_70584,_70586)),routes:do_or_die(...,...)) at /usr/lib/swipl/boot/init.pl:532
  [28] catch_with_backtrace('<garbage_collected>','<garbage_collected>','<garbage_collected>') at /usr/lib/swipl/boot/init.pl:582

While insert document with this:
[00:16,  8.41s/it][{'index': 'NYA', 'date': '1966-10-14', 'open': 435.73999, 'high': 435.73999, 'low': 435.73999, 'close': 435.73999, 'adj_close': 435.73999, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_ac69bef9dcc449f1b832955a0fb352d3'}, {'index': 'NYA', 'date': '1966-10-17', 'open': 440.609985, 'high': 440.609985, 'low': 440.609985, 'close': 440.609985, 'adj_close': 440.609985, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_c03f34b77c894c54a88231f1f3f019b4'}, {'index': 'NYA', 'date': '1966-10-18', 'open': 447.799988, 'high': 447.799988, 'low': 447.799988, 'close': 447.799988, 'adj_close': 447.799988, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_7e19232b70b44b2397821064430bab2a'}, {'index': 'NYA', 'date': '1966-10-19', 'open': 444.519989, 'high': 444.519989, 'low': 444.519989, 'close': 444.519989, 'adj_close': 444.519989, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_58b492ab7259449aacab228c5c9ff1c7'}, {'index': 'NYA', 'date': '1966-10-20', 'open': 443.25, 'high': 443.25, 'low': 443.25, 'close': 443.25, 'adj_close': 443.25, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_7714503df8f244afbb2533ff3d83a1a6'}, {'index': 'NYA', 'date': '1966-10-21', 'open': 444.839996, 'high': 444.839996, 'low': 444.839996, 'close': 444.839996, 'adj_close': 444.839996, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_c5e9bbb5f24a4edf8e74f28b1c3eff0e'}, {'index': 'NYA', 'date': '1966-10-24', 'open': 446.0, 'high': 446.0, 'low': 446.0, 'close': 446.0, 'adj_close': 446.0, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_41bfb1e914ad4a41b21c4e611a172079'}, {'index': 'NYA', 'date': '1966-10-25', 'open': 448.429993, 'high': 448.429993, 'low': 448.429993, 'close': 448.429993, 'adj_close': 448.429993, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_0f1318ae6c8a4b81ace6b97652e2a13b'}, {'index': 'NYA', 'date': '1966-10-26', 'open': 452.450012, 'high': 452.450012, 'low': 452.450012, 'close': 452.450012, 'adj_close': 452.450012, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_e4a1ec76f4c44f73b80e947f40f6d88d'}, {'index': 'NYA', 'date': '1966-10-27', 'open': 456.26001, 'high': 456.26001, 'low': 456.26001, 'close': 456.26001, 'adj_close': 456.26001, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_7de0599ee12948039b72634b0fef4533'}, {'index': 'NYA', 'date': '1966-10-28', 'open': 456.470001, 'high': 456.470001, 'low': 456.470001, 'close': 456.470001, 'adj_close': 456.470001, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_1629239709354f0abdb39e5c8594e541'}, {'index': 'NYA', 'date': '1966-10-31', 'open': 456.359985, 'high': 456.359985, 'low': 456.359985, 'close': 456.359985, 'adj_close': 456.359985, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_3fccf298aae441608e96ec5f27ac0a9a'}, {'index': 'NYA', 'date': '1966-11-01', 'open': 459.850006, 'high': 459.850006, 'low': 459.850006, 'close': 459.850006, 'adj_close': 459.850006, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_b4f774c9ee7c454282b4779d7dfcb769'}, {'index': 'NYA', 'date': '1966-11-02', 'open': 460.380005, 'high': 460.380005, 'low': 460.380005, 'close': 460.380005, 'adj_close': 460.380005, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_422de280413947349b3fc42e4daaed31'}, {'index': 'NYA', 'date': '1966-11-03', 'open': 458.790009, 'high': 458.790009, 'low': 458.790009, 'close': 458.790009, 'adj_close': 458.790009, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_d9f5370cdb994c46b627cd21133cc7e4'}, {'index': 'NYA', 'date': '1966-11-04', 'open': 460.269989, 'high': 460.269989, 'low': 460.269989, 'close': 460.269989, 'adj_close': 460.269989, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_7a4722f73e4542a5ba935cbe7f452f20'}, {'index': 'NYA', 'date': '1966-11-07', 'open': 460.170013, 'high': 460.170013, 'low': 460.170013, 'close': 460.170013, 'adj_close': 460.170013, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_2f29143c293443beafd861482958cdc4'}, {'index': 'NYA', 'date': '1966-11-09', 'open': 463.76001, 'high': 463.76001, 'low': 463.76001, 'close': 463.76001, 'adj_close': 463.76001, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_598c12211f5944a2ad97702e3d5a76ff'}, {'index': 'NYA', 'date': '1966-11-10', 'open': 466.829987, 'high': 466.829987, 'low': 466.829987, 'close': 466.829987, 'adj_close': 466.829987, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_b90bc27f854b4553b81780ef05985306'}, {'index': 'NYA', 'date': '1966-11-11', 'open': 467.25, 'high': 467.25, 'low': 467.25, 'close': 467.25, 'adj_close': 467.25, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_d7bf9c68bb604c309541d1f5e45f4e6c'}, {'index': 'NYA', 'date': '1966-11-14', 'open': 464.399994, 'high': 464.399994, 'low': 464.399994, 'close': 464.399994, 'adj_close': 464.399994, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_27c0c637dba3478abc8e06aeb2f1de13'}, {'index': 'NYA', 'date': '1966-11-15', 'open': 466.089996, 'high': 466.089996, 'low': 466.089996, 'close': 466.089996, 'adj_close': 466.089996, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_7e229ec3335f48e5af5b0babf6202674'}, {'index': 'NYA', 'date': '1966-11-16', 'open': 470.109985, 'high': 470.109985, 'low': 470.109985, 'close': 470.109985, 'adj_close': 470.109985, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_196ee5c117c94196a6c86d955ac1825b'}, {'index': 'NYA', 'date': '1966-11-17', 'open': 466.940002, 'high': 466.940002, 'low': 466.940002, 'close': 466.940002, 'adj_close': 466.940002, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_47104cbdfa9c42b0a82885e1fadb691a'}, {'index': 'NYA', 'date': '1966-11-18', 'open': 464.079987, 'high': 464.079987, 'low': 464.079987, 'close': 464.079987, 'adj_close': 464.079987, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_992a7fbdc05341ec9a2cb849893bfedd'}, {'index': 'NYA', 'date': '1966-11-21', 'open': 457.630005, 'high': 457.630005, 'low': 457.630005, 'close': 457.630005, 'adj_close': 457.630005, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_9eebb4f30617495d9b13f72db8039c55'}, {'index': 'NYA', 'date': '1966-11-22', 'open': 455.519989, 'high': 455.519989, 'low': 455.519989, 'close': 455.519989, 'adj_close': 455.519989, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_6befb11eb46a4d8f8cdf5e1f14e0bd7d'}, {'index': 'NYA', 'date': '1966-11-23', 'open': 458.899994, 'high': 458.899994, 'low': 458.899994, 'close': 458.899994, 'adj_close': 458.899994, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_f436b7462fc94d39ac24c1186565ed33'}, {'index': 'NYA', 'date': '1966-11-25', 'open': 462.600006, 'high': 462.600006, 'low': 462.600006, 'close': 462.600006, 'adj_close': 462.600006, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_3ed857eeaaf04c9c8ed5520e4b683d44'}, {'index': 'NYA', 'date': '1966-11-28', 'open': 462.179993, 'high': 462.179993, 'low': 462.179993, 'close': 462.179993, 'adj_close': 462.179993, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_be6cd17a1acc4c99be615447a4cd9a72'}, {'index': 'NYA', 'date': '1966-11-29', 'open': 460.589996, 'high': 460.589996, 'low': 460.589996, 'close': 460.589996, 'adj_close': 460.589996, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_96627271d0a847819ff3f493cb537612'}, {'index': 'NYA', 'date': '1966-11-30', 'open': 460.910004, 'high': 460.910004, 'low': 460.910004, 'close': 460.910004, 'adj_close': 460.910004, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_09ff579a912d4804a8a128fbfd608a63'}, {'index': 'NYA', 'date': '1966-12-01', 'open': 459.109985, 'high': 459.109985, 'low': 459.109985, 'close': 459.109985, 'adj_close': 459.109985, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_13f58d6daaee463b80164c1e9d8603cd'}, {'index': 'NYA', 'date': '1966-12-02', 'open': 459.320007, 'high': 459.320007, 'low': 459.320007, 'close': 459.320007, 'adj_close': 459.320007, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_bad15ef435da4d0088c690a26d2789b9'}, {'index': 'NYA', 'date': '1966-12-05', 'open': 459.75, 'high': 459.75, 'low': 459.75, 'close': 459.75, 'adj_close': 459.75, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_b87b7ea4cf6a4d3f93803f427665a7ed'}, {'index': 'NYA', 'date': '1966-12-06', 'open': 462.920013, 'high': 462.920013, 'low': 462.920013, 'close': 462.920013, 'adj_close': 462.920013, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_e2c359288ab140fbb7fe3389c4d9cf48'}, {'index': 'NYA', 'date': '1966-12-07', 'open': 467.570007, 'high': 467.570007, 'low': 467.570007, 'close': 467.570007, 'adj_close': 467.570007, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_cc7f06ae482146dcbbc12d44ef668a04'}, {'index': 'NYA', 'date': '1966-12-08', 'open': 469.470001, 'high': 469.470001, 'low': 469.470001, 'close': 469.470001, 'adj_close': 469.470001, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_4eaff3fb22014b378026b5fa18998d5f'}, {'index': 'NYA', 'date': '1966-12-09', 'open': 470.529999, 'high': 470.529999, 'low': 470.529999, 'close': 470.529999, 'adj_close': 470.529999, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_cdac45a2debd4cbebda0f6193b77a6fa'}, {'index': 'NYA', 'date': '1966-12-12', 'open': 475.079987, 'high': 475.079987, 'low': 475.079987, 'close': 475.079987, 'adj_close': 475.079987, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_48bbdba89bc046f78cd4852e8ce8af62'}, {'index': 'NYA', 'date': '1966-12-13', 'open': 473.809998, 'high': 473.809998, 'low': 473.809998, 'close': 473.809998, 'adj_close': 473.809998, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_8b09bfc77ba24dd2b449c8c4a1ad9e23'}, {'index': 'NYA', 'date': '1966-12-14', 'open': 473.700012, 'high': 473.700012, 'low': 473.700012, 'close': 473.700012, 'adj_close': 473.700012, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_b842ca919652455ba6640b96afe9a7da'}, {'index': 'NYA', 'date': '1966-12-15', 'open': 468.730011, 'high': 468.730011, 'low': 468.730011, 'close': 468.730011, 'adj_close': 468.730011, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_a1ea9fe9f02b486abea545b0ddedc802'}, {'index': 'NYA', 'date': '1966-12-16', 'open': 468.839996, 'high': 468.839996, 'low': 468.839996, 'close': 468.839996, 'adj_close': 468.839996, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_4fdc373cc99a4ea0b65e187f5310f5f7'}, {'index': 'NYA', 'date': '1966-12-19', 'open': 467.570007, 'high': 467.570007, 'low': 467.570007, 'close': 467.570007, 'adj_close': 467.570007, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_69387d1fef65455c9078fafc6d3bc003'}, {'index': 'NYA', 'date': '1966-12-20', 'open': 465.670013, 'high': 465.670013, 'low': 465.670013, 'close': 465.670013, 'adj_close': 465.670013, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_3a9c802396b24a8fa0c6fabfe101b9c8'}, {'index': 'NYA', 'date': '1966-12-21', 'open': 468.200012, 'high': 468.200012, 'low': 468.200012, 'close': 468.200012, 'adj_close': 468.200012, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_763800a0ba9d4b55aefdb61c7ade78c9'}, {'index': 'NYA', 'date': '1966-12-22', 'open': 470.109985, 'high': 470.109985, 'low': 470.109985, 'close': 470.109985, 'adj_close': 470.109985, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_a003681eb1114b468c7614429bad2cd8'}, {'index': 'NYA', 'date': '1966-12-23', 'open': 468.940002, 'high': 468.940002, 'low': 468.940002, 'close': 468.940002, 'adj_close': 468.940002, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_756f96892356400e9a5435d2c75d0f14'}, {'index': 'NYA', 'date': '1966-12-27', 'open': 466.51001, 'high': 466.51001, 'low': 466.51001, 'close': 466.51001, 'adj_close': 466.51001, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_e862ffdded764179aa1796c6146a248d'}, {'index': 'NYA', 'date': '1966-12-28', 'open': 464.190002, 'high': 464.190002, 'low': 464.190002, 'close': 464.190002, 'adj_close': 464.190002, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_62f9fe7072fc43518c3259e8e41c1102'}, {'index': 'NYA', 'date': '1966-12-29', 'open': 462.48999, 'high': 462.48999, 'low': 462.48999, 'close': 462.48999, 'adj_close': 462.48999, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_e45a7d3a48ab4d7298c9f30547a55189'}, {'index': 'NYA', 'date': '1966-12-30', 'open': 462.279999, 'high': 462.279999, 'low': 462.279999, 'close': 462.279999, 'adj_close': 462.279999, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_5c0cc58c00dd4e72b11d42d8bb52aa55'}, {'index': 'NYA', 'date': '1967-01-03', 'open': 462.48999, 'high': 462.48999, 'low': 462.48999, 'close': 462.48999, 'adj_close': 462.48999, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_b8224579041542a5b588d23bd725561b'}, {'index': 'NYA', 'date': '1967-01-04', 'open': 463.339996, 'high': 463.339996, 'low': 463.339996, 'close': 463.339996, 'adj_close': 463.339996, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_4908ee25f6cb4c6cbcf1429e78024153'}, {'index': 'NYA', 'date': '1967-01-05', 'open': 469.26001, 'high': 469.26001, 'low': 469.26001, 'close': 469.26001, 'adj_close': 469.26001, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_36aff27b24c84551939a81b41d5078b5'}, {'index': 'NYA', 'date': '1967-01-06', 'open': 472.75, 'high': 472.75, 'low': 472.75, 'close': 472.75, 'adj_close': 472.75, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_b0fa38527a934e6baa2ca2e154173da9'}, {'index': 'NYA', 'date': '1967-01-09', 'open': 476.660004, 'high': 476.660004, 'low': 476.660004, 'close': 476.660004, 'adj_close': 476.660004, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_ea6a640d367340c6915daf0fa656b1ca'}, {'index': 'NYA', 'date': '1967-01-10', 'open': 476.660004, 'high': 476.660004, 'low': 476.660004, 'close': 476.660004, 'adj_close': 476.660004, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_d1ccdefd919f479eb3e676a16263f85d'}, {'index': 'NYA', 'date': '1967-01-11', 'open': 480.579987, 'high': 480.579987, 'low': 480.579987, 'close': 480.579987, 'adj_close': 480.579987, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_55f7b42365f34d20b24d762c8fd9fa79'}, {'index': 'NYA', 'date': '1967-01-12', 'open': 483.429993, 'high': 483.429993, 'low': 483.429993, 'close': 483.429993, 'adj_close': 483.429993, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_2495360e59a44e17b527592f525cedc2'}, {'index': 'NYA', 'date': '1967-01-13', 'open': 486.809998, 'high': 486.809998, 'low': 486.809998, 'close': 486.809998, 'adj_close': 486.809998, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_c4b69088c4274358ab51d8fb6ec37a38'}, {'index': 'NYA', 'date': '1967-01-16', 'open': 485.970001, 'high': 485.970001, 'low': 485.970001, 'close': 485.970001, 'adj_close': 485.970001, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_19902d23c7bf42a8a6cf80a469228aa8'}, {'index': 'NYA', 'date': '1967-01-17', 'open': 491.26001, 'high': 491.26001, 'low': 491.26001, 'close': 491.26001, 'adj_close': 491.26001, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_a338832557304a0082d653d53e52e44b'}, {'index': 'NYA', 'date': '1967-01-18', 'open': 494.0, 'high': 494.0, 'low': 494.0, 'close': 494.0, 'adj_close': 494.0, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_1a66efbf81c147bba5efec461328f6f2'}, {'index': 'NYA', 'date': '1967-01-19', 'open': 494.220001, 'high': 494.220001, 'low': 494.220001, 'close': 494.220001, 'adj_close': 494.220001, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_7291ec3968bd4daa9c9ae21c6c817896'}, {'index': 'NYA', 'date': '1967-01-20', 'open': 496.01001, 'high': 496.01001, 'low': 496.01001, 'close': 496.01001, 'adj_close': 496.01001, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_188ebed56d5742e3a9bebb820513278c'}, {'index': 'NYA', 'date': '1967-01-23', 'open': 497.809998, 'high': 497.809998, 'low': 497.809998, 'close': 497.809998, 'adj_close': 497.809998, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_e676d68d055a4d3a9151ebc6668d2ee3'}, {'index': 'NYA', 'date': '1967-01-24', 'open': 498.76001, 'high': 498.76001, 'low': 498.76001, 'close': 498.76001, 'adj_close': 498.76001, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_c1c6a375d4ce42afa982e620c9a4ec3b'}, {'index': 'NYA', 'date': '1967-01-25', 'open': 495.380005, 'high': 495.380005, 'low': 495.380005, 'close': 495.380005, 'adj_close': 495.380005, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_9351ed7bb23842abb45b91e828c55e2a'}, {'index': 'NYA', 'date': '1967-01-26', 'open': 495.700012, 'high': 495.700012, 'low': 495.700012, 'close': 495.700012, 'adj_close': 495.700012, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_cc260c71c62943e9bb7f4576f2f719c7'}, {'index': 'NYA', 'date': '1967-01-27', 'open': 497.809998, 'high': 497.809998, 'low': 497.809998, 'close': 497.809998, 'adj_close': 497.809998, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_a5d780fcdb3048249d018f26ec7476a8'}, {'index': 'NYA', 'date': '1967-01-30', 'open': 500.670013, 'high': 500.670013, 'low': 500.670013, 'close': 500.670013, 'adj_close': 500.670013, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_57aea50b6cb5413dbac7f0c09883e24f'}, {'index': 'NYA', 'date': '1967-01-31', 'open': 500.140015, 'high': 500.140015, 'low': 500.140015, 'close': 500.140015, 'adj_close': 500.140015, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_2c76c472ce8d42098d837b9e38d03b45'}, {'index': 'NYA', 'date': '1967-02-01', 'open': 499.5, 'high': 499.5, 'low': 499.5, 'close': 499.5, 'adj_close': 499.5, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_6c5630d97c9648edae86375a33464765'}, {'index': 'NYA', 'date': '1967-02-02', 'open': 501.089996, 'high': 501.089996, 'low': 501.089996, 'close': 501.089996, 'adj_close': 501.089996, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_286f4582c20a4da5aa5adf08a3cd6458'}, {'index': 'NYA', 'date': '1967-02-03', 'open': 504.679993, 'high': 504.679993, 'low': 504.679993, 'close': 504.679993, 'adj_close': 504.679993, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_2f842d7ddf084fc395b37eac60f3b27d'}, {'index': 'NYA', 'date': '1967-02-06', 'open': 503.730011, 'high': 503.730011, 'low': 503.730011, 'close': 503.730011, 'adj_close': 503.730011, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_eecbc521e9504d35b9edd65cb29ca668'}, {'index': 'NYA', 'date': '1967-02-07', 'open': 502.679993, 'high': 502.679993, 'low': 502.679993, 'close': 502.679993, 'adj_close': 502.679993, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_7c357f0bba9b40fe9a280123bca76883'}, {'index': 'NYA', 'date': '1967-02-08', 'open': 507.119995, 'high': 507.119995, 'low': 507.119995, 'close': 507.119995, 'adj_close': 507.119995, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_6f1672bf7ce94f1eb5453ee42f25d289'}, {'index': 'NYA', 'date': '1967-02-09', 'open': 505.109985, 'high': 505.109985, 'low': 505.109985, 'close': 505.109985, 'adj_close': 505.109985, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_d3676d3881b242f6ba37676b339da629'}, {'index': 'NYA', 'date': '1967-02-10', 'open': 506.160004, 'high': 506.160004, 'low': 506.160004, 'close': 506.160004, 'adj_close': 506.160004, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_b0d83942f30b43209b4cd35eda075a5a'}, {'index': 'NYA', 'date': '1967-02-13', 'open': 506.899994, 'high': 506.899994, 'low': 506.899994, 'close': 506.899994, 'adj_close': 506.899994, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_76237c709f0a47088dced65b3219c489'}, {'index': 'NYA', 'date': '1967-02-14', 'open': 510.290009, 'high': 510.290009, 'low': 510.290009, 'close': 510.290009, 'adj_close': 510.290009, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_fa9acffbd77d4d49bddb7aabd3832f1b'}, {'index': 'NYA', 'date': '1967-02-15', 'open': 510.609985, 'high': 510.609985, 'low': 510.609985, 'close': 510.609985, 'adj_close': 510.609985, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_37ad9f23e08d4c678fec82c84855338e'}, {'index': 'NYA', 'date': '1967-02-16', 'open': 508.380005, 'high': 508.380005, 'low': 508.380005, 'close': 508.380005, 'adj_close': 508.380005, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_03aeb54fef0a4c60a86b67f8be97d0c8'}, {'index': 'NYA', 'date': '1967-02-17', 'open': 508.48999, 'high': 508.48999, 'low': 508.48999, 'close': 508.48999, 'adj_close': 508.48999, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_67febd829f0a43a1a3932e3c2b8e255c'}, {'index': 'NYA', 'date': '1967-02-20', 'open': 505.850006, 'high': 505.850006, 'low': 505.850006, 'close': 505.850006, 'adj_close': 505.850006, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_6b55825311e54d048dfe9d6aaa37acfc'}, {'index': 'NYA', 'date': '1967-02-21', 'open': 505.850006, 'high': 505.850006, 'low': 505.850006, 'close': 505.850006, 'adj_close': 505.850006, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_6165260771b2448ab65e75ca2e7ad39a'}, {'index': 'NYA', 'date': '1967-02-23', '@type': 'IndexDataOp', '@id': 'IndexDataOp_a4d281ab44e143c8aef0a398cf615ec8'}, {'index': 'NYA', 'date': '1967-02-24', 'open': 506.480011, 'high': 506.480011, 'low': 506.480011, 'close': 506.480011, 'adj_close': 506.480011, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_340f944337294879b5803c7b592b1944'}, {'index': 'NYA', 'date': '1967-02-27', 'open': 501.089996, 'high': 501.089996, 'low': 501.089996, 'close': 501.089996, 'adj_close': 501.089996, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_4905bfca283b4e4c8fa8fb59bbfe10bc'}, {'index': 'NYA', 'date': '1967-02-28', 'open': 502.890015, 'high': 502.890015, 'low': 502.890015, 'close': 502.890015, 'adj_close': 502.890015, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_45ad14a648e24e998dd1506b26812d82'}, {'index': 'NYA', 'date': '1967-03-01', 'open': 507.859985, 'high': 507.859985, 'low': 507.859985, 'close': 507.859985, 'adj_close': 507.859985, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_169bd2d888f34fd7962b1f4b77311ddd'}, {'index': 'NYA', 'date': '1967-03-02', 'open': 510.709991, 'high': 510.709991, 'low': 510.709991, 'close': 510.709991, 'adj_close': 510.709991, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_567b3c6178ea45169c82c110311af628'}, {'index': 'NYA', 'date': '1967-03-03', 'open': 511.450012, 'high': 511.450012, 'low': 511.450012, 'close': 511.450012, 'adj_close': 511.450012, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_b901176582bc4124a8f4dfe85e3f6e4c'}, {'index': 'NYA', 'date': '1967-03-06', 'open': 510.709991, 'high': 510.709991, 'low': 510.709991, 'close': 510.709991, 'adj_close': 510.709991, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_9032e62b60854c0ca10de0c3faffd565'}, {'index': 'NYA', 'date': '1967-03-07', 'open': 511.350006, 'high': 511.350006, 'low': 511.350006, 'close': 511.350006, 'adj_close': 511.350006, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_76dc8e44f64540eebab6717166ca22a2'}, {'index': 'NYA', 'date': '1967-03-08', 'open': 512.090027, 'high': 512.090027, 'low': 512.090027, 'close': 512.090027, 'adj_close': 512.090027, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_1590c7129b8d4e96adf3fad0c25f7741'}, {'index': 'NYA', 'date': '1967-03-09', 'open': 513.570007, 'high': 513.570007, 'low': 513.570007, 'close': 513.570007, 'adj_close': 513.570007, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_27866b6fcba04c32b490d5155849a043'}]

The schema is:
class IndexDataOp(DocumentTemplate):
    _key = RandomKey()
    adj_close: Optional[float]
    close: Optional[float]
    date: Optional[str]
    high: Optional[float]
    index: Optional[str]
    low: Optional[float]
    open: Optional[float]
    volume: Optional[int]

Sorry I don't have it in json",2021-08-26T10:41:33Z,Cheukting,https://github.com/terminusdb/terminusdb/issues/472#issuecomment-906292375,The error is telling me that volume is an integer and I have a 0.0. Is this expected behaviour?,the error be tell -PRON- that volume be an integer and i have a 00 is this expect behaviour,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,472,2021-08-26T10:39:15Z,Cheukting,Git a stack trace as an error,https://github.com/terminusdb/terminusdb/issues/472,"I got this stack trace:
terminusdb_client.errors.DatabaseError: Error: casting_error(0.0,'http://www.w3.org/2001/XMLSchema#integer')
  [46] throw(error(casting_error(0.0,'http://www.w3.org/2001/XMLSchema#integer'),_69862))
  [44] catch(api_document:do_or_die(...,...),error(casting_error(0.0,'http://www.w3.org/2001/XMLSchema#integer'),_69920),api_document:(...;...)) at /usr/lib/swipl/boot/init.pl:533
  [42] api_document:api_insert_document_(instance,transaction_object{commit_info:commit_info{},descriptor:branch_descriptor{branch_name:""main"",repository_descriptor: ...},inference_objects:[],instance_objects:[...],parent:transaction_object{descriptor: ...,inference_objects:[],instance_objects: ...,parent: ...,schema_objects: ...},schema_objects:[...]},<stream>(0x5614c9102b00),_69980) at /app/terminusdb/src/core/api/api_document.pl:185
  [41] '$bags':findall_loop(_70096,'<garbage_collected>',_70100,[]) at /usr/lib/swipl/boot/bags.pl:99
  [40] setup_call_catcher_cleanup('$bags':'$new_findall_bag','$bags':findall_loop(_70154,...,_70158,[]),_70136,'$bags':'$destroy_findall_bag') at /usr/lib/swipl/boot/init.pl:614
  [36] '<meta-call>'('<garbage_collected>') <foreign>
  [35] database:with_transaction_(query_context{all_witnesses:false,authorization:'terminusdb://system/data/admin',bindings:[],commit_info:commit_info{author:admin,message:'Document object inserted by Python client.'},default_collection:branch_descriptor{branch_name:""main"",repository_descriptor: ...},files:[],filter:type_filter{types: ...},prefixes:_70322{'@base':""terminusdb:///data/"",'@schema':""terminusdb:///schema#"",'@type':'Context',api:'http://terminusdb.com/schema/api#',owl:'http://www.w3.org/2002/07/owl#',rdf:'http://www.w3.org/1999/02/22-rdf-syntax-ns#',rdfs:'http://www.w3.org/2000/01/rdf-schema#',sys:'http://terminusdb.com/schema/sys#',vio:'http://terminusdb.com/schema/vio#',woql:'http://terminusdb.com/schema/woql#',xdd:'http://terminusdb.com/schema/xdd#',xsd:'http://www.w3.org/2001/XMLSchema#'},selected:[],system:system_descriptor{},transaction_objects:[...],update_guard:_70278,write_graph:branch_graph{branch_name:""main"",database_name:""stock_exchange"",organization_name:""Community"",repository_name:""local"",type:instance}},api_document:(...;...),_70240) at /app/terminusdb/src/core/transaction/database.pl:220
  [34] setup_call_catcher_cleanup(database:true,database:with_transaction_(...,...,_70482),_70460,database:true) at /usr/lib/swipl/boot/init.pl:614
  [30] '<meta-call>'('<garbage_collected>') <foreign>
  [29] catch(routes:(...,...),error(casting_error(0.0,'http://www.w3.org/2001/XMLSchema#integer'),context(_70584,_70586)),routes:do_or_die(...,...)) at /usr/lib/swipl/boot/init.pl:532
  [28] catch_with_backtrace('<garbage_collected>','<garbage_collected>','<garbage_collected>') at /usr/lib/swipl/boot/init.pl:582

While insert document with this:
[00:16,  8.41s/it][{'index': 'NYA', 'date': '1966-10-14', 'open': 435.73999, 'high': 435.73999, 'low': 435.73999, 'close': 435.73999, 'adj_close': 435.73999, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_ac69bef9dcc449f1b832955a0fb352d3'}, {'index': 'NYA', 'date': '1966-10-17', 'open': 440.609985, 'high': 440.609985, 'low': 440.609985, 'close': 440.609985, 'adj_close': 440.609985, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_c03f34b77c894c54a88231f1f3f019b4'}, {'index': 'NYA', 'date': '1966-10-18', 'open': 447.799988, 'high': 447.799988, 'low': 447.799988, 'close': 447.799988, 'adj_close': 447.799988, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_7e19232b70b44b2397821064430bab2a'}, {'index': 'NYA', 'date': '1966-10-19', 'open': 444.519989, 'high': 444.519989, 'low': 444.519989, 'close': 444.519989, 'adj_close': 444.519989, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_58b492ab7259449aacab228c5c9ff1c7'}, {'index': 'NYA', 'date': '1966-10-20', 'open': 443.25, 'high': 443.25, 'low': 443.25, 'close': 443.25, 'adj_close': 443.25, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_7714503df8f244afbb2533ff3d83a1a6'}, {'index': 'NYA', 'date': '1966-10-21', 'open': 444.839996, 'high': 444.839996, 'low': 444.839996, 'close': 444.839996, 'adj_close': 444.839996, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_c5e9bbb5f24a4edf8e74f28b1c3eff0e'}, {'index': 'NYA', 'date': '1966-10-24', 'open': 446.0, 'high': 446.0, 'low': 446.0, 'close': 446.0, 'adj_close': 446.0, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_41bfb1e914ad4a41b21c4e611a172079'}, {'index': 'NYA', 'date': '1966-10-25', 'open': 448.429993, 'high': 448.429993, 'low': 448.429993, 'close': 448.429993, 'adj_close': 448.429993, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_0f1318ae6c8a4b81ace6b97652e2a13b'}, {'index': 'NYA', 'date': '1966-10-26', 'open': 452.450012, 'high': 452.450012, 'low': 452.450012, 'close': 452.450012, 'adj_close': 452.450012, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_e4a1ec76f4c44f73b80e947f40f6d88d'}, {'index': 'NYA', 'date': '1966-10-27', 'open': 456.26001, 'high': 456.26001, 'low': 456.26001, 'close': 456.26001, 'adj_close': 456.26001, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_7de0599ee12948039b72634b0fef4533'}, {'index': 'NYA', 'date': '1966-10-28', 'open': 456.470001, 'high': 456.470001, 'low': 456.470001, 'close': 456.470001, 'adj_close': 456.470001, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_1629239709354f0abdb39e5c8594e541'}, {'index': 'NYA', 'date': '1966-10-31', 'open': 456.359985, 'high': 456.359985, 'low': 456.359985, 'close': 456.359985, 'adj_close': 456.359985, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_3fccf298aae441608e96ec5f27ac0a9a'}, {'index': 'NYA', 'date': '1966-11-01', 'open': 459.850006, 'high': 459.850006, 'low': 459.850006, 'close': 459.850006, 'adj_close': 459.850006, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_b4f774c9ee7c454282b4779d7dfcb769'}, {'index': 'NYA', 'date': '1966-11-02', 'open': 460.380005, 'high': 460.380005, 'low': 460.380005, 'close': 460.380005, 'adj_close': 460.380005, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_422de280413947349b3fc42e4daaed31'}, {'index': 'NYA', 'date': '1966-11-03', 'open': 458.790009, 'high': 458.790009, 'low': 458.790009, 'close': 458.790009, 'adj_close': 458.790009, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_d9f5370cdb994c46b627cd21133cc7e4'}, {'index': 'NYA', 'date': '1966-11-04', 'open': 460.269989, 'high': 460.269989, 'low': 460.269989, 'close': 460.269989, 'adj_close': 460.269989, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_7a4722f73e4542a5ba935cbe7f452f20'}, {'index': 'NYA', 'date': '1966-11-07', 'open': 460.170013, 'high': 460.170013, 'low': 460.170013, 'close': 460.170013, 'adj_close': 460.170013, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_2f29143c293443beafd861482958cdc4'}, {'index': 'NYA', 'date': '1966-11-09', 'open': 463.76001, 'high': 463.76001, 'low': 463.76001, 'close': 463.76001, 'adj_close': 463.76001, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_598c12211f5944a2ad97702e3d5a76ff'}, {'index': 'NYA', 'date': '1966-11-10', 'open': 466.829987, 'high': 466.829987, 'low': 466.829987, 'close': 466.829987, 'adj_close': 466.829987, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_b90bc27f854b4553b81780ef05985306'}, {'index': 'NYA', 'date': '1966-11-11', 'open': 467.25, 'high': 467.25, 'low': 467.25, 'close': 467.25, 'adj_close': 467.25, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_d7bf9c68bb604c309541d1f5e45f4e6c'}, {'index': 'NYA', 'date': '1966-11-14', 'open': 464.399994, 'high': 464.399994, 'low': 464.399994, 'close': 464.399994, 'adj_close': 464.399994, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_27c0c637dba3478abc8e06aeb2f1de13'}, {'index': 'NYA', 'date': '1966-11-15', 'open': 466.089996, 'high': 466.089996, 'low': 466.089996, 'close': 466.089996, 'adj_close': 466.089996, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_7e229ec3335f48e5af5b0babf6202674'}, {'index': 'NYA', 'date': '1966-11-16', 'open': 470.109985, 'high': 470.109985, 'low': 470.109985, 'close': 470.109985, 'adj_close': 470.109985, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_196ee5c117c94196a6c86d955ac1825b'}, {'index': 'NYA', 'date': '1966-11-17', 'open': 466.940002, 'high': 466.940002, 'low': 466.940002, 'close': 466.940002, 'adj_close': 466.940002, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_47104cbdfa9c42b0a82885e1fadb691a'}, {'index': 'NYA', 'date': '1966-11-18', 'open': 464.079987, 'high': 464.079987, 'low': 464.079987, 'close': 464.079987, 'adj_close': 464.079987, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_992a7fbdc05341ec9a2cb849893bfedd'}, {'index': 'NYA', 'date': '1966-11-21', 'open': 457.630005, 'high': 457.630005, 'low': 457.630005, 'close': 457.630005, 'adj_close': 457.630005, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_9eebb4f30617495d9b13f72db8039c55'}, {'index': 'NYA', 'date': '1966-11-22', 'open': 455.519989, 'high': 455.519989, 'low': 455.519989, 'close': 455.519989, 'adj_close': 455.519989, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_6befb11eb46a4d8f8cdf5e1f14e0bd7d'}, {'index': 'NYA', 'date': '1966-11-23', 'open': 458.899994, 'high': 458.899994, 'low': 458.899994, 'close': 458.899994, 'adj_close': 458.899994, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_f436b7462fc94d39ac24c1186565ed33'}, {'index': 'NYA', 'date': '1966-11-25', 'open': 462.600006, 'high': 462.600006, 'low': 462.600006, 'close': 462.600006, 'adj_close': 462.600006, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_3ed857eeaaf04c9c8ed5520e4b683d44'}, {'index': 'NYA', 'date': '1966-11-28', 'open': 462.179993, 'high': 462.179993, 'low': 462.179993, 'close': 462.179993, 'adj_close': 462.179993, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_be6cd17a1acc4c99be615447a4cd9a72'}, {'index': 'NYA', 'date': '1966-11-29', 'open': 460.589996, 'high': 460.589996, 'low': 460.589996, 'close': 460.589996, 'adj_close': 460.589996, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_96627271d0a847819ff3f493cb537612'}, {'index': 'NYA', 'date': '1966-11-30', 'open': 460.910004, 'high': 460.910004, 'low': 460.910004, 'close': 460.910004, 'adj_close': 460.910004, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_09ff579a912d4804a8a128fbfd608a63'}, {'index': 'NYA', 'date': '1966-12-01', 'open': 459.109985, 'high': 459.109985, 'low': 459.109985, 'close': 459.109985, 'adj_close': 459.109985, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_13f58d6daaee463b80164c1e9d8603cd'}, {'index': 'NYA', 'date': '1966-12-02', 'open': 459.320007, 'high': 459.320007, 'low': 459.320007, 'close': 459.320007, 'adj_close': 459.320007, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_bad15ef435da4d0088c690a26d2789b9'}, {'index': 'NYA', 'date': '1966-12-05', 'open': 459.75, 'high': 459.75, 'low': 459.75, 'close': 459.75, 'adj_close': 459.75, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_b87b7ea4cf6a4d3f93803f427665a7ed'}, {'index': 'NYA', 'date': '1966-12-06', 'open': 462.920013, 'high': 462.920013, 'low': 462.920013, 'close': 462.920013, 'adj_close': 462.920013, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_e2c359288ab140fbb7fe3389c4d9cf48'}, {'index': 'NYA', 'date': '1966-12-07', 'open': 467.570007, 'high': 467.570007, 'low': 467.570007, 'close': 467.570007, 'adj_close': 467.570007, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_cc7f06ae482146dcbbc12d44ef668a04'}, {'index': 'NYA', 'date': '1966-12-08', 'open': 469.470001, 'high': 469.470001, 'low': 469.470001, 'close': 469.470001, 'adj_close': 469.470001, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_4eaff3fb22014b378026b5fa18998d5f'}, {'index': 'NYA', 'date': '1966-12-09', 'open': 470.529999, 'high': 470.529999, 'low': 470.529999, 'close': 470.529999, 'adj_close': 470.529999, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_cdac45a2debd4cbebda0f6193b77a6fa'}, {'index': 'NYA', 'date': '1966-12-12', 'open': 475.079987, 'high': 475.079987, 'low': 475.079987, 'close': 475.079987, 'adj_close': 475.079987, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_48bbdba89bc046f78cd4852e8ce8af62'}, {'index': 'NYA', 'date': '1966-12-13', 'open': 473.809998, 'high': 473.809998, 'low': 473.809998, 'close': 473.809998, 'adj_close': 473.809998, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_8b09bfc77ba24dd2b449c8c4a1ad9e23'}, {'index': 'NYA', 'date': '1966-12-14', 'open': 473.700012, 'high': 473.700012, 'low': 473.700012, 'close': 473.700012, 'adj_close': 473.700012, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_b842ca919652455ba6640b96afe9a7da'}, {'index': 'NYA', 'date': '1966-12-15', 'open': 468.730011, 'high': 468.730011, 'low': 468.730011, 'close': 468.730011, 'adj_close': 468.730011, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_a1ea9fe9f02b486abea545b0ddedc802'}, {'index': 'NYA', 'date': '1966-12-16', 'open': 468.839996, 'high': 468.839996, 'low': 468.839996, 'close': 468.839996, 'adj_close': 468.839996, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_4fdc373cc99a4ea0b65e187f5310f5f7'}, {'index': 'NYA', 'date': '1966-12-19', 'open': 467.570007, 'high': 467.570007, 'low': 467.570007, 'close': 467.570007, 'adj_close': 467.570007, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_69387d1fef65455c9078fafc6d3bc003'}, {'index': 'NYA', 'date': '1966-12-20', 'open': 465.670013, 'high': 465.670013, 'low': 465.670013, 'close': 465.670013, 'adj_close': 465.670013, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_3a9c802396b24a8fa0c6fabfe101b9c8'}, {'index': 'NYA', 'date': '1966-12-21', 'open': 468.200012, 'high': 468.200012, 'low': 468.200012, 'close': 468.200012, 'adj_close': 468.200012, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_763800a0ba9d4b55aefdb61c7ade78c9'}, {'index': 'NYA', 'date': '1966-12-22', 'open': 470.109985, 'high': 470.109985, 'low': 470.109985, 'close': 470.109985, 'adj_close': 470.109985, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_a003681eb1114b468c7614429bad2cd8'}, {'index': 'NYA', 'date': '1966-12-23', 'open': 468.940002, 'high': 468.940002, 'low': 468.940002, 'close': 468.940002, 'adj_close': 468.940002, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_756f96892356400e9a5435d2c75d0f14'}, {'index': 'NYA', 'date': '1966-12-27', 'open': 466.51001, 'high': 466.51001, 'low': 466.51001, 'close': 466.51001, 'adj_close': 466.51001, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_e862ffdded764179aa1796c6146a248d'}, {'index': 'NYA', 'date': '1966-12-28', 'open': 464.190002, 'high': 464.190002, 'low': 464.190002, 'close': 464.190002, 'adj_close': 464.190002, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_62f9fe7072fc43518c3259e8e41c1102'}, {'index': 'NYA', 'date': '1966-12-29', 'open': 462.48999, 'high': 462.48999, 'low': 462.48999, 'close': 462.48999, 'adj_close': 462.48999, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_e45a7d3a48ab4d7298c9f30547a55189'}, {'index': 'NYA', 'date': '1966-12-30', 'open': 462.279999, 'high': 462.279999, 'low': 462.279999, 'close': 462.279999, 'adj_close': 462.279999, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_5c0cc58c00dd4e72b11d42d8bb52aa55'}, {'index': 'NYA', 'date': '1967-01-03', 'open': 462.48999, 'high': 462.48999, 'low': 462.48999, 'close': 462.48999, 'adj_close': 462.48999, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_b8224579041542a5b588d23bd725561b'}, {'index': 'NYA', 'date': '1967-01-04', 'open': 463.339996, 'high': 463.339996, 'low': 463.339996, 'close': 463.339996, 'adj_close': 463.339996, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_4908ee25f6cb4c6cbcf1429e78024153'}, {'index': 'NYA', 'date': '1967-01-05', 'open': 469.26001, 'high': 469.26001, 'low': 469.26001, 'close': 469.26001, 'adj_close': 469.26001, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_36aff27b24c84551939a81b41d5078b5'}, {'index': 'NYA', 'date': '1967-01-06', 'open': 472.75, 'high': 472.75, 'low': 472.75, 'close': 472.75, 'adj_close': 472.75, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_b0fa38527a934e6baa2ca2e154173da9'}, {'index': 'NYA', 'date': '1967-01-09', 'open': 476.660004, 'high': 476.660004, 'low': 476.660004, 'close': 476.660004, 'adj_close': 476.660004, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_ea6a640d367340c6915daf0fa656b1ca'}, {'index': 'NYA', 'date': '1967-01-10', 'open': 476.660004, 'high': 476.660004, 'low': 476.660004, 'close': 476.660004, 'adj_close': 476.660004, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_d1ccdefd919f479eb3e676a16263f85d'}, {'index': 'NYA', 'date': '1967-01-11', 'open': 480.579987, 'high': 480.579987, 'low': 480.579987, 'close': 480.579987, 'adj_close': 480.579987, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_55f7b42365f34d20b24d762c8fd9fa79'}, {'index': 'NYA', 'date': '1967-01-12', 'open': 483.429993, 'high': 483.429993, 'low': 483.429993, 'close': 483.429993, 'adj_close': 483.429993, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_2495360e59a44e17b527592f525cedc2'}, {'index': 'NYA', 'date': '1967-01-13', 'open': 486.809998, 'high': 486.809998, 'low': 486.809998, 'close': 486.809998, 'adj_close': 486.809998, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_c4b69088c4274358ab51d8fb6ec37a38'}, {'index': 'NYA', 'date': '1967-01-16', 'open': 485.970001, 'high': 485.970001, 'low': 485.970001, 'close': 485.970001, 'adj_close': 485.970001, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_19902d23c7bf42a8a6cf80a469228aa8'}, {'index': 'NYA', 'date': '1967-01-17', 'open': 491.26001, 'high': 491.26001, 'low': 491.26001, 'close': 491.26001, 'adj_close': 491.26001, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_a338832557304a0082d653d53e52e44b'}, {'index': 'NYA', 'date': '1967-01-18', 'open': 494.0, 'high': 494.0, 'low': 494.0, 'close': 494.0, 'adj_close': 494.0, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_1a66efbf81c147bba5efec461328f6f2'}, {'index': 'NYA', 'date': '1967-01-19', 'open': 494.220001, 'high': 494.220001, 'low': 494.220001, 'close': 494.220001, 'adj_close': 494.220001, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_7291ec3968bd4daa9c9ae21c6c817896'}, {'index': 'NYA', 'date': '1967-01-20', 'open': 496.01001, 'high': 496.01001, 'low': 496.01001, 'close': 496.01001, 'adj_close': 496.01001, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_188ebed56d5742e3a9bebb820513278c'}, {'index': 'NYA', 'date': '1967-01-23', 'open': 497.809998, 'high': 497.809998, 'low': 497.809998, 'close': 497.809998, 'adj_close': 497.809998, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_e676d68d055a4d3a9151ebc6668d2ee3'}, {'index': 'NYA', 'date': '1967-01-24', 'open': 498.76001, 'high': 498.76001, 'low': 498.76001, 'close': 498.76001, 'adj_close': 498.76001, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_c1c6a375d4ce42afa982e620c9a4ec3b'}, {'index': 'NYA', 'date': '1967-01-25', 'open': 495.380005, 'high': 495.380005, 'low': 495.380005, 'close': 495.380005, 'adj_close': 495.380005, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_9351ed7bb23842abb45b91e828c55e2a'}, {'index': 'NYA', 'date': '1967-01-26', 'open': 495.700012, 'high': 495.700012, 'low': 495.700012, 'close': 495.700012, 'adj_close': 495.700012, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_cc260c71c62943e9bb7f4576f2f719c7'}, {'index': 'NYA', 'date': '1967-01-27', 'open': 497.809998, 'high': 497.809998, 'low': 497.809998, 'close': 497.809998, 'adj_close': 497.809998, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_a5d780fcdb3048249d018f26ec7476a8'}, {'index': 'NYA', 'date': '1967-01-30', 'open': 500.670013, 'high': 500.670013, 'low': 500.670013, 'close': 500.670013, 'adj_close': 500.670013, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_57aea50b6cb5413dbac7f0c09883e24f'}, {'index': 'NYA', 'date': '1967-01-31', 'open': 500.140015, 'high': 500.140015, 'low': 500.140015, 'close': 500.140015, 'adj_close': 500.140015, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_2c76c472ce8d42098d837b9e38d03b45'}, {'index': 'NYA', 'date': '1967-02-01', 'open': 499.5, 'high': 499.5, 'low': 499.5, 'close': 499.5, 'adj_close': 499.5, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_6c5630d97c9648edae86375a33464765'}, {'index': 'NYA', 'date': '1967-02-02', 'open': 501.089996, 'high': 501.089996, 'low': 501.089996, 'close': 501.089996, 'adj_close': 501.089996, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_286f4582c20a4da5aa5adf08a3cd6458'}, {'index': 'NYA', 'date': '1967-02-03', 'open': 504.679993, 'high': 504.679993, 'low': 504.679993, 'close': 504.679993, 'adj_close': 504.679993, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_2f842d7ddf084fc395b37eac60f3b27d'}, {'index': 'NYA', 'date': '1967-02-06', 'open': 503.730011, 'high': 503.730011, 'low': 503.730011, 'close': 503.730011, 'adj_close': 503.730011, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_eecbc521e9504d35b9edd65cb29ca668'}, {'index': 'NYA', 'date': '1967-02-07', 'open': 502.679993, 'high': 502.679993, 'low': 502.679993, 'close': 502.679993, 'adj_close': 502.679993, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_7c357f0bba9b40fe9a280123bca76883'}, {'index': 'NYA', 'date': '1967-02-08', 'open': 507.119995, 'high': 507.119995, 'low': 507.119995, 'close': 507.119995, 'adj_close': 507.119995, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_6f1672bf7ce94f1eb5453ee42f25d289'}, {'index': 'NYA', 'date': '1967-02-09', 'open': 505.109985, 'high': 505.109985, 'low': 505.109985, 'close': 505.109985, 'adj_close': 505.109985, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_d3676d3881b242f6ba37676b339da629'}, {'index': 'NYA', 'date': '1967-02-10', 'open': 506.160004, 'high': 506.160004, 'low': 506.160004, 'close': 506.160004, 'adj_close': 506.160004, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_b0d83942f30b43209b4cd35eda075a5a'}, {'index': 'NYA', 'date': '1967-02-13', 'open': 506.899994, 'high': 506.899994, 'low': 506.899994, 'close': 506.899994, 'adj_close': 506.899994, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_76237c709f0a47088dced65b3219c489'}, {'index': 'NYA', 'date': '1967-02-14', 'open': 510.290009, 'high': 510.290009, 'low': 510.290009, 'close': 510.290009, 'adj_close': 510.290009, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_fa9acffbd77d4d49bddb7aabd3832f1b'}, {'index': 'NYA', 'date': '1967-02-15', 'open': 510.609985, 'high': 510.609985, 'low': 510.609985, 'close': 510.609985, 'adj_close': 510.609985, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_37ad9f23e08d4c678fec82c84855338e'}, {'index': 'NYA', 'date': '1967-02-16', 'open': 508.380005, 'high': 508.380005, 'low': 508.380005, 'close': 508.380005, 'adj_close': 508.380005, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_03aeb54fef0a4c60a86b67f8be97d0c8'}, {'index': 'NYA', 'date': '1967-02-17', 'open': 508.48999, 'high': 508.48999, 'low': 508.48999, 'close': 508.48999, 'adj_close': 508.48999, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_67febd829f0a43a1a3932e3c2b8e255c'}, {'index': 'NYA', 'date': '1967-02-20', 'open': 505.850006, 'high': 505.850006, 'low': 505.850006, 'close': 505.850006, 'adj_close': 505.850006, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_6b55825311e54d048dfe9d6aaa37acfc'}, {'index': 'NYA', 'date': '1967-02-21', 'open': 505.850006, 'high': 505.850006, 'low': 505.850006, 'close': 505.850006, 'adj_close': 505.850006, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_6165260771b2448ab65e75ca2e7ad39a'}, {'index': 'NYA', 'date': '1967-02-23', '@type': 'IndexDataOp', '@id': 'IndexDataOp_a4d281ab44e143c8aef0a398cf615ec8'}, {'index': 'NYA', 'date': '1967-02-24', 'open': 506.480011, 'high': 506.480011, 'low': 506.480011, 'close': 506.480011, 'adj_close': 506.480011, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_340f944337294879b5803c7b592b1944'}, {'index': 'NYA', 'date': '1967-02-27', 'open': 501.089996, 'high': 501.089996, 'low': 501.089996, 'close': 501.089996, 'adj_close': 501.089996, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_4905bfca283b4e4c8fa8fb59bbfe10bc'}, {'index': 'NYA', 'date': '1967-02-28', 'open': 502.890015, 'high': 502.890015, 'low': 502.890015, 'close': 502.890015, 'adj_close': 502.890015, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_45ad14a648e24e998dd1506b26812d82'}, {'index': 'NYA', 'date': '1967-03-01', 'open': 507.859985, 'high': 507.859985, 'low': 507.859985, 'close': 507.859985, 'adj_close': 507.859985, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_169bd2d888f34fd7962b1f4b77311ddd'}, {'index': 'NYA', 'date': '1967-03-02', 'open': 510.709991, 'high': 510.709991, 'low': 510.709991, 'close': 510.709991, 'adj_close': 510.709991, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_567b3c6178ea45169c82c110311af628'}, {'index': 'NYA', 'date': '1967-03-03', 'open': 511.450012, 'high': 511.450012, 'low': 511.450012, 'close': 511.450012, 'adj_close': 511.450012, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_b901176582bc4124a8f4dfe85e3f6e4c'}, {'index': 'NYA', 'date': '1967-03-06', 'open': 510.709991, 'high': 510.709991, 'low': 510.709991, 'close': 510.709991, 'adj_close': 510.709991, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_9032e62b60854c0ca10de0c3faffd565'}, {'index': 'NYA', 'date': '1967-03-07', 'open': 511.350006, 'high': 511.350006, 'low': 511.350006, 'close': 511.350006, 'adj_close': 511.350006, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_76dc8e44f64540eebab6717166ca22a2'}, {'index': 'NYA', 'date': '1967-03-08', 'open': 512.090027, 'high': 512.090027, 'low': 512.090027, 'close': 512.090027, 'adj_close': 512.090027, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_1590c7129b8d4e96adf3fad0c25f7741'}, {'index': 'NYA', 'date': '1967-03-09', 'open': 513.570007, 'high': 513.570007, 'low': 513.570007, 'close': 513.570007, 'adj_close': 513.570007, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_27866b6fcba04c32b490d5155849a043'}]

The schema is:
class IndexDataOp(DocumentTemplate):
    _key = RandomKey()
    adj_close: Optional[float]
    close: Optional[float]
    date: Optional[str]
    high: Optional[float]
    index: Optional[str]
    low: Optional[float]
    open: Optional[float]
    volume: Optional[int]

Sorry I don't have it in json",2021-08-26T11:28:12Z,matko,https://github.com/terminusdb/terminusdb/issues/472#issuecomment-906322335,"I think an error is to be expected, as you can't generally cast a float to an integer without losing precision (@GavinMendelGleason).
But this error is terrible. It should not return a stack trace but make that casting error into something more human readable.",i think an error be to be expect as -PRON- can not generally cast a float to an integer without lose precision ( @gavinmendelgleason ) but this error be terrible -PRON- should not return a stack trace but make that cast error into something more human readable,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,472,2021-08-26T10:39:15Z,Cheukting,Git a stack trace as an error,https://github.com/terminusdb/terminusdb/issues/472,"I got this stack trace:
terminusdb_client.errors.DatabaseError: Error: casting_error(0.0,'http://www.w3.org/2001/XMLSchema#integer')
  [46] throw(error(casting_error(0.0,'http://www.w3.org/2001/XMLSchema#integer'),_69862))
  [44] catch(api_document:do_or_die(...,...),error(casting_error(0.0,'http://www.w3.org/2001/XMLSchema#integer'),_69920),api_document:(...;...)) at /usr/lib/swipl/boot/init.pl:533
  [42] api_document:api_insert_document_(instance,transaction_object{commit_info:commit_info{},descriptor:branch_descriptor{branch_name:""main"",repository_descriptor: ...},inference_objects:[],instance_objects:[...],parent:transaction_object{descriptor: ...,inference_objects:[],instance_objects: ...,parent: ...,schema_objects: ...},schema_objects:[...]},<stream>(0x5614c9102b00),_69980) at /app/terminusdb/src/core/api/api_document.pl:185
  [41] '$bags':findall_loop(_70096,'<garbage_collected>',_70100,[]) at /usr/lib/swipl/boot/bags.pl:99
  [40] setup_call_catcher_cleanup('$bags':'$new_findall_bag','$bags':findall_loop(_70154,...,_70158,[]),_70136,'$bags':'$destroy_findall_bag') at /usr/lib/swipl/boot/init.pl:614
  [36] '<meta-call>'('<garbage_collected>') <foreign>
  [35] database:with_transaction_(query_context{all_witnesses:false,authorization:'terminusdb://system/data/admin',bindings:[],commit_info:commit_info{author:admin,message:'Document object inserted by Python client.'},default_collection:branch_descriptor{branch_name:""main"",repository_descriptor: ...},files:[],filter:type_filter{types: ...},prefixes:_70322{'@base':""terminusdb:///data/"",'@schema':""terminusdb:///schema#"",'@type':'Context',api:'http://terminusdb.com/schema/api#',owl:'http://www.w3.org/2002/07/owl#',rdf:'http://www.w3.org/1999/02/22-rdf-syntax-ns#',rdfs:'http://www.w3.org/2000/01/rdf-schema#',sys:'http://terminusdb.com/schema/sys#',vio:'http://terminusdb.com/schema/vio#',woql:'http://terminusdb.com/schema/woql#',xdd:'http://terminusdb.com/schema/xdd#',xsd:'http://www.w3.org/2001/XMLSchema#'},selected:[],system:system_descriptor{},transaction_objects:[...],update_guard:_70278,write_graph:branch_graph{branch_name:""main"",database_name:""stock_exchange"",organization_name:""Community"",repository_name:""local"",type:instance}},api_document:(...;...),_70240) at /app/terminusdb/src/core/transaction/database.pl:220
  [34] setup_call_catcher_cleanup(database:true,database:with_transaction_(...,...,_70482),_70460,database:true) at /usr/lib/swipl/boot/init.pl:614
  [30] '<meta-call>'('<garbage_collected>') <foreign>
  [29] catch(routes:(...,...),error(casting_error(0.0,'http://www.w3.org/2001/XMLSchema#integer'),context(_70584,_70586)),routes:do_or_die(...,...)) at /usr/lib/swipl/boot/init.pl:532
  [28] catch_with_backtrace('<garbage_collected>','<garbage_collected>','<garbage_collected>') at /usr/lib/swipl/boot/init.pl:582

While insert document with this:
[00:16,  8.41s/it][{'index': 'NYA', 'date': '1966-10-14', 'open': 435.73999, 'high': 435.73999, 'low': 435.73999, 'close': 435.73999, 'adj_close': 435.73999, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_ac69bef9dcc449f1b832955a0fb352d3'}, {'index': 'NYA', 'date': '1966-10-17', 'open': 440.609985, 'high': 440.609985, 'low': 440.609985, 'close': 440.609985, 'adj_close': 440.609985, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_c03f34b77c894c54a88231f1f3f019b4'}, {'index': 'NYA', 'date': '1966-10-18', 'open': 447.799988, 'high': 447.799988, 'low': 447.799988, 'close': 447.799988, 'adj_close': 447.799988, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_7e19232b70b44b2397821064430bab2a'}, {'index': 'NYA', 'date': '1966-10-19', 'open': 444.519989, 'high': 444.519989, 'low': 444.519989, 'close': 444.519989, 'adj_close': 444.519989, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_58b492ab7259449aacab228c5c9ff1c7'}, {'index': 'NYA', 'date': '1966-10-20', 'open': 443.25, 'high': 443.25, 'low': 443.25, 'close': 443.25, 'adj_close': 443.25, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_7714503df8f244afbb2533ff3d83a1a6'}, {'index': 'NYA', 'date': '1966-10-21', 'open': 444.839996, 'high': 444.839996, 'low': 444.839996, 'close': 444.839996, 'adj_close': 444.839996, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_c5e9bbb5f24a4edf8e74f28b1c3eff0e'}, {'index': 'NYA', 'date': '1966-10-24', 'open': 446.0, 'high': 446.0, 'low': 446.0, 'close': 446.0, 'adj_close': 446.0, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_41bfb1e914ad4a41b21c4e611a172079'}, {'index': 'NYA', 'date': '1966-10-25', 'open': 448.429993, 'high': 448.429993, 'low': 448.429993, 'close': 448.429993, 'adj_close': 448.429993, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_0f1318ae6c8a4b81ace6b97652e2a13b'}, {'index': 'NYA', 'date': '1966-10-26', 'open': 452.450012, 'high': 452.450012, 'low': 452.450012, 'close': 452.450012, 'adj_close': 452.450012, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_e4a1ec76f4c44f73b80e947f40f6d88d'}, {'index': 'NYA', 'date': '1966-10-27', 'open': 456.26001, 'high': 456.26001, 'low': 456.26001, 'close': 456.26001, 'adj_close': 456.26001, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_7de0599ee12948039b72634b0fef4533'}, {'index': 'NYA', 'date': '1966-10-28', 'open': 456.470001, 'high': 456.470001, 'low': 456.470001, 'close': 456.470001, 'adj_close': 456.470001, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_1629239709354f0abdb39e5c8594e541'}, {'index': 'NYA', 'date': '1966-10-31', 'open': 456.359985, 'high': 456.359985, 'low': 456.359985, 'close': 456.359985, 'adj_close': 456.359985, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_3fccf298aae441608e96ec5f27ac0a9a'}, {'index': 'NYA', 'date': '1966-11-01', 'open': 459.850006, 'high': 459.850006, 'low': 459.850006, 'close': 459.850006, 'adj_close': 459.850006, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_b4f774c9ee7c454282b4779d7dfcb769'}, {'index': 'NYA', 'date': '1966-11-02', 'open': 460.380005, 'high': 460.380005, 'low': 460.380005, 'close': 460.380005, 'adj_close': 460.380005, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_422de280413947349b3fc42e4daaed31'}, {'index': 'NYA', 'date': '1966-11-03', 'open': 458.790009, 'high': 458.790009, 'low': 458.790009, 'close': 458.790009, 'adj_close': 458.790009, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_d9f5370cdb994c46b627cd21133cc7e4'}, {'index': 'NYA', 'date': '1966-11-04', 'open': 460.269989, 'high': 460.269989, 'low': 460.269989, 'close': 460.269989, 'adj_close': 460.269989, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_7a4722f73e4542a5ba935cbe7f452f20'}, {'index': 'NYA', 'date': '1966-11-07', 'open': 460.170013, 'high': 460.170013, 'low': 460.170013, 'close': 460.170013, 'adj_close': 460.170013, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_2f29143c293443beafd861482958cdc4'}, {'index': 'NYA', 'date': '1966-11-09', 'open': 463.76001, 'high': 463.76001, 'low': 463.76001, 'close': 463.76001, 'adj_close': 463.76001, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_598c12211f5944a2ad97702e3d5a76ff'}, {'index': 'NYA', 'date': '1966-11-10', 'open': 466.829987, 'high': 466.829987, 'low': 466.829987, 'close': 466.829987, 'adj_close': 466.829987, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_b90bc27f854b4553b81780ef05985306'}, {'index': 'NYA', 'date': '1966-11-11', 'open': 467.25, 'high': 467.25, 'low': 467.25, 'close': 467.25, 'adj_close': 467.25, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_d7bf9c68bb604c309541d1f5e45f4e6c'}, {'index': 'NYA', 'date': '1966-11-14', 'open': 464.399994, 'high': 464.399994, 'low': 464.399994, 'close': 464.399994, 'adj_close': 464.399994, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_27c0c637dba3478abc8e06aeb2f1de13'}, {'index': 'NYA', 'date': '1966-11-15', 'open': 466.089996, 'high': 466.089996, 'low': 466.089996, 'close': 466.089996, 'adj_close': 466.089996, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_7e229ec3335f48e5af5b0babf6202674'}, {'index': 'NYA', 'date': '1966-11-16', 'open': 470.109985, 'high': 470.109985, 'low': 470.109985, 'close': 470.109985, 'adj_close': 470.109985, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_196ee5c117c94196a6c86d955ac1825b'}, {'index': 'NYA', 'date': '1966-11-17', 'open': 466.940002, 'high': 466.940002, 'low': 466.940002, 'close': 466.940002, 'adj_close': 466.940002, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_47104cbdfa9c42b0a82885e1fadb691a'}, {'index': 'NYA', 'date': '1966-11-18', 'open': 464.079987, 'high': 464.079987, 'low': 464.079987, 'close': 464.079987, 'adj_close': 464.079987, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_992a7fbdc05341ec9a2cb849893bfedd'}, {'index': 'NYA', 'date': '1966-11-21', 'open': 457.630005, 'high': 457.630005, 'low': 457.630005, 'close': 457.630005, 'adj_close': 457.630005, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_9eebb4f30617495d9b13f72db8039c55'}, {'index': 'NYA', 'date': '1966-11-22', 'open': 455.519989, 'high': 455.519989, 'low': 455.519989, 'close': 455.519989, 'adj_close': 455.519989, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_6befb11eb46a4d8f8cdf5e1f14e0bd7d'}, {'index': 'NYA', 'date': '1966-11-23', 'open': 458.899994, 'high': 458.899994, 'low': 458.899994, 'close': 458.899994, 'adj_close': 458.899994, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_f436b7462fc94d39ac24c1186565ed33'}, {'index': 'NYA', 'date': '1966-11-25', 'open': 462.600006, 'high': 462.600006, 'low': 462.600006, 'close': 462.600006, 'adj_close': 462.600006, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_3ed857eeaaf04c9c8ed5520e4b683d44'}, {'index': 'NYA', 'date': '1966-11-28', 'open': 462.179993, 'high': 462.179993, 'low': 462.179993, 'close': 462.179993, 'adj_close': 462.179993, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_be6cd17a1acc4c99be615447a4cd9a72'}, {'index': 'NYA', 'date': '1966-11-29', 'open': 460.589996, 'high': 460.589996, 'low': 460.589996, 'close': 460.589996, 'adj_close': 460.589996, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_96627271d0a847819ff3f493cb537612'}, {'index': 'NYA', 'date': '1966-11-30', 'open': 460.910004, 'high': 460.910004, 'low': 460.910004, 'close': 460.910004, 'adj_close': 460.910004, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_09ff579a912d4804a8a128fbfd608a63'}, {'index': 'NYA', 'date': '1966-12-01', 'open': 459.109985, 'high': 459.109985, 'low': 459.109985, 'close': 459.109985, 'adj_close': 459.109985, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_13f58d6daaee463b80164c1e9d8603cd'}, {'index': 'NYA', 'date': '1966-12-02', 'open': 459.320007, 'high': 459.320007, 'low': 459.320007, 'close': 459.320007, 'adj_close': 459.320007, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_bad15ef435da4d0088c690a26d2789b9'}, {'index': 'NYA', 'date': '1966-12-05', 'open': 459.75, 'high': 459.75, 'low': 459.75, 'close': 459.75, 'adj_close': 459.75, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_b87b7ea4cf6a4d3f93803f427665a7ed'}, {'index': 'NYA', 'date': '1966-12-06', 'open': 462.920013, 'high': 462.920013, 'low': 462.920013, 'close': 462.920013, 'adj_close': 462.920013, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_e2c359288ab140fbb7fe3389c4d9cf48'}, {'index': 'NYA', 'date': '1966-12-07', 'open': 467.570007, 'high': 467.570007, 'low': 467.570007, 'close': 467.570007, 'adj_close': 467.570007, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_cc7f06ae482146dcbbc12d44ef668a04'}, {'index': 'NYA', 'date': '1966-12-08', 'open': 469.470001, 'high': 469.470001, 'low': 469.470001, 'close': 469.470001, 'adj_close': 469.470001, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_4eaff3fb22014b378026b5fa18998d5f'}, {'index': 'NYA', 'date': '1966-12-09', 'open': 470.529999, 'high': 470.529999, 'low': 470.529999, 'close': 470.529999, 'adj_close': 470.529999, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_cdac45a2debd4cbebda0f6193b77a6fa'}, {'index': 'NYA', 'date': '1966-12-12', 'open': 475.079987, 'high': 475.079987, 'low': 475.079987, 'close': 475.079987, 'adj_close': 475.079987, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_48bbdba89bc046f78cd4852e8ce8af62'}, {'index': 'NYA', 'date': '1966-12-13', 'open': 473.809998, 'high': 473.809998, 'low': 473.809998, 'close': 473.809998, 'adj_close': 473.809998, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_8b09bfc77ba24dd2b449c8c4a1ad9e23'}, {'index': 'NYA', 'date': '1966-12-14', 'open': 473.700012, 'high': 473.700012, 'low': 473.700012, 'close': 473.700012, 'adj_close': 473.700012, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_b842ca919652455ba6640b96afe9a7da'}, {'index': 'NYA', 'date': '1966-12-15', 'open': 468.730011, 'high': 468.730011, 'low': 468.730011, 'close': 468.730011, 'adj_close': 468.730011, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_a1ea9fe9f02b486abea545b0ddedc802'}, {'index': 'NYA', 'date': '1966-12-16', 'open': 468.839996, 'high': 468.839996, 'low': 468.839996, 'close': 468.839996, 'adj_close': 468.839996, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_4fdc373cc99a4ea0b65e187f5310f5f7'}, {'index': 'NYA', 'date': '1966-12-19', 'open': 467.570007, 'high': 467.570007, 'low': 467.570007, 'close': 467.570007, 'adj_close': 467.570007, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_69387d1fef65455c9078fafc6d3bc003'}, {'index': 'NYA', 'date': '1966-12-20', 'open': 465.670013, 'high': 465.670013, 'low': 465.670013, 'close': 465.670013, 'adj_close': 465.670013, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_3a9c802396b24a8fa0c6fabfe101b9c8'}, {'index': 'NYA', 'date': '1966-12-21', 'open': 468.200012, 'high': 468.200012, 'low': 468.200012, 'close': 468.200012, 'adj_close': 468.200012, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_763800a0ba9d4b55aefdb61c7ade78c9'}, {'index': 'NYA', 'date': '1966-12-22', 'open': 470.109985, 'high': 470.109985, 'low': 470.109985, 'close': 470.109985, 'adj_close': 470.109985, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_a003681eb1114b468c7614429bad2cd8'}, {'index': 'NYA', 'date': '1966-12-23', 'open': 468.940002, 'high': 468.940002, 'low': 468.940002, 'close': 468.940002, 'adj_close': 468.940002, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_756f96892356400e9a5435d2c75d0f14'}, {'index': 'NYA', 'date': '1966-12-27', 'open': 466.51001, 'high': 466.51001, 'low': 466.51001, 'close': 466.51001, 'adj_close': 466.51001, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_e862ffdded764179aa1796c6146a248d'}, {'index': 'NYA', 'date': '1966-12-28', 'open': 464.190002, 'high': 464.190002, 'low': 464.190002, 'close': 464.190002, 'adj_close': 464.190002, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_62f9fe7072fc43518c3259e8e41c1102'}, {'index': 'NYA', 'date': '1966-12-29', 'open': 462.48999, 'high': 462.48999, 'low': 462.48999, 'close': 462.48999, 'adj_close': 462.48999, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_e45a7d3a48ab4d7298c9f30547a55189'}, {'index': 'NYA', 'date': '1966-12-30', 'open': 462.279999, 'high': 462.279999, 'low': 462.279999, 'close': 462.279999, 'adj_close': 462.279999, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_5c0cc58c00dd4e72b11d42d8bb52aa55'}, {'index': 'NYA', 'date': '1967-01-03', 'open': 462.48999, 'high': 462.48999, 'low': 462.48999, 'close': 462.48999, 'adj_close': 462.48999, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_b8224579041542a5b588d23bd725561b'}, {'index': 'NYA', 'date': '1967-01-04', 'open': 463.339996, 'high': 463.339996, 'low': 463.339996, 'close': 463.339996, 'adj_close': 463.339996, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_4908ee25f6cb4c6cbcf1429e78024153'}, {'index': 'NYA', 'date': '1967-01-05', 'open': 469.26001, 'high': 469.26001, 'low': 469.26001, 'close': 469.26001, 'adj_close': 469.26001, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_36aff27b24c84551939a81b41d5078b5'}, {'index': 'NYA', 'date': '1967-01-06', 'open': 472.75, 'high': 472.75, 'low': 472.75, 'close': 472.75, 'adj_close': 472.75, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_b0fa38527a934e6baa2ca2e154173da9'}, {'index': 'NYA', 'date': '1967-01-09', 'open': 476.660004, 'high': 476.660004, 'low': 476.660004, 'close': 476.660004, 'adj_close': 476.660004, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_ea6a640d367340c6915daf0fa656b1ca'}, {'index': 'NYA', 'date': '1967-01-10', 'open': 476.660004, 'high': 476.660004, 'low': 476.660004, 'close': 476.660004, 'adj_close': 476.660004, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_d1ccdefd919f479eb3e676a16263f85d'}, {'index': 'NYA', 'date': '1967-01-11', 'open': 480.579987, 'high': 480.579987, 'low': 480.579987, 'close': 480.579987, 'adj_close': 480.579987, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_55f7b42365f34d20b24d762c8fd9fa79'}, {'index': 'NYA', 'date': '1967-01-12', 'open': 483.429993, 'high': 483.429993, 'low': 483.429993, 'close': 483.429993, 'adj_close': 483.429993, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_2495360e59a44e17b527592f525cedc2'}, {'index': 'NYA', 'date': '1967-01-13', 'open': 486.809998, 'high': 486.809998, 'low': 486.809998, 'close': 486.809998, 'adj_close': 486.809998, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_c4b69088c4274358ab51d8fb6ec37a38'}, {'index': 'NYA', 'date': '1967-01-16', 'open': 485.970001, 'high': 485.970001, 'low': 485.970001, 'close': 485.970001, 'adj_close': 485.970001, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_19902d23c7bf42a8a6cf80a469228aa8'}, {'index': 'NYA', 'date': '1967-01-17', 'open': 491.26001, 'high': 491.26001, 'low': 491.26001, 'close': 491.26001, 'adj_close': 491.26001, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_a338832557304a0082d653d53e52e44b'}, {'index': 'NYA', 'date': '1967-01-18', 'open': 494.0, 'high': 494.0, 'low': 494.0, 'close': 494.0, 'adj_close': 494.0, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_1a66efbf81c147bba5efec461328f6f2'}, {'index': 'NYA', 'date': '1967-01-19', 'open': 494.220001, 'high': 494.220001, 'low': 494.220001, 'close': 494.220001, 'adj_close': 494.220001, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_7291ec3968bd4daa9c9ae21c6c817896'}, {'index': 'NYA', 'date': '1967-01-20', 'open': 496.01001, 'high': 496.01001, 'low': 496.01001, 'close': 496.01001, 'adj_close': 496.01001, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_188ebed56d5742e3a9bebb820513278c'}, {'index': 'NYA', 'date': '1967-01-23', 'open': 497.809998, 'high': 497.809998, 'low': 497.809998, 'close': 497.809998, 'adj_close': 497.809998, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_e676d68d055a4d3a9151ebc6668d2ee3'}, {'index': 'NYA', 'date': '1967-01-24', 'open': 498.76001, 'high': 498.76001, 'low': 498.76001, 'close': 498.76001, 'adj_close': 498.76001, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_c1c6a375d4ce42afa982e620c9a4ec3b'}, {'index': 'NYA', 'date': '1967-01-25', 'open': 495.380005, 'high': 495.380005, 'low': 495.380005, 'close': 495.380005, 'adj_close': 495.380005, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_9351ed7bb23842abb45b91e828c55e2a'}, {'index': 'NYA', 'date': '1967-01-26', 'open': 495.700012, 'high': 495.700012, 'low': 495.700012, 'close': 495.700012, 'adj_close': 495.700012, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_cc260c71c62943e9bb7f4576f2f719c7'}, {'index': 'NYA', 'date': '1967-01-27', 'open': 497.809998, 'high': 497.809998, 'low': 497.809998, 'close': 497.809998, 'adj_close': 497.809998, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_a5d780fcdb3048249d018f26ec7476a8'}, {'index': 'NYA', 'date': '1967-01-30', 'open': 500.670013, 'high': 500.670013, 'low': 500.670013, 'close': 500.670013, 'adj_close': 500.670013, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_57aea50b6cb5413dbac7f0c09883e24f'}, {'index': 'NYA', 'date': '1967-01-31', 'open': 500.140015, 'high': 500.140015, 'low': 500.140015, 'close': 500.140015, 'adj_close': 500.140015, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_2c76c472ce8d42098d837b9e38d03b45'}, {'index': 'NYA', 'date': '1967-02-01', 'open': 499.5, 'high': 499.5, 'low': 499.5, 'close': 499.5, 'adj_close': 499.5, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_6c5630d97c9648edae86375a33464765'}, {'index': 'NYA', 'date': '1967-02-02', 'open': 501.089996, 'high': 501.089996, 'low': 501.089996, 'close': 501.089996, 'adj_close': 501.089996, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_286f4582c20a4da5aa5adf08a3cd6458'}, {'index': 'NYA', 'date': '1967-02-03', 'open': 504.679993, 'high': 504.679993, 'low': 504.679993, 'close': 504.679993, 'adj_close': 504.679993, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_2f842d7ddf084fc395b37eac60f3b27d'}, {'index': 'NYA', 'date': '1967-02-06', 'open': 503.730011, 'high': 503.730011, 'low': 503.730011, 'close': 503.730011, 'adj_close': 503.730011, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_eecbc521e9504d35b9edd65cb29ca668'}, {'index': 'NYA', 'date': '1967-02-07', 'open': 502.679993, 'high': 502.679993, 'low': 502.679993, 'close': 502.679993, 'adj_close': 502.679993, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_7c357f0bba9b40fe9a280123bca76883'}, {'index': 'NYA', 'date': '1967-02-08', 'open': 507.119995, 'high': 507.119995, 'low': 507.119995, 'close': 507.119995, 'adj_close': 507.119995, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_6f1672bf7ce94f1eb5453ee42f25d289'}, {'index': 'NYA', 'date': '1967-02-09', 'open': 505.109985, 'high': 505.109985, 'low': 505.109985, 'close': 505.109985, 'adj_close': 505.109985, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_d3676d3881b242f6ba37676b339da629'}, {'index': 'NYA', 'date': '1967-02-10', 'open': 506.160004, 'high': 506.160004, 'low': 506.160004, 'close': 506.160004, 'adj_close': 506.160004, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_b0d83942f30b43209b4cd35eda075a5a'}, {'index': 'NYA', 'date': '1967-02-13', 'open': 506.899994, 'high': 506.899994, 'low': 506.899994, 'close': 506.899994, 'adj_close': 506.899994, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_76237c709f0a47088dced65b3219c489'}, {'index': 'NYA', 'date': '1967-02-14', 'open': 510.290009, 'high': 510.290009, 'low': 510.290009, 'close': 510.290009, 'adj_close': 510.290009, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_fa9acffbd77d4d49bddb7aabd3832f1b'}, {'index': 'NYA', 'date': '1967-02-15', 'open': 510.609985, 'high': 510.609985, 'low': 510.609985, 'close': 510.609985, 'adj_close': 510.609985, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_37ad9f23e08d4c678fec82c84855338e'}, {'index': 'NYA', 'date': '1967-02-16', 'open': 508.380005, 'high': 508.380005, 'low': 508.380005, 'close': 508.380005, 'adj_close': 508.380005, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_03aeb54fef0a4c60a86b67f8be97d0c8'}, {'index': 'NYA', 'date': '1967-02-17', 'open': 508.48999, 'high': 508.48999, 'low': 508.48999, 'close': 508.48999, 'adj_close': 508.48999, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_67febd829f0a43a1a3932e3c2b8e255c'}, {'index': 'NYA', 'date': '1967-02-20', 'open': 505.850006, 'high': 505.850006, 'low': 505.850006, 'close': 505.850006, 'adj_close': 505.850006, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_6b55825311e54d048dfe9d6aaa37acfc'}, {'index': 'NYA', 'date': '1967-02-21', 'open': 505.850006, 'high': 505.850006, 'low': 505.850006, 'close': 505.850006, 'adj_close': 505.850006, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_6165260771b2448ab65e75ca2e7ad39a'}, {'index': 'NYA', 'date': '1967-02-23', '@type': 'IndexDataOp', '@id': 'IndexDataOp_a4d281ab44e143c8aef0a398cf615ec8'}, {'index': 'NYA', 'date': '1967-02-24', 'open': 506.480011, 'high': 506.480011, 'low': 506.480011, 'close': 506.480011, 'adj_close': 506.480011, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_340f944337294879b5803c7b592b1944'}, {'index': 'NYA', 'date': '1967-02-27', 'open': 501.089996, 'high': 501.089996, 'low': 501.089996, 'close': 501.089996, 'adj_close': 501.089996, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_4905bfca283b4e4c8fa8fb59bbfe10bc'}, {'index': 'NYA', 'date': '1967-02-28', 'open': 502.890015, 'high': 502.890015, 'low': 502.890015, 'close': 502.890015, 'adj_close': 502.890015, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_45ad14a648e24e998dd1506b26812d82'}, {'index': 'NYA', 'date': '1967-03-01', 'open': 507.859985, 'high': 507.859985, 'low': 507.859985, 'close': 507.859985, 'adj_close': 507.859985, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_169bd2d888f34fd7962b1f4b77311ddd'}, {'index': 'NYA', 'date': '1967-03-02', 'open': 510.709991, 'high': 510.709991, 'low': 510.709991, 'close': 510.709991, 'adj_close': 510.709991, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_567b3c6178ea45169c82c110311af628'}, {'index': 'NYA', 'date': '1967-03-03', 'open': 511.450012, 'high': 511.450012, 'low': 511.450012, 'close': 511.450012, 'adj_close': 511.450012, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_b901176582bc4124a8f4dfe85e3f6e4c'}, {'index': 'NYA', 'date': '1967-03-06', 'open': 510.709991, 'high': 510.709991, 'low': 510.709991, 'close': 510.709991, 'adj_close': 510.709991, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_9032e62b60854c0ca10de0c3faffd565'}, {'index': 'NYA', 'date': '1967-03-07', 'open': 511.350006, 'high': 511.350006, 'low': 511.350006, 'close': 511.350006, 'adj_close': 511.350006, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_76dc8e44f64540eebab6717166ca22a2'}, {'index': 'NYA', 'date': '1967-03-08', 'open': 512.090027, 'high': 512.090027, 'low': 512.090027, 'close': 512.090027, 'adj_close': 512.090027, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_1590c7129b8d4e96adf3fad0c25f7741'}, {'index': 'NYA', 'date': '1967-03-09', 'open': 513.570007, 'high': 513.570007, 'low': 513.570007, 'close': 513.570007, 'adj_close': 513.570007, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_27866b6fcba04c32b490d5155849a043'}]

The schema is:
class IndexDataOp(DocumentTemplate):
    _key = RandomKey()
    adj_close: Optional[float]
    close: Optional[float]
    date: Optional[str]
    high: Optional[float]
    index: Optional[str]
    low: Optional[float]
    open: Optional[float]
    volume: Optional[int]

Sorry I don't have it in json",2021-08-31T08:37:43Z,spl,https://github.com/terminusdb/terminusdb/issues/472#issuecomment-909025641,"Here's a script with a much-simplified schema:
#!/bin/bash

set -ex

# Create the database.
xh 'http://admin:root@localhost:6363/api/db/admin/t' <<EOF
{""label"":""l"",""comment"":""c""}
EOF

# Create the schema.
xh 'http://admin:root@localhost:6363/api/document/admin/t?author=a&message=m&graph_type=schema' <<EOF
{
  ""@type"": ""Class"",
  ""@id"": ""OptionalInt"",
  ""integer"": {
    ""@class"": ""xsd:integer"",
    ""@type"": ""Optional""
  }
}
EOF

# Post the instance with 0.0 for an integer-typed field.
xh 'http://admin:root@localhost:6363/api/document/admin/t?author=a&message=m' <<EOF
{
  ""@id"": ""OptionalIntInstance"",
  ""@type"": ""OptionalInt"",
  ""integer"": 0.0
}
EOF
Here's the last HTTP response:
HTTP/1.1 400 Bad Request
connection: keep-alive
content-length: 415
content-type: application/json
date: Tue, 31 Aug 2021 08:31:15 GMT

{
    ""@type"": ""api:InsertDocumentErrorResponse"",
    ""api:error"": {
        ""@type"": ""api:BadCast"",
        ""api:document"": {
            ""@id"": ""OptionalIntInstance"",
            ""@type"": ""OptionalInt"",
            ""integer"": 0.0
        },
        ""api:type"": ""http://www.w3.org/2001/XMLSchema#integer"",
        ""api:value"": 0.0
    },
    ""api:message"": ""value 0.0 could not be casted to a 'http://www.w3.org/2001/XMLSchema#integer'"",
    ""api:status"": ""api:failure""
}
I think this is what you'd expect to see, @matko? I'm not sure why @Cheukting gets the above response.","here be a script with a much - simplify schema # /bin / bash set -ex # create the database xh ' http//adminroot@localhost6363 / api / db / admin / t ' < < eof { "" label""""l""""comment""""c "" } eof # create the schema xh ' http//adminroot@localhost6363 / api / document / admin / tauthor = a&message = m&graph_type = schema ' < < eof { "" @type "" "" class "" "" @id "" "" optionalint "" "" integer "" { "" @class "" "" xsdinteger "" "" @type "" "" optional "" } } eof # post the instance with 00 for an integer - type field xh ' http//adminroot@localhost6363 / api / document / admin / tauthor = a&message = m ' < < eof { "" @id "" "" optionalintinstance "" "" @type "" "" optionalint "" "" integer "" 00 } eof here be the last http response http/11 400 bad request connection keep - alive content - length 415 content - type application / json date tue 31 aug 2021 083115 gmt { "" @type "" "" apiinsertdocumenterrorresponse "" "" apierror "" { "" @type "" "" apibadcast "" "" apidocument "" { "" @id "" "" optionalintinstance "" "" @type "" "" optionalint "" "" integer "" 00 } "" apitype "" "" http//wwww3org/2001 / xmlschema#integer "" "" apivalue "" 00 } "" apimessage "" "" value 00 could not be cast to a ' http//wwww3org/2001 / xmlschema#integer ' "" "" apistatus "" "" apifailure "" } i think this be what -PRON- 'd expect to see @matko -PRON- be not sure why @cheukte get the above response",0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,472,2021-08-26T10:39:15Z,Cheukting,Git a stack trace as an error,https://github.com/terminusdb/terminusdb/issues/472,"I got this stack trace:
terminusdb_client.errors.DatabaseError: Error: casting_error(0.0,'http://www.w3.org/2001/XMLSchema#integer')
  [46] throw(error(casting_error(0.0,'http://www.w3.org/2001/XMLSchema#integer'),_69862))
  [44] catch(api_document:do_or_die(...,...),error(casting_error(0.0,'http://www.w3.org/2001/XMLSchema#integer'),_69920),api_document:(...;...)) at /usr/lib/swipl/boot/init.pl:533
  [42] api_document:api_insert_document_(instance,transaction_object{commit_info:commit_info{},descriptor:branch_descriptor{branch_name:""main"",repository_descriptor: ...},inference_objects:[],instance_objects:[...],parent:transaction_object{descriptor: ...,inference_objects:[],instance_objects: ...,parent: ...,schema_objects: ...},schema_objects:[...]},<stream>(0x5614c9102b00),_69980) at /app/terminusdb/src/core/api/api_document.pl:185
  [41] '$bags':findall_loop(_70096,'<garbage_collected>',_70100,[]) at /usr/lib/swipl/boot/bags.pl:99
  [40] setup_call_catcher_cleanup('$bags':'$new_findall_bag','$bags':findall_loop(_70154,...,_70158,[]),_70136,'$bags':'$destroy_findall_bag') at /usr/lib/swipl/boot/init.pl:614
  [36] '<meta-call>'('<garbage_collected>') <foreign>
  [35] database:with_transaction_(query_context{all_witnesses:false,authorization:'terminusdb://system/data/admin',bindings:[],commit_info:commit_info{author:admin,message:'Document object inserted by Python client.'},default_collection:branch_descriptor{branch_name:""main"",repository_descriptor: ...},files:[],filter:type_filter{types: ...},prefixes:_70322{'@base':""terminusdb:///data/"",'@schema':""terminusdb:///schema#"",'@type':'Context',api:'http://terminusdb.com/schema/api#',owl:'http://www.w3.org/2002/07/owl#',rdf:'http://www.w3.org/1999/02/22-rdf-syntax-ns#',rdfs:'http://www.w3.org/2000/01/rdf-schema#',sys:'http://terminusdb.com/schema/sys#',vio:'http://terminusdb.com/schema/vio#',woql:'http://terminusdb.com/schema/woql#',xdd:'http://terminusdb.com/schema/xdd#',xsd:'http://www.w3.org/2001/XMLSchema#'},selected:[],system:system_descriptor{},transaction_objects:[...],update_guard:_70278,write_graph:branch_graph{branch_name:""main"",database_name:""stock_exchange"",organization_name:""Community"",repository_name:""local"",type:instance}},api_document:(...;...),_70240) at /app/terminusdb/src/core/transaction/database.pl:220
  [34] setup_call_catcher_cleanup(database:true,database:with_transaction_(...,...,_70482),_70460,database:true) at /usr/lib/swipl/boot/init.pl:614
  [30] '<meta-call>'('<garbage_collected>') <foreign>
  [29] catch(routes:(...,...),error(casting_error(0.0,'http://www.w3.org/2001/XMLSchema#integer'),context(_70584,_70586)),routes:do_or_die(...,...)) at /usr/lib/swipl/boot/init.pl:532
  [28] catch_with_backtrace('<garbage_collected>','<garbage_collected>','<garbage_collected>') at /usr/lib/swipl/boot/init.pl:582

While insert document with this:
[00:16,  8.41s/it][{'index': 'NYA', 'date': '1966-10-14', 'open': 435.73999, 'high': 435.73999, 'low': 435.73999, 'close': 435.73999, 'adj_close': 435.73999, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_ac69bef9dcc449f1b832955a0fb352d3'}, {'index': 'NYA', 'date': '1966-10-17', 'open': 440.609985, 'high': 440.609985, 'low': 440.609985, 'close': 440.609985, 'adj_close': 440.609985, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_c03f34b77c894c54a88231f1f3f019b4'}, {'index': 'NYA', 'date': '1966-10-18', 'open': 447.799988, 'high': 447.799988, 'low': 447.799988, 'close': 447.799988, 'adj_close': 447.799988, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_7e19232b70b44b2397821064430bab2a'}, {'index': 'NYA', 'date': '1966-10-19', 'open': 444.519989, 'high': 444.519989, 'low': 444.519989, 'close': 444.519989, 'adj_close': 444.519989, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_58b492ab7259449aacab228c5c9ff1c7'}, {'index': 'NYA', 'date': '1966-10-20', 'open': 443.25, 'high': 443.25, 'low': 443.25, 'close': 443.25, 'adj_close': 443.25, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_7714503df8f244afbb2533ff3d83a1a6'}, {'index': 'NYA', 'date': '1966-10-21', 'open': 444.839996, 'high': 444.839996, 'low': 444.839996, 'close': 444.839996, 'adj_close': 444.839996, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_c5e9bbb5f24a4edf8e74f28b1c3eff0e'}, {'index': 'NYA', 'date': '1966-10-24', 'open': 446.0, 'high': 446.0, 'low': 446.0, 'close': 446.0, 'adj_close': 446.0, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_41bfb1e914ad4a41b21c4e611a172079'}, {'index': 'NYA', 'date': '1966-10-25', 'open': 448.429993, 'high': 448.429993, 'low': 448.429993, 'close': 448.429993, 'adj_close': 448.429993, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_0f1318ae6c8a4b81ace6b97652e2a13b'}, {'index': 'NYA', 'date': '1966-10-26', 'open': 452.450012, 'high': 452.450012, 'low': 452.450012, 'close': 452.450012, 'adj_close': 452.450012, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_e4a1ec76f4c44f73b80e947f40f6d88d'}, {'index': 'NYA', 'date': '1966-10-27', 'open': 456.26001, 'high': 456.26001, 'low': 456.26001, 'close': 456.26001, 'adj_close': 456.26001, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_7de0599ee12948039b72634b0fef4533'}, {'index': 'NYA', 'date': '1966-10-28', 'open': 456.470001, 'high': 456.470001, 'low': 456.470001, 'close': 456.470001, 'adj_close': 456.470001, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_1629239709354f0abdb39e5c8594e541'}, {'index': 'NYA', 'date': '1966-10-31', 'open': 456.359985, 'high': 456.359985, 'low': 456.359985, 'close': 456.359985, 'adj_close': 456.359985, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_3fccf298aae441608e96ec5f27ac0a9a'}, {'index': 'NYA', 'date': '1966-11-01', 'open': 459.850006, 'high': 459.850006, 'low': 459.850006, 'close': 459.850006, 'adj_close': 459.850006, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_b4f774c9ee7c454282b4779d7dfcb769'}, {'index': 'NYA', 'date': '1966-11-02', 'open': 460.380005, 'high': 460.380005, 'low': 460.380005, 'close': 460.380005, 'adj_close': 460.380005, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_422de280413947349b3fc42e4daaed31'}, {'index': 'NYA', 'date': '1966-11-03', 'open': 458.790009, 'high': 458.790009, 'low': 458.790009, 'close': 458.790009, 'adj_close': 458.790009, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_d9f5370cdb994c46b627cd21133cc7e4'}, {'index': 'NYA', 'date': '1966-11-04', 'open': 460.269989, 'high': 460.269989, 'low': 460.269989, 'close': 460.269989, 'adj_close': 460.269989, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_7a4722f73e4542a5ba935cbe7f452f20'}, {'index': 'NYA', 'date': '1966-11-07', 'open': 460.170013, 'high': 460.170013, 'low': 460.170013, 'close': 460.170013, 'adj_close': 460.170013, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_2f29143c293443beafd861482958cdc4'}, {'index': 'NYA', 'date': '1966-11-09', 'open': 463.76001, 'high': 463.76001, 'low': 463.76001, 'close': 463.76001, 'adj_close': 463.76001, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_598c12211f5944a2ad97702e3d5a76ff'}, {'index': 'NYA', 'date': '1966-11-10', 'open': 466.829987, 'high': 466.829987, 'low': 466.829987, 'close': 466.829987, 'adj_close': 466.829987, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_b90bc27f854b4553b81780ef05985306'}, {'index': 'NYA', 'date': '1966-11-11', 'open': 467.25, 'high': 467.25, 'low': 467.25, 'close': 467.25, 'adj_close': 467.25, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_d7bf9c68bb604c309541d1f5e45f4e6c'}, {'index': 'NYA', 'date': '1966-11-14', 'open': 464.399994, 'high': 464.399994, 'low': 464.399994, 'close': 464.399994, 'adj_close': 464.399994, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_27c0c637dba3478abc8e06aeb2f1de13'}, {'index': 'NYA', 'date': '1966-11-15', 'open': 466.089996, 'high': 466.089996, 'low': 466.089996, 'close': 466.089996, 'adj_close': 466.089996, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_7e229ec3335f48e5af5b0babf6202674'}, {'index': 'NYA', 'date': '1966-11-16', 'open': 470.109985, 'high': 470.109985, 'low': 470.109985, 'close': 470.109985, 'adj_close': 470.109985, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_196ee5c117c94196a6c86d955ac1825b'}, {'index': 'NYA', 'date': '1966-11-17', 'open': 466.940002, 'high': 466.940002, 'low': 466.940002, 'close': 466.940002, 'adj_close': 466.940002, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_47104cbdfa9c42b0a82885e1fadb691a'}, {'index': 'NYA', 'date': '1966-11-18', 'open': 464.079987, 'high': 464.079987, 'low': 464.079987, 'close': 464.079987, 'adj_close': 464.079987, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_992a7fbdc05341ec9a2cb849893bfedd'}, {'index': 'NYA', 'date': '1966-11-21', 'open': 457.630005, 'high': 457.630005, 'low': 457.630005, 'close': 457.630005, 'adj_close': 457.630005, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_9eebb4f30617495d9b13f72db8039c55'}, {'index': 'NYA', 'date': '1966-11-22', 'open': 455.519989, 'high': 455.519989, 'low': 455.519989, 'close': 455.519989, 'adj_close': 455.519989, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_6befb11eb46a4d8f8cdf5e1f14e0bd7d'}, {'index': 'NYA', 'date': '1966-11-23', 'open': 458.899994, 'high': 458.899994, 'low': 458.899994, 'close': 458.899994, 'adj_close': 458.899994, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_f436b7462fc94d39ac24c1186565ed33'}, {'index': 'NYA', 'date': '1966-11-25', 'open': 462.600006, 'high': 462.600006, 'low': 462.600006, 'close': 462.600006, 'adj_close': 462.600006, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_3ed857eeaaf04c9c8ed5520e4b683d44'}, {'index': 'NYA', 'date': '1966-11-28', 'open': 462.179993, 'high': 462.179993, 'low': 462.179993, 'close': 462.179993, 'adj_close': 462.179993, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_be6cd17a1acc4c99be615447a4cd9a72'}, {'index': 'NYA', 'date': '1966-11-29', 'open': 460.589996, 'high': 460.589996, 'low': 460.589996, 'close': 460.589996, 'adj_close': 460.589996, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_96627271d0a847819ff3f493cb537612'}, {'index': 'NYA', 'date': '1966-11-30', 'open': 460.910004, 'high': 460.910004, 'low': 460.910004, 'close': 460.910004, 'adj_close': 460.910004, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_09ff579a912d4804a8a128fbfd608a63'}, {'index': 'NYA', 'date': '1966-12-01', 'open': 459.109985, 'high': 459.109985, 'low': 459.109985, 'close': 459.109985, 'adj_close': 459.109985, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_13f58d6daaee463b80164c1e9d8603cd'}, {'index': 'NYA', 'date': '1966-12-02', 'open': 459.320007, 'high': 459.320007, 'low': 459.320007, 'close': 459.320007, 'adj_close': 459.320007, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_bad15ef435da4d0088c690a26d2789b9'}, {'index': 'NYA', 'date': '1966-12-05', 'open': 459.75, 'high': 459.75, 'low': 459.75, 'close': 459.75, 'adj_close': 459.75, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_b87b7ea4cf6a4d3f93803f427665a7ed'}, {'index': 'NYA', 'date': '1966-12-06', 'open': 462.920013, 'high': 462.920013, 'low': 462.920013, 'close': 462.920013, 'adj_close': 462.920013, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_e2c359288ab140fbb7fe3389c4d9cf48'}, {'index': 'NYA', 'date': '1966-12-07', 'open': 467.570007, 'high': 467.570007, 'low': 467.570007, 'close': 467.570007, 'adj_close': 467.570007, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_cc7f06ae482146dcbbc12d44ef668a04'}, {'index': 'NYA', 'date': '1966-12-08', 'open': 469.470001, 'high': 469.470001, 'low': 469.470001, 'close': 469.470001, 'adj_close': 469.470001, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_4eaff3fb22014b378026b5fa18998d5f'}, {'index': 'NYA', 'date': '1966-12-09', 'open': 470.529999, 'high': 470.529999, 'low': 470.529999, 'close': 470.529999, 'adj_close': 470.529999, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_cdac45a2debd4cbebda0f6193b77a6fa'}, {'index': 'NYA', 'date': '1966-12-12', 'open': 475.079987, 'high': 475.079987, 'low': 475.079987, 'close': 475.079987, 'adj_close': 475.079987, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_48bbdba89bc046f78cd4852e8ce8af62'}, {'index': 'NYA', 'date': '1966-12-13', 'open': 473.809998, 'high': 473.809998, 'low': 473.809998, 'close': 473.809998, 'adj_close': 473.809998, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_8b09bfc77ba24dd2b449c8c4a1ad9e23'}, {'index': 'NYA', 'date': '1966-12-14', 'open': 473.700012, 'high': 473.700012, 'low': 473.700012, 'close': 473.700012, 'adj_close': 473.700012, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_b842ca919652455ba6640b96afe9a7da'}, {'index': 'NYA', 'date': '1966-12-15', 'open': 468.730011, 'high': 468.730011, 'low': 468.730011, 'close': 468.730011, 'adj_close': 468.730011, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_a1ea9fe9f02b486abea545b0ddedc802'}, {'index': 'NYA', 'date': '1966-12-16', 'open': 468.839996, 'high': 468.839996, 'low': 468.839996, 'close': 468.839996, 'adj_close': 468.839996, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_4fdc373cc99a4ea0b65e187f5310f5f7'}, {'index': 'NYA', 'date': '1966-12-19', 'open': 467.570007, 'high': 467.570007, 'low': 467.570007, 'close': 467.570007, 'adj_close': 467.570007, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_69387d1fef65455c9078fafc6d3bc003'}, {'index': 'NYA', 'date': '1966-12-20', 'open': 465.670013, 'high': 465.670013, 'low': 465.670013, 'close': 465.670013, 'adj_close': 465.670013, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_3a9c802396b24a8fa0c6fabfe101b9c8'}, {'index': 'NYA', 'date': '1966-12-21', 'open': 468.200012, 'high': 468.200012, 'low': 468.200012, 'close': 468.200012, 'adj_close': 468.200012, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_763800a0ba9d4b55aefdb61c7ade78c9'}, {'index': 'NYA', 'date': '1966-12-22', 'open': 470.109985, 'high': 470.109985, 'low': 470.109985, 'close': 470.109985, 'adj_close': 470.109985, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_a003681eb1114b468c7614429bad2cd8'}, {'index': 'NYA', 'date': '1966-12-23', 'open': 468.940002, 'high': 468.940002, 'low': 468.940002, 'close': 468.940002, 'adj_close': 468.940002, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_756f96892356400e9a5435d2c75d0f14'}, {'index': 'NYA', 'date': '1966-12-27', 'open': 466.51001, 'high': 466.51001, 'low': 466.51001, 'close': 466.51001, 'adj_close': 466.51001, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_e862ffdded764179aa1796c6146a248d'}, {'index': 'NYA', 'date': '1966-12-28', 'open': 464.190002, 'high': 464.190002, 'low': 464.190002, 'close': 464.190002, 'adj_close': 464.190002, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_62f9fe7072fc43518c3259e8e41c1102'}, {'index': 'NYA', 'date': '1966-12-29', 'open': 462.48999, 'high': 462.48999, 'low': 462.48999, 'close': 462.48999, 'adj_close': 462.48999, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_e45a7d3a48ab4d7298c9f30547a55189'}, {'index': 'NYA', 'date': '1966-12-30', 'open': 462.279999, 'high': 462.279999, 'low': 462.279999, 'close': 462.279999, 'adj_close': 462.279999, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_5c0cc58c00dd4e72b11d42d8bb52aa55'}, {'index': 'NYA', 'date': '1967-01-03', 'open': 462.48999, 'high': 462.48999, 'low': 462.48999, 'close': 462.48999, 'adj_close': 462.48999, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_b8224579041542a5b588d23bd725561b'}, {'index': 'NYA', 'date': '1967-01-04', 'open': 463.339996, 'high': 463.339996, 'low': 463.339996, 'close': 463.339996, 'adj_close': 463.339996, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_4908ee25f6cb4c6cbcf1429e78024153'}, {'index': 'NYA', 'date': '1967-01-05', 'open': 469.26001, 'high': 469.26001, 'low': 469.26001, 'close': 469.26001, 'adj_close': 469.26001, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_36aff27b24c84551939a81b41d5078b5'}, {'index': 'NYA', 'date': '1967-01-06', 'open': 472.75, 'high': 472.75, 'low': 472.75, 'close': 472.75, 'adj_close': 472.75, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_b0fa38527a934e6baa2ca2e154173da9'}, {'index': 'NYA', 'date': '1967-01-09', 'open': 476.660004, 'high': 476.660004, 'low': 476.660004, 'close': 476.660004, 'adj_close': 476.660004, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_ea6a640d367340c6915daf0fa656b1ca'}, {'index': 'NYA', 'date': '1967-01-10', 'open': 476.660004, 'high': 476.660004, 'low': 476.660004, 'close': 476.660004, 'adj_close': 476.660004, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_d1ccdefd919f479eb3e676a16263f85d'}, {'index': 'NYA', 'date': '1967-01-11', 'open': 480.579987, 'high': 480.579987, 'low': 480.579987, 'close': 480.579987, 'adj_close': 480.579987, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_55f7b42365f34d20b24d762c8fd9fa79'}, {'index': 'NYA', 'date': '1967-01-12', 'open': 483.429993, 'high': 483.429993, 'low': 483.429993, 'close': 483.429993, 'adj_close': 483.429993, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_2495360e59a44e17b527592f525cedc2'}, {'index': 'NYA', 'date': '1967-01-13', 'open': 486.809998, 'high': 486.809998, 'low': 486.809998, 'close': 486.809998, 'adj_close': 486.809998, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_c4b69088c4274358ab51d8fb6ec37a38'}, {'index': 'NYA', 'date': '1967-01-16', 'open': 485.970001, 'high': 485.970001, 'low': 485.970001, 'close': 485.970001, 'adj_close': 485.970001, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_19902d23c7bf42a8a6cf80a469228aa8'}, {'index': 'NYA', 'date': '1967-01-17', 'open': 491.26001, 'high': 491.26001, 'low': 491.26001, 'close': 491.26001, 'adj_close': 491.26001, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_a338832557304a0082d653d53e52e44b'}, {'index': 'NYA', 'date': '1967-01-18', 'open': 494.0, 'high': 494.0, 'low': 494.0, 'close': 494.0, 'adj_close': 494.0, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_1a66efbf81c147bba5efec461328f6f2'}, {'index': 'NYA', 'date': '1967-01-19', 'open': 494.220001, 'high': 494.220001, 'low': 494.220001, 'close': 494.220001, 'adj_close': 494.220001, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_7291ec3968bd4daa9c9ae21c6c817896'}, {'index': 'NYA', 'date': '1967-01-20', 'open': 496.01001, 'high': 496.01001, 'low': 496.01001, 'close': 496.01001, 'adj_close': 496.01001, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_188ebed56d5742e3a9bebb820513278c'}, {'index': 'NYA', 'date': '1967-01-23', 'open': 497.809998, 'high': 497.809998, 'low': 497.809998, 'close': 497.809998, 'adj_close': 497.809998, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_e676d68d055a4d3a9151ebc6668d2ee3'}, {'index': 'NYA', 'date': '1967-01-24', 'open': 498.76001, 'high': 498.76001, 'low': 498.76001, 'close': 498.76001, 'adj_close': 498.76001, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_c1c6a375d4ce42afa982e620c9a4ec3b'}, {'index': 'NYA', 'date': '1967-01-25', 'open': 495.380005, 'high': 495.380005, 'low': 495.380005, 'close': 495.380005, 'adj_close': 495.380005, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_9351ed7bb23842abb45b91e828c55e2a'}, {'index': 'NYA', 'date': '1967-01-26', 'open': 495.700012, 'high': 495.700012, 'low': 495.700012, 'close': 495.700012, 'adj_close': 495.700012, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_cc260c71c62943e9bb7f4576f2f719c7'}, {'index': 'NYA', 'date': '1967-01-27', 'open': 497.809998, 'high': 497.809998, 'low': 497.809998, 'close': 497.809998, 'adj_close': 497.809998, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_a5d780fcdb3048249d018f26ec7476a8'}, {'index': 'NYA', 'date': '1967-01-30', 'open': 500.670013, 'high': 500.670013, 'low': 500.670013, 'close': 500.670013, 'adj_close': 500.670013, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_57aea50b6cb5413dbac7f0c09883e24f'}, {'index': 'NYA', 'date': '1967-01-31', 'open': 500.140015, 'high': 500.140015, 'low': 500.140015, 'close': 500.140015, 'adj_close': 500.140015, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_2c76c472ce8d42098d837b9e38d03b45'}, {'index': 'NYA', 'date': '1967-02-01', 'open': 499.5, 'high': 499.5, 'low': 499.5, 'close': 499.5, 'adj_close': 499.5, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_6c5630d97c9648edae86375a33464765'}, {'index': 'NYA', 'date': '1967-02-02', 'open': 501.089996, 'high': 501.089996, 'low': 501.089996, 'close': 501.089996, 'adj_close': 501.089996, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_286f4582c20a4da5aa5adf08a3cd6458'}, {'index': 'NYA', 'date': '1967-02-03', 'open': 504.679993, 'high': 504.679993, 'low': 504.679993, 'close': 504.679993, 'adj_close': 504.679993, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_2f842d7ddf084fc395b37eac60f3b27d'}, {'index': 'NYA', 'date': '1967-02-06', 'open': 503.730011, 'high': 503.730011, 'low': 503.730011, 'close': 503.730011, 'adj_close': 503.730011, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_eecbc521e9504d35b9edd65cb29ca668'}, {'index': 'NYA', 'date': '1967-02-07', 'open': 502.679993, 'high': 502.679993, 'low': 502.679993, 'close': 502.679993, 'adj_close': 502.679993, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_7c357f0bba9b40fe9a280123bca76883'}, {'index': 'NYA', 'date': '1967-02-08', 'open': 507.119995, 'high': 507.119995, 'low': 507.119995, 'close': 507.119995, 'adj_close': 507.119995, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_6f1672bf7ce94f1eb5453ee42f25d289'}, {'index': 'NYA', 'date': '1967-02-09', 'open': 505.109985, 'high': 505.109985, 'low': 505.109985, 'close': 505.109985, 'adj_close': 505.109985, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_d3676d3881b242f6ba37676b339da629'}, {'index': 'NYA', 'date': '1967-02-10', 'open': 506.160004, 'high': 506.160004, 'low': 506.160004, 'close': 506.160004, 'adj_close': 506.160004, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_b0d83942f30b43209b4cd35eda075a5a'}, {'index': 'NYA', 'date': '1967-02-13', 'open': 506.899994, 'high': 506.899994, 'low': 506.899994, 'close': 506.899994, 'adj_close': 506.899994, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_76237c709f0a47088dced65b3219c489'}, {'index': 'NYA', 'date': '1967-02-14', 'open': 510.290009, 'high': 510.290009, 'low': 510.290009, 'close': 510.290009, 'adj_close': 510.290009, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_fa9acffbd77d4d49bddb7aabd3832f1b'}, {'index': 'NYA', 'date': '1967-02-15', 'open': 510.609985, 'high': 510.609985, 'low': 510.609985, 'close': 510.609985, 'adj_close': 510.609985, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_37ad9f23e08d4c678fec82c84855338e'}, {'index': 'NYA', 'date': '1967-02-16', 'open': 508.380005, 'high': 508.380005, 'low': 508.380005, 'close': 508.380005, 'adj_close': 508.380005, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_03aeb54fef0a4c60a86b67f8be97d0c8'}, {'index': 'NYA', 'date': '1967-02-17', 'open': 508.48999, 'high': 508.48999, 'low': 508.48999, 'close': 508.48999, 'adj_close': 508.48999, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_67febd829f0a43a1a3932e3c2b8e255c'}, {'index': 'NYA', 'date': '1967-02-20', 'open': 505.850006, 'high': 505.850006, 'low': 505.850006, 'close': 505.850006, 'adj_close': 505.850006, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_6b55825311e54d048dfe9d6aaa37acfc'}, {'index': 'NYA', 'date': '1967-02-21', 'open': 505.850006, 'high': 505.850006, 'low': 505.850006, 'close': 505.850006, 'adj_close': 505.850006, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_6165260771b2448ab65e75ca2e7ad39a'}, {'index': 'NYA', 'date': '1967-02-23', '@type': 'IndexDataOp', '@id': 'IndexDataOp_a4d281ab44e143c8aef0a398cf615ec8'}, {'index': 'NYA', 'date': '1967-02-24', 'open': 506.480011, 'high': 506.480011, 'low': 506.480011, 'close': 506.480011, 'adj_close': 506.480011, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_340f944337294879b5803c7b592b1944'}, {'index': 'NYA', 'date': '1967-02-27', 'open': 501.089996, 'high': 501.089996, 'low': 501.089996, 'close': 501.089996, 'adj_close': 501.089996, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_4905bfca283b4e4c8fa8fb59bbfe10bc'}, {'index': 'NYA', 'date': '1967-02-28', 'open': 502.890015, 'high': 502.890015, 'low': 502.890015, 'close': 502.890015, 'adj_close': 502.890015, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_45ad14a648e24e998dd1506b26812d82'}, {'index': 'NYA', 'date': '1967-03-01', 'open': 507.859985, 'high': 507.859985, 'low': 507.859985, 'close': 507.859985, 'adj_close': 507.859985, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_169bd2d888f34fd7962b1f4b77311ddd'}, {'index': 'NYA', 'date': '1967-03-02', 'open': 510.709991, 'high': 510.709991, 'low': 510.709991, 'close': 510.709991, 'adj_close': 510.709991, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_567b3c6178ea45169c82c110311af628'}, {'index': 'NYA', 'date': '1967-03-03', 'open': 511.450012, 'high': 511.450012, 'low': 511.450012, 'close': 511.450012, 'adj_close': 511.450012, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_b901176582bc4124a8f4dfe85e3f6e4c'}, {'index': 'NYA', 'date': '1967-03-06', 'open': 510.709991, 'high': 510.709991, 'low': 510.709991, 'close': 510.709991, 'adj_close': 510.709991, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_9032e62b60854c0ca10de0c3faffd565'}, {'index': 'NYA', 'date': '1967-03-07', 'open': 511.350006, 'high': 511.350006, 'low': 511.350006, 'close': 511.350006, 'adj_close': 511.350006, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_76dc8e44f64540eebab6717166ca22a2'}, {'index': 'NYA', 'date': '1967-03-08', 'open': 512.090027, 'high': 512.090027, 'low': 512.090027, 'close': 512.090027, 'adj_close': 512.090027, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_1590c7129b8d4e96adf3fad0c25f7741'}, {'index': 'NYA', 'date': '1967-03-09', 'open': 513.570007, 'high': 513.570007, 'low': 513.570007, 'close': 513.570007, 'adj_close': 513.570007, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_27866b6fcba04c32b490d5155849a043'}]

The schema is:
class IndexDataOp(DocumentTemplate):
    _key = RandomKey()
    adj_close: Optional[float]
    close: Optional[float]
    date: Optional[str]
    high: Optional[float]
    index: Optional[str]
    low: Optional[float]
    open: Optional[float]
    volume: Optional[int]

Sorry I don't have it in json",2021-09-02T11:57:13Z,spl,https://github.com/terminusdb/terminusdb/issues/472#issuecomment-911590045,"I couldn't reproduce the problem even with the entire set of data given.
#!/bin/bash

set -ex

# Create the database.
xh 'http://admin:root@localhost:6363/api/db/admin/t' <<EOF
{""label"":""l"",""comment"":""c""}
EOF

# Create the schema.
xh 'http://admin:root@localhost:6363/api/document/admin/t?author=a&message=schema&graph_type=schema' <<EOF
{
  ""@type"": ""Class"",
  ""@id"": ""IndexDataOp"",
  ""@key"": {""@type"": ""Random""},
  ""adj_close"": {""@type"": ""Optional"", ""@class"": ""xsd:float""},
  ""close"": {""@type"": ""Optional"", ""@class"": ""xsd:float""},
  ""date"": {""@type"": ""Optional"", ""@class"": ""xsd:string""},
  ""high"": {""@type"": ""Optional"", ""@class"": ""xsd:float""},
  ""index"": {""@type"": ""Optional"", ""@class"": ""xsd:string""},
  ""low"": {""@type"": ""Optional"", ""@class"": ""xsd:float""},
  ""open"": {""@type"": ""Optional"", ""@class"": ""xsd:float""},
  ""volume"": {""@type"": ""Optional"", ""@class"": ""xsd:integer""}
}
EOF

# Post instances.
xh 'http://admin:root@localhost:6363/api/document/admin/t?author=a&message=instance' <<EOF
{""index"": ""NYA"", ""date"": ""1966-10-14"", ""open"": 435.73999, ""high"": 435.73999, ""low"": 435.73999, ""close"": 435.73999, ""adj_close"": 435.73999, ""volume"": 0.0, ""@type"": ""IndexDataOp"", ""@id"": ""IndexDataOp_ac69bef9dcc449f1b832955a0fb352d3""}
{""index"": ""NYA"", ""date"": ""1966-10-17"", ""open"": 440.609985, ""high"": 440.609985, ""low"": 440.609985, ""close"": 440.609985, ""adj_close"": 440.609985, ""volume"": 0.0, ""@type"": ""IndexDataOp"", ""@id"": ""IndexDataOp_c03f34b77c894c54a88231f1f3f019b4""}
{""index"": ""NYA"", ""date"": ""1966-10-18"", ""open"": 447.799988, ""high"": 447.799988, ""low"": 447.799988, ""close"": 447.799988, ""adj_close"": 447.799988, ""volume"": 0.0, ""@type"": ""IndexDataOp"", ""@id"": ""IndexDataOp_7e19232b70b44b2397821064430bab2a""}
{""index"": ""NYA"", ""date"": ""1966-10-19"", ""open"": 444.519989, ""high"": 444.519989, ""low"": 444.519989, ""close"": 444.519989, ""adj_close"": 444.519989, ""volume"": 0.0, ""@type"": ""IndexDataOp"", ""@id"": ""IndexDataOp_58b492ab7259449aacab228c5c9ff1c7""}
{""index"": ""NYA"", ""date"": ""1966-10-20"", ""open"": 443.25, ""high"": 443.25, ""low"": 443.25, ""close"": 443.25, ""adj_close"": 443.25, ""volume"": 0.0, ""@type"": ""IndexDataOp"", ""@id"": ""IndexDataOp_7714503df8f244afbb2533ff3d83a1a6""}
{""index"": ""NYA"", ""date"": ""1966-10-21"", ""open"": 444.839996, ""high"": 444.839996, ""low"": 444.839996, ""close"": 444.839996, ""adj_close"": 444.839996, ""volume"": 0.0, ""@type"": ""IndexDataOp"", ""@id"": ""IndexDataOp_c5e9bbb5f24a4edf8e74f28b1c3eff0e""}
{""index"": ""NYA"", ""date"": ""1966-10-24"", ""open"": 446.0, ""high"": 446.0, ""low"": 446.0, ""close"": 446.0, ""adj_close"": 446.0, ""volume"": 0.0, ""@type"": ""IndexDataOp"", ""@id"": ""IndexDataOp_41bfb1e914ad4a41b21c4e611a172079""}
{""index"": ""NYA"", ""date"": ""1966-10-25"", ""open"": 448.429993, ""high"": 448.429993, ""low"": 448.429993, ""close"": 448.429993, ""adj_close"": 448.429993, ""volume"": 0.0, ""@type"": ""IndexDataOp"", ""@id"": ""IndexDataOp_0f1318ae6c8a4b81ace6b97652e2a13b""}
{""index"": ""NYA"", ""date"": ""1966-10-26"", ""open"": 452.450012, ""high"": 452.450012, ""low"": 452.450012, ""close"": 452.450012, ""adj_close"": 452.450012, ""volume"": 0.0, ""@type"": ""IndexDataOp"", ""@id"": ""IndexDataOp_e4a1ec76f4c44f73b80e947f40f6d88d""}
{""index"": ""NYA"", ""date"": ""1966-10-27"", ""open"": 456.26001, ""high"": 456.26001, ""low"": 456.26001, ""close"": 456.26001, ""adj_close"": 456.26001, ""volume"": 0.0, ""@type"": ""IndexDataOp"", ""@id"": ""IndexDataOp_7de0599ee12948039b72634b0fef4533""}
{""index"": ""NYA"", ""date"": ""1966-10-28"", ""open"": 456.470001, ""high"": 456.470001, ""low"": 456.470001, ""close"": 456.470001, ""adj_close"": 456.470001, ""volume"": 0.0, ""@type"": ""IndexDataOp"", ""@id"": ""IndexDataOp_1629239709354f0abdb39e5c8594e541""}
{""index"": ""NYA"", ""date"": ""1966-10-31"", ""open"": 456.359985, ""high"": 456.359985, ""low"": 456.359985, ""close"": 456.359985, ""adj_close"": 456.359985, ""volume"": 0.0, ""@type"": ""IndexDataOp"", ""@id"": ""IndexDataOp_3fccf298aae441608e96ec5f27ac0a9a""}
{""index"": ""NYA"", ""date"": ""1966-11-01"", ""open"": 459.850006, ""high"": 459.850006, ""low"": 459.850006, ""close"": 459.850006, ""adj_close"": 459.850006, ""volume"": 0.0, ""@type"": ""IndexDataOp"", ""@id"": ""IndexDataOp_b4f774c9ee7c454282b4779d7dfcb769""}
{""index"": ""NYA"", ""date"": ""1966-11-02"", ""open"": 460.380005, ""high"": 460.380005, ""low"": 460.380005, ""close"": 460.380005, ""adj_close"": 460.380005, ""volume"": 0.0, ""@type"": ""IndexDataOp"", ""@id"": ""IndexDataOp_422de280413947349b3fc42e4daaed31""}
{""index"": ""NYA"", ""date"": ""1966-11-03"", ""open"": 458.790009, ""high"": 458.790009, ""low"": 458.790009, ""close"": 458.790009, ""adj_close"": 458.790009, ""volume"": 0.0, ""@type"": ""IndexDataOp"", ""@id"": ""IndexDataOp_d9f5370cdb994c46b627cd21133cc7e4""}
{""index"": ""NYA"", ""date"": ""1966-11-04"", ""open"": 460.269989, ""high"": 460.269989, ""low"": 460.269989, ""close"": 460.269989, ""adj_close"": 460.269989, ""volume"": 0.0, ""@type"": ""IndexDataOp"", ""@id"": ""IndexDataOp_7a4722f73e4542a5ba935cbe7f452f20""}
{""index"": ""NYA"", ""date"": ""1966-11-07"", ""open"": 460.170013, ""high"": 460.170013, ""low"": 460.170013, ""close"": 460.170013, ""adj_close"": 460.170013, ""volume"": 0.0, ""@type"": ""IndexDataOp"", ""@id"": ""IndexDataOp_2f29143c293443beafd861482958cdc4""}
{""index"": ""NYA"", ""date"": ""1966-11-09"", ""open"": 463.76001, ""high"": 463.76001, ""low"": 463.76001, ""close"": 463.76001, ""adj_close"": 463.76001, ""volume"": 0.0, ""@type"": ""IndexDataOp"", ""@id"": ""IndexDataOp_598c12211f5944a2ad97702e3d5a76ff""}
{""index"": ""NYA"", ""date"": ""1966-11-10"", ""open"": 466.829987, ""high"": 466.829987, ""low"": 466.829987, ""close"": 466.829987, ""adj_close"": 466.829987, ""volume"": 0.0, ""@type"": ""IndexDataOp"", ""@id"": ""IndexDataOp_b90bc27f854b4553b81780ef05985306""}
{""index"": ""NYA"", ""date"": ""1966-11-11"", ""open"": 467.25, ""high"": 467.25, ""low"": 467.25, ""close"": 467.25, ""adj_close"": 467.25, ""volume"": 0.0, ""@type"": ""IndexDataOp"", ""@id"": ""IndexDataOp_d7bf9c68bb604c309541d1f5e45f4e6c""}
{""index"": ""NYA"", ""date"": ""1966-11-14"", ""open"": 464.399994, ""high"": 464.399994, ""low"": 464.399994, ""close"": 464.399994, ""adj_close"": 464.399994, ""volume"": 0.0, ""@type"": ""IndexDataOp"", ""@id"": ""IndexDataOp_27c0c637dba3478abc8e06aeb2f1de13""}
{""index"": ""NYA"", ""date"": ""1966-11-15"", ""open"": 466.089996, ""high"": 466.089996, ""low"": 466.089996, ""close"": 466.089996, ""adj_close"": 466.089996, ""volume"": 0.0, ""@type"": ""IndexDataOp"", ""@id"": ""IndexDataOp_7e229ec3335f48e5af5b0babf6202674""}
{""index"": ""NYA"", ""date"": ""1966-11-16"", ""open"": 470.109985, ""high"": 470.109985, ""low"": 470.109985, ""close"": 470.109985, ""adj_close"": 470.109985, ""volume"": 0.0, ""@type"": ""IndexDataOp"", ""@id"": ""IndexDataOp_196ee5c117c94196a6c86d955ac1825b""}
{""index"": ""NYA"", ""date"": ""1966-11-17"", ""open"": 466.940002, ""high"": 466.940002, ""low"": 466.940002, ""close"": 466.940002, ""adj_close"": 466.940002, ""volume"": 0.0, ""@type"": ""IndexDataOp"", ""@id"": ""IndexDataOp_47104cbdfa9c42b0a82885e1fadb691a""}
{""index"": ""NYA"", ""date"": ""1966-11-18"", ""open"": 464.079987, ""high"": 464.079987, ""low"": 464.079987, ""close"": 464.079987, ""adj_close"": 464.079987, ""volume"": 0.0, ""@type"": ""IndexDataOp"", ""@id"": ""IndexDataOp_992a7fbdc05341ec9a2cb849893bfedd""}
{""index"": ""NYA"", ""date"": ""1966-11-21"", ""open"": 457.630005, ""high"": 457.630005, ""low"": 457.630005, ""close"": 457.630005, ""adj_close"": 457.630005, ""volume"": 0.0, ""@type"": ""IndexDataOp"", ""@id"": ""IndexDataOp_9eebb4f30617495d9b13f72db8039c55""}
{""index"": ""NYA"", ""date"": ""1966-11-22"", ""open"": 455.519989, ""high"": 455.519989, ""low"": 455.519989, ""close"": 455.519989, ""adj_close"": 455.519989, ""volume"": 0.0, ""@type"": ""IndexDataOp"", ""@id"": ""IndexDataOp_6befb11eb46a4d8f8cdf5e1f14e0bd7d""}
{""index"": ""NYA"", ""date"": ""1966-11-23"", ""open"": 458.899994, ""high"": 458.899994, ""low"": 458.899994, ""close"": 458.899994, ""adj_close"": 458.899994, ""volume"": 0.0, ""@type"": ""IndexDataOp"", ""@id"": ""IndexDataOp_f436b7462fc94d39ac24c1186565ed33""}
{""index"": ""NYA"", ""date"": ""1966-11-25"", ""open"": 462.600006, ""high"": 462.600006, ""low"": 462.600006, ""close"": 462.600006, ""adj_close"": 462.600006, ""volume"": 0.0, ""@type"": ""IndexDataOp"", ""@id"": ""IndexDataOp_3ed857eeaaf04c9c8ed5520e4b683d44""}
{""index"": ""NYA"", ""date"": ""1966-11-28"", ""open"": 462.179993, ""high"": 462.179993, ""low"": 462.179993, ""close"": 462.179993, ""adj_close"": 462.179993, ""volume"": 0.0, ""@type"": ""IndexDataOp"", ""@id"": ""IndexDataOp_be6cd17a1acc4c99be615447a4cd9a72""}
{""index"": ""NYA"", ""date"": ""1966-11-29"", ""open"": 460.589996, ""high"": 460.589996, ""low"": 460.589996, ""close"": 460.589996, ""adj_close"": 460.589996, ""volume"": 0.0, ""@type"": ""IndexDataOp"", ""@id"": ""IndexDataOp_96627271d0a847819ff3f493cb537612""}
{""index"": ""NYA"", ""date"": ""1966-11-30"", ""open"": 460.910004, ""high"": 460.910004, ""low"": 460.910004, ""close"": 460.910004, ""adj_close"": 460.910004, ""volume"": 0.0, ""@type"": ""IndexDataOp"", ""@id"": ""IndexDataOp_09ff579a912d4804a8a128fbfd608a63""}
{""index"": ""NYA"", ""date"": ""1966-12-01"", ""open"": 459.109985, ""high"": 459.109985, ""low"": 459.109985, ""close"": 459.109985, ""adj_close"": 459.109985, ""volume"": 0.0, ""@type"": ""IndexDataOp"", ""@id"": ""IndexDataOp_13f58d6daaee463b80164c1e9d8603cd""}
{""index"": ""NYA"", ""date"": ""1966-12-02"", ""open"": 459.320007, ""high"": 459.320007, ""low"": 459.320007, ""close"": 459.320007, ""adj_close"": 459.320007, ""volume"": 0.0, ""@type"": ""IndexDataOp"", ""@id"": ""IndexDataOp_bad15ef435da4d0088c690a26d2789b9""}
{""index"": ""NYA"", ""date"": ""1966-12-05"", ""open"": 459.75, ""high"": 459.75, ""low"": 459.75, ""close"": 459.75, ""adj_close"": 459.75, ""volume"": 0.0, ""@type"": ""IndexDataOp"", ""@id"": ""IndexDataOp_b87b7ea4cf6a4d3f93803f427665a7ed""}
{""index"": ""NYA"", ""date"": ""1966-12-06"", ""open"": 462.920013, ""high"": 462.920013, ""low"": 462.920013, ""close"": 462.920013, ""adj_close"": 462.920013, ""volume"": 0.0, ""@type"": ""IndexDataOp"", ""@id"": ""IndexDataOp_e2c359288ab140fbb7fe3389c4d9cf48""}
{""index"": ""NYA"", ""date"": ""1966-12-07"", ""open"": 467.570007, ""high"": 467.570007, ""low"": 467.570007, ""close"": 467.570007, ""adj_close"": 467.570007, ""volume"": 0.0, ""@type"": ""IndexDataOp"", ""@id"": ""IndexDataOp_cc7f06ae482146dcbbc12d44ef668a04""}
{""index"": ""NYA"", ""date"": ""1966-12-08"", ""open"": 469.470001, ""high"": 469.470001, ""low"": 469.470001, ""close"": 469.470001, ""adj_close"": 469.470001, ""volume"": 0.0, ""@type"": ""IndexDataOp"", ""@id"": ""IndexDataOp_4eaff3fb22014b378026b5fa18998d5f""}
{""index"": ""NYA"", ""date"": ""1966-12-09"", ""open"": 470.529999, ""high"": 470.529999, ""low"": 470.529999, ""close"": 470.529999, ""adj_close"": 470.529999, ""volume"": 0.0, ""@type"": ""IndexDataOp"", ""@id"": ""IndexDataOp_cdac45a2debd4cbebda0f6193b77a6fa""}
{""index"": ""NYA"", ""date"": ""1966-12-12"", ""open"": 475.079987, ""high"": 475.079987, ""low"": 475.079987, ""close"": 475.079987, ""adj_close"": 475.079987, ""volume"": 0.0, ""@type"": ""IndexDataOp"", ""@id"": ""IndexDataOp_48bbdba89bc046f78cd4852e8ce8af62""}
{""index"": ""NYA"", ""date"": ""1966-12-13"", ""open"": 473.809998, ""high"": 473.809998, ""low"": 473.809998, ""close"": 473.809998, ""adj_close"": 473.809998, ""volume"": 0.0, ""@type"": ""IndexDataOp"", ""@id"": ""IndexDataOp_8b09bfc77ba24dd2b449c8c4a1ad9e23""}
{""index"": ""NYA"", ""date"": ""1966-12-14"", ""open"": 473.700012, ""high"": 473.700012, ""low"": 473.700012, ""close"": 473.700012, ""adj_close"": 473.700012, ""volume"": 0.0, ""@type"": ""IndexDataOp"", ""@id"": ""IndexDataOp_b842ca919652455ba6640b96afe9a7da""}
{""index"": ""NYA"", ""date"": ""1966-12-15"", ""open"": 468.730011, ""high"": 468.730011, ""low"": 468.730011, ""close"": 468.730011, ""adj_close"": 468.730011, ""volume"": 0.0, ""@type"": ""IndexDataOp"", ""@id"": ""IndexDataOp_a1ea9fe9f02b486abea545b0ddedc802""}
{""index"": ""NYA"", ""date"": ""1966-12-16"", ""open"": 468.839996, ""high"": 468.839996, ""low"": 468.839996, ""close"": 468.839996, ""adj_close"": 468.839996, ""volume"": 0.0, ""@type"": ""IndexDataOp"", ""@id"": ""IndexDataOp_4fdc373cc99a4ea0b65e187f5310f5f7""}
{""index"": ""NYA"", ""date"": ""1966-12-19"", ""open"": 467.570007, ""high"": 467.570007, ""low"": 467.570007, ""close"": 467.570007, ""adj_close"": 467.570007, ""volume"": 0.0, ""@type"": ""IndexDataOp"", ""@id"": ""IndexDataOp_69387d1fef65455c9078fafc6d3bc003""}
{""index"": ""NYA"", ""date"": ""1966-12-20"", ""open"": 465.670013, ""high"": 465.670013, ""low"": 465.670013, ""close"": 465.670013, ""adj_close"": 465.670013, ""volume"": 0.0, ""@type"": ""IndexDataOp"", ""@id"": ""IndexDataOp_3a9c802396b24a8fa0c6fabfe101b9c8""}
{""index"": ""NYA"", ""date"": ""1966-12-21"", ""open"": 468.200012, ""high"": 468.200012, ""low"": 468.200012, ""close"": 468.200012, ""adj_close"": 468.200012, ""volume"": 0.0, ""@type"": ""IndexDataOp"", ""@id"": ""IndexDataOp_763800a0ba9d4b55aefdb61c7ade78c9""}
{""index"": ""NYA"", ""date"": ""1966-12-22"", ""open"": 470.109985, ""high"": 470.109985, ""low"": 470.109985, ""close"": 470.109985, ""adj_close"": 470.109985, ""volume"": 0.0, ""@type"": ""IndexDataOp"", ""@id"": ""IndexDataOp_a003681eb1114b468c7614429bad2cd8""}
{""index"": ""NYA"", ""date"": ""1966-12-23"", ""open"": 468.940002, ""high"": 468.940002, ""low"": 468.940002, ""close"": 468.940002, ""adj_close"": 468.940002, ""volume"": 0.0, ""@type"": ""IndexDataOp"", ""@id"": ""IndexDataOp_756f96892356400e9a5435d2c75d0f14""}
{""index"": ""NYA"", ""date"": ""1966-12-27"", ""open"": 466.51001, ""high"": 466.51001, ""low"": 466.51001, ""close"": 466.51001, ""adj_close"": 466.51001, ""volume"": 0.0, ""@type"": ""IndexDataOp"", ""@id"": ""IndexDataOp_e862ffdded764179aa1796c6146a248d""}
{""index"": ""NYA"", ""date"": ""1966-12-28"", ""open"": 464.190002, ""high"": 464.190002, ""low"": 464.190002, ""close"": 464.190002, ""adj_close"": 464.190002, ""volume"": 0.0, ""@type"": ""IndexDataOp"", ""@id"": ""IndexDataOp_62f9fe7072fc43518c3259e8e41c1102""}
{""index"": ""NYA"", ""date"": ""1966-12-29"", ""open"": 462.48999, ""high"": 462.48999, ""low"": 462.48999, ""close"": 462.48999, ""adj_close"": 462.48999, ""volume"": 0.0, ""@type"": ""IndexDataOp"", ""@id"": ""IndexDataOp_e45a7d3a48ab4d7298c9f30547a55189""}
{""index"": ""NYA"", ""date"": ""1966-12-30"", ""open"": 462.279999, ""high"": 462.279999, ""low"": 462.279999, ""close"": 462.279999, ""adj_close"": 462.279999, ""volume"": 0.0, ""@type"": ""IndexDataOp"", ""@id"": ""IndexDataOp_5c0cc58c00dd4e72b11d42d8bb52aa55""}
{""index"": ""NYA"", ""date"": ""1967-01-03"", ""open"": 462.48999, ""high"": 462.48999, ""low"": 462.48999, ""close"": 462.48999, ""adj_close"": 462.48999, ""volume"": 0.0, ""@type"": ""IndexDataOp"", ""@id"": ""IndexDataOp_b8224579041542a5b588d23bd725561b""}
{""index"": ""NYA"", ""date"": ""1967-01-04"", ""open"": 463.339996, ""high"": 463.339996, ""low"": 463.339996, ""close"": 463.339996, ""adj_close"": 463.339996, ""volume"": 0.0, ""@type"": ""IndexDataOp"", ""@id"": ""IndexDataOp_4908ee25f6cb4c6cbcf1429e78024153""}
{""index"": ""NYA"", ""date"": ""1967-01-05"", ""open"": 469.26001, ""high"": 469.26001, ""low"": 469.26001, ""close"": 469.26001, ""adj_close"": 469.26001, ""volume"": 0.0, ""@type"": ""IndexDataOp"", ""@id"": ""IndexDataOp_36aff27b24c84551939a81b41d5078b5""}
{""index"": ""NYA"", ""date"": ""1967-01-06"", ""open"": 472.75, ""high"": 472.75, ""low"": 472.75, ""close"": 472.75, ""adj_close"": 472.75, ""volume"": 0.0, ""@type"": ""IndexDataOp"", ""@id"": ""IndexDataOp_b0fa38527a934e6baa2ca2e154173da9""}
{""index"": ""NYA"", ""date"": ""1967-01-09"", ""open"": 476.660004, ""high"": 476.660004, ""low"": 476.660004, ""close"": 476.660004, ""adj_close"": 476.660004, ""volume"": 0.0, ""@type"": ""IndexDataOp"", ""@id"": ""IndexDataOp_ea6a640d367340c6915daf0fa656b1ca""}
{""index"": ""NYA"", ""date"": ""1967-01-10"", ""open"": 476.660004, ""high"": 476.660004, ""low"": 476.660004, ""close"": 476.660004, ""adj_close"": 476.660004, ""volume"": 0.0, ""@type"": ""IndexDataOp"", ""@id"": ""IndexDataOp_d1ccdefd919f479eb3e676a16263f85d""}
{""index"": ""NYA"", ""date"": ""1967-01-11"", ""open"": 480.579987, ""high"": 480.579987, ""low"": 480.579987, ""close"": 480.579987, ""adj_close"": 480.579987, ""volume"": 0.0, ""@type"": ""IndexDataOp"", ""@id"": ""IndexDataOp_55f7b42365f34d20b24d762c8fd9fa79""}
{""index"": ""NYA"", ""date"": ""1967-01-12"", ""open"": 483.429993, ""high"": 483.429993, ""low"": 483.429993, ""close"": 483.429993, ""adj_close"": 483.429993, ""volume"": 0.0, ""@type"": ""IndexDataOp"", ""@id"": ""IndexDataOp_2495360e59a44e17b527592f525cedc2""}
{""index"": ""NYA"", ""date"": ""1967-01-13"", ""open"": 486.809998, ""high"": 486.809998, ""low"": 486.809998, ""close"": 486.809998, ""adj_close"": 486.809998, ""volume"": 0.0, ""@type"": ""IndexDataOp"", ""@id"": ""IndexDataOp_c4b69088c4274358ab51d8fb6ec37a38""}
{""index"": ""NYA"", ""date"": ""1967-01-16"", ""open"": 485.970001, ""high"": 485.970001, ""low"": 485.970001, ""close"": 485.970001, ""adj_close"": 485.970001, ""volume"": 0.0, ""@type"": ""IndexDataOp"", ""@id"": ""IndexDataOp_19902d23c7bf42a8a6cf80a469228aa8""}
{""index"": ""NYA"", ""date"": ""1967-01-17"", ""open"": 491.26001, ""high"": 491.26001, ""low"": 491.26001, ""close"": 491.26001, ""adj_close"": 491.26001, ""volume"": 0.0, ""@type"": ""IndexDataOp"", ""@id"": ""IndexDataOp_a338832557304a0082d653d53e52e44b""}
{""index"": ""NYA"", ""date"": ""1967-01-18"", ""open"": 494.0, ""high"": 494.0, ""low"": 494.0, ""close"": 494.0, ""adj_close"": 494.0, ""volume"": 0.0, ""@type"": ""IndexDataOp"", ""@id"": ""IndexDataOp_1a66efbf81c147bba5efec461328f6f2""}
{""index"": ""NYA"", ""date"": ""1967-01-19"", ""open"": 494.220001, ""high"": 494.220001, ""low"": 494.220001, ""close"": 494.220001, ""adj_close"": 494.220001, ""volume"": 0.0, ""@type"": ""IndexDataOp"", ""@id"": ""IndexDataOp_7291ec3968bd4daa9c9ae21c6c817896""}
{""index"": ""NYA"", ""date"": ""1967-01-20"", ""open"": 496.01001, ""high"": 496.01001, ""low"": 496.01001, ""close"": 496.01001, ""adj_close"": 496.01001, ""volume"": 0.0, ""@type"": ""IndexDataOp"", ""@id"": ""IndexDataOp_188ebed56d5742e3a9bebb820513278c""}
{""index"": ""NYA"", ""date"": ""1967-01-23"", ""open"": 497.809998, ""high"": 497.809998, ""low"": 497.809998, ""close"": 497.809998, ""adj_close"": 497.809998, ""volume"": 0.0, ""@type"": ""IndexDataOp"", ""@id"": ""IndexDataOp_e676d68d055a4d3a9151ebc6668d2ee3""}
{""index"": ""NYA"", ""date"": ""1967-01-24"", ""open"": 498.76001, ""high"": 498.76001, ""low"": 498.76001, ""close"": 498.76001, ""adj_close"": 498.76001, ""volume"": 0.0, ""@type"": ""IndexDataOp"", ""@id"": ""IndexDataOp_c1c6a375d4ce42afa982e620c9a4ec3b""}
{""index"": ""NYA"", ""date"": ""1967-01-25"", ""open"": 495.380005, ""high"": 495.380005, ""low"": 495.380005, ""close"": 495.380005, ""adj_close"": 495.380005, ""volume"": 0.0, ""@type"": ""IndexDataOp"", ""@id"": ""IndexDataOp_9351ed7bb23842abb45b91e828c55e2a""}
{""index"": ""NYA"", ""date"": ""1967-01-26"", ""open"": 495.700012, ""high"": 495.700012, ""low"": 495.700012, ""close"": 495.700012, ""adj_close"": 495.700012, ""volume"": 0.0, ""@type"": ""IndexDataOp"", ""@id"": ""IndexDataOp_cc260c71c62943e9bb7f4576f2f719c7""}
{""index"": ""NYA"", ""date"": ""1967-01-27"", ""open"": 497.809998, ""high"": 497.809998, ""low"": 497.809998, ""close"": 497.809998, ""adj_close"": 497.809998, ""volume"": 0.0, ""@type"": ""IndexDataOp"", ""@id"": ""IndexDataOp_a5d780fcdb3048249d018f26ec7476a8""}
{""index"": ""NYA"", ""date"": ""1967-01-30"", ""open"": 500.670013, ""high"": 500.670013, ""low"": 500.670013, ""close"": 500.670013, ""adj_close"": 500.670013, ""volume"": 0.0, ""@type"": ""IndexDataOp"", ""@id"": ""IndexDataOp_57aea50b6cb5413dbac7f0c09883e24f""}
{""index"": ""NYA"", ""date"": ""1967-01-31"", ""open"": 500.140015, ""high"": 500.140015, ""low"": 500.140015, ""close"": 500.140015, ""adj_close"": 500.140015, ""volume"": 0.0, ""@type"": ""IndexDataOp"", ""@id"": ""IndexDataOp_2c76c472ce8d42098d837b9e38d03b45""}
{""index"": ""NYA"", ""date"": ""1967-02-01"", ""open"": 499.5, ""high"": 499.5, ""low"": 499.5, ""close"": 499.5, ""adj_close"": 499.5, ""volume"": 0.0, ""@type"": ""IndexDataOp"", ""@id"": ""IndexDataOp_6c5630d97c9648edae86375a33464765""}
{""index"": ""NYA"", ""date"": ""1967-02-02"", ""open"": 501.089996, ""high"": 501.089996, ""low"": 501.089996, ""close"": 501.089996, ""adj_close"": 501.089996, ""volume"": 0.0, ""@type"": ""IndexDataOp"", ""@id"": ""IndexDataOp_286f4582c20a4da5aa5adf08a3cd6458""}
{""index"": ""NYA"", ""date"": ""1967-02-03"", ""open"": 504.679993, ""high"": 504.679993, ""low"": 504.679993, ""close"": 504.679993, ""adj_close"": 504.679993, ""volume"": 0.0, ""@type"": ""IndexDataOp"", ""@id"": ""IndexDataOp_2f842d7ddf084fc395b37eac60f3b27d""}
{""index"": ""NYA"", ""date"": ""1967-02-06"", ""open"": 503.730011, ""high"": 503.730011, ""low"": 503.730011, ""close"": 503.730011, ""adj_close"": 503.730011, ""volume"": 0.0, ""@type"": ""IndexDataOp"", ""@id"": ""IndexDataOp_eecbc521e9504d35b9edd65cb29ca668""}
{""index"": ""NYA"", ""date"": ""1967-02-07"", ""open"": 502.679993, ""high"": 502.679993, ""low"": 502.679993, ""close"": 502.679993, ""adj_close"": 502.679993, ""volume"": 0.0, ""@type"": ""IndexDataOp"", ""@id"": ""IndexDataOp_7c357f0bba9b40fe9a280123bca76883""}
{""index"": ""NYA"", ""date"": ""1967-02-08"", ""open"": 507.119995, ""high"": 507.119995, ""low"": 507.119995, ""close"": 507.119995, ""adj_close"": 507.119995, ""volume"": 0.0, ""@type"": ""IndexDataOp"", ""@id"": ""IndexDataOp_6f1672bf7ce94f1eb5453ee42f25d289""}
{""index"": ""NYA"", ""date"": ""1967-02-09"", ""open"": 505.109985, ""high"": 505.109985, ""low"": 505.109985, ""close"": 505.109985, ""adj_close"": 505.109985, ""volume"": 0.0, ""@type"": ""IndexDataOp"", ""@id"": ""IndexDataOp_d3676d3881b242f6ba37676b339da629""}
{""index"": ""NYA"", ""date"": ""1967-02-10"", ""open"": 506.160004, ""high"": 506.160004, ""low"": 506.160004, ""close"": 506.160004, ""adj_close"": 506.160004, ""volume"": 0.0, ""@type"": ""IndexDataOp"", ""@id"": ""IndexDataOp_b0d83942f30b43209b4cd35eda075a5a""}
{""index"": ""NYA"", ""date"": ""1967-02-13"", ""open"": 506.899994, ""high"": 506.899994, ""low"": 506.899994, ""close"": 506.899994, ""adj_close"": 506.899994, ""volume"": 0.0, ""@type"": ""IndexDataOp"", ""@id"": ""IndexDataOp_76237c709f0a47088dced65b3219c489""}
{""index"": ""NYA"", ""date"": ""1967-02-14"", ""open"": 510.290009, ""high"": 510.290009, ""low"": 510.290009, ""close"": 510.290009, ""adj_close"": 510.290009, ""volume"": 0.0, ""@type"": ""IndexDataOp"", ""@id"": ""IndexDataOp_fa9acffbd77d4d49bddb7aabd3832f1b""}
{""index"": ""NYA"", ""date"": ""1967-02-15"", ""open"": 510.609985, ""high"": 510.609985, ""low"": 510.609985, ""close"": 510.609985, ""adj_close"": 510.609985, ""volume"": 0.0, ""@type"": ""IndexDataOp"", ""@id"": ""IndexDataOp_37ad9f23e08d4c678fec82c84855338e""}
{""index"": ""NYA"", ""date"": ""1967-02-16"", ""open"": 508.380005, ""high"": 508.380005, ""low"": 508.380005, ""close"": 508.380005, ""adj_close"": 508.380005, ""volume"": 0.0, ""@type"": ""IndexDataOp"", ""@id"": ""IndexDataOp_03aeb54fef0a4c60a86b67f8be97d0c8""}
{""index"": ""NYA"", ""date"": ""1967-02-17"", ""open"": 508.48999, ""high"": 508.48999, ""low"": 508.48999, ""close"": 508.48999, ""adj_close"": 508.48999, ""volume"": 0.0, ""@type"": ""IndexDataOp"", ""@id"": ""IndexDataOp_67febd829f0a43a1a3932e3c2b8e255c""}
{""index"": ""NYA"", ""date"": ""1967-02-20"", ""open"": 505.850006, ""high"": 505.850006, ""low"": 505.850006, ""close"": 505.850006, ""adj_close"": 505.850006, ""volume"": 0.0, ""@type"": ""IndexDataOp"", ""@id"": ""IndexDataOp_6b55825311e54d048dfe9d6aaa37acfc""}
{""index"": ""NYA"", ""date"": ""1967-02-21"", ""open"": 505.850006, ""high"": 505.850006, ""low"": 505.850006, ""close"": 505.850006, ""adj_close"": 505.850006, ""volume"": 0.0, ""@type"": ""IndexDataOp"", ""@id"": ""IndexDataOp_6165260771b2448ab65e75ca2e7ad39a""}
{""index"": ""NYA"", ""date"": ""1967-02-23"", ""@type"": ""IndexDataOp"", ""@id"": ""IndexDataOp_a4d281ab44e143c8aef0a398cf615ec8""}
{""index"": ""NYA"", ""date"": ""1967-02-24"", ""open"": 506.480011, ""high"": 506.480011, ""low"": 506.480011, ""close"": 506.480011, ""adj_close"": 506.480011, ""volume"": 0.0, ""@type"": ""IndexDataOp"", ""@id"": ""IndexDataOp_340f944337294879b5803c7b592b1944""}
{""index"": ""NYA"", ""date"": ""1967-02-27"", ""open"": 501.089996, ""high"": 501.089996, ""low"": 501.089996, ""close"": 501.089996, ""adj_close"": 501.089996, ""volume"": 0.0, ""@type"": ""IndexDataOp"", ""@id"": ""IndexDataOp_4905bfca283b4e4c8fa8fb59bbfe10bc""}
{""index"": ""NYA"", ""date"": ""1967-02-28"", ""open"": 502.890015, ""high"": 502.890015, ""low"": 502.890015, ""close"": 502.890015, ""adj_close"": 502.890015, ""volume"": 0.0, ""@type"": ""IndexDataOp"", ""@id"": ""IndexDataOp_45ad14a648e24e998dd1506b26812d82""}
{""index"": ""NYA"", ""date"": ""1967-03-01"", ""open"": 507.859985, ""high"": 507.859985, ""low"": 507.859985, ""close"": 507.859985, ""adj_close"": 507.859985, ""volume"": 0.0, ""@type"": ""IndexDataOp"", ""@id"": ""IndexDataOp_169bd2d888f34fd7962b1f4b77311ddd""}
{""index"": ""NYA"", ""date"": ""1967-03-02"", ""open"": 510.709991, ""high"": 510.709991, ""low"": 510.709991, ""close"": 510.709991, ""adj_close"": 510.709991, ""volume"": 0.0, ""@type"": ""IndexDataOp"", ""@id"": ""IndexDataOp_567b3c6178ea45169c82c110311af628""}
{""index"": ""NYA"", ""date"": ""1967-03-03"", ""open"": 511.450012, ""high"": 511.450012, ""low"": 511.450012, ""close"": 511.450012, ""adj_close"": 511.450012, ""volume"": 0.0, ""@type"": ""IndexDataOp"", ""@id"": ""IndexDataOp_b901176582bc4124a8f4dfe85e3f6e4c""}
{""index"": ""NYA"", ""date"": ""1967-03-06"", ""open"": 510.709991, ""high"": 510.709991, ""low"": 510.709991, ""close"": 510.709991, ""adj_close"": 510.709991, ""volume"": 0.0, ""@type"": ""IndexDataOp"", ""@id"": ""IndexDataOp_9032e62b60854c0ca10de0c3faffd565""}
{""index"": ""NYA"", ""date"": ""1967-03-07"", ""open"": 511.350006, ""high"": 511.350006, ""low"": 511.350006, ""close"": 511.350006, ""adj_close"": 511.350006, ""volume"": 0.0, ""@type"": ""IndexDataOp"", ""@id"": ""IndexDataOp_76dc8e44f64540eebab6717166ca22a2""}
{""index"": ""NYA"", ""date"": ""1967-03-08"", ""open"": 512.090027, ""high"": 512.090027, ""low"": 512.090027, ""close"": 512.090027, ""adj_close"": 512.090027, ""volume"": 0.0, ""@type"": ""IndexDataOp"", ""@id"": ""IndexDataOp_1590c7129b8d4e96adf3fad0c25f7741""}
{""index"": ""NYA"", ""date"": ""1967-03-09"", ""open"": 513.570007, ""high"": 513.570007, ""low"": 513.570007, ""close"": 513.570007, ""adj_close"": 513.570007, ""volume"": 0.0, ""@type"": ""IndexDataOp"", ""@id"": ""IndexDataOp_27866b6fcba04c32b490d5155849a043""}
EOF
HTTP/1.1 400 Bad Request
connection: keep-alive
content-length: 612
content-type: application/json
date: Thu, 02 Sep 2021 11:54:42 GMT

{
    ""@type"": ""api:InsertDocumentErrorResponse"",
    ""api:error"": {
        ""@type"": ""api:BadCast"",
        ""api:document"": {
            ""@id"": ""IndexDataOp_ac69bef9dcc449f1b832955a0fb352d3"",
            ""@type"": ""IndexDataOp"",
            ""adj_close"": 435.73999,
            ""close"": 435.73999,
            ""date"": ""1966-10-14"",
            ""high"": 435.73999,
            ""index"": ""NYA"",
            ""low"": 435.73999,
            ""open"": 435.73999,
            ""volume"": 0.0
        },
        ""api:type"": ""http://www.w3.org/2001/XMLSchema#integer"",
        ""api:value"": 0.0
    },
    ""api:message"": ""value 0.0 could not be casted to a 'http://www.w3.org/2001/XMLSchema#integer'"",
    ""api:status"": ""api:failure""
}","i could not reproduce the problem even with the entire set of datum give # /bin / bash set -ex # create the database xh ' http//adminroot@localhost6363 / api / db / admin / t ' < < eof { "" label""""l""""comment""""c "" } eof # create the schema xh ' http//adminroot@localhost6363 / api / document / admin / tauthor = a&message = schema&graph_type = schema ' < < eof { "" @type "" "" class "" "" @id "" "" indexdataop "" "" @key "" { "" @type "" "" random "" } "" adj_close "" { "" @type "" "" optional "" "" @class "" "" xsdfloat "" } "" close "" { "" @type "" "" optional "" "" @class "" "" xsdfloat "" } "" date "" { "" @type "" "" optional "" "" @class "" "" xsdstring "" } "" high "" { "" @type "" "" optional "" "" @class "" "" xsdfloat "" } "" index "" { "" @type "" "" optional "" "" @class "" "" xsdstring "" } "" low "" { "" @type "" "" optional "" "" @class "" "" xsdfloat "" } "" open "" { "" @type "" "" optional "" "" @class "" "" xsdfloat "" } "" volume "" { "" @type "" "" optional "" "" @class "" "" xsdinteger "" } } eof # post instance xh ' http//adminroot@localhost6363 / api / document / admin / tauthor = a&message = instance ' < < eof { "" index "" "" nya "" "" date "" "" 1966 - 10 - 14 "" "" open "" 43573999 "" high "" 43573999 "" low "" 43573999 "" close "" 43573999 "" adj_close "" 43573999 "" volume "" 00 "" @type "" "" indexdataop "" "" @id "" "" indexdataop_ac69bef9dcc449f1b832955a0fb352d3 "" } { "" index "" "" nya "" "" date "" "" 1966 - 10 - 17 "" "" open "" 440609985 "" high "" 440609985 "" low "" 440609985 "" close "" 440609985 "" adj_close "" 440609985 "" volume "" 00 "" @type "" "" indexdataop "" "" @id "" "" indexdataop_c03f34b77c894c54a88231f1f3f019b4 "" } { "" index "" "" nya "" "" date "" "" 1966 - 10 - 18 "" "" open "" 447799988 "" high "" 447799988 "" low "" 447799988 "" close "" 447799988 "" adj_close "" 447799988 "" volume "" 00 "" @type "" "" indexdataop "" "" @id "" "" indexdataop_7e19232b70b44b2397821064430bab2a "" } { "" index "" "" nya "" "" date "" "" 1966 - 10 - 19 "" "" open "" 444519989 "" high "" 444519989 "" low "" 444519989 "" close "" 444519989 "" adj_close "" 444519989 "" volume "" 00 "" @type "" "" indexdataop "" "" @id "" "" indexdataop_58b492ab7259449aacab228c5c9ff1c7 "" } { "" index "" "" nya "" "" date "" "" 1966 - 10 - 20 "" "" open "" 44325 "" high "" 44325 "" low "" 44325 "" close "" 44325 "" adj_close "" 44325 "" volume "" 00 "" @type "" "" indexdataop "" "" @id "" "" indexdataop_7714503df8f244afbb2533ff3d83a1a6 "" } { "" index "" "" nya "" "" date "" "" 1966 - 10 - 21 "" "" open "" 444839996 "" high "" 444839996 "" low "" 444839996 "" close "" 444839996 "" adj_close "" 444839996 "" volume "" 00 "" @type "" "" indexdataop "" "" @id "" "" indexdataop_c5e9bbb5f24a4edf8e74f28b1c3eff0e "" } { "" index "" "" nya "" "" date "" "" 1966 - 10 - 24 "" "" open "" 4460 "" high "" 4460 "" low "" 4460 "" close "" 4460 "" adj_close "" 4460 "" volume "" 00 "" @type "" "" indexdataop "" "" @id "" "" indexdataop_41bfb1e914ad4a41b21c4e611a172079 "" } { "" index "" "" nya "" "" date "" "" 1966 - 10 - 25 "" "" open "" 448429993 "" high "" 448429993 "" low "" 448429993 "" close "" 448429993 "" adj_close "" 448429993 "" volume "" 00 "" @type "" "" indexdataop "" "" @id "" "" indexdataop_0f1318ae6c8a4b81ace6b97652e2a13b "" } { "" index "" "" nya "" "" date "" "" 1966 - 10 - 26 "" "" open "" 452450012 "" high "" 452450012 "" low "" 452450012 "" close "" 452450012 "" adj_close "" 452450012 "" volume "" 00 "" @type "" "" indexdataop "" "" @id "" "" indexdataop_e4a1ec76f4c44f73b80e947f40f6d88d "" } { "" index "" "" nya "" "" date "" "" 1966 - 10 - 27 "" "" open "" 45626001 "" high "" 45626001 "" low "" 45626001 "" close "" 45626001 "" adj_close "" 45626001 "" volume "" 00 "" @type "" "" indexdataop "" "" @id "" "" indexdataop_7de0599ee12948039b72634b0fef4533 "" } { "" index "" "" nya "" "" date "" "" 1966 - 10 - 28 "" "" open "" 456470001 "" high "" 456470001 "" low "" 456470001 "" close "" 456470001 "" adj_close "" 456470001 "" volume "" 00 "" @type "" "" indexdataop "" "" @id "" "" indexdataop_1629239709354f0abdb39e5c8594e541 "" } { "" index "" "" nya "" "" date "" "" 1966 - 10 - 31 "" "" open "" 456359985 "" high "" 456359985 "" low "" 456359985 "" close "" 456359985 "" adj_close "" 456359985 "" volume "" 00 "" @type "" "" indexdataop "" "" @id "" "" indexdataop_3fccf298aae441608e96ec5f27ac0a9a "" } { "" index "" "" nya "" "" date "" "" 1966 - 11 - 01 "" "" open "" 459850006 "" high "" 459850006 "" low "" 459850006 "" close "" 459850006 "" adj_close "" 459850006 "" volume "" 00 "" @type "" "" indexdataop "" "" @id "" "" indexdataop_b4f774c9ee7c454282b4779d7dfcb769 "" } { "" index "" "" nya "" "" date "" "" 1966 - 11 - 02 "" "" open "" 460380005 "" high "" 460380005 "" low "" 460380005 "" close "" 460380005 "" adj_close "" 460380005 "" volume "" 00 "" @type "" "" indexdataop "" "" @id "" "" indexdataop_422de280413947349b3fc42e4daaed31 "" } { "" index "" "" nya "" "" date "" "" 1966 - 11 - 03 "" "" open "" 458790009 "" high "" 458790009 "" low "" 458790009 "" close "" 458790009 "" adj_close "" 458790009 "" volume "" 00 "" @type "" "" indexdataop "" "" @id "" "" indexdataop_d9f5370cdb994c46b627cd21133cc7e4 "" } { "" index "" "" nya "" "" date "" "" 1966 - 11 - 04 "" "" open "" 460269989 "" high "" 460269989 "" low "" 460269989 "" close "" 460269989 "" adj_close "" 460269989 "" volume "" 00 "" @type "" "" indexdataop "" "" @id "" "" indexdataop_7a4722f73e4542a5ba935cbe7f452f20 "" } { "" index "" "" nya "" "" date "" "" 1966 - 11 - 07 "" "" open "" 460170013 "" high "" 460170013 "" low "" 460170013 "" close "" 460170013 "" adj_close "" 460170013 "" volume "" 00 "" @type "" "" indexdataop "" "" @id "" "" indexdataop_2f29143c293443beafd861482958cdc4 "" } { "" index "" "" nya "" "" date "" "" 1966 - 11 - 09 "" "" open "" 46376001 "" high "" 46376001 "" low "" 46376001 "" close "" 46376001 "" adj_close "" 46376001 "" volume "" 00 "" @type "" "" indexdataop "" "" @id "" "" indexdataop_598c12211f5944a2ad97702e3d5a76ff "" } { "" index "" "" nya "" "" date "" "" 1966 - 11 - 10 "" "" open "" 466829987 "" high "" 466829987 "" low "" 466829987 "" close "" 466829987 "" adj_close "" 466829987 "" volume "" 00 "" @type "" "" indexdataop "" "" @id "" "" indexdataop_b90bc27f854b4553b81780ef05985306 "" } { "" index "" "" nya "" "" date "" "" 1966 - 11 - 11 "" "" open "" 46725 "" high "" 46725 "" low "" 46725 "" close "" 46725 "" adj_close "" 46725 "" volume "" 00 "" @type "" "" indexdataop "" "" @id "" "" indexdataop_d7bf9c68bb604c309541d1f5e45f4e6c "" } { "" index "" "" nya "" "" date "" "" 1966 - 11 - 14 "" "" open "" 464399994 "" high "" 464399994 "" low "" 464399994 "" close "" 464399994 "" adj_close "" 464399994 "" volume "" 00 "" @type "" "" indexdataop "" "" @id "" "" indexdataop_27c0c637dba3478abc8e06aeb2f1de13 "" } { "" index "" "" nya "" "" date "" "" 1966 - 11 - 15 "" "" open "" 466089996 "" high "" 466089996 "" low "" 466089996 "" close "" 466089996 "" adj_close "" 466089996 "" volume "" 00 "" @type "" "" indexdataop "" "" @id "" "" indexdataop_7e229ec3335f48e5af5b0babf6202674 "" } { "" index "" "" nya "" "" date "" "" 1966 - 11 - 16 "" "" open "" 470109985 "" high "" 470109985 "" low "" 470109985 "" close "" 470109985 "" adj_close "" 470109985 "" volume "" 00 "" @type "" "" indexdataop "" "" @id "" "" indexdataop_196ee5c117c94196a6c86d955ac1825b "" } { "" index "" "" nya "" "" date "" "" 1966 - 11 - 17 "" "" open "" 466940002 "" high "" 466940002 "" low "" 466940002 "" close "" 466940002 "" adj_close "" 466940002 "" volume "" 00 "" @type "" "" indexdataop "" "" @id "" "" indexdataop_47104cbdfa9c42b0a82885e1fadb691a "" } { "" index "" "" nya "" "" date "" "" 1966 - 11 - 18 "" "" open "" 464079987 "" high "" 464079987 "" low "" 464079987 "" close "" 464079987 "" adj_close "" 464079987 "" volume "" 00 "" @type "" "" indexdataop "" "" @id "" "" indexdataop_992a7fbdc05341ec9a2cb849893bfedd "" } { "" index "" "" nya "" "" date "" "" 1966 - 11 - 21 "" "" open "" 457630005 "" high "" 457630005 "" low "" 457630005 "" close "" 457630005 "" adj_close "" 457630005 "" volume "" 00 "" @type "" "" indexdataop "" "" @id "" "" indexdataop_9eebb4f30617495d9b13f72db8039c55 "" } { "" index "" "" nya "" "" date "" "" 1966 - 11 - 22 "" "" open "" 455519989 "" high "" 455519989 "" low "" 455519989 "" close "" 455519989 "" adj_close "" 455519989 "" volume "" 00 "" @type "" "" indexdataop "" "" @id "" "" indexdataop_6befb11eb46a4d8f8cdf5e1f14e0bd7d "" } { "" index "" "" nya "" "" date "" "" 1966 - 11 - 23 "" "" open "" 458899994 "" high "" 458899994 "" low "" 458899994 "" close "" 458899994 "" adj_close "" 458899994 "" volume "" 00 "" @type "" "" indexdataop "" "" @id "" "" indexdataop_f436b7462fc94d39ac24c1186565ed33 "" } { "" index "" "" nya "" "" date "" "" 1966 - 11 - 25 "" "" open "" 462600006 "" high "" 462600006 "" low "" 462600006 "" close "" 462600006 "" adj_close "" 462600006 "" volume "" 00 "" @type "" "" indexdataop "" "" @id "" "" indexdataop_3ed857eeaaf04c9c8ed5520e4b683d44 "" } { "" index "" "" nya "" "" date "" "" 1966 - 11 - 28 "" "" open "" 462179993 "" high "" 462179993 "" low "" 462179993 "" close "" 462179993 "" adj_close "" 462179993 "" volume "" 00 "" @type "" "" indexdataop "" "" @id "" "" indexdataop_be6cd17a1acc4c99be615447a4cd9a72 "" } { "" index "" "" nya "" "" date "" "" 1966 - 11 - 29 "" "" open "" 460589996 "" high "" 460589996 "" low "" 460589996 "" close "" 460589996 "" adj_close "" 460589996 "" volume "" 00 "" @type "" "" indexdataop "" "" @id "" "" indexdataop_96627271d0a847819ff3f493cb537612 "" } { "" index "" "" nya "" "" date "" "" 1966 - 11 - 30 "" "" open "" 460910004 "" high "" 460910004 "" low "" 460910004 "" close "" 460910004 "" adj_close "" 460910004 "" volume "" 00 "" @type "" "" indexdataop "" "" @id "" "" indexdataop_09ff579a912d4804a8a128fbfd608a63 "" } { "" index "" "" nya "" "" date "" "" 1966 - 12 - 01 "" "" open "" 459109985 "" high "" 459109985 "" low "" 459109985 "" close "" 459109985 "" adj_close "" 459109985 "" volume "" 00 "" @type "" "" indexdataop "" "" @id "" "" indexdataop_13f58d6daaee463b80164c1e9d8603cd "" } { "" index "" "" nya "" "" date "" "" 1966 - 12 - 02 "" "" open "" 459320007 "" high "" 459320007 "" low "" 459320007 "" close "" 459320007 "" adj_close "" 459320007 "" volume "" 00 "" @type "" "" indexdataop "" "" @id "" "" indexdataop_bad15ef435da4d0088c690a26d2789b9 "" } { "" index "" "" nya "" "" date "" "" 1966 - 12 - 05 "" "" open "" 45975 "" high "" 45975 "" low "" 45975 "" close "" 45975 "" adj_close "" 45975 "" volume "" 00 "" @type "" "" indexdataop "" "" @id "" "" indexdataop_b87b7ea4cf6a4d3f93803f427665a7ed "" } { "" index "" "" nya "" "" date "" "" 1966 - 12 - 06 "" "" open "" 462920013 "" high "" 462920013 "" low "" 462920013 "" close "" 462920013 "" adj_close "" 462920013 "" volume "" 00 "" @type "" "" indexdataop "" "" @id "" "" indexdataop_e2c359288ab140fbb7fe3389c4d9cf48 "" } { "" index "" "" nya "" "" date "" "" 1966 - 12 - 07 "" "" open "" 467570007 "" high "" 467570007 "" low "" 467570007 "" close "" 467570007 "" adj_close "" 467570007 "" volume "" 00 "" @type "" "" indexdataop "" "" @id "" "" indexdataop_cc7f06ae482146dcbbc12d44ef668a04 "" } { "" index "" "" nya "" "" date "" "" 1966 - 12 - 08 "" "" open "" 469470001 "" high "" 469470001 "" low "" 469470001 "" close "" 469470001 "" adj_close "" 469470001 "" volume "" 00 "" @type "" "" indexdataop "" "" @id "" "" indexdataop_4eaff3fb22014b378026b5fa18998d5f "" } { "" index "" "" nya "" "" date "" "" 1966 - 12 - 09 "" "" open "" 470529999 "" high "" 470529999 "" low "" 470529999 "" close "" 470529999 "" adj_close "" 470529999 "" volume "" 00 "" @type "" "" indexdataop "" "" @id "" "" indexdataop_cdac45a2debd4cbebda0f6193b77a6fa "" } { "" index "" "" nya "" "" date "" "" 1966 - 12 - 12 "" "" open "" 475079987 "" high "" 475079987 "" low "" 475079987 "" close "" 475079987 "" adj_close "" 475079987 "" volume "" 00 "" @type "" "" indexdataop "" "" @id "" "" indexdataop_48bbdba89bc046f78cd4852e8ce8af62 "" } { "" index "" "" nya "" "" date "" "" 1966 - 12 - 13 "" "" open "" 473809998 "" high "" 473809998 "" low "" 473809998 "" close "" 473809998 "" adj_close "" 473809998 "" volume "" 00 "" @type "" "" indexdataop "" "" @id "" "" indexdataop_8b09bfc77ba24dd2b449c8c4a1ad9e23 "" } { "" index "" "" nya "" "" date "" "" 1966 - 12 - 14 "" "" open "" 473700012 "" high "" 473700012 "" low "" 473700012 "" close "" 473700012 "" adj_close "" 473700012 "" volume "" 00 "" @type "" "" indexdataop "" "" @id "" "" indexdataop_b842ca919652455ba6640b96afe9a7da "" } { "" index "" "" nya "" "" date "" "" 1966 - 12 - 15 "" "" open "" 468730011 "" high "" 468730011 "" low "" 468730011 "" close "" 468730011 "" adj_close "" 468730011 "" volume "" 00 "" @type "" "" indexdataop "" "" @id "" "" indexdataop_a1ea9fe9f02b486abea545b0ddedc802 "" } { "" index "" "" nya "" "" date "" "" 1966 - 12 - 16 "" "" open "" 468839996 "" high "" 468839996 "" low "" 468839996 "" close "" 468839996 "" adj_close "" 468839996 "" volume "" 00 "" @type "" "" indexdataop "" "" @id "" "" indexdataop_4fdc373cc99a4ea0b65e187f5310f5f7 "" } { "" index "" "" nya "" "" date "" "" 1966 - 12 - 19 "" "" open "" 467570007 "" high "" 467570007 "" low "" 467570007 "" close "" 467570007 "" adj_close "" 467570007 "" volume "" 00 "" @type "" "" indexdataop "" "" @id "" "" indexdataop_69387d1fef65455c9078fafc6d3bc003 "" } { "" index "" "" nya "" "" date "" "" 1966 - 12 - 20 "" "" open "" 465670013 "" high "" 465670013 "" low "" 465670013 "" close "" 465670013 "" adj_close "" 465670013 "" volume "" 00 "" @type "" "" indexdataop "" "" @id "" "" indexdataop_3a9c802396b24a8fa0c6fabfe101b9c8 "" } { "" index "" "" nya "" "" date "" "" 1966 - 12 - 21 "" "" open "" 468200012 "" high "" 468200012 "" low "" 468200012 "" close "" 468200012 "" adj_close "" 468200012 "" volume "" 00 "" @type "" "" indexdataop "" "" @id "" "" indexdataop_763800a0ba9d4b55aefdb61c7ade78c9 "" } { "" index "" "" nya "" "" date "" "" 1966 - 12 - 22 "" "" open "" 470109985 "" high "" 470109985 "" low "" 470109985 "" close "" 470109985 "" adj_close "" 470109985 "" volume "" 00 "" @type "" "" indexdataop "" "" @id "" "" indexdataop_a003681eb1114b468c7614429bad2cd8 "" } { "" index "" "" nya "" "" date "" "" 1966 - 12 - 23 "" "" open "" 468940002 "" high "" 468940002 "" low "" 468940002 "" close "" 468940002 "" adj_close "" 468940002 "" volume "" 00 "" @type "" "" indexdataop "" "" @id "" "" indexdataop_756f96892356400e9a5435d2c75d0f14 "" } { "" index "" "" nya "" "" date "" "" 1966 - 12 - 27 "" "" open "" 46651001 "" high "" 46651001 "" low "" 46651001 "" close "" 46651001 "" adj_close "" 46651001 "" volume "" 00 "" @type "" "" indexdataop "" "" @id "" "" indexdataop_e862ffdded764179aa1796c6146a248d "" } { "" index "" "" nya "" "" date "" "" 1966 - 12 - 28 "" "" open "" 464190002 "" high "" 464190002 "" low "" 464190002 "" close "" 464190002 "" adj_close "" 464190002 "" volume "" 00 "" @type "" "" indexdataop "" "" @id "" "" indexdataop_62f9fe7072fc43518c3259e8e41c1102 "" } { "" index "" "" nya "" "" date "" "" 1966 - 12 - 29 "" "" open "" 46248999 "" high "" 46248999 "" low "" 46248999 "" close "" 46248999 "" adj_close "" 46248999 "" volume "" 00 "" @type "" "" indexdataop "" "" @id "" "" indexdataop_e45a7d3a48ab4d7298c9f30547a55189 "" } { "" index "" "" nya "" "" date "" "" 1966 - 12 - 30 "" "" open "" 462279999 "" high "" 462279999 "" low "" 462279999 "" close "" 462279999 "" adj_close "" 462279999 "" volume "" 00 "" @type "" "" indexdataop "" "" @id "" "" indexdataop_5c0cc58c00dd4e72b11d42d8bb52aa55 "" } { "" index "" "" nya "" "" date "" "" 1967 - 01 - 03 "" "" open "" 46248999 "" high "" 46248999 "" low "" 46248999 "" close "" 46248999 "" adj_close "" 46248999 "" volume "" 00 "" @type "" "" indexdataop "" "" @id "" "" indexdataop_b8224579041542a5b588d23bd725561b "" } { "" index "" "" nya "" "" date "" "" 1967 - 01 - 04 "" "" open "" 463339996 "" high "" 463339996 "" low "" 463339996 "" close "" 463339996 "" adj_close "" 463339996 "" volume "" 00 "" @type "" "" indexdataop "" "" @id "" "" indexdataop_4908ee25f6cb4c6cbcf1429e78024153 "" } { "" index "" "" nya "" "" date "" "" 1967 - 01 - 05 "" "" open "" 46926001 "" high "" 46926001 "" low "" 46926001 "" close "" 46926001 "" adj_close "" 46926001 "" volume "" 00 "" @type "" "" indexdataop "" "" @id "" "" indexdataop_36aff27b24c84551939a81b41d5078b5 "" } { "" index "" "" nya "" "" date "" "" 1967 - 01 - 06 "" "" open "" 47275 "" high "" 47275 "" low "" 47275 "" close "" 47275 "" adj_close "" 47275 "" volume "" 00 "" @type "" "" indexdataop "" "" @id "" "" indexdataop_b0fa38527a934e6baa2ca2e154173da9 "" } { "" index "" "" nya "" "" date "" "" 1967 - 01 - 09 "" "" open "" 476660004 "" high "" 476660004 "" low "" 476660004 "" close "" 476660004 "" adj_close "" 476660004 "" volume "" 00 "" @type "" "" indexdataop "" "" @id "" "" indexdataop_ea6a640d367340c6915daf0fa656b1ca "" } { "" index "" "" nya "" "" date "" "" 1967 - 01 - 10 "" "" open "" 476660004 "" high "" 476660004 "" low "" 476660004 "" close "" 476660004 "" adj_close "" 476660004 "" volume "" 00 "" @type "" "" indexdataop "" "" @id "" "" indexdataop_d1ccdefd919f479eb3e676a16263f85d "" } { "" index "" "" nya "" "" date "" "" 1967 - 01 - 11 "" "" open "" 480579987 "" high "" 480579987 "" low "" 480579987 "" close "" 480579987 "" adj_close "" 480579987 "" volume "" 00 "" @type "" "" indexdataop "" "" @id "" "" indexdataop_55f7b42365f34d20b24d762c8fd9fa79 "" } { "" index "" "" nya "" "" date "" "" 1967 - 01 - 12 "" "" open "" 483429993 "" high "" 483429993 "" low "" 483429993 "" close "" 483429993 "" adj_close "" 483429993 "" volume "" 00 "" @type "" "" indexdataop "" "" @id "" "" indexdataop_2495360e59a44e17b527592f525cedc2 "" } { "" index "" "" nya "" "" date "" "" 1967 - 01 - 13 "" "" open "" 486809998 "" high "" 486809998 "" low "" 486809998 "" close "" 486809998 "" adj_close "" 486809998 "" volume "" 00 "" @type "" "" indexdataop "" "" @id "" "" indexdataop_c4b69088c4274358ab51d8fb6ec37a38 "" } { "" index "" "" nya "" "" date "" "" 1967 - 01 - 16 "" "" open "" 485970001 "" high "" 485970001 "" low "" 485970001 "" close "" 485970001 "" adj_close "" 485970001 "" volume "" 00 "" @type "" "" indexdataop "" "" @id "" "" indexdataop_19902d23c7bf42a8a6cf80a469228aa8 "" } { "" index "" "" nya "" "" date "" "" 1967 - 01 - 17 "" "" open "" 49126001 "" high "" 49126001 "" low "" 49126001 "" close "" 49126001 "" adj_close "" 49126001 "" volume "" 00 "" @type "" "" indexdataop "" "" @id "" "" indexdataop_a338832557304a0082d653d53e52e44b "" } { "" index "" "" nya "" "" date "" "" 1967 - 01 - 18 "" "" open "" 4940 "" high "" 4940 "" low "" 4940 "" close "" 4940 "" adj_close "" 4940 "" volume "" 00 "" @type "" "" indexdataop "" "" @id "" "" indexdataop_1a66efbf81c147bba5efec461328f6f2 "" } { "" index "" "" nya "" "" date "" "" 1967 - 01 - 19 "" "" open "" 494220001 "" high "" 494220001 "" low "" 494220001 "" close "" 494220001 "" adj_close "" 494220001 "" volume "" 00 "" @type "" "" indexdataop "" "" @id "" "" indexdataop_7291ec3968bd4daa9c9ae21c6c817896 "" } { "" index "" "" nya "" "" date "" "" 1967 - 01 - 20 "" "" open "" 49601001 "" high "" 49601001 "" low "" 49601001 "" close "" 49601001 "" adj_close "" 49601001 "" volume "" 00 "" @type "" "" indexdataop "" "" @id "" "" indexdataop_188ebed56d5742e3a9bebb820513278c "" } { "" index "" "" nya "" "" date "" "" 1967 - 01 - 23 "" "" open "" 497809998 "" high "" 497809998 "" low "" 497809998 "" close "" 497809998 "" adj_close "" 497809998 "" volume "" 00 "" @type "" "" indexdataop "" "" @id "" "" indexdataop_e676d68d055a4d3a9151ebc6668d2ee3 "" } { "" index "" "" nya "" "" date "" "" 1967 - 01 - 24 "" "" open "" 49876001 "" high "" 49876001 "" low "" 49876001 "" close "" 49876001 "" adj_close "" 49876001 "" volume "" 00 "" @type "" "" indexdataop "" "" @id "" "" indexdataop_c1c6a375d4ce42afa982e620c9a4ec3b "" } { "" index "" "" nya "" "" date "" "" 1967 - 01 - 25 "" "" open "" 495380005 "" high "" 495380005 "" low "" 495380005 "" close "" 495380005 "" adj_close "" 495380005 "" volume "" 00 "" @type "" "" indexdataop "" "" @id "" "" indexdataop_9351ed7bb23842abb45b91e828c55e2a "" } { "" index "" "" nya "" "" date "" "" 1967 - 01 - 26 "" "" open "" 495700012 "" high "" 495700012 "" low "" 495700012 "" close "" 495700012 "" adj_close "" 495700012 "" volume "" 00 "" @type "" "" indexdataop "" "" @id "" "" indexdataop_cc260c71c62943e9bb7f4576f2f719c7 "" } { "" index "" "" nya "" "" date "" "" 1967 - 01 - 27 "" "" open "" 497809998 "" high "" 497809998 "" low "" 497809998 "" close "" 497809998 "" adj_close "" 497809998 "" volume "" 00 "" @type "" "" indexdataop "" "" @id "" "" indexdataop_a5d780fcdb3048249d018f26ec7476a8 "" } { "" index "" "" nya "" "" date "" "" 1967 - 01 - 30 "" "" open "" 500670013 "" high "" 500670013 "" low "" 500670013 "" close "" 500670013 "" adj_close "" 500670013 "" volume "" 00 "" @type "" "" indexdataop "" "" @id "" "" indexdataop_57aea50b6cb5413dbac7f0c09883e24f "" } { "" index "" "" nya "" "" date "" "" 1967 - 01 - 31 "" "" open "" 500140015 "" high "" 500140015 "" low "" 500140015 "" close "" 500140015 "" adj_close "" 500140015 "" volume "" 00 "" @type "" "" indexdataop "" "" @id "" "" indexdataop_2c76c472ce8d42098d837b9e38d03b45 "" } { "" index "" "" nya "" "" date "" "" 1967 - 02 - 01 "" "" open "" 4995 "" high "" 4995 "" low "" 4995 "" close "" 4995 "" adj_close "" 4995 "" volume "" 00 "" @type "" "" indexdataop "" "" @id "" "" indexdataop_6c5630d97c9648edae86375a33464765 "" } { "" index "" "" nya "" "" date "" "" 1967 - 02 - 02 "" "" open "" 501089996 "" high "" 501089996 "" low "" 501089996 "" close "" 501089996 "" adj_close "" 501089996 "" volume "" 00 "" @type "" "" indexdataop "" "" @id "" "" indexdataop_286f4582c20a4da5aa5adf08a3cd6458 "" } { "" index "" "" nya "" "" date "" "" 1967 - 02 - 03 "" "" open "" 504679993 "" high "" 504679993 "" low "" 504679993 "" close "" 504679993 "" adj_close "" 504679993 "" volume "" 00 "" @type "" "" indexdataop "" "" @id "" "" indexdataop_2f842d7ddf084fc395b37eac60f3b27d "" } { "" index "" "" nya "" "" date "" "" 1967 - 02 - 06 "" "" open "" 503730011 "" high "" 503730011 "" low "" 503730011 "" close "" 503730011 "" adj_close "" 503730011 "" volume "" 00 "" @type "" "" indexdataop "" "" @id "" "" indexdataop_eecbc521e9504d35b9edd65cb29ca668 "" } { "" index "" "" nya "" "" date "" "" 1967 - 02 - 07 "" "" open "" 502679993 "" high "" 502679993 "" low "" 502679993 "" close "" 502679993 "" adj_close "" 502679993 "" volume "" 00 "" @type "" "" indexdataop "" "" @id "" "" indexdataop_7c357f0bba9b40fe9a280123bca76883 "" } { "" index "" "" nya "" "" date "" "" 1967 - 02 - 08 "" "" open "" 507119995 "" high "" 507119995 "" low "" 507119995 "" close "" 507119995 "" adj_close "" 507119995 "" volume "" 00 "" @type "" "" indexdataop "" "" @id "" "" indexdataop_6f1672bf7ce94f1eb5453ee42f25d289 "" } { "" index "" "" nya "" "" date "" "" 1967 - 02 - 09 "" "" open "" 505109985 "" high "" 505109985 "" low "" 505109985 "" close "" 505109985 "" adj_close "" 505109985 "" volume "" 00 "" @type "" "" indexdataop "" "" @id "" "" indexdataop_d3676d3881b242f6ba37676b339da629 "" } { "" index "" "" nya "" "" date "" "" 1967 - 02 - 10 "" "" open "" 506160004 "" high "" 506160004 "" low "" 506160004 "" close "" 506160004 "" adj_close "" 506160004 "" volume "" 00 "" @type "" "" indexdataop "" "" @id "" "" indexdataop_b0d83942f30b43209b4cd35eda075a5a "" } { "" index "" "" nya "" "" date "" "" 1967 - 02 - 13 "" "" open "" 506899994 "" high "" 506899994 "" low "" 506899994 "" close "" 506899994 "" adj_close "" 506899994 "" volume "" 00 "" @type "" "" indexdataop "" "" @id "" "" indexdataop_76237c709f0a47088dced65b3219c489 "" } { "" index "" "" nya "" "" date "" "" 1967 - 02 - 14 "" "" open "" 510290009 "" high "" 510290009 "" low "" 510290009 "" close "" 510290009 "" adj_close "" 510290009 "" volume "" 00 "" @type "" "" indexdataop "" "" @id "" "" indexdataop_fa9acffbd77d4d49bddb7aabd3832f1b "" } { "" index "" "" nya "" "" date "" "" 1967 - 02 - 15 "" "" open "" 510609985 "" high "" 510609985 "" low "" 510609985 "" close "" 510609985 "" adj_close "" 510609985 "" volume "" 00 "" @type "" "" indexdataop "" "" @id "" "" indexdataop_37ad9f23e08d4c678fec82c84855338e "" } { "" index "" "" nya "" "" date "" "" 1967 - 02 - 16 "" "" open "" 508380005 "" high "" 508380005 "" low "" 508380005 "" close "" 508380005 "" adj_close "" 508380005 "" volume "" 00 "" @type "" "" indexdataop "" "" @id "" "" indexdataop_03aeb54fef0a4c60a86b67f8be97d0c8 "" } { "" index "" "" nya "" "" date "" "" 1967 - 02 - 17 "" "" open "" 50848999 "" high "" 50848999 "" low "" 50848999 "" close "" 50848999 "" adj_close "" 50848999 "" volume "" 00 "" @type "" "" indexdataop "" "" @id "" "" indexdataop_67febd829f0a43a1a3932e3c2b8e255c "" } { "" index "" "" nya "" "" date "" "" 1967 - 02 - 20 "" "" open "" 505850006 "" high "" 505850006 "" low "" 505850006 "" close "" 505850006 "" adj_close "" 505850006 "" volume "" 00 "" @type "" "" indexdataop "" "" @id "" "" indexdataop_6b55825311e54d048dfe9d6aaa37acfc "" } { "" index "" "" nya "" "" date "" "" 1967 - 02 - 21 "" "" open "" 505850006 "" high "" 505850006 "" low "" 505850006 "" close "" 505850006 "" adj_close "" 505850006 "" volume "" 00 "" @type "" "" indexdataop "" "" @id "" "" indexdataop_6165260771b2448ab65e75ca2e7ad39a "" } { "" index "" "" nya "" "" date "" "" 1967 - 02 - 23 "" "" @type "" "" indexdataop "" "" @id "" "" indexdataop_a4d281ab44e143c8aef0a398cf615ec8 "" } { "" index "" "" nya "" "" date "" "" 1967 - 02 - 24 "" "" open "" 506480011 "" high "" 506480011 "" low "" 506480011 "" close "" 506480011 "" adj_close "" 506480011 "" volume "" 00 "" @type "" "" indexdataop "" "" @id "" "" indexdataop_340f944337294879b5803c7b592b1944 "" } { "" index "" "" nya "" "" date "" "" 1967 - 02 - 27 "" "" open "" 501089996 "" high "" 501089996 "" low "" 501089996 "" close "" 501089996 "" adj_close "" 501089996 "" volume "" 00 "" @type "" "" indexdataop "" "" @id "" "" indexdataop_4905bfca283b4e4c8fa8fb59bbfe10bc "" } { "" index "" "" nya "" "" date "" "" 1967 - 02 - 28 "" "" open "" 502890015 "" high "" 502890015 "" low "" 502890015 "" close "" 502890015 "" adj_close "" 502890015 "" volume "" 00 "" @type "" "" indexdataop "" "" @id "" "" indexdataop_45ad14a648e24e998dd1506b26812d82 "" } { "" index "" "" nya "" "" date "" "" 1967 - 03 - 01 "" "" open "" 507859985 "" high "" 507859985 "" low "" 507859985 "" close "" 507859985 "" adj_close "" 507859985 "" volume "" 00 "" @type "" "" indexdataop "" "" @id "" "" indexdataop_169bd2d888f34fd7962b1f4b77311ddd "" } { "" index "" "" nya "" "" date "" "" 1967 - 03 - 02 "" "" open "" 510709991 "" high "" 510709991 "" low "" 510709991 "" close "" 510709991 "" adj_close "" 510709991 "" volume "" 00 "" @type "" "" indexdataop "" "" @id "" "" indexdataop_567b3c6178ea45169c82c110311af628 "" } { "" index "" "" nya "" "" date "" "" 1967 - 03 - 03 "" "" open "" 511450012 "" high "" 511450012 "" low "" 511450012 "" close "" 511450012 "" adj_close "" 511450012 "" volume "" 00 "" @type "" "" indexdataop "" "" @id "" "" indexdataop_b901176582bc4124a8f4dfe85e3f6e4c "" } { "" index "" "" nya "" "" date "" "" 1967 - 03 - 06 "" "" open "" 510709991 "" high "" 510709991 "" low "" 510709991 "" close "" 510709991 "" adj_close "" 510709991 "" volume "" 00 "" @type "" "" indexdataop "" "" @id "" "" indexdataop_9032e62b60854c0ca10de0c3faffd565 "" } { "" index "" "" nya "" "" date "" "" 1967 - 03 - 07 "" "" open "" 511350006 "" high "" 511350006 "" low "" 511350006 "" close "" 511350006 "" adj_close "" 511350006 "" volume "" 00 "" @type "" "" indexdataop "" "" @id "" "" indexdataop_76dc8e44f64540eebab6717166ca22a2 "" } { "" index "" "" nya "" "" date "" "" 1967 - 03 - 08 "" "" open "" 512090027 "" high "" 512090027 "" low "" 512090027 "" close "" 512090027 "" adj_close "" 512090027 "" volume "" 00 "" @type "" "" indexdataop "" "" @id "" "" indexdataop_1590c7129b8d4e96adf3fad0c25f7741 "" } { "" index "" "" nya "" "" date "" "" 1967 - 03 - 09 "" "" open "" 513570007 "" high "" 513570007 "" low "" 513570007 "" close "" 513570007 "" adj_close "" 513570007 "" volume "" 00 "" @type "" "" indexdataop "" "" @id "" "" indexdataop_27866b6fcba04c32b490d5155849a043 "" } eof http/11 400 bad request connection keep - alive content - length 612 content - type application / json date thu 02 sep 2021 115442 gmt { "" @type "" "" apiinsertdocumenterrorresponse "" "" apierror "" { "" @type "" "" apibadcast "" "" apidocument "" { "" @id "" "" indexdataop_ac69bef9dcc449f1b832955a0fb352d3 "" "" @type "" "" indexdataop "" "" adj_close "" 43573999 "" close "" 43573999 "" date "" "" 1966 - 10 - 14 "" "" high "" 43573999 "" index "" "" nya "" "" low "" 43573999 "" open "" 43573999 "" volume "" 00 } "" apitype "" "" http//wwww3org/2001 / xmlschema#integer "" "" apivalue "" 00 } "" apimessage "" "" value 00 could not be cast to a ' http//wwww3org/2001 / xmlschema#integer ' "" "" apistatus "" "" apifailure "" }",0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,472,2021-08-26T10:39:15Z,Cheukting,Git a stack trace as an error,https://github.com/terminusdb/terminusdb/issues/472,"I got this stack trace:
terminusdb_client.errors.DatabaseError: Error: casting_error(0.0,'http://www.w3.org/2001/XMLSchema#integer')
  [46] throw(error(casting_error(0.0,'http://www.w3.org/2001/XMLSchema#integer'),_69862))
  [44] catch(api_document:do_or_die(...,...),error(casting_error(0.0,'http://www.w3.org/2001/XMLSchema#integer'),_69920),api_document:(...;...)) at /usr/lib/swipl/boot/init.pl:533
  [42] api_document:api_insert_document_(instance,transaction_object{commit_info:commit_info{},descriptor:branch_descriptor{branch_name:""main"",repository_descriptor: ...},inference_objects:[],instance_objects:[...],parent:transaction_object{descriptor: ...,inference_objects:[],instance_objects: ...,parent: ...,schema_objects: ...},schema_objects:[...]},<stream>(0x5614c9102b00),_69980) at /app/terminusdb/src/core/api/api_document.pl:185
  [41] '$bags':findall_loop(_70096,'<garbage_collected>',_70100,[]) at /usr/lib/swipl/boot/bags.pl:99
  [40] setup_call_catcher_cleanup('$bags':'$new_findall_bag','$bags':findall_loop(_70154,...,_70158,[]),_70136,'$bags':'$destroy_findall_bag') at /usr/lib/swipl/boot/init.pl:614
  [36] '<meta-call>'('<garbage_collected>') <foreign>
  [35] database:with_transaction_(query_context{all_witnesses:false,authorization:'terminusdb://system/data/admin',bindings:[],commit_info:commit_info{author:admin,message:'Document object inserted by Python client.'},default_collection:branch_descriptor{branch_name:""main"",repository_descriptor: ...},files:[],filter:type_filter{types: ...},prefixes:_70322{'@base':""terminusdb:///data/"",'@schema':""terminusdb:///schema#"",'@type':'Context',api:'http://terminusdb.com/schema/api#',owl:'http://www.w3.org/2002/07/owl#',rdf:'http://www.w3.org/1999/02/22-rdf-syntax-ns#',rdfs:'http://www.w3.org/2000/01/rdf-schema#',sys:'http://terminusdb.com/schema/sys#',vio:'http://terminusdb.com/schema/vio#',woql:'http://terminusdb.com/schema/woql#',xdd:'http://terminusdb.com/schema/xdd#',xsd:'http://www.w3.org/2001/XMLSchema#'},selected:[],system:system_descriptor{},transaction_objects:[...],update_guard:_70278,write_graph:branch_graph{branch_name:""main"",database_name:""stock_exchange"",organization_name:""Community"",repository_name:""local"",type:instance}},api_document:(...;...),_70240) at /app/terminusdb/src/core/transaction/database.pl:220
  [34] setup_call_catcher_cleanup(database:true,database:with_transaction_(...,...,_70482),_70460,database:true) at /usr/lib/swipl/boot/init.pl:614
  [30] '<meta-call>'('<garbage_collected>') <foreign>
  [29] catch(routes:(...,...),error(casting_error(0.0,'http://www.w3.org/2001/XMLSchema#integer'),context(_70584,_70586)),routes:do_or_die(...,...)) at /usr/lib/swipl/boot/init.pl:532
  [28] catch_with_backtrace('<garbage_collected>','<garbage_collected>','<garbage_collected>') at /usr/lib/swipl/boot/init.pl:582

While insert document with this:
[00:16,  8.41s/it][{'index': 'NYA', 'date': '1966-10-14', 'open': 435.73999, 'high': 435.73999, 'low': 435.73999, 'close': 435.73999, 'adj_close': 435.73999, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_ac69bef9dcc449f1b832955a0fb352d3'}, {'index': 'NYA', 'date': '1966-10-17', 'open': 440.609985, 'high': 440.609985, 'low': 440.609985, 'close': 440.609985, 'adj_close': 440.609985, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_c03f34b77c894c54a88231f1f3f019b4'}, {'index': 'NYA', 'date': '1966-10-18', 'open': 447.799988, 'high': 447.799988, 'low': 447.799988, 'close': 447.799988, 'adj_close': 447.799988, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_7e19232b70b44b2397821064430bab2a'}, {'index': 'NYA', 'date': '1966-10-19', 'open': 444.519989, 'high': 444.519989, 'low': 444.519989, 'close': 444.519989, 'adj_close': 444.519989, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_58b492ab7259449aacab228c5c9ff1c7'}, {'index': 'NYA', 'date': '1966-10-20', 'open': 443.25, 'high': 443.25, 'low': 443.25, 'close': 443.25, 'adj_close': 443.25, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_7714503df8f244afbb2533ff3d83a1a6'}, {'index': 'NYA', 'date': '1966-10-21', 'open': 444.839996, 'high': 444.839996, 'low': 444.839996, 'close': 444.839996, 'adj_close': 444.839996, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_c5e9bbb5f24a4edf8e74f28b1c3eff0e'}, {'index': 'NYA', 'date': '1966-10-24', 'open': 446.0, 'high': 446.0, 'low': 446.0, 'close': 446.0, 'adj_close': 446.0, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_41bfb1e914ad4a41b21c4e611a172079'}, {'index': 'NYA', 'date': '1966-10-25', 'open': 448.429993, 'high': 448.429993, 'low': 448.429993, 'close': 448.429993, 'adj_close': 448.429993, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_0f1318ae6c8a4b81ace6b97652e2a13b'}, {'index': 'NYA', 'date': '1966-10-26', 'open': 452.450012, 'high': 452.450012, 'low': 452.450012, 'close': 452.450012, 'adj_close': 452.450012, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_e4a1ec76f4c44f73b80e947f40f6d88d'}, {'index': 'NYA', 'date': '1966-10-27', 'open': 456.26001, 'high': 456.26001, 'low': 456.26001, 'close': 456.26001, 'adj_close': 456.26001, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_7de0599ee12948039b72634b0fef4533'}, {'index': 'NYA', 'date': '1966-10-28', 'open': 456.470001, 'high': 456.470001, 'low': 456.470001, 'close': 456.470001, 'adj_close': 456.470001, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_1629239709354f0abdb39e5c8594e541'}, {'index': 'NYA', 'date': '1966-10-31', 'open': 456.359985, 'high': 456.359985, 'low': 456.359985, 'close': 456.359985, 'adj_close': 456.359985, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_3fccf298aae441608e96ec5f27ac0a9a'}, {'index': 'NYA', 'date': '1966-11-01', 'open': 459.850006, 'high': 459.850006, 'low': 459.850006, 'close': 459.850006, 'adj_close': 459.850006, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_b4f774c9ee7c454282b4779d7dfcb769'}, {'index': 'NYA', 'date': '1966-11-02', 'open': 460.380005, 'high': 460.380005, 'low': 460.380005, 'close': 460.380005, 'adj_close': 460.380005, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_422de280413947349b3fc42e4daaed31'}, {'index': 'NYA', 'date': '1966-11-03', 'open': 458.790009, 'high': 458.790009, 'low': 458.790009, 'close': 458.790009, 'adj_close': 458.790009, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_d9f5370cdb994c46b627cd21133cc7e4'}, {'index': 'NYA', 'date': '1966-11-04', 'open': 460.269989, 'high': 460.269989, 'low': 460.269989, 'close': 460.269989, 'adj_close': 460.269989, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_7a4722f73e4542a5ba935cbe7f452f20'}, {'index': 'NYA', 'date': '1966-11-07', 'open': 460.170013, 'high': 460.170013, 'low': 460.170013, 'close': 460.170013, 'adj_close': 460.170013, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_2f29143c293443beafd861482958cdc4'}, {'index': 'NYA', 'date': '1966-11-09', 'open': 463.76001, 'high': 463.76001, 'low': 463.76001, 'close': 463.76001, 'adj_close': 463.76001, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_598c12211f5944a2ad97702e3d5a76ff'}, {'index': 'NYA', 'date': '1966-11-10', 'open': 466.829987, 'high': 466.829987, 'low': 466.829987, 'close': 466.829987, 'adj_close': 466.829987, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_b90bc27f854b4553b81780ef05985306'}, {'index': 'NYA', 'date': '1966-11-11', 'open': 467.25, 'high': 467.25, 'low': 467.25, 'close': 467.25, 'adj_close': 467.25, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_d7bf9c68bb604c309541d1f5e45f4e6c'}, {'index': 'NYA', 'date': '1966-11-14', 'open': 464.399994, 'high': 464.399994, 'low': 464.399994, 'close': 464.399994, 'adj_close': 464.399994, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_27c0c637dba3478abc8e06aeb2f1de13'}, {'index': 'NYA', 'date': '1966-11-15', 'open': 466.089996, 'high': 466.089996, 'low': 466.089996, 'close': 466.089996, 'adj_close': 466.089996, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_7e229ec3335f48e5af5b0babf6202674'}, {'index': 'NYA', 'date': '1966-11-16', 'open': 470.109985, 'high': 470.109985, 'low': 470.109985, 'close': 470.109985, 'adj_close': 470.109985, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_196ee5c117c94196a6c86d955ac1825b'}, {'index': 'NYA', 'date': '1966-11-17', 'open': 466.940002, 'high': 466.940002, 'low': 466.940002, 'close': 466.940002, 'adj_close': 466.940002, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_47104cbdfa9c42b0a82885e1fadb691a'}, {'index': 'NYA', 'date': '1966-11-18', 'open': 464.079987, 'high': 464.079987, 'low': 464.079987, 'close': 464.079987, 'adj_close': 464.079987, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_992a7fbdc05341ec9a2cb849893bfedd'}, {'index': 'NYA', 'date': '1966-11-21', 'open': 457.630005, 'high': 457.630005, 'low': 457.630005, 'close': 457.630005, 'adj_close': 457.630005, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_9eebb4f30617495d9b13f72db8039c55'}, {'index': 'NYA', 'date': '1966-11-22', 'open': 455.519989, 'high': 455.519989, 'low': 455.519989, 'close': 455.519989, 'adj_close': 455.519989, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_6befb11eb46a4d8f8cdf5e1f14e0bd7d'}, {'index': 'NYA', 'date': '1966-11-23', 'open': 458.899994, 'high': 458.899994, 'low': 458.899994, 'close': 458.899994, 'adj_close': 458.899994, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_f436b7462fc94d39ac24c1186565ed33'}, {'index': 'NYA', 'date': '1966-11-25', 'open': 462.600006, 'high': 462.600006, 'low': 462.600006, 'close': 462.600006, 'adj_close': 462.600006, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_3ed857eeaaf04c9c8ed5520e4b683d44'}, {'index': 'NYA', 'date': '1966-11-28', 'open': 462.179993, 'high': 462.179993, 'low': 462.179993, 'close': 462.179993, 'adj_close': 462.179993, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_be6cd17a1acc4c99be615447a4cd9a72'}, {'index': 'NYA', 'date': '1966-11-29', 'open': 460.589996, 'high': 460.589996, 'low': 460.589996, 'close': 460.589996, 'adj_close': 460.589996, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_96627271d0a847819ff3f493cb537612'}, {'index': 'NYA', 'date': '1966-11-30', 'open': 460.910004, 'high': 460.910004, 'low': 460.910004, 'close': 460.910004, 'adj_close': 460.910004, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_09ff579a912d4804a8a128fbfd608a63'}, {'index': 'NYA', 'date': '1966-12-01', 'open': 459.109985, 'high': 459.109985, 'low': 459.109985, 'close': 459.109985, 'adj_close': 459.109985, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_13f58d6daaee463b80164c1e9d8603cd'}, {'index': 'NYA', 'date': '1966-12-02', 'open': 459.320007, 'high': 459.320007, 'low': 459.320007, 'close': 459.320007, 'adj_close': 459.320007, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_bad15ef435da4d0088c690a26d2789b9'}, {'index': 'NYA', 'date': '1966-12-05', 'open': 459.75, 'high': 459.75, 'low': 459.75, 'close': 459.75, 'adj_close': 459.75, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_b87b7ea4cf6a4d3f93803f427665a7ed'}, {'index': 'NYA', 'date': '1966-12-06', 'open': 462.920013, 'high': 462.920013, 'low': 462.920013, 'close': 462.920013, 'adj_close': 462.920013, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_e2c359288ab140fbb7fe3389c4d9cf48'}, {'index': 'NYA', 'date': '1966-12-07', 'open': 467.570007, 'high': 467.570007, 'low': 467.570007, 'close': 467.570007, 'adj_close': 467.570007, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_cc7f06ae482146dcbbc12d44ef668a04'}, {'index': 'NYA', 'date': '1966-12-08', 'open': 469.470001, 'high': 469.470001, 'low': 469.470001, 'close': 469.470001, 'adj_close': 469.470001, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_4eaff3fb22014b378026b5fa18998d5f'}, {'index': 'NYA', 'date': '1966-12-09', 'open': 470.529999, 'high': 470.529999, 'low': 470.529999, 'close': 470.529999, 'adj_close': 470.529999, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_cdac45a2debd4cbebda0f6193b77a6fa'}, {'index': 'NYA', 'date': '1966-12-12', 'open': 475.079987, 'high': 475.079987, 'low': 475.079987, 'close': 475.079987, 'adj_close': 475.079987, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_48bbdba89bc046f78cd4852e8ce8af62'}, {'index': 'NYA', 'date': '1966-12-13', 'open': 473.809998, 'high': 473.809998, 'low': 473.809998, 'close': 473.809998, 'adj_close': 473.809998, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_8b09bfc77ba24dd2b449c8c4a1ad9e23'}, {'index': 'NYA', 'date': '1966-12-14', 'open': 473.700012, 'high': 473.700012, 'low': 473.700012, 'close': 473.700012, 'adj_close': 473.700012, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_b842ca919652455ba6640b96afe9a7da'}, {'index': 'NYA', 'date': '1966-12-15', 'open': 468.730011, 'high': 468.730011, 'low': 468.730011, 'close': 468.730011, 'adj_close': 468.730011, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_a1ea9fe9f02b486abea545b0ddedc802'}, {'index': 'NYA', 'date': '1966-12-16', 'open': 468.839996, 'high': 468.839996, 'low': 468.839996, 'close': 468.839996, 'adj_close': 468.839996, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_4fdc373cc99a4ea0b65e187f5310f5f7'}, {'index': 'NYA', 'date': '1966-12-19', 'open': 467.570007, 'high': 467.570007, 'low': 467.570007, 'close': 467.570007, 'adj_close': 467.570007, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_69387d1fef65455c9078fafc6d3bc003'}, {'index': 'NYA', 'date': '1966-12-20', 'open': 465.670013, 'high': 465.670013, 'low': 465.670013, 'close': 465.670013, 'adj_close': 465.670013, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_3a9c802396b24a8fa0c6fabfe101b9c8'}, {'index': 'NYA', 'date': '1966-12-21', 'open': 468.200012, 'high': 468.200012, 'low': 468.200012, 'close': 468.200012, 'adj_close': 468.200012, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_763800a0ba9d4b55aefdb61c7ade78c9'}, {'index': 'NYA', 'date': '1966-12-22', 'open': 470.109985, 'high': 470.109985, 'low': 470.109985, 'close': 470.109985, 'adj_close': 470.109985, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_a003681eb1114b468c7614429bad2cd8'}, {'index': 'NYA', 'date': '1966-12-23', 'open': 468.940002, 'high': 468.940002, 'low': 468.940002, 'close': 468.940002, 'adj_close': 468.940002, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_756f96892356400e9a5435d2c75d0f14'}, {'index': 'NYA', 'date': '1966-12-27', 'open': 466.51001, 'high': 466.51001, 'low': 466.51001, 'close': 466.51001, 'adj_close': 466.51001, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_e862ffdded764179aa1796c6146a248d'}, {'index': 'NYA', 'date': '1966-12-28', 'open': 464.190002, 'high': 464.190002, 'low': 464.190002, 'close': 464.190002, 'adj_close': 464.190002, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_62f9fe7072fc43518c3259e8e41c1102'}, {'index': 'NYA', 'date': '1966-12-29', 'open': 462.48999, 'high': 462.48999, 'low': 462.48999, 'close': 462.48999, 'adj_close': 462.48999, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_e45a7d3a48ab4d7298c9f30547a55189'}, {'index': 'NYA', 'date': '1966-12-30', 'open': 462.279999, 'high': 462.279999, 'low': 462.279999, 'close': 462.279999, 'adj_close': 462.279999, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_5c0cc58c00dd4e72b11d42d8bb52aa55'}, {'index': 'NYA', 'date': '1967-01-03', 'open': 462.48999, 'high': 462.48999, 'low': 462.48999, 'close': 462.48999, 'adj_close': 462.48999, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_b8224579041542a5b588d23bd725561b'}, {'index': 'NYA', 'date': '1967-01-04', 'open': 463.339996, 'high': 463.339996, 'low': 463.339996, 'close': 463.339996, 'adj_close': 463.339996, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_4908ee25f6cb4c6cbcf1429e78024153'}, {'index': 'NYA', 'date': '1967-01-05', 'open': 469.26001, 'high': 469.26001, 'low': 469.26001, 'close': 469.26001, 'adj_close': 469.26001, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_36aff27b24c84551939a81b41d5078b5'}, {'index': 'NYA', 'date': '1967-01-06', 'open': 472.75, 'high': 472.75, 'low': 472.75, 'close': 472.75, 'adj_close': 472.75, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_b0fa38527a934e6baa2ca2e154173da9'}, {'index': 'NYA', 'date': '1967-01-09', 'open': 476.660004, 'high': 476.660004, 'low': 476.660004, 'close': 476.660004, 'adj_close': 476.660004, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_ea6a640d367340c6915daf0fa656b1ca'}, {'index': 'NYA', 'date': '1967-01-10', 'open': 476.660004, 'high': 476.660004, 'low': 476.660004, 'close': 476.660004, 'adj_close': 476.660004, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_d1ccdefd919f479eb3e676a16263f85d'}, {'index': 'NYA', 'date': '1967-01-11', 'open': 480.579987, 'high': 480.579987, 'low': 480.579987, 'close': 480.579987, 'adj_close': 480.579987, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_55f7b42365f34d20b24d762c8fd9fa79'}, {'index': 'NYA', 'date': '1967-01-12', 'open': 483.429993, 'high': 483.429993, 'low': 483.429993, 'close': 483.429993, 'adj_close': 483.429993, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_2495360e59a44e17b527592f525cedc2'}, {'index': 'NYA', 'date': '1967-01-13', 'open': 486.809998, 'high': 486.809998, 'low': 486.809998, 'close': 486.809998, 'adj_close': 486.809998, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_c4b69088c4274358ab51d8fb6ec37a38'}, {'index': 'NYA', 'date': '1967-01-16', 'open': 485.970001, 'high': 485.970001, 'low': 485.970001, 'close': 485.970001, 'adj_close': 485.970001, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_19902d23c7bf42a8a6cf80a469228aa8'}, {'index': 'NYA', 'date': '1967-01-17', 'open': 491.26001, 'high': 491.26001, 'low': 491.26001, 'close': 491.26001, 'adj_close': 491.26001, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_a338832557304a0082d653d53e52e44b'}, {'index': 'NYA', 'date': '1967-01-18', 'open': 494.0, 'high': 494.0, 'low': 494.0, 'close': 494.0, 'adj_close': 494.0, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_1a66efbf81c147bba5efec461328f6f2'}, {'index': 'NYA', 'date': '1967-01-19', 'open': 494.220001, 'high': 494.220001, 'low': 494.220001, 'close': 494.220001, 'adj_close': 494.220001, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_7291ec3968bd4daa9c9ae21c6c817896'}, {'index': 'NYA', 'date': '1967-01-20', 'open': 496.01001, 'high': 496.01001, 'low': 496.01001, 'close': 496.01001, 'adj_close': 496.01001, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_188ebed56d5742e3a9bebb820513278c'}, {'index': 'NYA', 'date': '1967-01-23', 'open': 497.809998, 'high': 497.809998, 'low': 497.809998, 'close': 497.809998, 'adj_close': 497.809998, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_e676d68d055a4d3a9151ebc6668d2ee3'}, {'index': 'NYA', 'date': '1967-01-24', 'open': 498.76001, 'high': 498.76001, 'low': 498.76001, 'close': 498.76001, 'adj_close': 498.76001, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_c1c6a375d4ce42afa982e620c9a4ec3b'}, {'index': 'NYA', 'date': '1967-01-25', 'open': 495.380005, 'high': 495.380005, 'low': 495.380005, 'close': 495.380005, 'adj_close': 495.380005, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_9351ed7bb23842abb45b91e828c55e2a'}, {'index': 'NYA', 'date': '1967-01-26', 'open': 495.700012, 'high': 495.700012, 'low': 495.700012, 'close': 495.700012, 'adj_close': 495.700012, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_cc260c71c62943e9bb7f4576f2f719c7'}, {'index': 'NYA', 'date': '1967-01-27', 'open': 497.809998, 'high': 497.809998, 'low': 497.809998, 'close': 497.809998, 'adj_close': 497.809998, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_a5d780fcdb3048249d018f26ec7476a8'}, {'index': 'NYA', 'date': '1967-01-30', 'open': 500.670013, 'high': 500.670013, 'low': 500.670013, 'close': 500.670013, 'adj_close': 500.670013, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_57aea50b6cb5413dbac7f0c09883e24f'}, {'index': 'NYA', 'date': '1967-01-31', 'open': 500.140015, 'high': 500.140015, 'low': 500.140015, 'close': 500.140015, 'adj_close': 500.140015, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_2c76c472ce8d42098d837b9e38d03b45'}, {'index': 'NYA', 'date': '1967-02-01', 'open': 499.5, 'high': 499.5, 'low': 499.5, 'close': 499.5, 'adj_close': 499.5, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_6c5630d97c9648edae86375a33464765'}, {'index': 'NYA', 'date': '1967-02-02', 'open': 501.089996, 'high': 501.089996, 'low': 501.089996, 'close': 501.089996, 'adj_close': 501.089996, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_286f4582c20a4da5aa5adf08a3cd6458'}, {'index': 'NYA', 'date': '1967-02-03', 'open': 504.679993, 'high': 504.679993, 'low': 504.679993, 'close': 504.679993, 'adj_close': 504.679993, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_2f842d7ddf084fc395b37eac60f3b27d'}, {'index': 'NYA', 'date': '1967-02-06', 'open': 503.730011, 'high': 503.730011, 'low': 503.730011, 'close': 503.730011, 'adj_close': 503.730011, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_eecbc521e9504d35b9edd65cb29ca668'}, {'index': 'NYA', 'date': '1967-02-07', 'open': 502.679993, 'high': 502.679993, 'low': 502.679993, 'close': 502.679993, 'adj_close': 502.679993, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_7c357f0bba9b40fe9a280123bca76883'}, {'index': 'NYA', 'date': '1967-02-08', 'open': 507.119995, 'high': 507.119995, 'low': 507.119995, 'close': 507.119995, 'adj_close': 507.119995, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_6f1672bf7ce94f1eb5453ee42f25d289'}, {'index': 'NYA', 'date': '1967-02-09', 'open': 505.109985, 'high': 505.109985, 'low': 505.109985, 'close': 505.109985, 'adj_close': 505.109985, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_d3676d3881b242f6ba37676b339da629'}, {'index': 'NYA', 'date': '1967-02-10', 'open': 506.160004, 'high': 506.160004, 'low': 506.160004, 'close': 506.160004, 'adj_close': 506.160004, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_b0d83942f30b43209b4cd35eda075a5a'}, {'index': 'NYA', 'date': '1967-02-13', 'open': 506.899994, 'high': 506.899994, 'low': 506.899994, 'close': 506.899994, 'adj_close': 506.899994, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_76237c709f0a47088dced65b3219c489'}, {'index': 'NYA', 'date': '1967-02-14', 'open': 510.290009, 'high': 510.290009, 'low': 510.290009, 'close': 510.290009, 'adj_close': 510.290009, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_fa9acffbd77d4d49bddb7aabd3832f1b'}, {'index': 'NYA', 'date': '1967-02-15', 'open': 510.609985, 'high': 510.609985, 'low': 510.609985, 'close': 510.609985, 'adj_close': 510.609985, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_37ad9f23e08d4c678fec82c84855338e'}, {'index': 'NYA', 'date': '1967-02-16', 'open': 508.380005, 'high': 508.380005, 'low': 508.380005, 'close': 508.380005, 'adj_close': 508.380005, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_03aeb54fef0a4c60a86b67f8be97d0c8'}, {'index': 'NYA', 'date': '1967-02-17', 'open': 508.48999, 'high': 508.48999, 'low': 508.48999, 'close': 508.48999, 'adj_close': 508.48999, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_67febd829f0a43a1a3932e3c2b8e255c'}, {'index': 'NYA', 'date': '1967-02-20', 'open': 505.850006, 'high': 505.850006, 'low': 505.850006, 'close': 505.850006, 'adj_close': 505.850006, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_6b55825311e54d048dfe9d6aaa37acfc'}, {'index': 'NYA', 'date': '1967-02-21', 'open': 505.850006, 'high': 505.850006, 'low': 505.850006, 'close': 505.850006, 'adj_close': 505.850006, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_6165260771b2448ab65e75ca2e7ad39a'}, {'index': 'NYA', 'date': '1967-02-23', '@type': 'IndexDataOp', '@id': 'IndexDataOp_a4d281ab44e143c8aef0a398cf615ec8'}, {'index': 'NYA', 'date': '1967-02-24', 'open': 506.480011, 'high': 506.480011, 'low': 506.480011, 'close': 506.480011, 'adj_close': 506.480011, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_340f944337294879b5803c7b592b1944'}, {'index': 'NYA', 'date': '1967-02-27', 'open': 501.089996, 'high': 501.089996, 'low': 501.089996, 'close': 501.089996, 'adj_close': 501.089996, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_4905bfca283b4e4c8fa8fb59bbfe10bc'}, {'index': 'NYA', 'date': '1967-02-28', 'open': 502.890015, 'high': 502.890015, 'low': 502.890015, 'close': 502.890015, 'adj_close': 502.890015, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_45ad14a648e24e998dd1506b26812d82'}, {'index': 'NYA', 'date': '1967-03-01', 'open': 507.859985, 'high': 507.859985, 'low': 507.859985, 'close': 507.859985, 'adj_close': 507.859985, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_169bd2d888f34fd7962b1f4b77311ddd'}, {'index': 'NYA', 'date': '1967-03-02', 'open': 510.709991, 'high': 510.709991, 'low': 510.709991, 'close': 510.709991, 'adj_close': 510.709991, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_567b3c6178ea45169c82c110311af628'}, {'index': 'NYA', 'date': '1967-03-03', 'open': 511.450012, 'high': 511.450012, 'low': 511.450012, 'close': 511.450012, 'adj_close': 511.450012, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_b901176582bc4124a8f4dfe85e3f6e4c'}, {'index': 'NYA', 'date': '1967-03-06', 'open': 510.709991, 'high': 510.709991, 'low': 510.709991, 'close': 510.709991, 'adj_close': 510.709991, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_9032e62b60854c0ca10de0c3faffd565'}, {'index': 'NYA', 'date': '1967-03-07', 'open': 511.350006, 'high': 511.350006, 'low': 511.350006, 'close': 511.350006, 'adj_close': 511.350006, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_76dc8e44f64540eebab6717166ca22a2'}, {'index': 'NYA', 'date': '1967-03-08', 'open': 512.090027, 'high': 512.090027, 'low': 512.090027, 'close': 512.090027, 'adj_close': 512.090027, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_1590c7129b8d4e96adf3fad0c25f7741'}, {'index': 'NYA', 'date': '1967-03-09', 'open': 513.570007, 'high': 513.570007, 'low': 513.570007, 'close': 513.570007, 'adj_close': 513.570007, 'volume': 0.0, '@type': 'IndexDataOp', '@id': 'IndexDataOp_27866b6fcba04c32b490d5155849a043'}]

The schema is:
class IndexDataOp(DocumentTemplate):
    _key = RandomKey()
    adj_close: Optional[float]
    close: Optional[float]
    date: Optional[str]
    high: Optional[float]
    index: Optional[str]
    low: Optional[float]
    open: Optional[float]
    volume: Optional[int]

Sorry I don't have it in json",2021-09-02T12:14:16Z,spl,https://github.com/terminusdb/terminusdb/issues/472#issuecomment-911607328,It looks like this was fixed by 82576c5.,-PRON- look like this be fix by 82576c5,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,473,2021-08-26T10:43:17Z,matko,Request body parse failures are not reported properly,https://github.com/terminusdb/terminusdb/issues/473,"When a request body is parsed, and this fails with for example a json parse error, this is reported back to the user as a 'ViolationWithDatatypeObject'. This is caused by old code that assumes such all parse errors must occur in the context of converting something to an xsd datatype.
We need to return the proper error, mentioning the proper response type corresponding with the request, and using a valid inner error. This currently already happens in the document interface, but not everywhere else.
For example, see this example from #470:
echo '' | xh 'http://localhost:6363/api/db/unknown' # POST

HTTP/1.1 400 Bad Request
connection: keep-alive
content-length: 173
content-type: application/json
date: Thu, 26 Aug 2021 08:51:43 GMT

{
    ""api:status"": ""api:failure"",
    ""system:witnesses"": [
        {
            ""@type"": ""vio:ViolationWithDatatypeObject"",
            ""vio:literal"": ""json(unexpected_end_of_file)""
        }
    ]
}",2021-09-06T11:04:11Z,spl,https://github.com/terminusdb/terminusdb/issues/473#issuecomment-913558827,Reopened by #523.,reopen by # 523,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,476,2021-08-26T12:34:53Z,matko,Have an endpoint for maintainance on a collection,https://github.com/terminusdb/terminusdb/issues/476,"There's various conditions, like historical bugs or limitations in the current implementation, that can leave a graph in a state where it contains garbage data. It would be good to have an endpoint which allows you to scan a collection and fix issues in it.",,,,,,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,478,2021-08-26T14:15:34Z,matko,Discussion: Should arrays have nulls?,https://github.com/terminusdb/terminusdb/issues/478,"Currently our array type allows for nulls as valid elements. As far as I am aware, this is our only collection type that behaves in this way.
I'm doubting whether this should be a default. I think in most cases, users would not expect to find nulls in their arrays. Right now there's no way to enforce that through the schema though.
I'm proposing that we either remove nulls from arrays entirely, or that we create some new type, NullableArray, that retains the old behavior while making it illegal for Arrays to contain nulls.
Interested in other people's thoughts on this.",2021-08-26T15:04:57Z,Cheukting,https://github.com/terminusdb/terminusdb/issues/478#issuecomment-906493623,"I think null in array makes sense cause it’s ordered. I may have a list of
stuff that has empty slots on it. For set then it doesn’t make sense to
have null.
…
On Thu, 26 Aug 2021 at 15:15, Matthijs van Otterdijk < ***@***.***> wrote:
 Currently our array type allows for nulls as valid elements. As far as I
 am aware, this is our only collection type that behaves in this way.

 I'm doubting whether this should be a default. I think in most cases,
 users would not expect to find nulls in their arrays. Right now there's no
 way to enforce that through the schema though.

 I'm proposing that we either remove nulls from arrays entirely, or that we
 create some new type, NullableArray, that retains the old behavior while
 making it illegal for Arrays to contain nulls.

 Interested in other people's thoughts on this.

 —
 You are receiving this because you are subscribed to this thread.
 Reply to this email directly, view it on GitHub
 <#478>, or unsubscribe
 <https://github.com/notifications/unsubscribe-auth/AG3N26IDJHHBYYAVKP3MMWLT6ZEBFANCNFSM5C3OZ7GA>
 .",i think null in array make sense cause -PRON- ’ order i may have a list of stuff that have empty slot on -PRON- for set then -PRON- do not make sense to have null … on thu 26 aug 2021 at 1515 matthijs van otterdijk < * * * @ * * * * * * > write currently -PRON- array type allow for null as valid element as far as i be aware this be -PRON- only collection type that behave in this way -PRON- be doubt whether this should be a default i think in most case user would not expect to find null in -PRON- array right now there be no way to enforce that through the schema though -PRON- be propose that -PRON- either remove null from array entirely or that -PRON- create some new type nullablearray that retain the old behavior while make -PRON- illegal for array to contain null interested in other people 's thought on this — -PRON- be receive this because -PRON- be subscribe to this thread reply to this email directly view -PRON- on github < # 478 > or unsubscribe < https//githubcom / notification / unsubscribe - auth / ag3n26idjhhbyyavkp3mmwlt6zebfancnfsm5c3oz7ga >,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,478,2021-08-26T14:15:34Z,matko,Discussion: Should arrays have nulls?,https://github.com/terminusdb/terminusdb/issues/478,"Currently our array type allows for nulls as valid elements. As far as I am aware, this is our only collection type that behaves in this way.
I'm doubting whether this should be a default. I think in most cases, users would not expect to find nulls in their arrays. Right now there's no way to enforce that through the schema though.
I'm proposing that we either remove nulls from arrays entirely, or that we create some new type, NullableArray, that retains the old behavior while making it illegal for Arrays to contain nulls.
Interested in other people's thoughts on this.",2021-08-26T15:16:11Z,matko,https://github.com/terminusdb/terminusdb/issues/478#issuecomment-906503264,"If that's the case, should we also have nulls in linked lists?
My problem with this is that when it comes to SQL, we often (rightly) criticize it for being so permissive with null values in databases, which can often lead to nasty surprises at runtime. We can still support nulls with a special type, I just don't think it should be the default.",if that be the case should -PRON- also have null in link list -PRON- problem with this be that when -PRON- come to sql -PRON- often ( rightly ) criticize -PRON- for be so permissive with null value in database which can often lead to nasty surprise at runtime -PRON- can still support null with a special type i just do not think -PRON- should be the default,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,478,2021-08-26T14:15:34Z,matko,Discussion: Should arrays have nulls?,https://github.com/terminusdb/terminusdb/issues/478,"Currently our array type allows for nulls as valid elements. As far as I am aware, this is our only collection type that behaves in this way.
I'm doubting whether this should be a default. I think in most cases, users would not expect to find nulls in their arrays. Right now there's no way to enforce that through the schema though.
I'm proposing that we either remove nulls from arrays entirely, or that we create some new type, NullableArray, that retains the old behavior while making it illegal for Arrays to contain nulls.
Interested in other people's thoughts on this.",2021-08-26T19:20:24Z,Cheukting,https://github.com/terminusdb/terminusdb/issues/478#issuecomment-906677956,"IMO it’s easier to convert btn SQL databases if the null in a list behaves
the same way lol
…
On Thu, 26 Aug 2021 at 18:16, Matthijs van Otterdijk < ***@***.***> wrote:
 If that's the case, should we also have nulls in linked lists?

 My problem with this is that when it comes to SQL, we often (rightly)
 criticize it for being so permissive with null values in databases, which
 can often lead to nasty surprises at runtime. We can still support nulls
 with a special type, I just don't think it should be the default.

 —
 You are receiving this because you commented.


 Reply to this email directly, view it on GitHub
 <#478 (comment)>,
 or unsubscribe
 <https://github.com/notifications/unsubscribe-auth/AG3N26OVVKIMBXNHA7IAO3DT6ZLENANCNFSM5C3OZ7GA>
 .",imo -PRON- ’ easy to convert btn sql database if the null in a list behave the same way lol … on thu 26 aug 2021 at 1816 matthijs van otterdijk < * * * @ * * * * * * > write if that be the case should -PRON- also have null in link list -PRON- problem with this be that when -PRON- come to sql -PRON- often ( rightly ) criticize -PRON- for be so permissive with null value in database which can often lead to nasty surprise at runtime -PRON- can still support null with a special type i just do not think -PRON- should be the default — -PRON- be receive this because -PRON- comment reply to this email directly view -PRON- on github < # 478 ( comment ) > or unsubscribe < https//githubcom / notification / unsubscribe - auth / ag3n26ovvkimbxnha7iao3dt6zlenancnfsm5c3oz7ga >,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,478,2021-08-26T14:15:34Z,matko,Discussion: Should arrays have nulls?,https://github.com/terminusdb/terminusdb/issues/478,"Currently our array type allows for nulls as valid elements. As far as I am aware, this is our only collection type that behaves in this way.
I'm doubting whether this should be a default. I think in most cases, users would not expect to find nulls in their arrays. Right now there's no way to enforce that through the schema though.
I'm proposing that we either remove nulls from arrays entirely, or that we create some new type, NullableArray, that retains the old behavior while making it illegal for Arrays to contain nulls.
Interested in other people's thoughts on this.",2021-09-02T16:35:59Z,matko,https://github.com/terminusdb/terminusdb/issues/478#issuecomment-911865111,"So in conversation it was decided to not allow nulls, and to later on allow nulls through optional types inside lists and arrays. This has been implemented now.",so in conversation -PRON- be decide to not allow null and to later on allow null through optional type inside list and array this have be implement now,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,482,2021-08-28T07:46:51Z,GavinMendelGleason,Subdocuments should not be replaced unless changed,https://github.com/terminusdb/terminusdb/issues/482,"If a subdocument has no changes, then we should not regenerate the ID and should submit no edge changes. This will pollute the history with spurious changes even in the case where nothing substantial has changed.


 This feature would also enable us to have a class-level auto-increment key which would be even more convenient for subdocuments then random (and improve reproducibility substantially).


An example for the subdocument autoincrement syntax might be:
{ ""@type"" : ""Class"", 
  ""@id"" : ""Address"", 
  ""@subdocument"" : [],
  ""@base"" : ""Address/"",
  ""@key"" : { ""@type"" : ""AutoIncrement"",
                   ""start"" : 0 },
  ""street"" : ""xsd:string"",
  ""province"" : ""xsd:string"",
  ""country"" : ""xsd:string"" }
The first generated id for an address would then be: ""Address/0""",2021-09-02T20:33:07Z,GavinMendelGleason,https://github.com/terminusdb/terminusdb/issues/482#issuecomment-912034600,"This is no longer relevant as subdocuments now have correct addressing with lexical, hash and valuehash. If you use Random you must supply the ID or all bets are off.",this be no long relevant as subdocument now have correct address with lexical hash and valuehash if -PRON- use random -PRON- must supply the -PRON- d or all bet be off,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,483,2021-08-30T08:47:43Z,spl,Redirect GitHub pages to docs,https://github.com/terminusdb/terminusdb/issues/483,"We should refer people who arrive at https://terminusdb.github.io/terminusdb/ to https://terminusdb.github.io/terminusdb-docs/. If we can do a meta refresh to redirect, we should probably do that, too.",2021-09-06T14:04:45Z,spl,https://github.com/terminusdb/terminusdb/issues/483#issuecomment-913675145,💯,💯,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,485,2021-08-30T13:30:21Z,matko,Schema checking does not properly check list/array structure,https://github.com/terminusdb/terminusdb/issues/485,"It is possible to delete the 'first' property from a cell without running into a schema error. Nevertheless, our interpretation of lists is broken if this property is missing. We should therefore check for this property.
Similarly, it is possible to delete array elements (but not their position). In this case, we interpret it as a 'null'. Based on discussion during today's meeting though, it's a good idea to not allow this.
So both lists and array need to do cardinality checking properly, ensuring there's exactly one element for each position.",,,,,,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,486,2021-08-30T13:43:32Z,spl,Increase transaction retry count,https://github.com/terminusdb/terminusdb/issues/486,From #466.,,,,,,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,487,2021-08-31T12:43:18Z,matko,sending a - as post to the document interface causes wrong error,https://github.com/terminusdb/terminusdb/issues/487,"matthijs@leviathan:~/datachemist/src/terminus-server$ curl -X POST \
  -H ""Content-Type: application/json"" \
  -u ""admin:root"" \
  ""http://localhost:6363/api/document/admin/moo?graph_type=schema&author=me&message=me"" \
  --data-binary -@
{
  ""api:message"":""Type error for - which should be dict"",
  ""api:status"":""api:failure"",
  ""system:witnesses"": [
    {
      ""@type"":""vio:ViolationWithDatatypeObject"",
      ""vio:literal"":""-"",
      ""vio:message"":""Type error for - which should be dict"",
      ""vio:type"":""dict""
    }
  ]
}

This should be returning a proper response type.",2021-09-06T14:03:54Z,spl,https://github.com/terminusdb/terminusdb/issues/487#issuecomment-913674517,May be changed by #524.,may be change by # 524,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,487,2021-08-31T12:43:18Z,matko,sending a - as post to the document interface causes wrong error,https://github.com/terminusdb/terminusdb/issues/487,"matthijs@leviathan:~/datachemist/src/terminus-server$ curl -X POST \
  -H ""Content-Type: application/json"" \
  -u ""admin:root"" \
  ""http://localhost:6363/api/document/admin/moo?graph_type=schema&author=me&message=me"" \
  --data-binary -@
{
  ""api:message"":""Type error for - which should be dict"",
  ""api:status"":""api:failure"",
  ""system:witnesses"": [
    {
      ""@type"":""vio:ViolationWithDatatypeObject"",
      ""vio:literal"":""-"",
      ""vio:message"":""Type error for - which should be dict"",
      ""vio:type"":""dict""
    }
  ]
}

This should be returning a proper response type.",2021-09-07T07:28:56Z,spl,https://github.com/terminusdb/terminusdb/issues/487#issuecomment-914061081,"The problem still manifests with this script using af04956:
#!/bin/bash
set -ex

# Create a database.
xh 'http://admin:root@localhost:6363/api/db/admin/t' <<EOF
{""label"":""l"",""comment"":""c""}
EOF

# Post a dash for a schema document.
echo -n '-' | xh 'http://admin:root@localhost:6363/api/document/admin/t?author=a&message=m&graph_type=schema'
Output from the last command:
HTTP/1.1 400 Bad Request
connection: keep-alive
content-length: 289
content-type: application/json
date: Tue, 07 Sep 2021 07:27:00 GMT

{
    ""api:message"": ""Type error for - which should be dict"",
    ""api:status"": ""api:failure"",
    ""system:witnesses"": [
        {
            ""@type"": ""vio:ViolationWithDatatypeObject"",
            ""vio:literal"": ""-"",
            ""vio:message"": ""Type error for - which should be dict"",
            ""vio:type"": ""dict""
        }
    ]
}","the problem still manifest with this script use af04956 # /bin / bash set -ex # create a database xh ' http//adminroot@localhost6363 / api / db / admin / t ' < < eof { "" label""""l""""comment""""c "" } eof # post a dash for a schema document echo -n ' - ' | xh ' http//adminroot@localhost6363 / api / document / admin / tauthor = a&message = m&graph_type = schema ' output from the last command http/11 400 bad request connection keep - alive content - length 289 content - type application / json date tue 07 sep 2021 072700 gmt { "" apimessage "" "" type error for - which should be dict "" "" apistatus "" "" apifailure "" "" systemwitnesse "" [ { "" @type "" "" vioviolationwithdatatypeobject "" "" violiteral "" "" - "" "" viomessage "" "" type error for - which should be dict "" "" viotype "" "" dict "" } ] }",0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,487,2021-08-31T12:43:18Z,matko,sending a - as post to the document interface causes wrong error,https://github.com/terminusdb/terminusdb/issues/487,"matthijs@leviathan:~/datachemist/src/terminus-server$ curl -X POST \
  -H ""Content-Type: application/json"" \
  -u ""admin:root"" \
  ""http://localhost:6363/api/document/admin/moo?graph_type=schema&author=me&message=me"" \
  --data-binary -@
{
  ""api:message"":""Type error for - which should be dict"",
  ""api:status"":""api:failure"",
  ""system:witnesses"": [
    {
      ""@type"":""vio:ViolationWithDatatypeObject"",
      ""vio:literal"":""-"",
      ""vio:message"":""Type error for - which should be dict"",
      ""vio:type"":""dict""
    }
  ]
}

This should be returning a proper response type.",2021-09-07T07:41:17Z,spl,https://github.com/terminusdb/terminusdb/issues/487#issuecomment-914068688,"This is likely the source of the -:
?- json:atom_json_dict(""-"", Out, []).
Out =  (-).","this be likely the source of the - - jsonatom_json_dict(""- "" out [ ] ) out = ( - )",0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,487,2021-08-31T12:43:18Z,matko,sending a - as post to the document interface causes wrong error,https://github.com/terminusdb/terminusdb/issues/487,"matthijs@leviathan:~/datachemist/src/terminus-server$ curl -X POST \
  -H ""Content-Type: application/json"" \
  -u ""admin:root"" \
  ""http://localhost:6363/api/document/admin/moo?graph_type=schema&author=me&message=me"" \
  --data-binary -@
{
  ""api:message"":""Type error for - which should be dict"",
  ""api:status"":""api:failure"",
  ""system:witnesses"": [
    {
      ""@type"":""vio:ViolationWithDatatypeObject"",
      ""vio:literal"":""-"",
      ""vio:message"":""Type error for - which should be dict"",
      ""vio:type"":""dict""
    }
  ]
}

This should be returning a proper response type.",2021-09-07T08:02:25Z,spl,https://github.com/terminusdb/terminusdb/issues/487#issuecomment-914082417,I reported #487 (comment) at SWI-Prolog/packages-http#147.,i report # 487 ( comment ) at swi - prolog / package - http#147,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,487,2021-08-31T12:43:18Z,matko,sending a - as post to the document interface causes wrong error,https://github.com/terminusdb/terminusdb/issues/487,"matthijs@leviathan:~/datachemist/src/terminus-server$ curl -X POST \
  -H ""Content-Type: application/json"" \
  -u ""admin:root"" \
  ""http://localhost:6363/api/document/admin/moo?graph_type=schema&author=me&message=me"" \
  --data-binary -@
{
  ""api:message"":""Type error for - which should be dict"",
  ""api:status"":""api:failure"",
  ""system:witnesses"": [
    {
      ""@type"":""vio:ViolationWithDatatypeObject"",
      ""vio:literal"":""-"",
      ""vio:message"":""Type error for - which should be dict"",
      ""vio:type"":""dict""
    }
  ]
}

This should be returning a proper response type.",2021-09-07T08:14:32Z,spl,https://github.com/terminusdb/terminusdb/issues/487#issuecomment-914091225,"Since it appears to be a problem with SWI-Prolog and doesn't cause any problems with TerminusDB, we'll close this as a won't-fix.",since -PRON- appear to be a problem with swi - prolog and do not cause any problem with terminusdb -PRON- will close this as a won't - fix,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,487,2021-08-31T12:43:18Z,matko,sending a - as post to the document interface causes wrong error,https://github.com/terminusdb/terminusdb/issues/487,"matthijs@leviathan:~/datachemist/src/terminus-server$ curl -X POST \
  -H ""Content-Type: application/json"" \
  -u ""admin:root"" \
  ""http://localhost:6363/api/document/admin/moo?graph_type=schema&author=me&message=me"" \
  --data-binary -@
{
  ""api:message"":""Type error for - which should be dict"",
  ""api:status"":""api:failure"",
  ""system:witnesses"": [
    {
      ""@type"":""vio:ViolationWithDatatypeObject"",
      ""vio:literal"":""-"",
      ""vio:message"":""Type error for - which should be dict"",
      ""vio:type"":""dict""
    }
  ]
}

This should be returning a proper response type.",2021-09-07T09:14:12Z,spl,https://github.com/terminusdb/terminusdb/issues/487#issuecomment-914135617,Fixed in http/json (SWI-Prolog/packages-http#147 (comment))!,fix in http / json ( swi - prolog / package - http#147 ( comment ) ),0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,488,2021-08-31T13:24:18Z,matko,Objects in the system graph template have wrong ids,https://github.com/terminusdb/terminusdb/issues/488,"Some objects in the system instance template have an explicit id specified for them, which do not match the key strategy of their type. This is a problem if these objects are ever updated.
This is the case for Organization admin_organization, Capability server_access, User anonymous, and User admin.
We need to make sure the template contains the valid id. Also, when inserted originally, I'd have expected the schema checker to error on these invalid keys, which did not happen. This needs to be investigated.",2021-09-02T20:41:27Z,GavinMendelGleason,https://github.com/terminusdb/terminusdb/issues/488#issuecomment-912040263,Fixed in #499,fix in # 499,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,490,2021-08-31T13:40:50Z,matko,cannot insert instance documents along with their schema data,https://github.com/terminusdb/terminusdb/issues/490,"When inserting both a new type into the schema, and documents of that type into the instance, a type not found error is triggered.
I would expect the type checker to use the newest schema to do type checking, but it seems to be using the previous version.",2021-09-08T11:19:33Z,matko,https://github.com/terminusdb/terminusdb/issues/490#issuecomment-915148132,"This was a misunderstanding.
Document insertion requires a fully built schema in order to do elaboration on the incoming json document. Therefore, the schema must have committed before we can insert the document. This happens way before schema checking.
It should be possible to build an endpoint which does this, by first building a schema layer out of the schema changes, inserting the new schema layer into the transaction, then inserting the instance objects, finally committing both in one operation. However, at present we have no endpoint that requires both schema and instance changes to happen at once, making this a non-issue.
closing.",this be a misunderstanding document insertion require a fully build schema in order to do elaboration on the incoming json document therefore the schema must have commit before -PRON- can insert the document this happen way before schema check -PRON- should be possible to build an endpoint which do this by first build a schema layer out of the schema change insert the new schema layer into the transaction then insert the instance object finally commit both in one operation however at present -PRON- have no endpoint that require both schema and instance change to happen at once make this a non - issue closing,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,492,2021-09-01T07:56:28Z,GavinMendelGleason,Replace graph_filter with graph in schema and WOQL JSON conversion and clients,https://github.com/terminusdb/terminusdb/issues/492,"Describe the bug
Currently the compiler is not using a consistent naming convention for the graph / graph_filter. This needs to be the same everywhere.",,,,,,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,496,2021-09-01T13:19:11Z,AstroChelonian,error with terminusdb db delete database,https://github.com/terminusdb/terminusdb/issues/496,"After creating a database using ./terminusdb db create, an error occurs.
To reproduce
Step 1: Create a new empty database:
./terminusdb db create test_delete_db --label label --comment comment --prefixes '{""@base"":""http://base/"",""@schema"":""http://schema/""}'

Database admin/test_delete_db created

Step 2: try deleting the database above:
 ./terminusdb db delete admin/test_delete_db 

The following error occurs:
[17] terminus_store:delete_named_graph(<store>,'admin%7ctest_delete_db')
  [16] triplestore:safe_delete_named_graph(<store>,'admin|test_delete_db') at /home/astrochelonian/Documents/terminusdb/src/core/triple/triplestore.pl:213
  [14] utils:do_or_die('<garbage_collected>',error(database_files_do_not_exist(""admin"",""test_delete_db""),_10888)) at /home/astrochelonian/Documents/terminusdb/src/core/util/utils.pl:106
  [13] db_delete:delete_db('<garbage_collected>','terminusdb://system/data/admin',""admin"",""test_delete_db"",false) at /home/astrochelonian/Documents/terminusdb/src/core/api/db_delete.pl:48
  [12] catch(cli:delete_db(...,'terminusdb://system/data/admin',""admin"",""test_delete_db"",false),error(existence_error(procedure,...),context(...,_11008)),cli:do_or_die(...,...)) at /usr/lib/swi-prolog/boot/init.pl:532
  [11] catch_with_backtrace('<garbage_collected>','<garbage_collected>','<garbage_collected>') at /usr/lib/swi-prolog/boot/init.pl:582

Step 3: Try force deleting the database (add --force).
 ./terminusdb db delete admin/test_delete_db  --force

The same error occurs:
  [17] terminus_store:delete_named_graph(<store>,'admin%7ctest_delete_db')
  [16] triplestore:safe_delete_named_graph(<store>,'admin|test_delete_db') at /home/astrochelonian/Documents/terminusdb/src/core/triple/triplestore.pl:213
  [14] db_delete:force_delete_db(""admin"",""test_delete_db"") at /home/astrochelonian/Documents/terminusdb/src/core/api/db_delete.pl:96
  [12] catch(cli:delete_db(...,'terminusdb://system/data/admin',""admin"",""test_delete_db"",true),error(existence_error(procedure,...),context(...,_1102)),cli:do_or_die(...,...)) at /usr/lib/swi-prolog/boot/init.pl:532
  [11] catch_with_backtrace(cli:delete_db(...,'terminusdb://system/data/admin',""admin"",""test_delete_db"",true),error(existence_error(procedure,...),context(...,_1184)),cli:do_or_die(...,...)) at /usr/lib/swi-prolog/boot/init.pl:582",2021-09-01T13:53:00Z,spl,https://github.com/terminusdb/terminusdb/issues/496#issuecomment-910307349,"I can't reproduce with a freshly initialized store. And, just to make sure it wasn't a problem with my system configuration, I did it in the docker container:
$ docker run -it terminusdb/terminusdb-server:dev /bin/bash
root@fa4edb2016b2:/app/terminusdb# ./terminusdb store init --force
Successfully initialised database!!!
root@fa4edb2016b2:/app/terminusdb# ./terminusdb db create test_delete_db --label label --comment comment --prefixes '{""@base"":""http://base/"",""@schema"":""http://schema/""}'
Database admin/test_delete_db created
root@fa4edb2016b2:/app/terminusdb#  ./terminusdb db delete admin/test_delete_db 
Database admin/test_delete_db deleted","i can not reproduce with a freshly initialize store and just to make sure -PRON- be not a problem with -PRON- system configuration i do -PRON- in the docker container $ docker run -it terminusdb / terminusdb - serverdev /bin / bash root@fa4edb2016b2 / app / terminusdb # /terminusdb store init --force successfully initialise database root@fa4edb2016b2 / app / terminusdb # /terminusdb db create test_delete_db --label label --comment comment --prefixe ' { "" @base""""http//base/""""@schema""""http//schema/ "" } ' database admin / test_delete_db create root@fa4edb2016b2 / app / terminusdb # /terminusdb db delete admin / test_delete_db database admin / test_delete_db delete",0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,496,2021-09-01T13:19:11Z,AstroChelonian,error with terminusdb db delete database,https://github.com/terminusdb/terminusdb/issues/496,"After creating a database using ./terminusdb db create, an error occurs.
To reproduce
Step 1: Create a new empty database:
./terminusdb db create test_delete_db --label label --comment comment --prefixes '{""@base"":""http://base/"",""@schema"":""http://schema/""}'

Database admin/test_delete_db created

Step 2: try deleting the database above:
 ./terminusdb db delete admin/test_delete_db 

The following error occurs:
[17] terminus_store:delete_named_graph(<store>,'admin%7ctest_delete_db')
  [16] triplestore:safe_delete_named_graph(<store>,'admin|test_delete_db') at /home/astrochelonian/Documents/terminusdb/src/core/triple/triplestore.pl:213
  [14] utils:do_or_die('<garbage_collected>',error(database_files_do_not_exist(""admin"",""test_delete_db""),_10888)) at /home/astrochelonian/Documents/terminusdb/src/core/util/utils.pl:106
  [13] db_delete:delete_db('<garbage_collected>','terminusdb://system/data/admin',""admin"",""test_delete_db"",false) at /home/astrochelonian/Documents/terminusdb/src/core/api/db_delete.pl:48
  [12] catch(cli:delete_db(...,'terminusdb://system/data/admin',""admin"",""test_delete_db"",false),error(existence_error(procedure,...),context(...,_11008)),cli:do_or_die(...,...)) at /usr/lib/swi-prolog/boot/init.pl:532
  [11] catch_with_backtrace('<garbage_collected>','<garbage_collected>','<garbage_collected>') at /usr/lib/swi-prolog/boot/init.pl:582

Step 3: Try force deleting the database (add --force).
 ./terminusdb db delete admin/test_delete_db  --force

The same error occurs:
  [17] terminus_store:delete_named_graph(<store>,'admin%7ctest_delete_db')
  [16] triplestore:safe_delete_named_graph(<store>,'admin|test_delete_db') at /home/astrochelonian/Documents/terminusdb/src/core/triple/triplestore.pl:213
  [14] db_delete:force_delete_db(""admin"",""test_delete_db"") at /home/astrochelonian/Documents/terminusdb/src/core/api/db_delete.pl:96
  [12] catch(cli:delete_db(...,'terminusdb://system/data/admin',""admin"",""test_delete_db"",true),error(existence_error(procedure,...),context(...,_1102)),cli:do_or_die(...,...)) at /usr/lib/swi-prolog/boot/init.pl:532
  [11] catch_with_backtrace(cli:delete_db(...,'terminusdb://system/data/admin',""admin"",""test_delete_db"",true),error(existence_error(procedure,...),context(...,_1184)),cli:do_or_die(...,...)) at /usr/lib/swi-prolog/boot/init.pl:582",2021-09-06T17:16:18Z,AstroChelonian,https://github.com/terminusdb/terminusdb/issues/496#issuecomment-913779779,"I can't reproduce with a freshly initialized store. And, just to make sure it wasn't a problem with my system configuration, I did it in the docker container:
$ docker run -it terminusdb/terminusdb-server:dev /bin/bash
root@fa4edb2016b2:/app/terminusdb# ./terminusdb store init --force
Successfully initialised database!!!
root@fa4edb2016b2:/app/terminusdb# ./terminusdb db create test_delete_db --label label --comment comment --prefixes '{""@base"":""http://base/"",""@schema"":""http://schema/""}'
Database admin/test_delete_db created
root@fa4edb2016b2:/app/terminusdb#  ./terminusdb db delete admin/test_delete_db 
Database admin/test_delete_db deleted

When i have a brand new store, I can't reproduce it as well. I tried your approach in docker and it works.
My next step is to nuke my old store, start a new one, and see what happens.","i can not reproduce with a freshly initialize store and just to make sure -PRON- be not a problem with -PRON- system configuration i do -PRON- in the docker container $ docker run -it terminusdb / terminusdb - serverdev /bin / bash root@fa4edb2016b2 / app / terminusdb # /terminusdb store init --force successfully initialise database root@fa4edb2016b2 / app / terminusdb # /terminusdb db create test_delete_db --label label --comment comment --prefixe ' { "" @base""""http//base/""""@schema""""http//schema/ "" } ' database admin / test_delete_db create root@fa4edb2016b2 / app / terminusdb # /terminusdb db delete admin / test_delete_db database admin / test_delete_db delete when i have a brand new store i can not reproduce -PRON- as well i try -PRON- approach in docker and -PRON- work -PRON- next step be to nuke -PRON- old store start a new one and see what happen",0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,496,2021-09-01T13:19:11Z,AstroChelonian,error with terminusdb db delete database,https://github.com/terminusdb/terminusdb/issues/496,"After creating a database using ./terminusdb db create, an error occurs.
To reproduce
Step 1: Create a new empty database:
./terminusdb db create test_delete_db --label label --comment comment --prefixes '{""@base"":""http://base/"",""@schema"":""http://schema/""}'

Database admin/test_delete_db created

Step 2: try deleting the database above:
 ./terminusdb db delete admin/test_delete_db 

The following error occurs:
[17] terminus_store:delete_named_graph(<store>,'admin%7ctest_delete_db')
  [16] triplestore:safe_delete_named_graph(<store>,'admin|test_delete_db') at /home/astrochelonian/Documents/terminusdb/src/core/triple/triplestore.pl:213
  [14] utils:do_or_die('<garbage_collected>',error(database_files_do_not_exist(""admin"",""test_delete_db""),_10888)) at /home/astrochelonian/Documents/terminusdb/src/core/util/utils.pl:106
  [13] db_delete:delete_db('<garbage_collected>','terminusdb://system/data/admin',""admin"",""test_delete_db"",false) at /home/astrochelonian/Documents/terminusdb/src/core/api/db_delete.pl:48
  [12] catch(cli:delete_db(...,'terminusdb://system/data/admin',""admin"",""test_delete_db"",false),error(existence_error(procedure,...),context(...,_11008)),cli:do_or_die(...,...)) at /usr/lib/swi-prolog/boot/init.pl:532
  [11] catch_with_backtrace('<garbage_collected>','<garbage_collected>','<garbage_collected>') at /usr/lib/swi-prolog/boot/init.pl:582

Step 3: Try force deleting the database (add --force).
 ./terminusdb db delete admin/test_delete_db  --force

The same error occurs:
  [17] terminus_store:delete_named_graph(<store>,'admin%7ctest_delete_db')
  [16] triplestore:safe_delete_named_graph(<store>,'admin|test_delete_db') at /home/astrochelonian/Documents/terminusdb/src/core/triple/triplestore.pl:213
  [14] db_delete:force_delete_db(""admin"",""test_delete_db"") at /home/astrochelonian/Documents/terminusdb/src/core/api/db_delete.pl:96
  [12] catch(cli:delete_db(...,'terminusdb://system/data/admin',""admin"",""test_delete_db"",true),error(existence_error(procedure,...),context(...,_1102)),cli:do_or_die(...,...)) at /usr/lib/swi-prolog/boot/init.pl:532
  [11] catch_with_backtrace(cli:delete_db(...,'terminusdb://system/data/admin',""admin"",""test_delete_db"",true),error(existence_error(procedure,...),context(...,_1184)),cli:do_or_die(...,...)) at /usr/lib/swi-prolog/boot/init.pl:582",2021-09-07T08:47:23Z,AstroChelonian,https://github.com/terminusdb/terminusdb/issues/496#issuecomment-914113568,This error was happening because I didn't update to the newest terminus_store_prolog-0.19.1.,this error be happen because i do not update to the new terminus_store_prolog-0191,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,497,2021-09-01T13:42:41Z,matko,return empty array on document retrieval when there's no elements,https://github.com/terminusdb/terminusdb/issues/497,"when a document has a property which is an array, and that array is empty, the returned document will have that property missing.
It should instead return an empty list.",2021-09-06T14:04:38Z,Cheukting,https://github.com/terminusdb/terminusdb/issues/497#issuecomment-913675067,Will test with Python client when fixed,will test with python client when fix,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,497,2021-09-01T13:42:41Z,matko,return empty array on document retrieval when there's no elements,https://github.com/terminusdb/terminusdb/issues/497,"when a document has a property which is an array, and that array is empty, the returned document will have that property missing.
It should instead return an empty list.",2021-09-13T11:36:39Z,AstroChelonian,https://github.com/terminusdb/terminusdb/issues/497#issuecomment-918105376,"to replicate:
#!/bin/bash
Create the database.
xh 'http://admin:root@localhost:6363/api/db/admin/test497' <<EOF {""label"":""l"",""comment"":""c""} EOF
Create a type that contains an array.
curl -X POST -H ""Content-Type: application/json"" -u ""admin:root"" ""http://localhost:6363/api/document/admin/test497?graph_type=schema&author=me&message=me"" --data-binary '{""@id"": ""ArrayTestDoc"", ""@type"": ""Class"", ""array"": {""@type"": ""Array"", ""@class"": ""xsd:string""}}'
Create a document in ArrayTestDoc.
curl -X POST -H ""Content-Type: application/json"" -u ""admin:root"" ""http://localhost:6363/api/document/admin/test497?graph_type=instance&author=me&message=me"" --data-binary '{""@id"": ""ArrayTestDoc/IdForArrayDoc"", ""@type"": ""ArrayTestDoc"", ""array"": [""array1"", ""array2"", ""array3""]}'
Insert a document with an empty field array.
curl -X POST -H ""Content-Type: application/json"" -u ""admin:root"" ""http://localhost:6363/api/document/admin/test497?graph_type=instance&author=me&message=me"" --data-binary '{""@id"": ""ArrayTestDoc/IdForArrayDocEmpty"", ""@type"": ""ArrayTestDoc"", ""array"": []}' 
Read the document with the field array.
curl -X GET -H ""Content-Type: application/json"" -u ""admin:root"" ""http://localhost:6363/api/document/admin/test497?graph_type=instance""
Output:
 {""@id"":""ArrayTestDoc/IdForArrayDoc"", ""@type"":""ArrayTestDoc"", ""array"": [""array1"", ""array2"", ""array3"" ]}
{""@id"":""ArrayTestDoc/IdForArrayDocEmpty"", ""@type"":""ArrayTestDoc""}","to replicate # /bin / bash create the database xh ' http//adminroot@localhost6363 / api / db / admin / test497 ' < < eof { "" label""""l""""comment""""c "" } eof create a type that contain an array curl -x post -h "" content - type application / json "" -u "" adminroot "" "" http//localhost6363 / api / document / admin / test497graph_type = schema&author = me&message = -PRON- "" --data - binary ' { "" @id "" "" arraytestdoc "" "" @type "" "" class "" "" array "" { "" @type "" "" array "" "" @class "" "" xsdstring "" } } ' create a document in arraytestdoc curl -x post -h "" content - type application / json "" -u "" adminroot "" "" http//localhost6363 / api / document / admin / test497graph_type = instance&author = me&message = -PRON- "" --data - binary ' { "" @id "" "" arraytestdoc / idforarraydoc "" "" @type "" "" arraytestdoc "" "" array "" [ "" array1 "" "" array2 "" "" array3 "" ] } ' insert a document with an empty field array curl -x post -h "" content - type application / json "" -u "" adminroot "" "" http//localhost6363 / api / document / admin / test497graph_type = instance&author = me&message = -PRON- "" --data - binary ' { "" @id "" "" arraytestdoc / idforarraydocempty "" "" @type "" "" arraytestdoc "" "" array "" [ ] } ' read the document with the field array curl -x get -h "" content - type application / json "" -u "" adminroot "" "" http//localhost6363 / api / document / admin / test497graph_type = instance "" output { "" @id""""arraytestdoc / idforarraydoc "" "" @type""""arraytestdoc "" "" array "" [ "" array1 "" "" array2 "" "" array3 "" ] } { "" @id""""arraytestdoc / idforarraydocempty "" "" @type""""arraytestdoc "" }",0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,497,2021-09-01T13:42:41Z,matko,return empty array on document retrieval when there's no elements,https://github.com/terminusdb/terminusdb/issues/497,"when a document has a property which is an array, and that array is empty, the returned document will have that property missing.
It should instead return an empty list.",2021-09-13T14:50:41Z,AstroChelonian,https://github.com/terminusdb/terminusdb/issues/497#issuecomment-918272617,Accepted this as the default behaviour for now. We might re-visit this later.,accept this as the default behaviour for now -PRON- may re - visit this later,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,498,2021-09-01T13:54:57Z,matko,list cardinality refutation should return better witness,https://github.com/terminusdb/terminusdb/issues/498,"The witness for a missing property that should point to a list of some Thing is the same as the witness we get for a missing property that should point to that Thing directly. In both cases, you get the following witness:
{
   ""@type"": ""instance_nost_cardinality_one"",
   ""instance"": ""Some_Subject_ID"",
   ""predicate"": ""some_predicate"",
   ""class"": ""Thing""
}

In the case of a list, we should return a slightly different ""class"" that makes it clear that this is a list of Things, and not a Thing directly.",,,,,,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,500,2021-09-02T03:10:04Z,LogicalDash,API endpoint for commit log access,https://github.com/terminusdb/terminusdb/issues/500,"Is your feature request related to a problem? Please describe.
I have an app that would benefit from a display of recent changes to the graph.
Describe the solution you'd like
I'd like an API endpoint that returns an array of commit messages in a particular branch and timeframe.
Describe alternatives you've considered
Right now I'd have to use the CLI to generate that data ahead of time.
Additional context
Wouldn't this be useful when using client.ref(..) as well? Maybe I don't have a shell in the server, and need to look at the commit log to decide which commit I want to check out.",2021-09-06T13:57:44Z,GavinMendelGleason,https://github.com/terminusdb/terminusdb/issues/500#issuecomment-913670279,"That's a good idea @LogicalDash - we'll put it somewhere in our roadmap.
In the meantime there are library functions for this in the python and javascript client. Are you writing your own client?",that be a good idea @logicaldash - -PRON- will put -PRON- somewhere in -PRON- roadmap in the meantime there be library function for this in the python and javascript client be -PRON- write -PRON- own client,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,500,2021-09-02T03:10:04Z,LogicalDash,API endpoint for commit log access,https://github.com/terminusdb/terminusdb/issues/500,"Is your feature request related to a problem? Please describe.
I have an app that would benefit from a display of recent changes to the graph.
Describe the solution you'd like
I'd like an API endpoint that returns an array of commit messages in a particular branch and timeframe.
Describe alternatives you've considered
Right now I'd have to use the CLI to generate that data ahead of time.
Additional context
Wouldn't this be useful when using client.ref(..) as well? Maybe I don't have a shell in the server, and need to look at the commit log to decide which commit I want to check out.",2021-09-06T15:16:44Z,LogicalDash,https://github.com/terminusdb/terminusdb/issues/500#issuecomment-913722184,"Is this what you mean? Seriously, I can't tell... I guess this issue might be interpreted as a request for documentation.",be this what -PRON- mean seriously i can not tell i guess this issue may be interpret as a request for documentation,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,500,2021-09-02T03:10:04Z,LogicalDash,API endpoint for commit log access,https://github.com/terminusdb/terminusdb/issues/500,"Is your feature request related to a problem? Please describe.
I have an app that would benefit from a display of recent changes to the graph.
Describe the solution you'd like
I'd like an API endpoint that returns an array of commit messages in a particular branch and timeframe.
Describe alternatives you've considered
Right now I'd have to use the CLI to generate that data ahead of time.
Additional context
Wouldn't this be useful when using client.ref(..) as well? Maybe I don't have a shell in the server, and need to look at the commit log to decide which commit I want to check out.",2021-09-06T15:23:35Z,LogicalDash,https://github.com/terminusdb/terminusdb/issues/500#issuecomment-913726052,"To answer your question, I'd prefer not to implement my own client, and would like direction to the correct bits of the official clients.",to answer -PRON- question -PRON- 'd prefer not to implement -PRON- own client and would like direction to the correct bit of the official client,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,500,2021-09-02T03:10:04Z,LogicalDash,API endpoint for commit log access,https://github.com/terminusdb/terminusdb/issues/500,"Is your feature request related to a problem? Please describe.
I have an app that would benefit from a display of recent changes to the graph.
Describe the solution you'd like
I'd like an API endpoint that returns an array of commit messages in a particular branch and timeframe.
Describe alternatives you've considered
Right now I'd have to use the CLI to generate that data ahead of time.
Additional context
Wouldn't this be useful when using client.ref(..) as well? Maybe I don't have a shell in the server, and need to look at the commit log to decide which commit I want to check out.",2021-09-06T15:48:06Z,Cheukting,https://github.com/terminusdb/terminusdb/issues/500#issuecomment-913738729,Hi @LogicalDash we would really know what is your use case and help. Are you using our official JS client or Python client? If you are using Python client I know there is a method that may be useful to you? So far we have no direct endpoint to get commit history yet. Like @GavinMendelGleason said we will put this in our roadmap but right now you can get around by either making a WOQL query or the Python method (that wrap the query in its call) to get it from the database.,hi @logicaldash -PRON- would really know what be -PRON- use case and help be -PRON- use -PRON- official js client or python client if -PRON- be use python client i know there be a method that may be useful to -PRON- so far -PRON- have no direct endpoint to get commit history yet like @gavinmendelgleason say -PRON- will put this in -PRON- roadmap but right now -PRON- can get around by either make a woql query or the python method ( that wrap the query in -PRON- call ) to get -PRON- from the database,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,500,2021-09-02T03:10:04Z,LogicalDash,API endpoint for commit log access,https://github.com/terminusdb/terminusdb/issues/500,"Is your feature request related to a problem? Please describe.
I have an app that would benefit from a display of recent changes to the graph.
Describe the solution you'd like
I'd like an API endpoint that returns an array of commit messages in a particular branch and timeframe.
Describe alternatives you've considered
Right now I'd have to use the CLI to generate that data ahead of time.
Additional context
Wouldn't this be useful when using client.ref(..) as well? Maybe I don't have a shell in the server, and need to look at the commit log to decide which commit I want to check out.",2021-09-06T15:50:09Z,LogicalDash,https://github.com/terminusdb/terminusdb/issues/500#issuecomment-913739741,"the app is for organizing a cooperative. The org chart for the cooperative
is expected to change over time. We want to display its current state and
also a ticker of recent changes. We're using javascript.
…
On Mon, Sep 6, 2021, 11:48 AM Cheuk Ting Ho ***@***.***> wrote:
 Hi @LogicalDash <https://github.com/LogicalDash> would really know what
 is your use case? Are you using our official JS client or Python client? If
 you are using Python client I know there is a method
 <https://terminusdb.github.io/terminusdb-client-python/woqlClient.html#terminusdb_client.WOQLClient.get_commit_history>
 that may be useful to you? So far we have no direct endpoint to get commit
 history yet. Like @GavinMendelGleason
 <https://github.com/GavinMendelGleason> said we will put this in our
 roadmap but right now you can get around by either making a WOQL query or
 the Python method (that wrap the query in its call) to get it directly.

 —
 You are receiving this because you were mentioned.
 Reply to this email directly, view it on GitHub
 <#500 (comment)>,
 or unsubscribe
 <https://github.com/notifications/unsubscribe-auth/AAKFYVXLJDQZJHCBA5VGAZDUATPEBANCNFSM5DH6ANBA>
 .
 Triage notifications on the go with GitHub Mobile for iOS
 <https://apps.apple.com/app/apple-store/id1477376905?ct=notification-email&mt=8&pt=524675>
 or Android
 <https://play.google.com/store/apps/details?id=com.github.android&referrer=utm_campaign%3Dnotification-email%26utm_medium%3Demail%26utm_source%3Dgithub>.",the app be for organize a cooperative the org chart for the cooperative be expect to change over time -PRON- want to display -PRON- current state and also a ticker of recent change -PRON- be use javascript … on mon sep 6 2021 1148 am cheuk ting ho * * * @ * * * * * * > write hi @logicaldash < https//githubcom / logicaldash > would really know what be -PRON- use case be -PRON- use -PRON- official js client or python client if -PRON- be use python client i know there be a method < https//terminusdbgithubio / terminusdb - client - python / woqlclienthtml#terminusdb_clientwoqlclientget_commit_history > that may be useful to -PRON- so far -PRON- have no direct endpoint to get commit history yet like @gavinmendelgleason < https//githubcom / gavinmendelgleason > say -PRON- will put this in -PRON- roadmap but right now -PRON- can get around by either make a woql query or the python method ( that wrap the query in -PRON- call ) to get -PRON- directly — -PRON- be receive this because -PRON- be mention reply to this email directly view -PRON- on github < # 500 ( comment ) > or unsubscribe < https//githubcom / notification / unsubscribe - auth / aakfyvxljdqzjhcba5vgazduatpebancnfsm5dh6anba > triage notification on the go with github mobile for ios < https//appsapplecom / app / apple - store / id1477376905ct = notification - email&mt=8&pt=524675 > or android < https//playgooglecom / store / app / detailsid = comgithubandroid&referrer = utm_campaign%3dnotification - email%26utm_medium%3demail%26utm_source%3dgithub >,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,500,2021-09-02T03:10:04Z,LogicalDash,API endpoint for commit log access,https://github.com/terminusdb/terminusdb/issues/500,"Is your feature request related to a problem? Please describe.
I have an app that would benefit from a display of recent changes to the graph.
Describe the solution you'd like
I'd like an API endpoint that returns an array of commit messages in a particular branch and timeframe.
Describe alternatives you've considered
Right now I'd have to use the CLI to generate that data ahead of time.
Additional context
Wouldn't this be useful when using client.ref(..) as well? Maybe I don't have a shell in the server, and need to look at the commit log to decide which commit I want to check out.",2021-09-06T17:45:39Z,Cheukting,https://github.com/terminusdb/terminusdb/issues/500#issuecomment-913789322,"I see, there is definitely something (maybe a WOQL query) in the JS client to get the commit history, the team was using it for developing the dashboard. I cannot say for sure what it is on their behalf. @Francesca-Bit or @KittyJose would maybe give you some insight?",i see there be definitely something ( maybe a woql query ) in the js client to get the commit history the team be use -PRON- for develop the dashboard i can not say for sure what -PRON- be on -PRON- behalf @francesca - bit or @kittyjose would maybe give -PRON- some insight,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,500,2021-09-02T03:10:04Z,LogicalDash,API endpoint for commit log access,https://github.com/terminusdb/terminusdb/issues/500,"Is your feature request related to a problem? Please describe.
I have an app that would benefit from a display of recent changes to the graph.
Describe the solution you'd like
I'd like an API endpoint that returns an array of commit messages in a particular branch and timeframe.
Describe alternatives you've considered
Right now I'd have to use the CLI to generate that data ahead of time.
Additional context
Wouldn't this be useful when using client.ref(..) as well? Maybe I don't have a shell in the server, and need to look at the commit log to decide which commit I want to check out.",2021-09-06T21:33:36Z,Francesca-Bit,https://github.com/terminusdb/terminusdb/issues/500#issuecomment-913861081,"Hi we have 3 method in WOQL/WOQLLibrary
1)
let query = WOQL.lib().commits()
/**
 * get all the commits of a specific branch
 * if a timestamp is given, gets all the commits before the specified timestamp
 * @param {string} branch - the branch name
 * @param {number} limit - the max number of result
 * @param {number} timestamp - Unix timestamp in seconds
 */

WOQLLibrary.prototype.commits = function (branch=""main"",limit=10,timestamp=0)...




const = WOQL.lib().previousCommits('m8vpxewh2aovfauebfkbzwmj4qwr5lb')
/** 
*get commits older than the specified commit id
*/
WOQLLibrary.prototype.previousCommits = function(commit_id,limit=10){





WOQL.lib().first_commit()

/**
return the first commit
*/
WOQLLibrary.prototype.first_commit = function()...","hi -PRON- have 3 method in woql / woqllibrary 1 ) let query = woqllib()commits ( ) / * * * get all the commit of a specific branch * if a timestamp be give get all the commit before the specify timestamp * @param { string } branch - the branch name * @param { number } limit - the max number of result * @param { number } timestamp - unix timestamp in second * / woqllibraryprototypecommit = function ( branch=""main""limit=10timestamp=0 ) const = woqllib()previouscommits('m8vpxewh2aovfauebfkbzwmj4qwr5 lb ' ) / * * * get commit old than the specified commit -PRON- d * / woqllibraryprototypepreviouscommit = function(commit_idlimit=10 ) { woqllib()first_commit ( ) / * * return the first commit * / woqllibraryprototypefirst_commit = function ( )",0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,500,2021-09-02T03:10:04Z,LogicalDash,API endpoint for commit log access,https://github.com/terminusdb/terminusdb/issues/500,"Is your feature request related to a problem? Please describe.
I have an app that would benefit from a display of recent changes to the graph.
Describe the solution you'd like
I'd like an API endpoint that returns an array of commit messages in a particular branch and timeframe.
Describe alternatives you've considered
Right now I'd have to use the CLI to generate that data ahead of time.
Additional context
Wouldn't this be useful when using client.ref(..) as well? Maybe I don't have a shell in the server, and need to look at the commit log to decide which commit I want to check out.",2021-09-06T23:11:11Z,LogicalDash,https://github.com/terminusdb/terminusdb/issues/500#issuecomment-913892782,"This appears to be completely undocumented https://terminusdb.github.io/terminusdb-client-js/#/api/woqlClient.js?id=terminusdb-client-api
ctrl+f ""lib"", no results.","this appear to be completely undocumented https//terminusdbgithubio / terminusdb - client - js/#/api / woqlclientjsid = terminusdb - client - api ctrl+f "" lib "" no result",0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,500,2021-09-02T03:10:04Z,LogicalDash,API endpoint for commit log access,https://github.com/terminusdb/terminusdb/issues/500,"Is your feature request related to a problem? Please describe.
I have an app that would benefit from a display of recent changes to the graph.
Describe the solution you'd like
I'd like an API endpoint that returns an array of commit messages in a particular branch and timeframe.
Describe alternatives you've considered
Right now I'd have to use the CLI to generate that data ahead of time.
Additional context
Wouldn't this be useful when using client.ref(..) as well? Maybe I don't have a shell in the server, and need to look at the commit log to decide which commit I want to check out.",2021-09-07T09:31:47Z,Francesca-Bit,https://github.com/terminusdb/terminusdb/issues/500#issuecomment-914149917,"It is WOQL Object no WOQLClient
I added the example in the documentation too
this is the link
https://terminusdb.github.io/terminusdb-client-js/#/api/woql.js?id=lib",-PRON- be woql object no woqlclient i add the example in the documentation too this be the link https//terminusdbgithubio / terminusdb - client - js/#/api / woqljsid = lib,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,501,2021-09-02T09:28:46Z,matko,documentation property is not cardinality checked,https://github.com/terminusdb/terminusdb/issues/501,"@Francesca-Bit discovered that it is possible to have multiple documentation properties for a particular type, which subsequently causes errors while constructing a document out of the type.
We have to ensure that the documentation property is properly cardinality-checked.",2021-09-21T11:35:01Z,spl,https://github.com/terminusdb/terminusdb/issues/501#issuecomment-923895412,"In the following script, I create a database, insert a type with @documentation (POST), and replace that type with a new @documentation (PUT).
#!/bin/bash
set -ex

xh 'http://admin:root@localhost:6363/api/db/admin/tdb' label=l comment=c

xh 'http://admin:root@localhost:6363/api/document/admin/tdb' graph_type==schema author==a message==m <<EOF
{
  ""@id"": ""D"",
  ""@type"": ""Class"",
  ""@documentation"": {
    ""@comment"": ""comment 1""
  }
}
EOF

xh PUT 'http://admin:root@localhost:6363/api/document/admin/tdb' graph_type==schema author==a message==m <<EOF
{
  ""@id"": ""D"",
  ""@type"": ""Class"",
  ""@documentation"": {
    ""@comment"": ""comment 2""
  }
}
EOF

xh 'http://admin:root@localhost:6363/api/document/admin/tdb' graph_type==schema
Output from the last command:
HTTP/1.1 200 OK
Connection: keep-alive
Content-Length: 160
Content-Type: application/json; stream=true; charset=UTF-8
Date: Tue, 21 Sep 2021 09:02:15 GMT

{""@base"":""terminusdb:///data/"", ""@schema"":""terminusdb:///schema#"", ""@type"":""@context""}
{""@documentation"": {""@comment"":""comment 2""}, ""@id"":""D"", ""@type"":""Class""}
The @documentation is the one from the PUT.
I also dump the triples for the schema:
xh 'http://admin:root@localhost:6363/api/triples/admin/tdb/local/branch/main/schema'
@base <terminusdb:///schema#> .
@prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
@prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> .
@prefix woql: <http://terminusdb.com/schema/woql#> .
@prefix xsd: <http://www.w3.org/2001/XMLSchema#> .
@prefix xdd: <http://terminusdb.com/schema/xdd#> .
@prefix vio: <http://terminusdb.com/schema/vio#> .
@prefix sys: <http://terminusdb.com/schema/sys#> .
@prefix api: <http://terminusdb.com/schema/api#> .
@prefix owl: <http://www.w3.org/2002/07/owl#> .
@prefix doc: <data/> .

<schema#D>
  a sys:Class ;
  sys:documentation <schema#D/documentation/Documentation> .

<schema#D/documentation/Documentation>
  a sys:Documentation ;
  sys:comment ""comment 2"" .

<terminusdb://context>
  a sys:Context ;
  sys:base ""terminusdb:///data/"" ;
  sys:schema ""terminusdb:///schema#"" .
I believe this is what we'd expect.","in the follow script i create a database insert a type with @documentation ( post ) and replace that type with a new @documentation ( put ) # /bin / bash set -ex xh ' http//adminroot@localhost6363 / api / db / admin / tdb ' label = l comment = c xh ' http//adminroot@localhost6363 / api / document / admin / tdb ' graph_type==schema author==a message==m < < eof { "" @id "" "" d "" "" @type "" "" class "" "" @documentation "" { "" @comment "" "" comment 1 "" } } eof xh put ' http//adminroot@localhost6363 / api / document / admin / tdb ' graph_type==schema author==a message==m < < eof { "" @id "" "" d "" "" @type "" "" class "" "" @documentation "" { "" @comment "" "" comment 2 "" } } eof xh ' http//adminroot@localhost6363 / api / document / admin / tdb ' graph_type==schema output from the last command http/11 200 ok connection keep - alive content - length 160 content - type application / json stream = true charset = utf-8 date tue 21 sep 2021 090215 gmt { "" @base""""terminusdb///data/ "" "" @schema""""terminusdb///schema # "" "" @type""""@context "" } { "" @documentation "" { "" @comment""""comment 2 "" } "" @id""""d "" "" @type""""class "" } the @documentation be the one from the put i also dump the triple for the schema xh ' http//adminroot@localhost6363 / api / triple / admin / tdb / local / branch / main / schema ' @base < terminusdb///schema # > @prefix rdf < http//wwww3org/1999/02/22-rdf - syntax - ns # > @prefix rdfs < http//wwww3org/2000/01 / rdf - schema # > @prefix woql < http//terminusdbcom / schema / woql # > @prefix xsd < http//wwww3org/2001 / xmlschema # > @prefix xdd < http//terminusdbcom / schema / xdd # > @prefix vio < http//terminusdbcom / schema / vio # > @prefix sys < http//terminusdbcom / schema / sys # > @prefix api < http//terminusdbcom / schema / api # > @prefix owl < http//wwww3org/2002/07 / owl # > @prefix doc < data/ > < schema#d > a sysclass sysdocumentation < schema#d / documentation / documentation > < schema#d / documentation / documentation > a sysdocumentation syscomment "" comment 2 "" < terminusdb//context > a syscontext sysbase "" terminusdb///data/ "" sysschema "" terminusdb///schema # "" i believe this be what -PRON- 'd expect",0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,501,2021-09-02T09:28:46Z,matko,documentation property is not cardinality checked,https://github.com/terminusdb/terminusdb/issues/501,"@Francesca-Bit discovered that it is possible to have multiple documentation properties for a particular type, which subsequently causes errors while constructing a document out of the type.
We have to ensure that the documentation property is properly cardinality-checked.",2021-09-21T12:10:15Z,GavinMendelGleason,https://github.com/terminusdb/terminusdb/issues/501#issuecomment-923921613,"Yes, this appears to be correct. Since it's now hard to get this error to trigger, I would suggest moving it out of the milestone and we can fix it later.",yes this appear to be correct since -PRON- be now hard to get this error to trigger i would suggest move -PRON- out of the milestone and -PRON- can fix -PRON- later,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,501,2021-09-02T09:28:46Z,matko,documentation property is not cardinality checked,https://github.com/terminusdb/terminusdb/issues/501,"@Francesca-Bit discovered that it is possible to have multiple documentation properties for a particular type, which subsequently causes errors while constructing a document out of the type.
We have to ensure that the documentation property is properly cardinality-checked.",2021-09-21T12:17:35Z,spl,https://github.com/terminusdb/terminusdb/issues/501#issuecomment-923927916,Agreed. Done.,agree do,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,502,2021-09-02T09:54:42Z,matko,Rewrite unit tests to not run as admin,https://github.com/terminusdb/terminusdb/issues/502,"We've set up the vast majority of our unit tests to run as admin. This means that most paths skip the authorization code paths, and these end up not getting tested.
I think it'd be a good idea to change the majority of tests over to run as a normal user, so we do test these paths.",,,,,,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,505,2021-09-02T10:53:25Z,KittyJose,Response Time is high for large data set when adding a new document class ,https://github.com/terminusdb/terminusdb/issues/505,"Describe the bug
Response Time is high for large data set when adding a new document class. Maybe we should not do schema check when adding a new class and only do if we change type of a property or delete a link etc? I dont know the right way :)
To Reproduce
I used the stock ticker data set and added a new document class via Model building tool. Response time was 2.4 mins",2021-09-02T16:36:21Z,GavinMendelGleason,https://github.com/terminusdb/terminusdb/issues/505#issuecomment-911865394,Blast radius is way too high. This requires more clever checking about when to do schema_instance checking.,blast radius be way too high this require more clever checking about when to do schema_instance check,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,506,2021-09-02T13:45:10Z,spl,No error when creating an instance with no field,https://github.com/terminusdb/terminusdb/issues/506,"I expected to get an error when I created a document that, according to a schema, required a field, but the field was missing from the document.
Script:
#!/bin/bash

set -ex

# Create the database.
xh 'http://admin:root@localhost:6363/api/db/admin/t' <<EOF
{""label"":""l"",""comment"":""c""}
EOF

# Create the schema.
xh 'http://admin:root@localhost:6363/api/document/admin/t?author=a&message=m&graph_type=schema' <<EOF
{
  ""@type"": ""Class"",
  ""@id"": ""String"",
  ""s"": ""xsd:string""
}
EOF

# Post the instances.
xh 'http://admin:root@localhost:6363/api/document/admin/t?author=a&message=m' <<EOF
{
  ""@id"": ""without-field"",
  ""@type"": ""String""
}
EOF

# View the instance.
xh 'http://admin:root@localhost:6363/api/document/admin/t'
Output
+ xh http://admin:root@localhost:6363/api/db/admin/t
HTTP/1.1 200 OK
connection: keep-alive
content-length: 60
content-type: application/json; charset=UTF-8
date: Thu, 02 Sep 2021 13:44:11 GMT

{
    ""@type"": ""api:DbCreateResponse"",
    ""api:status"": ""api:success""
}


+ xh 'http://admin:root@localhost:6363/api/document/admin/t?author=a&message=m&graph_type=schema'
HTTP/1.1 200 OK
connection: keep-alive
content-length: 12
content-type: application/json; charset=UTF-8
date: Thu, 02 Sep 2021 13:44:12 GMT

[
    ""String""
]


+ xh 'http://admin:root@localhost:6363/api/document/admin/t?author=a&message=m'
HTTP/1.1 200 OK
connection: keep-alive
content-length: 38
content-type: application/json; charset=UTF-8
date: Thu, 02 Sep 2021 13:44:14 GMT

[
    ""terminusdb:///data/without-field""
]


+ xh http://admin:root@localhost:6363/api/document/admin/t
HTTP/1.1 200 OK
connection: keep-alive
content-length: 42
content-type: application/json; stream=true; charset=UTF-8
date: Thu, 02 Sep 2021 13:44:14 GMT

{
    ""@id"": ""without-field"",
    ""@type"": ""String""
}",2021-09-02T15:12:41Z,matko,https://github.com/terminusdb/terminusdb/issues/506#issuecomment-911793119,"It looks like the assumed default in the HTTP api is schemaless. If you explicitely set schema to true as part of the input document, it will actually schema check.
I am not sure if we should change this default. Right now though it doesn't match what we do in the cli tool.",-PRON- look like the assumed default in the http api be schemaless if -PRON- explicitely set schema to true as part of the input document -PRON- will actually schema check i be not sure if -PRON- should change this default right now though -PRON- do not match what -PRON- do in the cli tool,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,506,2021-09-02T13:45:10Z,spl,No error when creating an instance with no field,https://github.com/terminusdb/terminusdb/issues/506,"I expected to get an error when I created a document that, according to a schema, required a field, but the field was missing from the document.
Script:
#!/bin/bash

set -ex

# Create the database.
xh 'http://admin:root@localhost:6363/api/db/admin/t' <<EOF
{""label"":""l"",""comment"":""c""}
EOF

# Create the schema.
xh 'http://admin:root@localhost:6363/api/document/admin/t?author=a&message=m&graph_type=schema' <<EOF
{
  ""@type"": ""Class"",
  ""@id"": ""String"",
  ""s"": ""xsd:string""
}
EOF

# Post the instances.
xh 'http://admin:root@localhost:6363/api/document/admin/t?author=a&message=m' <<EOF
{
  ""@id"": ""without-field"",
  ""@type"": ""String""
}
EOF

# View the instance.
xh 'http://admin:root@localhost:6363/api/document/admin/t'
Output
+ xh http://admin:root@localhost:6363/api/db/admin/t
HTTP/1.1 200 OK
connection: keep-alive
content-length: 60
content-type: application/json; charset=UTF-8
date: Thu, 02 Sep 2021 13:44:11 GMT

{
    ""@type"": ""api:DbCreateResponse"",
    ""api:status"": ""api:success""
}


+ xh 'http://admin:root@localhost:6363/api/document/admin/t?author=a&message=m&graph_type=schema'
HTTP/1.1 200 OK
connection: keep-alive
content-length: 12
content-type: application/json; charset=UTF-8
date: Thu, 02 Sep 2021 13:44:12 GMT

[
    ""String""
]


+ xh 'http://admin:root@localhost:6363/api/document/admin/t?author=a&message=m'
HTTP/1.1 200 OK
connection: keep-alive
content-length: 38
content-type: application/json; charset=UTF-8
date: Thu, 02 Sep 2021 13:44:14 GMT

[
    ""terminusdb:///data/without-field""
]


+ xh http://admin:root@localhost:6363/api/document/admin/t
HTTP/1.1 200 OK
connection: keep-alive
content-length: 42
content-type: application/json; stream=true; charset=UTF-8
date: Thu, 02 Sep 2021 13:44:14 GMT

{
    ""@id"": ""without-field"",
    ""@type"": ""String""
}",2021-09-02T18:04:47Z,spl,https://github.com/terminusdb/terminusdb/issues/506#issuecomment-911931655,"Indeed, I see an error now!
#!/bin/bash

set -ex

# Create the database.
xh 'http://admin:root@localhost:6363/api/db/admin/t' <<EOF
{""label"":""l"",""comment"":""c""}
EOF

# Create the schema.
xh 'http://admin:root@localhost:6363/api/document/admin/t?author=a&message=m&graph_type=schema' <<EOF
{""@type"": ""Class"",""@id"": ""String"",""s"": ""xsd:string""}
EOF

# Post the instance.
xh 'http://admin:root@localhost:6363/api/document/admin/t?author=a&message=m' <<EOF
{""@type"": ""String""}
EOF
HTTP/1.1 400 Bad Request
connection: keep-alive
content-length: 427
content-type: application/json
date: Thu, 02 Sep 2021 18:02:14 GMT

{
    ""@type"": ""api:InsertDocumentErrorResponse"",
    ""api:error"": {
        ""@type"": ""api:SchemaCheckFailure"",
        ""api:witnesses"": [
            {
                ""@type"": ""instance_not_cardinality_one"",
                ""class"": ""http://www.w3.org/2001/XMLSchema#string"",
                ""instance"": ""terminusdb:///data/String/54729217485e094b04e5939f90f39b94"",
                ""predicate"": ""terminusdb:///schema#s""
            }
        ]
    },
    ""api:message"": ""Schema check failure"",
    ""api:status"": ""api:failure""
}","indeed i see an error now # /bin / bash set -ex # create the database xh ' http//adminroot@localhost6363 / api / db / admin / t ' < < eof { "" label""""l""""comment""""c "" } eof # create the schema xh ' http//adminroot@localhost6363 / api / document / admin / tauthor = a&message = m&graph_type = schema ' < < eof { "" @type "" "" class""""@id "" "" string""""s "" "" xsdstring "" } eof # post the instance xh ' http//adminroot@localhost6363 / api / document / admin / tauthor = a&message = m ' < < eof { "" @type "" "" string "" } eof http/11 400 bad request connection keep - alive content - length 427 content - type application / json date thu 02 sep 2021 180214 gmt { "" @type "" "" apiinsertdocumenterrorresponse "" "" apierror "" { "" @type "" "" apischemacheckfailure "" "" apiwitnesse "" [ { "" @type "" "" instance_not_cardinality_one "" "" class "" "" http//wwww3org/2001 / xmlschema#string "" "" instance "" "" terminusdb///data / string/54729217485e094b04e5939f90f39b94 "" "" predicate "" "" terminusdb///schema#s "" } ] } "" apimessage "" "" schema check failure "" "" apistatus "" "" apifailure "" }",0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,509,2021-09-02T18:00:00Z,spl,SubmittedIdDoesNotMatchBase error is cryptic,https://github.com/terminusdb/terminusdb/issues/509,"As of f58d7a0, we have a wonderful new error message. I'm guessing it has to do with a mismatch between the @key @type and the provided @id, but it is difficult to understand. Can we improve on the error message?
Script (run from a freshly initialized store):
#!/bin/bash

set -ex

# Create the database.
xh 'http://admin:root@localhost:6363/api/db/admin/t' <<EOF
{""label"":""l"",""comment"":""c""}
EOF

# Create the schema.
xh 'http://admin:root@localhost:6363/api/document/admin/t?author=a&message=m&graph_type=schema' <<EOF
{""@type"": ""Class"",""@id"": ""String""}
EOF

# Post an instance.
xh 'http://admin:root@localhost:6363/api/document/admin/t?author=a&message=m' <<EOF
{""@id"": ""something"",""@type"": ""String""}
EOF
Output of last command:
HTTP/1.1 400 Bad Request
connection: keep-alive
content-length: 402
content-type: application/json
date: Thu, 02 Sep 2021 17:47:32 GMT

{
    ""@type"": ""api:InsertDocumentErrorResponse"",
    ""api:error"": {
        ""@type"": ""api:SubmittedIdDoesNotMatchBase"",
        ""api:base"": ""String/"",
        ""api:document"": {
            ""@id"": ""something"",
            ""@type"": ""String""
        },
        ""api:submitted_id"": ""terminusdb:///data/something""
    },
    ""api:message"": ""Document was submitted with id 'terminusdb:///data/something' which does not match base 'String/'"",
    ""api:status"": ""api:failure""
}
Some questions that arise from reading the above:

What is api:base? Does it have anything to do with @base? If so, how? If not, then perhaps it should be called something else.
Why does String/ have the / at the end?
Where did terminusdb:///data/something come from? I gave the @id something, but why am I seeing terminusdb:///data/ in front of it? If terminusdb:///data/ is the default @base, should String/ also have terminusdb:///data/ in front of it?
Is terminusdb:///data/something is supposed to match String/? How?",2021-09-02T20:29:51Z,GavinMendelGleason,https://github.com/terminusdb/terminusdb/issues/509#issuecomment-912032213,"Id's should match the default 'base' that is used to form their id. You can set the base to the empty string to get no base (and no constraint).
The response message should return either an expanded Base (in which case you'll see terminusdb:///data/something/BASE or they should both be truncated so that the comparison is more obvious.
The URI base is formed from either the @base provided with the class, or with the type by default.",-PRON- d 's should match the default ' base ' that be use to form -PRON- -PRON- d -PRON- can set the base to the empty string to get no base ( and no constraint ) the response message should return either an expand base ( in which case -PRON- will see terminusdb///data / something / base or -PRON- should both be truncate so that the comparison be more obvious the uri base be form from either the @base provide with the class or with the type by default,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,509,2021-09-02T18:00:00Z,spl,SubmittedIdDoesNotMatchBase error is cryptic,https://github.com/terminusdb/terminusdb/issues/509,"As of f58d7a0, we have a wonderful new error message. I'm guessing it has to do with a mismatch between the @key @type and the provided @id, but it is difficult to understand. Can we improve on the error message?
Script (run from a freshly initialized store):
#!/bin/bash

set -ex

# Create the database.
xh 'http://admin:root@localhost:6363/api/db/admin/t' <<EOF
{""label"":""l"",""comment"":""c""}
EOF

# Create the schema.
xh 'http://admin:root@localhost:6363/api/document/admin/t?author=a&message=m&graph_type=schema' <<EOF
{""@type"": ""Class"",""@id"": ""String""}
EOF

# Post an instance.
xh 'http://admin:root@localhost:6363/api/document/admin/t?author=a&message=m' <<EOF
{""@id"": ""something"",""@type"": ""String""}
EOF
Output of last command:
HTTP/1.1 400 Bad Request
connection: keep-alive
content-length: 402
content-type: application/json
date: Thu, 02 Sep 2021 17:47:32 GMT

{
    ""@type"": ""api:InsertDocumentErrorResponse"",
    ""api:error"": {
        ""@type"": ""api:SubmittedIdDoesNotMatchBase"",
        ""api:base"": ""String/"",
        ""api:document"": {
            ""@id"": ""something"",
            ""@type"": ""String""
        },
        ""api:submitted_id"": ""terminusdb:///data/something""
    },
    ""api:message"": ""Document was submitted with id 'terminusdb:///data/something' which does not match base 'String/'"",
    ""api:status"": ""api:failure""
}
Some questions that arise from reading the above:

What is api:base? Does it have anything to do with @base? If so, how? If not, then perhaps it should be called something else.
Why does String/ have the / at the end?
Where did terminusdb:///data/something come from? I gave the @id something, but why am I seeing terminusdb:///data/ in front of it? If terminusdb:///data/ is the default @base, should String/ also have terminusdb:///data/ in front of it?
Is terminusdb:///data/something is supposed to match String/? How?",2021-09-03T10:24:31Z,matko,https://github.com/terminusdb/terminusdb/issues/509#issuecomment-912432669,"It may be confusing that we have both @base for the entire schema, and @base for individual types. For the schema, it denotes the default prefix for all nodes in the instance graph. For individual types, it denotes part of the prefix specific to that type. By default, this defaults to the class name but it is overridable.
So for example, if you have defined a class Person, and your schema base is 'http://base/', then ids will be like

http://base/Person/mark
http://base/Person/sean

Here, 'http://base/' is the @base for the entire schema, and then 'Person/' is the @base for the Person type.
It is perhaps confusing that we have these two meanings for @base. I'm not sure if in this stage we can still alter that.",-PRON- may be confusing that -PRON- have both @base for the entire schema and @base for individual type for the schema -PRON- denote the default prefix for all node in the instance graph for individual type -PRON- denote part of the prefix specific to that type by default this default to the class name but -PRON- be overridable so for example if -PRON- have define a class person and -PRON- schema base be ' http//base/ ' then ids will be like http//base / person / mark http//base / person / sean here ' http//base/ ' be the @base for the entire schema and then ' person/ ' be the @base for the person type -PRON- be perhaps confusing that -PRON- have these two meaning for @base -PRON- be not sure if in this stage -PRON- can still alter that,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,509,2021-09-02T18:00:00Z,spl,SubmittedIdDoesNotMatchBase error is cryptic,https://github.com/terminusdb/terminusdb/issues/509,"As of f58d7a0, we have a wonderful new error message. I'm guessing it has to do with a mismatch between the @key @type and the provided @id, but it is difficult to understand. Can we improve on the error message?
Script (run from a freshly initialized store):
#!/bin/bash

set -ex

# Create the database.
xh 'http://admin:root@localhost:6363/api/db/admin/t' <<EOF
{""label"":""l"",""comment"":""c""}
EOF

# Create the schema.
xh 'http://admin:root@localhost:6363/api/document/admin/t?author=a&message=m&graph_type=schema' <<EOF
{""@type"": ""Class"",""@id"": ""String""}
EOF

# Post an instance.
xh 'http://admin:root@localhost:6363/api/document/admin/t?author=a&message=m' <<EOF
{""@id"": ""something"",""@type"": ""String""}
EOF
Output of last command:
HTTP/1.1 400 Bad Request
connection: keep-alive
content-length: 402
content-type: application/json
date: Thu, 02 Sep 2021 17:47:32 GMT

{
    ""@type"": ""api:InsertDocumentErrorResponse"",
    ""api:error"": {
        ""@type"": ""api:SubmittedIdDoesNotMatchBase"",
        ""api:base"": ""String/"",
        ""api:document"": {
            ""@id"": ""something"",
            ""@type"": ""String""
        },
        ""api:submitted_id"": ""terminusdb:///data/something""
    },
    ""api:message"": ""Document was submitted with id 'terminusdb:///data/something' which does not match base 'String/'"",
    ""api:status"": ""api:failure""
}
Some questions that arise from reading the above:

What is api:base? Does it have anything to do with @base? If so, how? If not, then perhaps it should be called something else.
Why does String/ have the / at the end?
Where did terminusdb:///data/something come from? I gave the @id something, but why am I seeing terminusdb:///data/ in front of it? If terminusdb:///data/ is the default @base, should String/ also have terminusdb:///data/ in front of it?
Is terminusdb:///data/something is supposed to match String/? How?",2021-09-03T20:23:56Z,GavinMendelGleason,https://github.com/terminusdb/terminusdb/issues/509#issuecomment-912790609,It probably is possible to alter it at this point as it is auto-translated internally to a fully qualified URL. As long as we keep the fully qualified URL the same it should be ok.,-PRON- probably be possible to alter -PRON- at this point as -PRON- be auto - translate internally to a fully qualified url as long as -PRON- keep the fully qualified url the same -PRON- should be ok,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,511,2021-09-03T12:46:56Z,GavinMendelGleason,Add more explicit documentation that keys are not inherited,https://github.com/terminusdb/terminusdb/issues/511,,2021-09-03T20:26:31Z,GavinMendelGleason,https://github.com/terminusdb/terminusdb/issues/511#issuecomment-912791809,This is now covered in the Beta documentation - both the reference and the explainer,this be now cover in the beta documentation - both the reference and the explainer,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,512,2021-09-03T13:38:59Z,matko,Escape slashes from ids,https://github.com/terminusdb/terminusdb/issues/512,"Since slashes have a special meaning in id paths, we should not allow them in names of things. We must ensure that they're escaped everywhere.",2021-09-03T20:25:27Z,GavinMendelGleason,https://github.com/terminusdb/terminusdb/issues/512#issuecomment-912791285,Fixed in #516,fix in # 516,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,515,2021-09-03T14:49:36Z,spl,true and false unexpectedly converted to strings,https://github.com/terminusdb/terminusdb/issues/515,"This issue is similar to #450, but it needs to be handled differently.
The problem is that true and false get converted to strings when passed to a field whose type is xsd:string.
#!/bin/bash
set -ex

# Create the database.
xh 'http://admin:root@localhost:6363/api/db/admin/t' <<EOF
{""label"":""l"",""comment"":""c""}
EOF

# Create the schema.
xh 'http://admin:root@localhost:6363/api/document/admin/t?author=a&message=m&graph_type=schema' <<EOF
{""@type"": ""Class"",""@id"": ""String"",""s"": ""xsd:string""}
EOF

# Post the instance.
xh 'http://admin:root@localhost:6363/api/document/admin/t?author=a&message=m' <<EOF
{""@type"": ""String"",""s"":false}
EOF

# View the instance.
xh 'http://admin:root@localhost:6363/api/document/admin/t'
Output of the last command:
HTTP/1.1 200 OK
connection: keep-alive
content-length: 89
content-type: application/json; stream=true; charset=UTF-8
date: Fri, 03 Sep 2021 14:45:53 GMT

{
    ""@id"": ""String/1d18344d4b3cf6f5835fbe32691822b0"",
    ""@type"": ""String"",
    ""s"": ""false""
}
Note the ""false"" in the output.",2021-09-03T15:00:29Z,spl,https://github.com/terminusdb/terminusdb/issues/515#issuecomment-912603645,"This change:
diff --git a/src/core/document/json.pl b/src/core/document/json.pl
index d4a9359a..0bbaf268 100644
--- a/src/core/document/json.pl
+++ b/src/core/document/json.pl
@@ -556,6 +556,10 @@ value_expand_list([Value|Vs], DB, Context, Elt_Type, [Expanded|Exs]) :-
 % Unit type expansion
 context_value_expand(_,_,[],json{},[]) :-
     !.
+context_value_expand(_,_,Value,_,V) :-
+    memberchk(Value, [true, false]),
+    !,
+    V = json{'@type': ""http://www.w3.org/2001/XMLSchema#boolean"", '@value': Value}.
 context_value_expand(DB,Context,Value,Expansion,V) :-
     get_dict('@container', Expansion, _),
     !,
produces this error:
HTTP/1.1 500 Internal Server Error
connection: keep-alive
content-length: 3295
content-type: application/json
date: Fri, 03 Sep 2021 14:58:40 GMT

{
    ""api:message"": ""Error: unknown_type_casting_error(\""http://www.w3.org/2001/XMLSchema#boolean\"")\n  [46] throw(error(unknown_type_casting_error(\""http://www.w3.org/2001/XMLSchema#boolean\""),_6360))\n  [44] catch(api_document:do_or_die(...,...),error(unknown_type_casting_error(\""http://www.w3.org/2001/XMLSchema#boolean\""),_6416),api_document:(...;...)) at /usr/local/Cellar/swi-prolog/8.2.4/libexec/lib/swipl/boot/init.pl:533\n  [42] api_document:api_insert_document_(instance,transaction_object{commit_info:commit_info{},descriptor:branch_descriptor{branch_name:\""main\"",repository_descriptor: ...},inference_objects:[],instance_objects:[...],parent:transaction_object{descriptor: ...,inference_objects:[],instance_objects: ...,parent: ...,schema_objects: ...},schema_objects:[...]},<stream>(0x7fbb9479fa40),_6474) at /Users/leather/projects/terminusdb/terminusdb/src/core/api/api_document.pl:188\n  [41] '$bags':findall_loop(_6590,api_document:api_insert_document_(instance,...,<stream>(0x7fbb9479fa40),_6612),_6594,[]) at /usr/local/Cellar/swi-prolog/8.2.4/libexec/lib/swipl/boot/bags.pl:99\n  [40] setup_call_catcher_cleanup('$bags':'$new_findall_bag','$bags':findall_loop(_6664,...,_6668,[]),_6646,'$bags':'$destroy_findall_bag') at /usr/local/Cellar/swi-prolog/8.2.4/libexec/lib/swipl/boot/init.pl:614\n  [36] '<meta-call>'(api_document:(...;...)) <foreign>\n  [35] database:with_transaction_(query_context{all_witnesses:false,authorization:'terminusdb://system/data/User/admin',bindings:[],commit_info:commit_info{author:a,message:m},default_collection:branch_descriptor{branch_name:\""main\"",repository_descriptor: ...},files:[],filter:type_filter{types: ...},prefixes:_6838{'@base':\""terminusdb:///data/\"",'@schema':\""terminusdb:///schema#\"",'@type':'Context',api:'http://terminusdb.com/schema/api#',owl:'http://www.w3.org/2002/07/owl#',rdf:'http://www.w3.org/1999/02/22-rdf-syntax-ns#',rdfs:'http://www.w3.org/2000/01/rdf-schema#',sys:'http://terminusdb.com/schema/sys#',vio:'http://terminusdb.com/schema/vio#',woql:'http://terminusdb.com/schema/woql#',xdd:'http://terminusdb.com/schema/xdd#',xsd:'http://www.w3.org/2001/XMLSchema#'},selected:[],system:system_descriptor{},transaction_objects:[...],update_guard:_6808,write_graph:branch_graph{branch_name:\""main\"",database_name:\""t\"",organization_name:\""admin\"",repository_name:\""local\"",type:instance}},api_document:(...;...),_6762) at /Users/leather/projects/terminusdb/terminusdb/src/core/transaction/database.pl:220\n  [34] setup_call_catcher_cleanup(database:true,database:with_transaction_(...,...,_7004),_6982,database:true) at /usr/local/Cellar/swi-prolog/8.2.4/libexec/lib/swipl/boot/init.pl:614\n  [30] '<meta-call>'('<garbage_collected>') <foreign>\n  [29] catch(routes:(...,...),error(unknown_type_casting_error(\""http://www.w3.org/2001/XMLSchema#boolean\""),context(_7104,_7106)),routes:do_or_die(...,...)) at /usr/local/Cellar/swi-prolog/8.2.4/libexec/lib/swipl/boot/init.pl:532\n  [28] catch_with_backtrace('<garbage_collected>','<garbage_collected>','<garbage_collected>') at /usr/local/Cellar/swi-prolog/8.2.4/libexec/lib/swipl/boot/init.pl:582\n\nNote: some frames are missing due to last-call optimization.\nRe-run your program in debug mode (:- debug.) to get more detail.\n\n"",
    ""api:status"": ""api:server_error""
}

I'm not sure if I'm doing the change above correctly.","this change diff --git a / src / core / document / jsonpl b / src / core / document / jsonpl index d4a9359a0bbaf268 100644 --- a / src / core / document / jsonpl + + + b / src / core / document / jsonpl @@ -5566 +55610 @@ value_expand_list([value|vs ] db context elt_type [ expanded|exs ] ) - % unit type expansion context_value_expand(__[]json { } [ ] ) - + context_value_expand(__value_v ) - + memberchk(value [ true false ] ) + + v = json{'@type ' "" http//wwww3org/2001 / xmlschema#boolean "" ' @value ' value } context_value_expand(dbcontextvalueexpansionv ) - get_dict('@container ' expansion _ ) produce this error http/11 500 internal server error connection keep - alive content - length 3295 content - type application / json date fri 03 sep 2021 145840 gmt { "" apimessage "" "" error unknown_type_casting_error(\""http//wwww3org/2001 / xmlschema#boolean\"")\n [ 46 ] throw(error(unknown_type_casting_error(\""http//wwww3org/2001 / xmlschema#boolean\"")_6360))\n [ 44 ] catch(api_documentdo_or_die()error(unknown_type_casting_error(\""http//wwww3org/2001 / xmlschema#boolean\"")_6416)api_document ( ) ) at /usr / local / cellar / swi - prolog/824 / libexec / lib / swipl / boot / initpl533\n [ 42 ] api_documentapi_insert_document_(instancetransaction_object{commit_infocommit_info{}descriptorbranch_descriptor{branch_name\""main\""repository_descriptor } inference_objects[]instance_objects[]parenttransaction_object{descriptor inference_objects[]instance_objects parent schema_object } schema_objects[]}<stream>(0x7fbb9479fa40)_6474 ) at /users / leather / project / terminusdb / terminusdb / src / core / api / api_documentpl188\n [ 41 ] ' $ bags'findall_loop(_6590api_documentapi_insert_document_(instance < stream>(0x7fbb9479fa40)_6612)_6594 [ ] ) at /usr / local / cellar / swi - prolog/824 / libexec / lib / swipl / boot / bagspl99\n [ 40 ] setup_call_catcher_cleanup('$bags''$new_findall_bag''$bags'findall_loop(_6664_6668[])_6646'$bags''$destroy_findall_bag ' ) at /usr / local / cellar / swi - prolog/824 / libexec / lib / swipl / boot / initpl614\n [ 36 ] ' < meta - call>'(api_document ( ) ) < foreign>\n [ 35 ] databasewith_transaction_(query_context{all_witnessesfalseauthorization'terminusdb//system / datum / user / admin'bindings[]commit_infocommit_info{authoramessagem}default_collectionbranch_descriptor{branch_name\""main\""repository_descriptor } files[]filtertype_filter{types } prefixes_6838{'@base'\""terminusdb///data/\""'@schema'\""terminusdb///schema#\""'@type''context'api'http//terminusdbcom / schema / api#'owl'http//wwww3org/2002/07 / owl#'rdf'http//wwww3org/1999/02/22-rdf - syntax - ns#'rdfs'http//wwww3org/2000/01 / rdf - schema#'sys'http//terminusdbcom / schema / sys#'vio'http//terminusdbcom / schema / vio#'woql'http//terminusdbcom / schema / woql#'xdd'http//terminusdbcom / schema / xdd#'xsd'http//wwww3org/2001 / xmlschema#'}selected[]systemsystem_descriptor{}transaction_objects[]update_guard_6808write_graphbranch_graph{branch_name\""main\""database_name\""t\""organization_name\""admin\""repository_name\""local\""typeinstance}}api_document()_6762 ) at /users / leather / project / terminusdb / terminusdb / src / core / transaction / databasepl220\n [ 34 ] setup_call_catcher_cleanup(databasetruedatabasewith_transaction_(_7004)_6982databasetrue ) at /usr / local / cellar / swi - prolog/824 / libexec / lib / swipl / boot / initpl614\n [ 30 ] ' < meta - call>'('<garbage_collecte > ' ) < foreign>\n [ 29 ] catch(routes()error(unknown_type_casting_error(\""http//wwww3org/2001 / xmlschema#boolean\"")context(_7104_7106))routesdo_or_die ( ) ) at /usr / local / cellar / swi - prolog/824 / libexec / lib / swipl / boot / initpl532\n [ 28 ] catch_with_backtrace('<garbage_collected>''<garbage_collected>''<garbage_collecte > ' ) at /usr / local / cellar / swi - prolog/824 / libexec / lib / swipl / boot / initpl582\n\nnote some frame be miss due to last - call optimization\nre - run -PRON- program in debug mode ( - debug ) to get more detail\n\n "" "" apistatus "" "" apiserver_error "" } -PRON- be not sure if -PRON- be do the change above correctly",0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,515,2021-09-03T14:49:36Z,spl,true and false unexpectedly converted to strings,https://github.com/terminusdb/terminusdb/issues/515,"This issue is similar to #450, but it needs to be handled differently.
The problem is that true and false get converted to strings when passed to a field whose type is xsd:string.
#!/bin/bash
set -ex

# Create the database.
xh 'http://admin:root@localhost:6363/api/db/admin/t' <<EOF
{""label"":""l"",""comment"":""c""}
EOF

# Create the schema.
xh 'http://admin:root@localhost:6363/api/document/admin/t?author=a&message=m&graph_type=schema' <<EOF
{""@type"": ""Class"",""@id"": ""String"",""s"": ""xsd:string""}
EOF

# Post the instance.
xh 'http://admin:root@localhost:6363/api/document/admin/t?author=a&message=m' <<EOF
{""@type"": ""String"",""s"":false}
EOF

# View the instance.
xh 'http://admin:root@localhost:6363/api/document/admin/t'
Output of the last command:
HTTP/1.1 200 OK
connection: keep-alive
content-length: 89
content-type: application/json; stream=true; charset=UTF-8
date: Fri, 03 Sep 2021 14:45:53 GMT

{
    ""@id"": ""String/1d18344d4b3cf6f5835fbe32691822b0"",
    ""@type"": ""String"",
    ""s"": ""false""
}
Note the ""false"" in the output.",2021-09-03T17:35:32Z,GavinMendelGleason,https://github.com/terminusdb/terminusdb/issues/515#issuecomment-912699582,"That looked plausible!  Not sure what that casting error means, it's pretty spartan.",that look plausible not sure what that cast error mean -PRON- be pretty spartan,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,515,2021-09-03T14:49:36Z,spl,true and false unexpectedly converted to strings,https://github.com/terminusdb/terminusdb/issues/515,"This issue is similar to #450, but it needs to be handled differently.
The problem is that true and false get converted to strings when passed to a field whose type is xsd:string.
#!/bin/bash
set -ex

# Create the database.
xh 'http://admin:root@localhost:6363/api/db/admin/t' <<EOF
{""label"":""l"",""comment"":""c""}
EOF

# Create the schema.
xh 'http://admin:root@localhost:6363/api/document/admin/t?author=a&message=m&graph_type=schema' <<EOF
{""@type"": ""Class"",""@id"": ""String"",""s"": ""xsd:string""}
EOF

# Post the instance.
xh 'http://admin:root@localhost:6363/api/document/admin/t?author=a&message=m' <<EOF
{""@type"": ""String"",""s"":false}
EOF

# View the instance.
xh 'http://admin:root@localhost:6363/api/document/admin/t'
Output of the last command:
HTTP/1.1 200 OK
connection: keep-alive
content-length: 89
content-type: application/json; stream=true; charset=UTF-8
date: Fri, 03 Sep 2021 14:45:53 GMT

{
    ""@id"": ""String/1d18344d4b3cf6f5835fbe32691822b0"",
    ""@type"": ""String"",
    ""s"": ""false""
}
Note the ""false"" in the output.",2021-09-06T08:28:06Z,spl,https://github.com/terminusdb/terminusdb/issues/515#issuecomment-913453509,"@GavinMendelGleason This seems to be fixed in fc782b1. The response to:
xh 'http://admin:root@localhost:6363/api/document/admin/t?author=a&message=m' <<EOF
{""@type"": ""String"",""s"":false}
EOF
is:
HTTP/1.1 400 Bad Request
connection: keep-alive
content-length: 418
content-type: application/json
date: Mon, 06 Sep 2021 08:20:08 GMT

{
    ""api:message"": ""Type error for {\""@type\"":\""xsd:boolean\"",\""@value\"":false} which should be dict"",
    ""api:status"": ""api:failure"",
    ""system:witnesses"": [
        {
            ""@type"": ""vio:ViolationWithDatatypeObject"",
            ""vio:literal"": ""{\""@type\"":\""xsd:boolean\"",\""@value\"":false}"",
            ""vio:message"": ""Type error for {\""@type\"":\""xsd:boolean\"",\""@value\"":false} which should be dict"",
            ""vio:type"": ""dict""
        }
    ]
}
However, the error doesn't quite tell me (as the user) where the problem is. Ideally, we would have something like:

Expected something of type xsd:string in the field s of an instance of String but found false, which is of type xsd:boolean.","@gavinmendelgleason this seem to be fix in fc782b1 the response to xh ' http//adminroot@localhost6363 / api / document / admin / tauthor = a&message = m ' < < eof { "" @type "" "" string""""s""false } eof be http/11 400 bad request connection keep - alive content - length 418 content - type application / json date mon 06 sep 2021 082008 gmt { "" apimessage "" "" type error for { \""@type\""\""xsdboolean\""\""@value\""false } which should be dict "" "" apistatus "" "" apifailure "" "" systemwitnesse "" [ { "" @type "" "" vioviolationwithdatatypeobject "" "" violiteral "" "" { \""@type\""\""xsdboolean\""\""@value\""false } "" "" viomessage "" "" type error for { \""@type\""\""xsdboolean\""\""@value\""false } which should be dict "" "" viotype "" "" dict "" } ] } however the error do not quite tell -PRON- ( as the user ) where the problem be ideally -PRON- would have something like expect something of type xsdstring in the field s of an instance of string but find false which be of type xsdboolean",0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,515,2021-09-03T14:49:36Z,spl,true and false unexpectedly converted to strings,https://github.com/terminusdb/terminusdb/issues/515,"This issue is similar to #450, but it needs to be handled differently.
The problem is that true and false get converted to strings when passed to a field whose type is xsd:string.
#!/bin/bash
set -ex

# Create the database.
xh 'http://admin:root@localhost:6363/api/db/admin/t' <<EOF
{""label"":""l"",""comment"":""c""}
EOF

# Create the schema.
xh 'http://admin:root@localhost:6363/api/document/admin/t?author=a&message=m&graph_type=schema' <<EOF
{""@type"": ""Class"",""@id"": ""String"",""s"": ""xsd:string""}
EOF

# Post the instance.
xh 'http://admin:root@localhost:6363/api/document/admin/t?author=a&message=m' <<EOF
{""@type"": ""String"",""s"":false}
EOF

# View the instance.
xh 'http://admin:root@localhost:6363/api/document/admin/t'
Output of the last command:
HTTP/1.1 200 OK
connection: keep-alive
content-length: 89
content-type: application/json; stream=true; charset=UTF-8
date: Fri, 03 Sep 2021 14:45:53 GMT

{
    ""@id"": ""String/1d18344d4b3cf6f5835fbe32691822b0"",
    ""@type"": ""String"",
    ""s"": ""false""
}
Note the ""false"" in the output.",2021-09-14T14:09:00Z,spl,https://github.com/terminusdb/terminusdb/issues/515#issuecomment-919188311,"The relevant part of fc782b1 was effectively reverted by #582. Consequently, this issue is not resolved.",the relevant part of fc782b1 be effectively revert by # 582 consequently this issue be not resolve,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,515,2021-09-03T14:49:36Z,spl,true and false unexpectedly converted to strings,https://github.com/terminusdb/terminusdb/issues/515,"This issue is similar to #450, but it needs to be handled differently.
The problem is that true and false get converted to strings when passed to a field whose type is xsd:string.
#!/bin/bash
set -ex

# Create the database.
xh 'http://admin:root@localhost:6363/api/db/admin/t' <<EOF
{""label"":""l"",""comment"":""c""}
EOF

# Create the schema.
xh 'http://admin:root@localhost:6363/api/document/admin/t?author=a&message=m&graph_type=schema' <<EOF
{""@type"": ""Class"",""@id"": ""String"",""s"": ""xsd:string""}
EOF

# Post the instance.
xh 'http://admin:root@localhost:6363/api/document/admin/t?author=a&message=m' <<EOF
{""@type"": ""String"",""s"":false}
EOF

# View the instance.
xh 'http://admin:root@localhost:6363/api/document/admin/t'
Output of the last command:
HTTP/1.1 200 OK
connection: keep-alive
content-length: 89
content-type: application/json; stream=true; charset=UTF-8
date: Fri, 03 Sep 2021 14:45:53 GMT

{
    ""@id"": ""String/1d18344d4b3cf6f5835fbe32691822b0"",
    ""@type"": ""String"",
    ""s"": ""false""
}
Note the ""false"" in the output.",2021-09-14T14:21:59Z,spl,https://github.com/terminusdb/terminusdb/issues/515#issuecomment-919200237,"This is the test I think should pass (currently at cba16f2):
diff --git a/src/core/document/json.pl b/src/core/document/json.pl
index 6e3444da..6d5bba3f 100644
--- a/src/core/document/json.pl
+++ b/src/core/document/json.pl
@@ -5222,6 +5222,9 @@ schema10('
 { ""@type"" : ""Class"",
   ""@id"" : ""Boolean"",
   ""b"" : ""xsd:boolean"" }
+{ ""@type"" : ""Class"",
+  ""@id"" : ""String"",
+  ""s"" : ""xsd:string"" }
 ').
 
 test(boolean_in_boolean_field,
@@ -5244,6 +5247,27 @@ test(boolean_in_boolean_field,
         _
     ).
 
+test(boolean_in_string_field,
+     [
+         setup(
+             (   setup_temp_store(State),
+                 create_db_with_empty_schema(""admin"", ""foo""),
+                 resolve_absolute_string_descriptor(""admin/foo"", Desc),
+                 write_schema(schema10, Desc)
+             )),
+         cleanup(
+             teardown_temp_store(State)
+         ),
+         error(casting_error(false, 'http://www.w3.org/2001/XMLSchema#string'), _)
+     ]) :-
+    Document = _{ '@type': ""String"", s: false },
+    create_context(Desc, _{ author: ""a"", message: ""m"" }, Context),
+    with_transaction(
+        Context,
+        insert_document(Context, Document, _Id),
+        _
+    ).
+
 :- end_tests(json).
 
 :- begin_tests(schema_checker).
This is a part of the change I thought would fix it:
diff --git a/src/core/triple/casting.pl b/src/core/triple/casting.pl
index a19daa9e..28915676 100644
--- a/src/core/triple/casting.pl
+++ b/src/core/triple/casting.pl
@@ -58,12 +58,11 @@ typecast(Val, Type, Hint, Cast) :-
  *
  * Casts from source type to target type.
  */
-%%% xsd:string Self Cast
-%%% NOTE: Should not be necessary as we shouldn't be able to have an atom in the first place
-typecast_switch('http://www.w3.org/2001/XMLSchema#string', 'http://www.w3.org/2001/XMLSchema#string', Val, _, Val_String^^'http://www.w3.org/2001/XMLSchema#string') :-
+%%% Can't cast an atom to a string
+typecast_switch('http://www.w3.org/2001/XMLSchema#string', _, Val, _, _) :-
     atom(Val),
     !,
-    atom_string(Val,Val_String).
+    throw(error(casting_error(Val, 'http://www.w3.org/2001/XMLSchema#string'), _)).
 %%% Self Cast
 typecast_switch(Type, Type, Val, _, Val^^Type) :-
     !.
That is, we don't cast atoms to strings. However, that seems to run contrary to this test:

  
    
      terminusdb/src/core/triple/casting.pl
    
    
        Lines 795 to 797
      in
      3bb9d6e
    
    
    
    

        
          
           test(boolean_string, []) :- 
        

        
          
               typecast(true^^'http://www.w3.org/2001/XMLSchema#boolean', 'http://www.w3.org/2001/XMLSchema#string', [], ""true""^^'http://www.w3.org/2001/XMLSchema#string'), 
        

        
          
               typecast(false^^'http://www.w3.org/2001/XMLSchema#boolean', 'http://www.w3.org/2001/XMLSchema#string', [], ""false""^^'http://www.w3.org/2001/XMLSchema#string'). 
        
    
  


The atom-to-string casting also seems to be explicitly relied on for turtle:

  
    
      terminusdb/src/core/triple/literals.pl
    
    
        Lines 305 to 314
      in
      3bb9d6e
    
    
    
    

        
          
           /* 
        

        
          
            * literal_to_turtle(+Literal,-Turtle_Literal) is det. 
        

        
          
            * 
        

        
          
            * Deal with precularities of rdf_process_turtle 
        

        
          
            */ 
        

        
          
           literal_to_turtle(String@Lang,literal(lang(Lang,S))) :- 
        

        
          
               atom_string(S,String). 
        

        
          
           literal_to_turtle(Elt^^Type,literal(type(Type,S))) :- 
        

        
          
               typecast(Elt^^Type, 'http://www.w3.org/2001/XMLSchema#string', [], Val^^_), 
        

        
          
               atom_string(S,Val). 
        
    
  



  
    
      terminusdb/src/core/triple/literals.pl
    
    
        Lines 557 to 558
      in
      3bb9d6e
    
    
    
    

        
          
           test(bool, []) :- 
        

        
          
               literal_to_turtle(false^^'http://www.w3.org/2001/XMLSchema#boolean', literal(type('http://www.w3.org/2001/XMLSchema#boolean',false))). 
        
    
  


So, I'm wondering what the solution is, if there is one.","this be the test i think should pass ( currently at cba16f2 ) diff --git a / src / core / document / jsonpl b / src / core / document / jsonpl index 6e3444da6d5bba3f 100644 --- a / src / core / document / jsonpl + + + b / src / core / document / jsonpl @@ -52226 +52229 @@ schema10 ( ' { "" @type "" "" class "" "" @id "" "" boolean "" "" b "" "" xsdboolean "" } + { "" @type "" "" class "" + "" @id "" "" string "" + "" s "" "" xsdstring "" } ' ) test(boolean_in_boolean_field @@ -52446 +524727 @@ test(boolean_in_boolean_field _ ) + test(boolean_in_string_field + [ + setup ( + ( setup_temp_store(state ) + create_db_with_empty_schema(""admin "" "" foo "" ) + resolve_absolute_string_descriptor(""admin / foo "" desc ) + write_schema(schema10 desc ) + ) ) + cleanup ( + teardown_temp_store(state ) + ) + error(casting_error(false ' http//wwww3org/2001 / xmlschema#string ' ) _ ) + ] ) - + document = _ { ' @type ' "" string "" s false } + create_context(desc _ { author "" a "" message "" m "" } context ) + with_transaction ( + context + insert_document(context document _ -PRON- d ) + _ + ) + - end_tests(json ) - begin_tests(schema_checker ) this be a part of the change i think would fix -PRON- diff --git a / src / core / triple / castingpl b / src / core / triple / castingpl index a19daa9e28915676 100644 --- a / src / core / triple / castingpl + + + b / src / core / triple / castingpl @@ -5812 +5811 @@ typecast(val type hint cast ) - * * cast from source type to target type * / -%%% xsdstre self cast -%%% note should not be necessary as -PRON- should not be able to have an atom in the first place -typecast_switch('http//wwww3org/2001 / xmlschema#string ' ' http//wwww3org/2001 / xmlschema#string ' val _ val_string^^'http//wwww3org/2001 / xmlschema#string ' ) - + % % % can not cast an atom to a string + typecast_switch('http//wwww3org/2001 / xmlschema#string ' _ val _ _ ) - atom(val ) - atom_string(valval_string ) + throw(error(casting_error(val ' http//wwww3org/2001 / xmlschema#string ' ) _ ) ) % % % self cast typecast_switch(type type val _ val^^type ) - that is -PRON- do not cast atom to string however that seem to run contrary to this test terminusdb / src / core / triple / castingpl line 795 to 797 in 3bb9d6e test(boolean_string [ ] ) - typecast(true^^'http//wwww3org/2001 / xmlschema#boolean ' ' http//wwww3org/2001 / xmlschema#string ' [ ] "" true""^^'http//wwww3org/2001 / xmlschema#string ' ) typecast(false^^'http//wwww3org/2001 / xmlschema#boolean ' ' http//wwww3org/2001 / xmlschema#string ' [ ] "" false""^^'http//wwww3org/2001 / xmlschema#string ' ) the atom - to - string casting also seem to be explicitly rely on for turtle terminusdb / src / core / triple / literalspl line 305 to 314 in 3bb9d6e / * * literal_to_turtle(+literal - turtle_literal ) be det * * deal with precularitie of rdf_process_turtle * / literal_to_turtle(string@langliteral(lang(lang ) ) ) - atom_string(sstring ) literal_to_turtle(elt^^typeliteral(type(types ) ) ) - typecast(elt^^type ' http//wwww3org/2001 / xmlschema#string ' [ ] val^^ _ ) atom_string(sval ) terminusdb / src / core / triple / literalspl line 557 to 558 in 3bb9d6e test(bool [ ] ) - literal_to_turtle(false^^'http//wwww3org/2001 / xmlschema#boolean ' literal(type('http//wwww3org/2001 / xmlschema#boolean'false ) ) ) so -PRON- be wonder what the solution be if there be one",0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,517,2021-09-04T06:47:37Z,GavinMendelGleason,Rebase resulting in one issue when two need to be reported,https://github.com/terminusdb/terminusdb/issues/517,"The branch rebase_objects has a failing test in db_rebase.pl which results in only one witness when there should be two. This is a serious problem for programmatic fixups.
 This test also results in a schema_instance check rather than a local_instance_check which makes no sense.",,,,,,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,518,2021-09-04T07:14:13Z,GavinMendelGleason,Domain Sharding,https://github.com/terminusdb/terminusdb/issues/518,"Domain Sharding
TerminusDB and TerminusX allow you to query across different data-products via the Using WOQL word. The document interface doesn't have any support for multiple data-products. But extending our current offering in a non-breaking and backward-compatible way should be relatively easy.
Combined with object storage on TerminusX, this would give us Infinitely scalable domain sharding

Foreign Types
Currently when we reference foreign instances we are doing so by constructing an empty class, and tagging the instance as being of this class, or we are reproducing the entire class definition internally to the data product. This doesn't scale well and ties representations which other domain teams should be in charge of.
The way to fix this is to add a foreign type edge. This would be like rdf:type but instead use the tag system:foreign_type or something along those lines. All we would need to alter is the instance checking procedure to treat system:foreign_type tagged entities as opaque and terminal (they can have no exiting edges).
The reason to use a different edge from rdf:type is to facilitate a union operation. We would like to keep our strategy of only recording the principle type of an object, and never having two type designators - otherwise things will become significantly more complicated in the constraint reasoner.
This is probably a few-line change and shouldn't take more than a day and has no backward compatibility issues. This gets us 90% of the way there and we should do this and then build an example data product assemblage using Marketing, Finance and HumanResources for instance, which has links going in different directions.
Foreign Type Discovery
Assuming we use the above approach, it makes it very easy to have a ""late binding strategy"" where you simply have to look the object up in another data product to get its description.  It would be handy though to know which data products it might live in! We will at some point want a data product discovery which can tell you which data products have the URI available to you (it might live in more than one because there may be different versions of the data product which you have access to).
Union
To create assemblages for analysis it would be nice if you could create a union of branches from various data products and give it to the document API. Updates would always take place in the data product of which the type is defined, and reads could happen across the data products.
To implement the union, we just need to add some sort of descriptor for unions (a list should work fine) and extend the API so that you can send in a list of paths. At the lower level reads are across collections of descriptors anyhow, but writes will require discovery of the principle. We probably could have a quick check that there is no duplication in the schema when you do an assemblage.
Infinitely scalable domain sharding
With a few lines of code we can reasonably claim to scale up indefinitely for assemblages of data products. Simply shard your data by domain, and use pointers across the shards. The rest are really just convenience features.
We can produce some test examples but internally it would be extremely useful for us to pursue this approach for the system graph itself. If we shard by organisation for servers already, it probably makes sense to shard the data on this boundary as well. This will reduce the overheads to the system graph writes which currently will have difficulty scaling up.",2021-09-04T11:45:43Z,luke-feeney,https://github.com/terminusdb/terminusdb/issues/518#issuecomment-912960218,I like the sound of this! I particularly like the idea contained in the diagram.,i like the sound of this i particularly like the idea contain in the diagram,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,518,2021-09-04T07:14:13Z,GavinMendelGleason,Domain Sharding,https://github.com/terminusdb/terminusdb/issues/518,"Domain Sharding
TerminusDB and TerminusX allow you to query across different data-products via the Using WOQL word. The document interface doesn't have any support for multiple data-products. But extending our current offering in a non-breaking and backward-compatible way should be relatively easy.
Combined with object storage on TerminusX, this would give us Infinitely scalable domain sharding

Foreign Types
Currently when we reference foreign instances we are doing so by constructing an empty class, and tagging the instance as being of this class, or we are reproducing the entire class definition internally to the data product. This doesn't scale well and ties representations which other domain teams should be in charge of.
The way to fix this is to add a foreign type edge. This would be like rdf:type but instead use the tag system:foreign_type or something along those lines. All we would need to alter is the instance checking procedure to treat system:foreign_type tagged entities as opaque and terminal (they can have no exiting edges).
The reason to use a different edge from rdf:type is to facilitate a union operation. We would like to keep our strategy of only recording the principle type of an object, and never having two type designators - otherwise things will become significantly more complicated in the constraint reasoner.
This is probably a few-line change and shouldn't take more than a day and has no backward compatibility issues. This gets us 90% of the way there and we should do this and then build an example data product assemblage using Marketing, Finance and HumanResources for instance, which has links going in different directions.
Foreign Type Discovery
Assuming we use the above approach, it makes it very easy to have a ""late binding strategy"" where you simply have to look the object up in another data product to get its description.  It would be handy though to know which data products it might live in! We will at some point want a data product discovery which can tell you which data products have the URI available to you (it might live in more than one because there may be different versions of the data product which you have access to).
Union
To create assemblages for analysis it would be nice if you could create a union of branches from various data products and give it to the document API. Updates would always take place in the data product of which the type is defined, and reads could happen across the data products.
To implement the union, we just need to add some sort of descriptor for unions (a list should work fine) and extend the API so that you can send in a list of paths. At the lower level reads are across collections of descriptors anyhow, but writes will require discovery of the principle. We probably could have a quick check that there is no duplication in the schema when you do an assemblage.
Infinitely scalable domain sharding
With a few lines of code we can reasonably claim to scale up indefinitely for assemblages of data products. Simply shard your data by domain, and use pointers across the shards. The rest are really just convenience features.
We can produce some test examples but internally it would be extremely useful for us to pursue this approach for the system graph itself. If we shard by organisation for servers already, it probably makes sense to shard the data on this boundary as well. This will reduce the overheads to the system graph writes which currently will have difficulty scaling up.",2021-09-06T10:11:00Z,matko,https://github.com/terminusdb/terminusdb/issues/518#issuecomment-913525399,"I really like the idea of using foreign_type so that we can just union graphs and have something that works! We just need to make really sure that foreign types can't also be local types, so we know absolutely sure that there cannot be duplicate triples.",i really like the idea of use foreign_type so that -PRON- can just union graphs and have something that work -PRON- just need to make really sure that foreign type can not also be local type so -PRON- know absolutely sure that there can not be duplicate triple,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,520,2021-09-06T07:11:43Z,GavinMendelGleason,Underscores in names not encoding properly leading to collision dangers.,https://github.com/terminusdb/terminusdb/issues/520,Perhaps we can switch out the uri_encoded with the old www encoding strategy?,2021-09-06T14:04:24Z,Cheukting,https://github.com/terminusdb/terminusdb/issues/520#issuecomment-913674895,Will test with Python client when fixed,will test with python client when fix,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,520,2021-09-06T07:11:43Z,GavinMendelGleason,Underscores in names not encoding properly leading to collision dangers.,https://github.com/terminusdb/terminusdb/issues/520,Perhaps we can switch out the uri_encoded with the old www encoding strategy?,2021-09-10T08:29:00Z,GavinMendelGleason,https://github.com/terminusdb/terminusdb/issues/520#issuecomment-916729399,Fixed in #536,fix in # 536,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,521,2021-09-06T10:09:40Z,matko,Sharding a single data product,https://github.com/terminusdb/terminusdb/issues/521,"Since @GavinMendelGleason just wrote #518 , I feel like I should add to it with other forms of sharding. Domain sharding is great, and we should have it, but this sort of sharding really only works when the data products move entirely independently. There's another form of sharding that people want, which is sharding within a single data product, where the different shards are not independent from each other.
What's the difference? In domain sharding, we shard different kinds of types into different graphs. The sharding criteria is the domain of these types. Within a single domain, the sharding criteria has to be something inside of these types themselves.
the point of sharding within a single data product
So what is the point of this sort of sharding? There's a couple.
Breaking up transaction domains
One of the biggest reasons for doing this is that it'll allow concurrent write transactions, as long as these transactions happen in a different shard. Each shard would effectively be its own transaction domain.
Smaller runtime memory footprint
When querying, the only graphs that need to be loaded are graphs actually looked at. For very large graphs, this can significantly reduce the memory footprint.
Retain a shared history
This is more of a comparison with domain sharding. Sharding within a single domain retains a shared history, conceptually anyway. By guaranteeing that shards cannot contain any sort of overlapping data, the history of the sharded domain can be reconstructed by merging the individual histories of each shard. We could easily convert a sharded database back to one without shards, or one with more shards, by splitting the history accordingly.
Federated queries
A bit more future talk, but it could be possible some time to actually have different shards loaded on different machines, and use a federated protocol to perform a large query. This'd allow us to do queries on data sets that are too large to fit on any one machine, thus scaling to petabytes and beyond.
When this is desirable
Sharding would be very useful in situations where you have a lot of data that is receiving a lot of writes, but the individual writes tend to be isolated to just a small part of the graph. This is for example the case with our own system database, which holds user, organization and database data. Individual queries tend to only look at one organization and its associated database, and only some users. Right now, concurrent writes to the system graph are problematic. If we broke up the transaction domain with shards, these problems would (largely) go away.
How to do it
Sharding criteria
With our object model we have a pretty natural sharding strategy available to us. We can simply shard on document ids. Any subdocuments should live in the same shard as the document that points at them. ValueHash subdocuments can be replicated in all shards that use them.
We need some function that takes an object iri and turns it into a shard number. Ideally, this is something that the user is able to set up themselves. Shard numbers can be some arbitrary large number (though maybe we should limit it to a u32 or a u64), which we can then take a modulo of to determine the actual shard this object should live in.
With such a sharding strategy we can be sure that no shard contains duplicate data (except potentially duplicate ValueHash subdocuments, which is ok).
Retrieval
When retrieving documents by ids, we can use the same procedure to determine in what shard to look. Then, all subdocuments should live in the same graph. Similarly for updating documents by id, we know exactly where to store the document and all its subdocuments.
For more complex queries, such as ""find all people with 'mark' as first name"", we are still required to go over all shards, but having found a match in a shard, we know that any directly connected object (the owner document and the contained subdocument) will live in the same shard.
History and metadata graphs
Each shard would have its own history. In other words, each shard would have its own 'shard graph', that contains the commit information relevant to that shard, and points to individual shard layers containing the actual data. To properly split up transaction domains, these shard graphs should be directly pointed at by a store label, so that they can be updated independently. We still ought to point at them through the existing metadata graph structures though.
In my opinion, shards should live on the branch/named graph level. When sharding a named graph, we ought to create a special commit on them which contains pointers to all the shards, as well as the instruction on the specific sharding strategy. After that, all further updates are done in the shard graphs, until such a time that the shards are updated.
When updating the sharding strategy or the shard count, we can merge back these individual histories into the commit graph, and then create a new shard commit on top of that with the new strategy. This way we always maintain the history properly.
Complex updates
When updates need to involve multiple shards at once, we need to do something that's being planned for a while now: multi-domain transactions. Plans for this have been drawn up but sort of assume big transaction domains delineated by a metadata graph. This would no longer be the case with these shards.
Instead, each shard needs its own metadata graph, where transactions can live. The total graph structure then becomes
metadata -> commit -> shard meta -> shard commit -> data.
This may seem pretty excessive, but the extra overhead may be worth it for the cases where one wants sharding.
What we need before we could implement this

Multi-domain transactions
(Ideally) historyless graphs (for the new metadata layer)",2021-09-06T11:39:19Z,matko,https://github.com/terminusdb/terminusdb/issues/521#issuecomment-913580111,"Probably, we are only interested in sharding the instance graph. the schema graph is small enough to load fully all the time, and we probably need the whole thing all the time anyway.",probably -PRON- be only interested in sharde the instance graph the schema graph be small enough to load fully all the time and -PRON- probably need the whole thing all the time anyway,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,521,2021-09-06T10:09:40Z,matko,Sharding a single data product,https://github.com/terminusdb/terminusdb/issues/521,"Since @GavinMendelGleason just wrote #518 , I feel like I should add to it with other forms of sharding. Domain sharding is great, and we should have it, but this sort of sharding really only works when the data products move entirely independently. There's another form of sharding that people want, which is sharding within a single data product, where the different shards are not independent from each other.
What's the difference? In domain sharding, we shard different kinds of types into different graphs. The sharding criteria is the domain of these types. Within a single domain, the sharding criteria has to be something inside of these types themselves.
the point of sharding within a single data product
So what is the point of this sort of sharding? There's a couple.
Breaking up transaction domains
One of the biggest reasons for doing this is that it'll allow concurrent write transactions, as long as these transactions happen in a different shard. Each shard would effectively be its own transaction domain.
Smaller runtime memory footprint
When querying, the only graphs that need to be loaded are graphs actually looked at. For very large graphs, this can significantly reduce the memory footprint.
Retain a shared history
This is more of a comparison with domain sharding. Sharding within a single domain retains a shared history, conceptually anyway. By guaranteeing that shards cannot contain any sort of overlapping data, the history of the sharded domain can be reconstructed by merging the individual histories of each shard. We could easily convert a sharded database back to one without shards, or one with more shards, by splitting the history accordingly.
Federated queries
A bit more future talk, but it could be possible some time to actually have different shards loaded on different machines, and use a federated protocol to perform a large query. This'd allow us to do queries on data sets that are too large to fit on any one machine, thus scaling to petabytes and beyond.
When this is desirable
Sharding would be very useful in situations where you have a lot of data that is receiving a lot of writes, but the individual writes tend to be isolated to just a small part of the graph. This is for example the case with our own system database, which holds user, organization and database data. Individual queries tend to only look at one organization and its associated database, and only some users. Right now, concurrent writes to the system graph are problematic. If we broke up the transaction domain with shards, these problems would (largely) go away.
How to do it
Sharding criteria
With our object model we have a pretty natural sharding strategy available to us. We can simply shard on document ids. Any subdocuments should live in the same shard as the document that points at them. ValueHash subdocuments can be replicated in all shards that use them.
We need some function that takes an object iri and turns it into a shard number. Ideally, this is something that the user is able to set up themselves. Shard numbers can be some arbitrary large number (though maybe we should limit it to a u32 or a u64), which we can then take a modulo of to determine the actual shard this object should live in.
With such a sharding strategy we can be sure that no shard contains duplicate data (except potentially duplicate ValueHash subdocuments, which is ok).
Retrieval
When retrieving documents by ids, we can use the same procedure to determine in what shard to look. Then, all subdocuments should live in the same graph. Similarly for updating documents by id, we know exactly where to store the document and all its subdocuments.
For more complex queries, such as ""find all people with 'mark' as first name"", we are still required to go over all shards, but having found a match in a shard, we know that any directly connected object (the owner document and the contained subdocument) will live in the same shard.
History and metadata graphs
Each shard would have its own history. In other words, each shard would have its own 'shard graph', that contains the commit information relevant to that shard, and points to individual shard layers containing the actual data. To properly split up transaction domains, these shard graphs should be directly pointed at by a store label, so that they can be updated independently. We still ought to point at them through the existing metadata graph structures though.
In my opinion, shards should live on the branch/named graph level. When sharding a named graph, we ought to create a special commit on them which contains pointers to all the shards, as well as the instruction on the specific sharding strategy. After that, all further updates are done in the shard graphs, until such a time that the shards are updated.
When updating the sharding strategy or the shard count, we can merge back these individual histories into the commit graph, and then create a new shard commit on top of that with the new strategy. This way we always maintain the history properly.
Complex updates
When updates need to involve multiple shards at once, we need to do something that's being planned for a while now: multi-domain transactions. Plans for this have been drawn up but sort of assume big transaction domains delineated by a metadata graph. This would no longer be the case with these shards.
Instead, each shard needs its own metadata graph, where transactions can live. The total graph structure then becomes
metadata -> commit -> shard meta -> shard commit -> data.
This may seem pretty excessive, but the extra overhead may be worth it for the cases where one wants sharding.
What we need before we could implement this

Multi-domain transactions
(Ideally) historyless graphs (for the new metadata layer)",2021-09-06T11:44:16Z,GavinMendelGleason,https://github.com/terminusdb/terminusdb/issues/521#issuecomment-913582756,"This is fantastic!  In terms of the ""what we need"", do you have any thoughts on implementation of historyless graphs?","this be fantastic in term of the "" what -PRON- need "" do -PRON- have any thought on implementation of historyless graphs",0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,521,2021-09-06T10:09:40Z,matko,Sharding a single data product,https://github.com/terminusdb/terminusdb/issues/521,"Since @GavinMendelGleason just wrote #518 , I feel like I should add to it with other forms of sharding. Domain sharding is great, and we should have it, but this sort of sharding really only works when the data products move entirely independently. There's another form of sharding that people want, which is sharding within a single data product, where the different shards are not independent from each other.
What's the difference? In domain sharding, we shard different kinds of types into different graphs. The sharding criteria is the domain of these types. Within a single domain, the sharding criteria has to be something inside of these types themselves.
the point of sharding within a single data product
So what is the point of this sort of sharding? There's a couple.
Breaking up transaction domains
One of the biggest reasons for doing this is that it'll allow concurrent write transactions, as long as these transactions happen in a different shard. Each shard would effectively be its own transaction domain.
Smaller runtime memory footprint
When querying, the only graphs that need to be loaded are graphs actually looked at. For very large graphs, this can significantly reduce the memory footprint.
Retain a shared history
This is more of a comparison with domain sharding. Sharding within a single domain retains a shared history, conceptually anyway. By guaranteeing that shards cannot contain any sort of overlapping data, the history of the sharded domain can be reconstructed by merging the individual histories of each shard. We could easily convert a sharded database back to one without shards, or one with more shards, by splitting the history accordingly.
Federated queries
A bit more future talk, but it could be possible some time to actually have different shards loaded on different machines, and use a federated protocol to perform a large query. This'd allow us to do queries on data sets that are too large to fit on any one machine, thus scaling to petabytes and beyond.
When this is desirable
Sharding would be very useful in situations where you have a lot of data that is receiving a lot of writes, but the individual writes tend to be isolated to just a small part of the graph. This is for example the case with our own system database, which holds user, organization and database data. Individual queries tend to only look at one organization and its associated database, and only some users. Right now, concurrent writes to the system graph are problematic. If we broke up the transaction domain with shards, these problems would (largely) go away.
How to do it
Sharding criteria
With our object model we have a pretty natural sharding strategy available to us. We can simply shard on document ids. Any subdocuments should live in the same shard as the document that points at them. ValueHash subdocuments can be replicated in all shards that use them.
We need some function that takes an object iri and turns it into a shard number. Ideally, this is something that the user is able to set up themselves. Shard numbers can be some arbitrary large number (though maybe we should limit it to a u32 or a u64), which we can then take a modulo of to determine the actual shard this object should live in.
With such a sharding strategy we can be sure that no shard contains duplicate data (except potentially duplicate ValueHash subdocuments, which is ok).
Retrieval
When retrieving documents by ids, we can use the same procedure to determine in what shard to look. Then, all subdocuments should live in the same graph. Similarly for updating documents by id, we know exactly where to store the document and all its subdocuments.
For more complex queries, such as ""find all people with 'mark' as first name"", we are still required to go over all shards, but having found a match in a shard, we know that any directly connected object (the owner document and the contained subdocument) will live in the same shard.
History and metadata graphs
Each shard would have its own history. In other words, each shard would have its own 'shard graph', that contains the commit information relevant to that shard, and points to individual shard layers containing the actual data. To properly split up transaction domains, these shard graphs should be directly pointed at by a store label, so that they can be updated independently. We still ought to point at them through the existing metadata graph structures though.
In my opinion, shards should live on the branch/named graph level. When sharding a named graph, we ought to create a special commit on them which contains pointers to all the shards, as well as the instruction on the specific sharding strategy. After that, all further updates are done in the shard graphs, until such a time that the shards are updated.
When updating the sharding strategy or the shard count, we can merge back these individual histories into the commit graph, and then create a new shard commit on top of that with the new strategy. This way we always maintain the history properly.
Complex updates
When updates need to involve multiple shards at once, we need to do something that's being planned for a while now: multi-domain transactions. Plans for this have been drawn up but sort of assume big transaction domains delineated by a metadata graph. This would no longer be the case with these shards.
Instead, each shard needs its own metadata graph, where transactions can live. The total graph structure then becomes
metadata -> commit -> shard meta -> shard commit -> data.
This may seem pretty excessive, but the extra overhead may be worth it for the cases where one wants sharding.
What we need before we could implement this

Multi-domain transactions
(Ideally) historyless graphs (for the new metadata layer)",2021-09-06T11:50:30Z,matko,https://github.com/terminusdb/terminusdb/issues/521#issuecomment-913586043,"This is fantastic! In terms of the ""what we need"", do you have any thoughts on implementation of historyless graphs?

I've really not given that much thought. But it's been under consideration for our various (often small) graph types which see a lot of updates but for which we don't really care about the history at all, such as all the metadata graphs. Assuming we have such a thing, it makes sense to use that to keep track of any ongoing transactions in shards too.
I suspect any sort of persistent hash structure would do, and we should just evaluate a bunch and see what works. If we put a compatible api on it, terminus-server doesn't even have to notice the difference between different graph types.","this be fantastic in term of the "" what -PRON- need "" do -PRON- have any thought on implementation of historyless graphs -PRON- have really not give that much thought but -PRON- be be under consideration for -PRON- various ( often small ) graph type which see a lot of update but for which -PRON- do not really care about the history at all such as all the metadata graphs assume -PRON- have such a thing -PRON- make sense to use that to keep track of any ongoing transaction in shard too i suspect any sort of persistent hash structure would do and -PRON- should just evaluate a bunch and see what work if -PRON- put a compatible api on -PRON- terminus - server do not even have to notice the difference between different graph type",0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,528,2021-09-06T13:31:32Z,spl,SubmittedIdDoesNotMatchBase error should have field and type fully expanded,https://github.com/terminusdb/terminusdb/issues/528,This is just a part of resolving #509.,,,,,,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,529,2021-09-06T13:42:13Z,spl,Integration tests for routes,https://github.com/terminusdb/terminusdb/issues/529,,,,,,,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,532,2021-09-07T12:46:06Z,Francesca-Bit,get a document using POST call and a template query,https://github.com/terminusdb/terminusdb/issues/532,"If I try to get a document using a POST call with a template query that has count  field
{
""type"": ""Person"",
""count"": 10,
""query"": { ""age"": 42 },
}
I get the following error
{ ""api:message"": ""Type error for 10 which should be atom"", ""api:status"": ""api:failure"", ""system:witnesses"": [ { ""@type"": ""vio:ViolationWithDatatypeObject"", ""vio:literal"": ""10"", ""vio:message"": ""Type error for 10 which should be atom"", ""vio:type"": ""atom"" } ] }",,,,,,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,534,2021-09-07T13:30:12Z,GavinMendelGleason,Patch API,https://github.com/terminusdb/terminusdb/issues/534,"Currently we have POST, PUT and DELETE endpoints but no PATCH. It would be extremely useful to have a way to specify only the change to the document for alterations. It would also be nice if we had some tools to construct patches from two documents, apply patches to a document and give a list of alternatives for conflicting patch application stacks.
The PATCH interface should take a minus/plus format as well as a more minimal format that just asks for a given update. The minimal format can be elaborated into a minus/plus.
Further, we need a way to construct patches out of minus/plus triple layers so that these can be reported to users in a way that is more straightforward.  The layer-stack needs to be queried and a patch plus/minus pair should be constructed for each change.

 Patch API with minus/plus (swi)
 Elaborator from positive to minus/plus (swi)
 Query interface to layer-stack (rust)
 Patch construction from layer-stack (swi)
 Patch conflict detection and application (js/python client?)",2021-09-07T13:39:29Z,spl,https://github.com/terminusdb/terminusdb/issues/534#issuecomment-914315187,Should we have two endpoints – one for diff to query and one for patch to update – or just one that combines the two operations?,should -PRON- have two endpoint – one for diff to query and one for patch to update – or just one that combine the two operation,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,534,2021-09-07T13:30:12Z,GavinMendelGleason,Patch API,https://github.com/terminusdb/terminusdb/issues/534,"Currently we have POST, PUT and DELETE endpoints but no PATCH. It would be extremely useful to have a way to specify only the change to the document for alterations. It would also be nice if we had some tools to construct patches from two documents, apply patches to a document and give a list of alternatives for conflicting patch application stacks.
The PATCH interface should take a minus/plus format as well as a more minimal format that just asks for a given update. The minimal format can be elaborated into a minus/plus.
Further, we need a way to construct patches out of minus/plus triple layers so that these can be reported to users in a way that is more straightforward.  The layer-stack needs to be queried and a patch plus/minus pair should be constructed for each change.

 Patch API with minus/plus (swi)
 Elaborator from positive to minus/plus (swi)
 Query interface to layer-stack (rust)
 Patch construction from layer-stack (swi)
 Patch conflict detection and application (js/python client?)",2021-09-07T20:07:16Z,GavinMendelGleason,https://github.com/terminusdb/terminusdb/issues/534#issuecomment-914589664,Probably two endpoints unless there is some reason to do otherwise?,probably two endpoint unless there be some reason to do otherwise,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,534,2021-09-07T13:30:12Z,GavinMendelGleason,Patch API,https://github.com/terminusdb/terminusdb/issues/534,"Currently we have POST, PUT and DELETE endpoints but no PATCH. It would be extremely useful to have a way to specify only the change to the document for alterations. It would also be nice if we had some tools to construct patches from two documents, apply patches to a document and give a list of alternatives for conflicting patch application stacks.
The PATCH interface should take a minus/plus format as well as a more minimal format that just asks for a given update. The minimal format can be elaborated into a minus/plus.
Further, we need a way to construct patches out of minus/plus triple layers so that these can be reported to users in a way that is more straightforward.  The layer-stack needs to be queried and a patch plus/minus pair should be constructed for each change.

 Patch API with minus/plus (swi)
 Elaborator from positive to minus/plus (swi)
 Query interface to layer-stack (rust)
 Patch construction from layer-stack (swi)
 Patch conflict detection and application (js/python client?)",2021-09-09T12:31:33Z,GavinMendelGleason,https://github.com/terminusdb/terminusdb/issues/534#issuecomment-916049041,"For set, optional, cardinality and the default cardinality 1 a patch can just be ""{old"": {""property"": ""value_old""}, ""new"":{""property"" :""value_new""}}. However, for an array or list it's a bit trickier.
For now we can force a complete description of the before and after in a patch for lists and arrays.","for set optional cardinality and the default cardinality 1 a patch can just be "" { old "" { "" property "" "" value_old "" } "" new""{""property "" "" value_new "" } } however for an array or list -PRON- be a bit tricky for now -PRON- can force a complete description of the before and after in a patch for list and arrays",0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,535,2021-09-07T14:13:15Z,spl,Add commit IDs to query and update responses,https://github.com/terminusdb/terminusdb/issues/535,"This is produced from an off-hand comment by @matko. In general, it seems to be a good idea to provide the requester with the commit ID of the data that is being queried or has been updated.",2021-09-07T20:12:03Z,GavinMendelGleason,https://github.com/terminusdb/terminusdb/issues/535#issuecomment-914592399,Duplicate of #248,duplicate of # 248,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,535,2021-09-07T14:13:15Z,spl,Add commit IDs to query and update responses,https://github.com/terminusdb/terminusdb/issues/535,"This is produced from an off-hand comment by @matko. In general, it seems to be a good idea to provide the requester with the commit ID of the data that is being queried or has been updated.",2021-09-08T02:12:19Z,spl,https://github.com/terminusdb/terminusdb/issues/535#issuecomment-914829512,"Thanks, @GavinMendelGleason!",thanks @gavinmendelgleason,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,537,2021-09-07T15:50:56Z,AstroChelonian,Cryptic Error When Rebasing Branch,https://github.com/terminusdb/terminusdb/issues/537,"When rebasing a branch, and the ""rebase_from"" and/or the ""author"" values are not provided the error report does not reflect accurately the situation.
To reproduce:
echo '{""rebase_from"" : ""admin/t/local/branch/unknown""}'| xh 'http://admin:root@localhost:6363/api/rebase/admin/t/local/branch/main'

Output
HTTP/1.1 500 Internal Server Error
Connection: close
Content-Length: 69
Content-Type: application/json; charset=UTF-8
Date: Tue, 07 Sep 2021 14:32:08 GMT

{
    ""code"": 500,
    ""message"": ""Arguments are not sufficiently instantiated""
}",,,,,,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,541,2021-09-09T12:46:01Z,matko,no context in schema results in bad error when replacing schema,https://github.com/terminusdb/terminusdb/issues/541,"When submitting a new schema as a post with full_replace=true, a context has to be provided in the submitted schema. If it's missing, an error is returned. This error contains a stacktrace. It should be a pretty error in the same format as our other pretty errors.",,,,,,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,544,2021-09-09T13:23:51Z,matko,proposal: new option ignore_duplicates on document insert,https://github.com/terminusdb/terminusdb/issues/544,"I would like a flag that allows document insertion to ignore duplicates, while still erroring on differences.
When data originates from another system, the workflow for getting that into terminusdb is by doing some export in that other system, followed by an import into terminusdb. For example, take a bank statement. This will contain all transactions for a particular time range. Banks typically allow you to export this information in various formats, such as CSV.
When such statements overlap, then on import, terminusdb will give an error. This can happen for example if you had previously imported an incomplete monthly statement, and then later on try to import the complete statement.
It'd be good if in this case, the document insertion endpoint could be told to verify that the old document for a particular id matches the new document, and simply ignore the new document if these match.",2021-09-09T14:08:52Z,matko,https://github.com/terminusdb/terminusdb/issues/544#issuecomment-916133451,"The code is currently already doing this properly for value hash objects, which can be reinserted as many times as desired with no errors being returned.",the code be currently already do this properly for value hash object which can be reinserte as many time as desire with no error be return,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,545,2021-09-09T14:41:17Z,matko,Cannot insert documents with expanded properties,https://github.com/terminusdb/terminusdb/issues/545,"When trying to insert a document with the properties expanded, insert document fails:
$ curl -u ""admin:root"" -X POST -H ""Content-Type: application/json"" 'http://localhost:6363/api/document/admin/foo?graph_type=instance&author=x&message=x&ignore_duplicates=true' --data-binary @-
{
  ""@type"":""terminusdb:///schema#Thing"",
  ""terminusdb:///schema#x"":""c""
}
{
  ""@type"":""api:InsertDocumentErrorResponse"",
  ""api:error"": {
    ""@type"":""api:UnrecognizedProperty"",
    ""api:document"": {
      ""@type"":""terminusdb:///schema#Thing"",
      ""terminusdb:///schema#x"":""c""
    },
    ""api:property"":""terminusdb:///schema#x"",
    ""api:type"":""terminusdb:///schema#Thing""
  },
  ""api:message"":""Submitted document contained unrecognized property 'terminusdb:///schema#x' for type \""terminusdb:///schema#Thing\"""",
  ""api:status"":""api:failure""
}

But it's fine with the compressed property:
$ curl -u ""admin:root"" -X POST -H ""Content-Type: application/json"" 'http://localhost:6363/api/document/admin/foo?graph_type=instance&author=x&message=x&ignore_duplicates=true' --data-binary @-
{
  ""@type"":""terminusdb:///schema#Thing"",
  ""x"":""c""
}
[""terminusdb:///data/Thing/c"" ]",,,,,,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,546,2021-09-09T14:45:35Z,spl,OPTIONS request: Empty reply from server,https://github.com/terminusdb/terminusdb/issues/546,"$ curl -X OPTIONS 'http://localhost:6363/api/ok'
curl: (52) Empty reply from server
$ curl -X OPTIONS 'http://admin:root@localhost:6363/api/db/admin/t'
curl: (52) Empty reply from server
I don't know if we're expecting to use the OPTIONS request at any point, but, if we do, I don't think it should do this.",2021-09-09T16:02:07Z,matko,https://github.com/terminusdb/terminusdb/issues/546#issuecomment-916231744,It's my understanding that OPTIONS is actually supposed to only send headers but no data. I'll have to check the rfc.,-PRON- be -PRON- understanding that option be actually suppose to only send header but no data -PRON- will have to check the rfc,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,546,2021-09-09T14:45:35Z,spl,OPTIONS request: Empty reply from server,https://github.com/terminusdb/terminusdb/issues/546,"$ curl -X OPTIONS 'http://localhost:6363/api/ok'
curl: (52) Empty reply from server
$ curl -X OPTIONS 'http://admin:root@localhost:6363/api/db/admin/t'
curl: (52) Empty reply from server
I don't know if we're expecting to use the OPTIONS request at any point, but, if we do, I don't think it should do this.",2021-09-09T16:08:11Z,spl,https://github.com/terminusdb/terminusdb/issues/546#issuecomment-916237355,It doesn't seem to be returning headers.,-PRON- do not seem to be return header,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,546,2021-09-09T14:45:35Z,spl,OPTIONS request: Empty reply from server,https://github.com/terminusdb/terminusdb/issues/546,"$ curl -X OPTIONS 'http://localhost:6363/api/ok'
curl: (52) Empty reply from server
$ curl -X OPTIONS 'http://admin:root@localhost:6363/api/db/admin/t'
curl: (52) Empty reply from server
I don't know if we're expecting to use the OPTIONS request at any point, but, if we do, I don't think it should do this.",2021-09-09T16:08:55Z,matko,https://github.com/terminusdb/terminusdb/issues/546#issuecomment-916238010,"But is that actually wrong?
One way to get it to return a header is to submit an Origin which will result in CORS headers being submitted.
But usually we just don't have headers for this situation.",but be that actually wrong one way to get -PRON- to return a header be to submit an origin which will result in cor header be submit but usually -PRON- just do not have header for this situation,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,546,2021-09-09T14:45:35Z,spl,OPTIONS request: Empty reply from server,https://github.com/terminusdb/terminusdb/issues/546,"$ curl -X OPTIONS 'http://localhost:6363/api/ok'
curl: (52) Empty reply from server
$ curl -X OPTIONS 'http://admin:root@localhost:6363/api/db/admin/t'
curl: (52) Empty reply from server
I don't know if we're expecting to use the OPTIONS request at any point, but, if we do, I don't think it should do this.",2021-09-09T17:26:20Z,spl,https://github.com/terminusdb/terminusdb/issues/546#issuecomment-916293476,"But is that actually wrong?

It doesn't seem to be the expected thing. According to this MDN article, you would use it to “find out which request methods a server supports.”

One way to get it to return a header is to submit an Origin which will result in CORS headers being submitted.

Indeed, that does work:
$ xh OPTIONS 'http://127.0.0.1:6363/api/ok' Origin:https://example.com
HTTP/1.1 200 OK
access-control-allow-credentials: true
access-control-allow-headers: Authorization, Authorization-Remote, Accept, Accept-Encoding, Accept-Language, Host, Origin, Referer, Content-Type, Content-Length, Content-Range, Content-Disposition, Content-Description
access-control-allow-methods: GET, POST, PUT, DELETE, OPTIONS
access-control-allow-origin: https://example.com
access-control-max-age: 1728000
connection: keep-alive
content-length: 0
date: Thu, 09 Sep 2021 17:22:57 GMT","but be that actually wrong -PRON- do not seem to be the expect thing accord to this mdn article -PRON- would use -PRON- to "" find out which request method a server support "" one way to get -PRON- to return a header be to submit an origin which will result in cor header be submit indeed that do work $ xh option ' http//1270016363 / api / ok ' originhttps//examplecom http/11 200 ok access - control - allow - credential true access - control - allow - header authorization authorization - remote accept accept - encode accept - language host origin referer content - type content - length content - range content - disposition content - description access - control - allow - method get post put delete option access - control - allow - origin https//examplecom access - control - max - age 1728000 connection keep - alive content - length 0 date thu 09 sep 2021 172257 gmt",0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,546,2021-09-09T14:45:35Z,spl,OPTIONS request: Empty reply from server,https://github.com/terminusdb/terminusdb/issues/546,"$ curl -X OPTIONS 'http://localhost:6363/api/ok'
curl: (52) Empty reply from server
$ curl -X OPTIONS 'http://admin:root@localhost:6363/api/db/admin/t'
curl: (52) Empty reply from server
I don't know if we're expecting to use the OPTIONS request at any point, but, if we do, I don't think it should do this.",2021-09-09T17:32:55Z,spl,https://github.com/terminusdb/terminusdb/issues/546#issuecomment-916298647,"Two interesting notes:


The Access-Control-Allow-Headers header in the response above lists GET, POST, PUT, DELETE, OPTIONS, but only GET and OPTIONS have been specified in the server for this route.


I learned that the Allow header “must be sent if the server responds with a 405 Method Not Allowed,” and the server is not doing that:
$ xh POST 'http://127.0.0.1:6363/api/ok'
HTTP/1.1 405 Method Not Allowed
connection: close
content-length: 99
content-type: application/json; charset=UTF-8
date: Thu, 09 Sep 2021 14:47:18 GMT

{
    ""code"": 405,
    ""location"": ""/api/ok"",
    ""message"": ""Method not allowed: POST"",
    ""method"": ""POST""
}","two interesting note the access - control - allow - header header in the response above list get post put delete option but only get and option have be specify in the server for this route i learn that the allow header "" must be send if the server respond with a 405 method not allow "" and the server be not do that $ xh post ' http//1270016363 / api / ok ' http/11 405 method not allow connection close content - length 99 content - type application / json charset = utf-8 date thu 09 sep 2021 144718 gmt { "" code "" 405 "" location "" "" /api / ok "" "" message "" "" method not allow post "" "" method "" "" post "" }",0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,548,2021-09-09T15:30:41Z,matko,ids for newly inserted documents are no longer being compressed,https://github.com/terminusdb/terminusdb/issues/548,"When inserting documents, you get a list of ids back. These used to be compressed, but this no longer seems to be the case.
Example:
$ curl -u ""admin:root"" -X POST -H ""Content-Type: application/json"" 'http://localhost:6363/api/document/admin/foo?graph_type=instance&author=x&message=x&ignore_duplicates=true' --data-binary '{""@type"": ""Thing"", ""x"": ""asdf""}'
[""terminusdb:///data/Thing/asdf"" ]

The expected output was
[""Thing/asdf""]",2021-09-09T23:14:54Z,matko,https://github.com/terminusdb/terminusdb/issues/548#issuecomment-916503348,"To be fair I'm not sure if this is what we should do.
It's clear to me that something needs to change though, as currently, schema inserts and modifications will return a compressed id (or actually probably just whatever you send in), while the instance inserts and modifications always return an expanded ID. We need to make this consistent.
It could make sense to allow the user to choose if they want their prefixes back compressed or expanded. We just need to pick what we want to do by default.",to be fair -PRON- be not sure if this be what -PRON- should do -PRON- be clear to -PRON- that something need to change though as currently schema insert and modification will return a compress -PRON- d ( or actually probably just whatever -PRON- send in ) while the instance insert and modification always return an expand -PRON- d -PRON- need to make this consistent -PRON- could make sense to allow the user to choose if -PRON- want -PRON- prefix back compress or expand -PRON- just need to pick what -PRON- want to do by default,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,548,2021-09-09T15:30:41Z,matko,ids for newly inserted documents are no longer being compressed,https://github.com/terminusdb/terminusdb/issues/548,"When inserting documents, you get a list of ids back. These used to be compressed, but this no longer seems to be the case.
Example:
$ curl -u ""admin:root"" -X POST -H ""Content-Type: application/json"" 'http://localhost:6363/api/document/admin/foo?graph_type=instance&author=x&message=x&ignore_duplicates=true' --data-binary '{""@type"": ""Thing"", ""x"": ""asdf""}'
[""terminusdb:///data/Thing/asdf"" ]

The expected output was
[""Thing/asdf""]",2021-09-10T08:26:51Z,GavinMendelGleason,https://github.com/terminusdb/terminusdb/issues/548#issuecomment-916728078,"Compressed by default has some downsides. It means that everything is centred around a particular data-product as the default prefixes may be different in the federated scenario. Also, while it is nicer to look at, we're often using it programmatically so it may be less important. I'd say it's safer to choose expanded by default.",compress by default have some downside -PRON- mean that everything be centre around a particular data - product as the default prefix may be different in the federated scenario also while -PRON- be nice to look at -PRON- be often use -PRON- programmatically so -PRON- may be less important -PRON- 'd say -PRON- be safe to choose expand by default,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,550,2021-09-09T18:15:44Z,spl,"Use newline-delimited, minimized JSON stream in response",https://github.com/terminusdb/terminusdb/issues/550,"The TerminusDB server currently sends nicely formatted streams of JSON objects, in which each object covers multiple lines and the fields are indented. They are pleasant to inspect with curl/xh. However, there are two not insignificant issues:

The format is not widely supported by clients. I have already run into this issue twice. In both cases, support for newline-delimited JSON (minimized JSON objects separated by newlines) was easily found, but the current format was not supported.
The whitespace adds a large number of bytes to the content length, which results in more time (a) printing the response, (b) sending it over the network, and (c) parsing it in the receiver. With large and/or many responses, this can make a difference.

While I have enjoyed being able to easily read the JSON responses, I think the priority – and, therefore, the default – should be a standard format that is also efficient. We can always add, say, an HTTP request header that requests prettily printed JSON.",2021-09-09T18:19:07Z,matko,https://github.com/terminusdb/terminusdb/issues/550#issuecomment-916331295,"We already have this.
if you specify minimized=true as one of the query parameters, json objects will come out newline delineated.
Though I guess you're also saying we should make it the default. That's possible.",-PRON- already have this if -PRON- specify minimize = true as one of the query parameter json object will come out newline delineate though i guess -PRON- be also say -PRON- should make -PRON- the default that be possible,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,550,2021-09-09T18:15:44Z,spl,"Use newline-delimited, minimized JSON stream in response",https://github.com/terminusdb/terminusdb/issues/550,"The TerminusDB server currently sends nicely formatted streams of JSON objects, in which each object covers multiple lines and the fields are indented. They are pleasant to inspect with curl/xh. However, there are two not insignificant issues:

The format is not widely supported by clients. I have already run into this issue twice. In both cases, support for newline-delimited JSON (minimized JSON objects separated by newlines) was easily found, but the current format was not supported.
The whitespace adds a large number of bytes to the content length, which results in more time (a) printing the response, (b) sending it over the network, and (c) parsing it in the receiver. With large and/or many responses, this can make a difference.

While I have enjoyed being able to easily read the JSON responses, I think the priority – and, therefore, the default – should be a standard format that is also efficient. We can always add, say, an HTTP request header that requests prettily printed JSON.",2021-09-09T18:56:12Z,spl,https://github.com/terminusdb/terminusdb/issues/550#issuecomment-916354840,Great! Yes!,great yes,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,552,2021-09-09T22:34:42Z,matko,PUT document fails with stack trace if submitted without id or with nonexistent id,https://github.com/terminusdb/terminusdb/issues/552,Submitting a document without an id results in an error with a stack trace. Should be a proper error.,,,,,,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,555,2021-09-10T09:05:11Z,matko,switch to sha2 hashing,https://github.com/terminusdb/terminusdb/issues/555,"We're currently using sha1 as a hashing strategy. Sometimes, we're even using md5. Before releasing beta, we should switch to sha2.",,,,,,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,557,2021-09-10T14:30:32Z,Cheukting,Query Document is not happy if property is optional,https://github.com/terminusdb/terminusdb/issues/557,"I have this schema:
[{'@base': 'terminusdb:///data/', '@Schema': 'terminusdb:///schema#', '@type': '@context'}, {'@id': 'Grades', '@key': {'@type': 'Random'}, '@type': 'Class', 'final': {'@Class': 'xsd:decimal', '@type': 'Optional'}, 'first_name': {'@Class': 'xsd:string', '@type': 'Optional'}, 'grade': {'@Class': 'xsd:string', '@type': 'Optional'}, 'last_name': {'@Class': 'xsd:string', '@type': 'Optional'}, 'ssn': {'@Class': 'xsd:string', '@type': 'Optional'}, 'test1': {'@Class': 'xsd:decimal', '@type': 'Optional'}, 'test2': {'@Class': 'xsd:decimal', '@type': 'Optional'}, 'test3': {'@Class': 'xsd:decimal', '@type': 'Optional'}, 'test4': {'@Class': 'xsd:decimal', '@type': 'Optional'}}]
so grade is optional, but when I query:
{'@type': 'Grades', 'grade': 'B-'}

I have such error:
Type error for ""B-"" which should be dict

I think it should accept the query",,,,,,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,558,2021-09-10T16:30:30Z,Cheukting,Missing key error throws stack trace,https://github.com/terminusdb/terminusdb/issues/558,"The schema:
[{'@base': 'terminusdb:///data/', '@schema': 'terminusdb:///schema#', '@type': '@context'}, {'@documentation': {'@comment': 'This is address'}, '@id': 'Address', '@key': {'@type': 'Random'}, '@subdocument': [], '@type': 'Class', '_id': 'xsd:string', 'country': 'Country', 'postal_code': 'xsd:string', 'street': 'xsd:string'}, {'@documentation': {'@comment': 'TaggedUnion allow options for types'}, '@id': 'Contact', '@inherits': 'TaggedUnion', '@key': {'@type': 'Random'}, '@type': 'Class', '_id': 'xsd:string', 'international': 'xsd:string', 'local_number': 'xsd:integer'}, {'@abstract': [], '@id': 'Coordinate', '@key': {'@type': 'Random'}, '@type': 'Class', '_id': 'xsd:string', 'x': 'xsd:decimal', 'y': 'xsd:decimal'}, {'@documentation': {'@comment': ""This is Country.\nCountry is a class object in the schema. It's class attributes will be the properties of the object. Therefore, a Country object will have a name which is string and a list of alias names that is called 'also_know_as'""}, '@id': 'Country', '@key': {'@fields': ['name'], '@type': 'Hash'}, '@type': 'Class', '_id': 'xsd:string', 'also_know_as': {'@class': 'xsd:string', '@type': 'List'}, 'name': 'xsd:string'}, {'@documentation': {'@comment': 'Employee will inherits the attributes from Person'}, '@id': 'Employee', '@inherits': 'Person', '@key': {'@type': 'Random'}, '@type': 'Class', '_id': 'xsd:string', 'address_of': 'Address', 'age': 'xsd:integer', 'contact_number': {'@class': 'xsd:string', '@type': 'Optional'}, 'friend_of': {'@class': 'Person', '@type': 'Set'}, 'managed_by': 'Employee', 'name': 'xsd:string'}, {'@abstract': [], '@documentation': {'@comment': 'Location is inherits from Address and Coordinate\nClass can have multiple inheritance. It will inherits both the attibutes from Address and Coordinate.'}, '@id': 'Location', '@inherits': ['Address', 'Coordinate'], '@key': {'@type': 'Random'}, '@subdocument': [], '@type': 'Class', '_id': 'xsd:string', 'country': 'Country', 'name': 'xsd:string', 'postal_code': 'xsd:string', 'street': 'xsd:string', 'x': 'xsd:decimal', 'y': 'xsd:decimal'}, {'@documentation': {'@comment': 'This is a person\nCan store the explanation to the attributes in the docstring. Docstrings needs to be in numpydoc format.', '@properties': {'age': 'Age of the person.', 'name': 'Name of the person.'}}, '@id': 'Person', '@key': {'@type': 'Random'}, '@type': 'Class', '_id': 'xsd:string', 'age': 'xsd:integer', 'friend_of': {'@class': 'Person', '@type': 'Set'}, 'name': 'xsd:string'}, {'@id': 'Team', '@type': 'Enum', '@value': ['Information Technology', 'Marketing']}]

The document that I tried to insert:
[{'@type': 'Country', '_id': 'Country/84cc08e3c292b9c19176919caab546f17d1fb8f20599fd5cefeb0aec3c5a9420', 'name': 'United Kingdom', 'also_know_as': ['Great Britian']}, {'@type': 'Country', '_id': 'Country/c9cab0238debc6e55498e2c0963a1d8e25bd29c95ddd53a3fff841ee636b6bc5', 'name': 'Austria', 'also_know_as': []}, {'@type': 'Address', 'street': 'Some Ave', 'postal_code': 'EC12 345', 'country': {'_id': 'Country/84cc08e3c292b9c19176919caab546f17d1fb8f20599fd5cefeb0aec3c5a9420', '@type': 'Country'}}, {'@type': 'Address', 'street': 'Some Place', 'postal_code': 'T65 987', 'country': {'_id': 'Country/84cc08e3c292b9c19176919caab546f17d1fb8f20599fd5cefeb0aec3c5a9420', '@type': 'Country'}}, {'@type': 'Address', 'street': 'Some Field', 'postal_code': '012 478', 'country': {'_id': 'Country/c9cab0238debc6e55498e2c0963a1d8e25bd29c95ddd53a3fff841ee636b6bc5', '@type': 'Country'}}, {'@type': 'Employee', '_id': 'Employee/10413e84ab6a4d098166ee03445c9466', 'address_of': {'@type': 'Address', 'street': 'Some Field', 'postal_code': '012 478', 'country': {'_id': 'Country/c9cab0238debc6e55498e2c0963a1d8e25bd29c95ddd53a3fff841ee636b6bc5', '@type': 'Country'}}, 'managed_by': {'_id': 'Employee/10413e84ab6a4d098166ee03445c9466', '@type': 'Employee'}, 'name': 'Gavin', 'age': 1000, 'friend_of': []}, {'@type': 'Employee', '_id': 'Employee/c49c58fda1794b5c8317aadc90517d56', 'address_of': {'@type': 'Address', 'street': 'Some Place', 'postal_code': 'T65 987', 'country': {'_id': 'Country/84cc08e3c292b9c19176919caab546f17d1fb8f20599fd5cefeb0aec3c5a9420', '@type': 'Country'}}, 'managed_by': {'_id': 'Employee/10413e84ab6a4d098166ee03445c9466', '@type': 'Employee'}, 'name': 'Dani', 'age': 18, 'friend_of': [{'_id': 'Employee/4341711a837d41149cf070517e4304f5', '@type': 'Employee'}]}, {'@type': 'Employee', '_id': 'Employee/4341711a837d41149cf070517e4304f5', 'address_of': {'@type': 'Address', 'street': 'Some Ave', 'postal_code': 'EC12 345', 'country': {'_id': 'Country/84cc08e3c292b9c19176919caab546f17d1fb8f20599fd5cefeb0aec3c5a9420', '@type': 'Country'}}, 'managed_by': {'_id': 'Employee/10413e84ab6a4d098166ee03445c9466', '@type': 'Employee'}, 'name': 'Cheuk', 'age': 21, 'friend_of': [{'_id': 'Employee/c49c58fda1794b5c8317aadc90517d56', '@type': 'Employee'}]}]

I am trying to use my own _id for the Python client so not to conflict as the official @id but then I got this error:
Error: missing_key('terminusdb:///schema#name',json{'@id':_19608,'@type':'terminusdb:///schema#Country','terminusdb:///schema#_id':json{'@type':'http://www.w3.org/2001/XMLSchema#string','@value':""Country/84cc08e3c292b9c19176919caab546f17d1fb8f20599fd5cefeb0aec3c5a9420""}})
  [46] throw(error(missing_key('terminusdb:///schema#name',...),_19676))
  [44] catch('<garbage_collected>','<garbage_collected>','<garbage_collected>') at /usr/lib/swipl/boot/init.pl:533
  [42] api_document:api_insert_document_(instance,transaction_object{commit_info:commit_info{},descriptor:branch_descriptor{branch_name:""main"",repository_descriptor: ...},inference_objects:[],instance_objects:[...],parent:transaction_object{descriptor: ...,inference_objects:[],instance_objects: ...,parent: ...,schema_objects: ...},schema_objects:[...]},<stream>(0x55e13fd28100),_19758) at /app/terminusdb/src/core/api/api_document.pl:188
  [41] '$bags':findall_loop(_19874,'<garbage_collected>',_19878,[]) at /usr/lib/swipl/boot/bags.pl:99
  [40] setup_call_catcher_cleanup('$bags':'$new_findall_bag','$bags':findall_loop(_19932,...,_19936,[]),_19914,'$bags':'$destroy_findall_bag') at /usr/lib/swipl/boot/init.pl:614
  [36] '<meta-call>'('<garbage_collected>') <foreign>
  [35] database:with_transaction_(query_context{all_witnesses:false,authorization:'terminusdb://system/data/User/admin',bindings:[],commit_info:commit_info{author:admin,message:'Document object inserted by Python client.'},default_collection:branch_descriptor{branch_name:""main"",repository_descriptor: ...},files:[],filter:type_filter{types: ...},prefixes:_20094{'@base':""terminusdb:///data/"",'@schema':""terminusdb:///schema#"",'@type':'Context',api:'http://terminusdb.com/schema/api#',owl:'http://www.w3.org/2002/07/owl#',rdf:'http://www.w3.org/1999/02/22-rdf-syntax-ns#',rdfs:'http://www.w3.org/2000/01/rdf-schema#',sys:'http://terminusdb.com/schema/sys#',vio:'http://terminusdb.com/schema/vio#',woql:'http://terminusdb.com/schema/woql#',xdd:'http://terminusdb.com/schema/xdd#',xsd:'http://www.w3.org/2001/XMLSchema#'},selected:[],system:system_descriptor{},transaction_objects:[...],update_guard:_20064,write_graph:branch_graph{branch_name:""main"",database_name:""somenewdb"",organization_name:""admin"",repository_name:""local"",type:instance}},api_document:(...;...),_20018) at /app/terminusdb/src/core/transaction/database.pl:220
  [34] setup_call_catcher_cleanup(database:true,database:with_transaction_(...,...,_20260),_20238,database:true) at /usr/lib/swipl/boot/init.pl:614
  [30] '<meta-call>'('<garbage_collected>') <foreign>
  [29] catch(routes:(...,...),error(missing_key('terminusdb:///schema#name',...),context(_20362,_20364)),routes:do_or_die(...,...)) at /usr/lib/swipl/boot/init.pl:532
  [28] catch_with_backtrace('<garbage_collected>','<garbage_collected>','<garbage_collected>') at /usr/lib/swipl/boot/init.pl:582

Note: some frames are missing due to last-call optimization.
Re-run your program in debug mode (:- debug.) to get more detail.",,,,,,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,562,2021-09-12T07:44:28Z,GavinMendelGleason,Stack trace rather than proper error for missing key fields,https://github.com/terminusdb/terminusdb/issues/562,"If you insert a schema object with a Lexical which is missing the fields parameter, you get a stack trace rather than a proper error.
key_missing_fields(json{'@type':""Lexical"",'@value':[""isotope_name""]},'http://terminusdb.com/schema/sys#Lexical')",,,,,,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,563,2021-09-12T07:59:19Z,GavinMendelGleason,No longer possible to set schema documentation,https://github.com/terminusdb/terminusdb/issues/563,"It appears that it is no longer possible to update the context document with information. We should make this work again.
{'@type': 'api:InsertDocumentErrorResponse', 
  'api:error': {'@type': 'api:NoIdInDocument', 
  'api:document': {'@base': 'http://example.com/elements/', 
                                  '@documentation': {'@authors': ['Gavin Mendel-Gleason'], 
                                                                          '@description': 'This collection gives the periodic table of the elements with all isotopes together with their names, symbols and masses', 
                                                                          '@title': 'Periodic Table of the Elements'}, 
                                 '@schema': 'http://example.com/elements#', 
                                 '@type': '@context'}}, 
  'api:message': ""No '@id' field found in document, but it was required"",
  'api:status': 'api:failure'}",2021-09-13T13:17:36Z,spl,https://github.com/terminusdb/terminusdb/issues/563#issuecomment-918181096,"Verify that this works with PUT.
This is for a POST. The error message is wrong. It should mention something about the context, e.g. that it already exists.",verify that this work with put this be for a post the error message be wrong -PRON- should mention something about the context eg that -PRON- already exist,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,563,2021-09-12T07:59:19Z,GavinMendelGleason,No longer possible to set schema documentation,https://github.com/terminusdb/terminusdb/issues/563,"It appears that it is no longer possible to update the context document with information. We should make this work again.
{'@type': 'api:InsertDocumentErrorResponse', 
  'api:error': {'@type': 'api:NoIdInDocument', 
  'api:document': {'@base': 'http://example.com/elements/', 
                                  '@documentation': {'@authors': ['Gavin Mendel-Gleason'], 
                                                                          '@description': 'This collection gives the periodic table of the elements with all isotopes together with their names, symbols and masses', 
                                                                          '@title': 'Periodic Table of the Elements'}, 
                                 '@schema': 'http://example.com/elements#', 
                                 '@type': '@context'}}, 
  'api:message': ""No '@id' field found in document, but it was required"",
  'api:status': 'api:failure'}",2021-09-13T15:37:54Z,matko,https://github.com/terminusdb/terminusdb/issues/563#issuecomment-918318451,"So just to add a little to this..
This is not about documentation specifically. This is about the POST endpoint not eating context documents when full_replace is unset or false.",so just to add a little to this this be not about documentation specifically this be about the post endpoint not eat context document when full_replace be unset or false,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,563,2021-09-12T07:59:19Z,GavinMendelGleason,No longer possible to set schema documentation,https://github.com/terminusdb/terminusdb/issues/563,"It appears that it is no longer possible to update the context document with information. We should make this work again.
{'@type': 'api:InsertDocumentErrorResponse', 
  'api:error': {'@type': 'api:NoIdInDocument', 
  'api:document': {'@base': 'http://example.com/elements/', 
                                  '@documentation': {'@authors': ['Gavin Mendel-Gleason'], 
                                                                          '@description': 'This collection gives the periodic table of the elements with all isotopes together with their names, symbols and masses', 
                                                                          '@title': 'Periodic Table of the Elements'}, 
                                 '@schema': 'http://example.com/elements#', 
                                 '@type': '@context'}}, 
  'api:message': ""No '@id' field found in document, but it was required"",
  'api:status': 'api:failure'}",2021-09-13T16:54:04Z,AstroChelonian,https://github.com/terminusdb/terminusdb/issues/563#issuecomment-918384592,"The error described above occurs when the following is typed in:
#!/bin/bash
set -ex
Create the database.
xh 'http://admin:root@localhost:6363/api/db/admin/test563' <<EOF
{""label"":""l"",""comment"":""c""}
EOF

Update a schema with a context document (use POST)
curl -X POST -H ""Content-Type: application/json"" -u ""admin:root"" ""http://localhost:6363/api/document/admin/test563?graph_type=schema&author=me&message=me"" --data-binary '{""@base"": ""http://example.com/elements/"", ""@documentation"": {""@authors"": [""Gavin Mendel-Gleason""], ""@description"": ""This collection gives the periodic table of the elements with all isotopes together with their names, symbols and masses"", ""@title"": ""Periodic Table of the Elements""}, ""@schema"": ""http://example.com/elements#"", ""@type"": ""@context""}'","the error describe above occur when the follow be type in # /bin / bash set -ex create the database xh ' http//adminroot@localhost6363 / api / db / admin / test563 ' < < eof { "" label""""l""""comment""""c "" } eof update a schema with a context document ( use post ) curl -x post -h "" content - type application / json "" -u "" adminroot "" "" http//localhost6363 / api / document / admin / test563graph_type = schema&author = me&message = -PRON- "" --data - binary ' { "" @base "" "" http//examplecom / elements/ "" "" @documentation "" { "" @author "" [ "" gavin mendel - gleason "" ] "" @description "" "" this collection give the periodic table of the element with all isotope together with -PRON- name symbol and masse "" "" @title "" "" periodic table of the element "" } "" @schema "" "" http//examplecom / element # "" "" @type "" "" @context "" } '",0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,566,2021-09-13T09:50:31Z,Cheukting,Stacktrace if I did not specify a Random key for subdocument,https://github.com/terminusdb/terminusdb/issues/566,"I get this:
Error: subdocument_key_type_restriction
E             [46] throw(error(subdocument_key_type_restriction,_6238))
E             [44] catch('<garbage_collected>','<garbage_collected>','<garbage_collected>') at /usr/lib/swipl/boot/init.pl:533
E             [42] api_document:api_insert_document_(schema,transaction_object{commit_info:commit_info{},descriptor:branch_descriptor{branch_name:""main"",repository_descriptor: ...},inference_objects:[],instance_objects:[...],parent:transaction_object{descriptor: ...,inference_objects:[],instance_objects: ...,parent: ...,schema_objects: ...},schema_objects:[...]},<stream>(0x55bde1421600),_6314) at /app/terminusdb/src/core/api/api_document.pl:175
E             [41] '$bags':findall_loop(_6430,'<garbage_collected>',_6434,[]) at /usr/lib/swipl/boot/bags.pl:99
E             [40] setup_call_catcher_cleanup('$bags':'$new_findall_bag','$bags':findall_loop(_6488,...,_6492,[]),_6470,'$bags':'$destroy_findall_bag') at /usr/lib/swipl/boot/init.pl:614
E             [36] '<meta-call>'('<garbage_collected>') <foreign>
E             [35] database:with_transaction_(query_context{all_witnesses:false,authorization:'terminusdb://system/data/User/admin',bindings:[],commit_info:commit_info{author:admin,message:'I am checking in the schema'},default_collection:branch_descriptor{branch_name:""main"",repository_descriptor: ...},files:[],filter:type_filter{types: ...},prefixes:_6650{'@base':""terminusdb:///data/"",'@schema':""terminusdb:///schema#"",'@type':'Context',api:'http://terminusdb.com/schema/api#',owl:'http://www.w3.org/2002/07/owl#',rdf:'http://www.w3.org/1999/02/22-rdf-syntax-ns#',rdfs:'http://www.w3.org/2000/01/rdf-schema#',sys:'http://terminusdb.com/schema/sys#',vio:'http://terminusdb.com/schema/vio#',woql:'http://terminusdb.com/schema/woql#',xdd:'http://terminusdb.com/schema/xdd#',xsd:'http://www.w3.org/2001/XMLSchema#'},selected:[],system:system_descriptor{},transaction_objects:[...],update_guard:_6620,write_graph:branch_graph{branch_name:""main"",database_name:""test_docapi"",organization_name:""admin"",repository_name:""local"",type:instance}},api_document:(...;...),_6574) at /app/terminusdb/src/core/transaction/database.pl:220
E             [34] setup_call_catcher_cleanup(database:true,database:with_transaction_(...,...,_6816),_6794,database:true) at /usr/lib/swipl/boot/init.pl:614
E             [30] '<meta-call>'('<garbage_collected>') <foreign>
E             [29] catch(routes:(...,...),error(subdocument_key_type_restriction,context(_6912,_6914)),routes:do_or_die(...,...)) at /usr/lib/swipl/boot/init.pl:532
E             [28] catch_with_backtrace('<garbage_collected>','<garbage_collected>','<garbage_collected>') at /usr/lib/swipl/boot/init.pl:582
E           
E           Note: some frames are missing due to last-call optimization.
E           Re-run your program in debug mode (:- debug.) to get more detail.

When I insert this schema:
[{'@type': 'Class', '@id': 'Address', '@documentation': {'@comment': 'This is address', '@properties': {}}, '@subdocument': [], 'street': 'xsd:string', 'postal_code': 'xsd:string', 'country': 'Country'}, {'@type': 'Class', '@id': 'Country', 'name': 'xsd:string', 'perimeter': {'@type': 'List', '@class': 'Coordinate'}}, {'@type': 'Class', '@id': 'Coordinate', 'x': 'xsd:decimal', 'y': 'xsd:decimal'}, {'@type': 'Enum', '@id': 'Role', '@value': ['Admin', 'Read', 'Write']}, {'@type': 'Class', '@id': 'Employee', '@inherits': ['Person'], 'address_of': 'Address', 'contact_number': {'@type': 'Optional', '@class': 'xsd:string'}, 'managed_by': 'Employee', 'member_of': 'Team', 'permisstion': {'@type': 'Set', '@class': 'Role'}, 'name': 'xsd:string', 'age': 'xsd:integer', 'friend_of': {'@type': 'Set', '@class': 'Person'}}, {'@type': 'Enum', '@id': 'Team', '@value': ['Information Technology', 'Marketing']}, {'@type': 'Class', '@id': 'Person', '@documentation': {'@comment': 'This is a person', '@properties': {'name': 'Name of the person.', 'age': 'Age of the person.'}}, 'name': 'xsd:string', 'age': 'xsd:integer', 'friend_of': {'@type': 'Set', '@class': 'Person'}}]
I think how I cause it is to not specify the subdocument to have a ""Random"" key. I think if I do so it will be Random by default and do not have to specify it?",2021-09-15T11:02:24Z,spl,https://github.com/terminusdb/terminusdb/issues/566#issuecomment-919919036,"Okay, I can reproduce:
cat test.json | xh 'http://admin:root@localhost:6363/api/document/admin/tdb' graph_type==schema author==a message==m
HTTP/1.1 500 Internal Server Error
Connection: keep-alive
Content-Length: 3092
Content-Type: application/json
Date: Wed, 15 Sep 2021 11:02:26 GMT

{
    ""api:message"": ""Error: subdocument_key_type_restriction\n  [46] throw(error(subdocument_key_type_restriction,_23892))\n  [44] catch(api_document:do_or_die(...,...),error(subdocument_key_type_restriction,_23944),api_document:(...;...)) at /usr/local/Cellar/swi-prolog/8.2.4/libexec/lib/swipl/boot/init.pl:533\n  [42] api_document:api_insert_document_(schema,transaction_object{commit_info:commit_info{},descriptor:branch_descriptor{branch_name:\""main\"",repository_descriptor: ...},inference_objects:[],instance_objects:[...],parent:transaction_object{descriptor: ...,inference_objects:[],instance_objects: ...,parent: ...,schema_objects: ...},schema_objects:[...]},<stream>(0x7f99ba1deca0),_23998) at /Users/leather/projects/terminusdb/terminusdb/src/core/api/api_document.pl:175\n  [41] '$bags':findall_loop(_24114,'<garbage_collected>',_24118,[]) at /usr/local/Cellar/swi-prolog/8.2.4/libexec/lib/swipl/boot/bags.pl:99\n  [40] setup_call_catcher_cleanup('$bags':'$new_findall_bag','$bags':findall_loop(_24172,...,_24176,[]),_24154,'$bags':'$destroy_findall_bag') at /usr/local/Cellar/swi-prolog/8.2.4/libexec/lib/swipl/boot/init.pl:614\n  [36] '<meta-call>'('<garbage_collected>') <foreign>\n  [35] database:with_transaction_(query_context{all_witnesses:false,authorization:'terminusdb://system/data/User/admin',bindings:[],commit_info:commit_info{author:a,message:m},default_collection:branch_descriptor{branch_name:\""main\"",repository_descriptor: ...},files:[],filter:type_filter{types: ...},prefixes:_24348{'@base':\""terminusdb:///data/\"",'@schema':\""terminusdb:///schema#\"",'@type':'Context',api:'http://terminusdb.com/schema/api#',owl:'http://www.w3.org/2002/07/owl#',rdf:'http://www.w3.org/1999/02/22-rdf-syntax-ns#',rdfs:'http://www.w3.org/2000/01/rdf-schema#',sys:'http://terminusdb.com/schema/sys#',vio:'http://terminusdb.com/schema/vio#',woql:'http://terminusdb.com/schema/woql#',xdd:'http://terminusdb.com/schema/xdd#',xsd:'http://www.w3.org/2001/XMLSchema#'},selected:[],system:system_descriptor{},transaction_objects:[...],update_guard:_24308,write_graph:branch_graph{branch_name:\""main\"",database_name:\""tdb\"",organization_name:\""admin\"",repository_name:\""local\"",type:instance}},api_document:(...;...),_24258) at /Users/leather/projects/terminusdb/terminusdb/src/core/transaction/database.pl:220\n  [34] setup_call_catcher_cleanup(database:true,database:with_transaction_(...,...,_24500),_24478,database:true) at /usr/local/Cellar/swi-prolog/8.2.4/libexec/lib/swipl/boot/init.pl:614\n  [30] '<meta-call>'('<garbage_collected>') <foreign>\n  [29] catch(routes:(...,...),error(subdocument_key_type_restriction,context(_24596,_24598)),routes:do_or_die(...,...)) at /usr/local/Cellar/swi-prolog/8.2.4/libexec/lib/swipl/boot/init.pl:532\n  [28] catch_with_backtrace('<garbage_collected>','<garbage_collected>','<garbage_collected>') at /usr/local/Cellar/swi-prolog/8.2.4/libexec/lib/swipl/boot/init.pl:582\n\nNote: some frames are missing due to last-call optimization.\nRe-run your program in debug mode (:- debug.) to get more detail.\n\n"",
    ""api:status"": ""api:server_error""
}
test.json:
[
  {
    ""@type"": ""Class"",
    ""@id"": ""Address"",
    ""@documentation"": {
      ""@comment"": ""This is address"",
      ""@properties"": {}
    },
    ""@subdocument"": [],
    ""street"": ""xsd:string"",
    ""postal_code"": ""xsd:string"",
    ""country"": ""Country""
  },
  {
    ""@type"": ""Class"",
    ""@id"": ""Country"",
    ""name"": ""xsd:string"",
    ""perimeter"": {
      ""@type"": ""List"",
      ""@class"": ""Coordinate""
    }
  },
  {
    ""@type"": ""Class"",
    ""@id"": ""Coordinate"",
    ""x"": ""xsd:decimal"",
    ""y"": ""xsd:decimal""
  },
  {
    ""@type"": ""Enum"",
    ""@id"": ""Role"",
    ""@value"": [
      ""Admin"",
      ""Read"",
      ""Write""
    ]
  },
  {
    ""@type"": ""Class"",
    ""@id"": ""Employee"",
    ""@inherits"": [
      ""Person""
    ],
    ""address_of"": ""Address"",
    ""contact_number"": {
      ""@type"": ""Optional"",
      ""@class"": ""xsd:string""
    },
    ""managed_by"": ""Employee"",
    ""member_of"": ""Team"",
    ""permisstion"": {
      ""@type"": ""Set"",
      ""@class"": ""Role""
    },
    ""name"": ""xsd:string"",
    ""age"": ""xsd:integer"",
    ""friend_of"": {
      ""@type"": ""Set"",
      ""@class"": ""Person""
    }
  },
  {
    ""@type"": ""Enum"",
    ""@id"": ""Team"",
    ""@value"": [
      ""Information Technology"",
      ""Marketing""
    ]
  },
  {
    ""@type"": ""Class"",
    ""@id"": ""Person"",
    ""@documentation"": {
      ""@comment"": ""This is a person"",
      ""@properties"": {
        ""name"": ""Name of the person."",
        ""age"": ""Age of the person.""
      }
    },
    ""name"": ""xsd:string"",
    ""age"": ""xsd:integer"",
    ""friend_of"": {
      ""@type"": ""Set"",
      ""@class"": ""Person""
    }
  }
]","okay i can reproduce cat testjson | xh ' http//adminroot@localhost6363 / api / document / admin / tdb ' graph_type==schema author==a message==m http/11 500 internal server error connection keep - alive content - length 3092 content - type application / json date -PRON- d 15 sep 2021 110226 gmt { "" apimessage "" "" error subdocument_key_type_restriction\n [ 46 ] throw(error(subdocument_key_type_restriction_23892))\n [ 44 ] catch(api_documentdo_or_die()error(subdocument_key_type_restriction_23944)api_document ( ) ) at /usr / local / cellar / swi - prolog/824 / libexec / lib / swipl / boot / initpl533\n [ 42 ] api_documentapi_insert_document_(schematransaction_object{commit_infocommit_info{}descriptorbranch_descriptor{branch_name\""main\""repository_descriptor } inference_objects[]instance_objects[]parenttransaction_object{descriptor inference_objects[]instance_objects parent schema_object } schema_objects[]}<stream>(0x7f99ba1deca0)_23998 ) at /users / leather / project / terminusdb / terminusdb / src / core / api / api_documentpl175\n [ 41 ] ' $ bags'findall_loop(_24114'<garbage_collected>'_24118 [ ] ) at /usr / local / cellar / swi - prolog/824 / libexec / lib / swipl / boot / bagspl99\n [ 40 ] setup_call_catcher_cleanup('$bags''$new_findall_bag''$bags'findall_loop(_24172_24176[])_24154'$bags''$destroy_findall_bag ' ) at /usr / local / cellar / swi - prolog/824 / libexec / lib / swipl / boot / initpl614\n [ 36 ] ' < meta - call>'('<garbage_collecte > ' ) < foreign>\n [ 35 ] databasewith_transaction_(query_context{all_witnessesfalseauthorization'terminusdb//system / datum / user / admin'bindings[]commit_infocommit_info{authoramessagem}default_collectionbranch_descriptor{branch_name\""main\""repository_descriptor } files[]filtertype_filter{types } prefixes_24348{'@base'\""terminusdb///data/\""'@schema'\""terminusdb///schema#\""'@type''context'api'http//terminusdbcom / schema / api#'owl'http//wwww3org/2002/07 / owl#'rdf'http//wwww3org/1999/02/22-rdf - syntax - ns#'rdfs'http//wwww3org/2000/01 / rdf - schema#'sys'http//terminusdbcom / schema / sys#'vio'http//terminusdbcom / schema / vio#'woql'http//terminusdbcom / schema / woql#'xdd'http//terminusdbcom / schema / xdd#'xsd'http//wwww3org/2001 / xmlschema#'}selected[]systemsystem_descriptor{}transaction_objects[]update_guard_24308write_graphbranch_graph{branch_name\""main\""database_name\""tdb\""organization_name\""admin\""repository_name\""local\""typeinstance}}api_document()_24258 ) at /users / leather / project / terminusdb / terminusdb / src / core / transaction / databasepl220\n [ 34 ] setup_call_catcher_cleanup(databasetruedatabasewith_transaction_(_24500)_24478databasetrue ) at /usr / local / cellar / swi - prolog/824 / libexec / lib / swipl / boot / initpl614\n [ 30 ] ' < meta - call>'('<garbage_collecte > ' ) < foreign>\n [ 29 ] catch(routes()error(subdocument_key_type_restrictioncontext(_24596_24598))routesdo_or_die ( ) ) at /usr / local / cellar / swi - prolog/824 / libexec / lib / swipl / boot / initpl532\n [ 28 ] catch_with_backtrace('<garbage_collected>''<garbage_collected>''<garbage_collecte > ' ) at /usr / local / cellar / swi - prolog/824 / libexec / lib / swipl / boot / initpl582\n\nnote some frame be miss due to last - call optimization\nre - run -PRON- program in debug mode ( - debug ) to get more detail\n\n "" "" apistatus "" "" apiserver_error "" } testjson [ { "" @type "" "" class "" "" @id "" "" address "" "" @documentation "" { "" @comment "" "" this be address "" "" @propertie "" { } } "" @subdocument "" [ ] "" street "" "" xsdstring "" "" postal_code "" "" xsdstring "" "" country "" "" country "" } { "" @type "" "" class "" "" @id "" "" country "" "" name "" "" xsdstring "" "" perimeter "" { "" @type "" "" list "" "" @class "" "" coordinate "" } } { "" @type "" "" class "" "" @id "" "" coordinate "" "" x "" "" xsddecimal "" "" y "" "" xsddecimal "" } { "" @type "" "" enum "" "" @id "" "" role "" "" @value "" [ "" admin "" "" read "" "" write "" ] } { "" @type "" "" class "" "" @id "" "" employee "" "" @inherit "" [ "" person "" ] "" address_of "" "" address "" "" contact_number "" { "" @type "" "" optional "" "" @class "" "" xsdstring "" } "" managed_by "" "" employee "" "" member_of "" "" team "" "" permisstion "" { "" @type "" "" set "" "" @class "" "" role "" } "" name "" "" xsdstring "" "" age "" "" xsdinteger "" "" friend_of "" { "" @type "" "" set "" "" @class "" "" person "" } } { "" @type "" "" enum "" "" @id "" "" team "" "" @value "" [ "" information technology "" "" marketing "" ] } { "" @type "" "" class "" "" @id "" "" person "" "" @documentation "" { "" @comment "" "" this be a person "" "" @propertie "" { "" name "" "" name of the person "" "" age "" "" age of the person "" } } "" name "" "" xsdstring "" "" age "" "" xsdinteger "" "" friend_of "" { "" @type "" "" set "" "" @class "" "" person "" } } ]",0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,567,2021-09-13T10:57:46Z,spl,GET /api/remote/blah: stack trace error for database_does_not_exist,https://github.com/terminusdb/terminusdb/issues/567,"$ xh 'http://localhost:6363/api/remote/blah'
HTTP/1.1 500 Internal Server Error
connection: keep-alive
content-length: 1407
content-type: application/json
date: Mon, 13 Sep 2021 10:57:04 GMT

{
    ""api:message"": ""Error: database_does_not_exist(\""blah\"",\""_meta\"")\n  [34] throw(error(database_does_not_exist(\""blah\"",\""_meta\""),_15748))\n  [32] capabilities:check_descriptor_auth_(database_descriptor{database_name:\""_meta\"",organization_name:\""blah\""},'@schema':'Action/meta_write_access',anonymous,transaction_object{descriptor:system_descriptor{},inference_objects:[],instance_objects:[...],schema_objects:[...]}) at /Users/leather/projects/terminusdb/terminusdb/src/core/account/capabilities.pl:494\n  [28] api_remote:list_remotes(transaction_object{descriptor:system_descriptor{},inference_objects:[],instance_objects:[...],schema_objects:[...]},anonymous,blah,_15892) at /Users/leather/projects/terminusdb/terminusdb/src/core/api/api_remote.pl:113\n  [27] '<meta-call>'(routes:(...;...)) <foreign>\n  [26] catch(routes:(...;...),error(database_does_not_exist(\""blah\"",\""_meta\""),context(_16036,_16038)),routes:do_or_die(...,...)) at /usr/local/Cellar/swi-prolog/8.2.4/libexec/lib/swipl/boot/init.pl:532\n  [25] catch_with_backtrace(routes:(...;...),error(database_does_not_exist(\""blah\"",\""_meta\""),context(_16112,_16114)),routes:do_or_die(...,...)) at /usr/local/Cellar/swi-prolog/8.2.4/libexec/lib/swipl/boot/init.pl:582\n\nNote: some frames are missing due to last-call optimization.\nRe-run your program in debug mode (:- debug.) to get more detail.\n\n"",
    ""api:status"": ""api:server_error""
}",2021-09-13T13:12:52Z,spl,https://github.com/terminusdb/terminusdb/issues/567#issuecomment-918176735,Note that I'm not doing any authentication here.,note that -PRON- be not do any authentication here,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,567,2021-09-13T10:57:46Z,spl,GET /api/remote/blah: stack trace error for database_does_not_exist,https://github.com/terminusdb/terminusdb/issues/567,"$ xh 'http://localhost:6363/api/remote/blah'
HTTP/1.1 500 Internal Server Error
connection: keep-alive
content-length: 1407
content-type: application/json
date: Mon, 13 Sep 2021 10:57:04 GMT

{
    ""api:message"": ""Error: database_does_not_exist(\""blah\"",\""_meta\"")\n  [34] throw(error(database_does_not_exist(\""blah\"",\""_meta\""),_15748))\n  [32] capabilities:check_descriptor_auth_(database_descriptor{database_name:\""_meta\"",organization_name:\""blah\""},'@schema':'Action/meta_write_access',anonymous,transaction_object{descriptor:system_descriptor{},inference_objects:[],instance_objects:[...],schema_objects:[...]}) at /Users/leather/projects/terminusdb/terminusdb/src/core/account/capabilities.pl:494\n  [28] api_remote:list_remotes(transaction_object{descriptor:system_descriptor{},inference_objects:[],instance_objects:[...],schema_objects:[...]},anonymous,blah,_15892) at /Users/leather/projects/terminusdb/terminusdb/src/core/api/api_remote.pl:113\n  [27] '<meta-call>'(routes:(...;...)) <foreign>\n  [26] catch(routes:(...;...),error(database_does_not_exist(\""blah\"",\""_meta\""),context(_16036,_16038)),routes:do_or_die(...,...)) at /usr/local/Cellar/swi-prolog/8.2.4/libexec/lib/swipl/boot/init.pl:532\n  [25] catch_with_backtrace(routes:(...;...),error(database_does_not_exist(\""blah\"",\""_meta\""),context(_16112,_16114)),routes:do_or_die(...,...)) at /usr/local/Cellar/swi-prolog/8.2.4/libexec/lib/swipl/boot/init.pl:582\n\nNote: some frames are missing due to last-call optimization.\nRe-run your program in debug mode (:- debug.) to get more detail.\n\n"",
    ""api:status"": ""api:server_error""
}",2021-09-20T13:23:08Z,spl,https://github.com/terminusdb/terminusdb/issues/567#issuecomment-922924202,This should be a 404.,this should be a 404,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,568,2021-09-13T11:01:37Z,spl,GET /api/remote/: 400 error with BadAbsoluteDescriptor: /_meta,https://github.com/terminusdb/terminusdb/issues/568,"$ xh 'http://localhost:6363/api/remote/'
HTTP/1.1 400 Bad Request
connection: keep-alive
content-length: 261
content-type: application/json
date: Mon, 13 Sep 2021 11:00:59 GMT

{
    ""@type"": ""api:RemoteErrorResponse"",
    ""api:error"": {
        ""@type"": ""api:BadAbsoluteDescriptor"",
        ""api:absolute_descriptor"": ""/_meta""
    },
    ""api:message"": ""The following absolute resource descriptor string is invalid: '/_meta'"",
    ""api:status"": ""api:failure""
}",,,,,,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,569,2021-09-13T12:05:09Z,spl,POST /api/db/admin/t: Schema check failure when comment or label is null,https://github.com/terminusdb/terminusdb/issues/569,"From a fresh store:
#!/bin/bash
set -ex

xh 'http://admin:root@localhost:6363/api/db/admin/t' <<EOF
{""comment"":null,""label"":""l""}
EOF
HTTP/1.1 400 Bad Request
connection: keep-alive
content-length: 403
content-type: application/json
date: Mon, 13 Sep 2021 12:04:34 GMT

{
    ""api:message"": ""Schema check failure"",
    ""api:status"": ""api:failure"",
    ""system:witnesses"": [
        {
            ""@type"": ""instance_not_cardinality_one"",
            ""class"": ""http://www.w3.org/2001/XMLSchema#string"",
            ""instance"": ""terminusdb://system/data/UserDatabase/001937d8700681688235f1518a5104ccf8dfa89baeb9d58aa95d85f723bc55ac"",
            ""predicate"": ""http://terminusdb.com/schema/system#comment""
        }
    ]
}
#!/bin/bash
set -ex

xh 'http://admin:root@localhost:6363/api/db/admin/t' <<EOF
{""comment"":""c"",""label"":null}
EOF
HTTP/1.1 400 Bad Request
connection: keep-alive
content-length: 401
content-type: application/json
date: Mon, 13 Sep 2021 12:05:24 GMT

{
    ""api:message"": ""Schema check failure"",
    ""api:status"": ""api:failure"",
    ""system:witnesses"": [
        {
            ""@type"": ""instance_not_cardinality_one"",
            ""class"": ""http://www.w3.org/2001/XMLSchema#string"",
            ""instance"": ""terminusdb://system/data/UserDatabase/82c036c1755d6007e0a3c2290048efc6c7625e36c7b3a504adfe3c7a8aebfaf2"",
            ""predicate"": ""http://terminusdb.com/schema/system#label""
        }
    ]
}",,,,,,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,570,2021-09-13T12:13:59Z,matko,Ensure Lexicals over optionals work,https://github.com/terminusdb/terminusdb/issues/570,"There's some uncertainty over what lexicals over optionals do right now, and if it is the right thing. Better get that checked.",2021-09-13T15:25:23Z,AstroChelonian,https://github.com/terminusdb/terminusdb/issues/570#issuecomment-918306866,"To replicate:
#!/bin/bash
set -ex
Create the database.
xh 'http://admin:root@localhost:6363/api/db/admin/test570' <<EOF
{""label"":""l"",""comment"":""c""}
EOF


Create a schema with an optional field and a lexical key.
curl -X POST -H ""Content-Type: application/json"" -u ""admin:root"" ""http://localhost:6363/api/document/admin/test570?graph_type=schema&author=me&message=me"" --data-binary '{""@id"": ""Doc"", ""@type"": ""Class"", ""optional_field"": {""@type"": ""Optional"", ""@class"": ""xsd:string""}, ""@key"": { ""@type"" : ""Lexical"", ""@fields"" : [""optional_field""]}}'

Add something in the optional field.
curl -X POST -H ""Content-Type: application/json"" -u ""admin:root"" ""http://localhost:6363/api/document/admin/test570?graph_type=instance&author=me&message=me"" --data-binary '{""@type"": ""Doc"", ""optional_field"": ""string""}'

Omit the optional field.
curl -X POST -H ""Content-Type: application/json"" -u ""admin:root"" ""http://localhost:6363/api/document/admin/test570?graph_type=instance&author=me&message=me"" --data-binary '{""@type"": ""Doc""}'

When the optional field is omitted, the following error occurs:

{
  ""api:message"":""Error: missing_key('terminusdb:///schema#optional_field',json{'@id':_6198,'@type':'terminusdb:///schema#Doc'})\n  [46] throw(error(missing_key('terminusdb:///schema#optional_field',...),_6250))\n  [44] catch('<garbage_collected>','<garbage_collected>','<garbage_collected>') at /usr/lib/swi-prolog/boot/init.pl:533\n  [42] api_document:api_insert_document_(instance,transaction_object{commit_info:commit_info{},descriptor:branch_descriptor{branch_name:\""main\"",repository_descriptor: ...},inference_objects:[],instance_objects:[...],parent:transaction_object{descriptor: ...,inference_objects:[],instance_objects: ...,parent: ...,schema_objects: ...},schema_objects:[...]},<stream>(0x7f1388009760),_6332) at /home/astrochelonian/Documents/terminusdb/src/core/api/api_document.pl:188\n  [41] '$bags':findall_loop(_6448,'<garbage_collected>',_6452,[]) at /usr/lib/swi-prolog/boot/bags.pl:99\n  [40] setup_call_catcher_cleanup('$bags':'$new_findall_bag','$bags':findall_loop(_6506,...,_6510,[]),_6488,'$bags':'$destroy_findall_bag') at /usr/lib/swi-prolog/boot/init.pl:614\n  [36] '<meta-call>'('<garbage_collected>') <foreign>\n  [35] database:with_transaction_(query_context{all_witnesses:false,authorization:'terminusdb://system/data/User/admin',bindings:[],commit_info:commit_info{author:me,message:me},default_collection:branch_descriptor{branch_name:\""main\"",repository_descriptor: ...},files:[],filter:type_filter{types: ...},prefixes:_6718{'@base':\""terminusdb:///data/\"",'@schema':\""terminusdb:///schema#\"",'@type':'Context',api:'http://terminusdb.com/schema/api#',owl:'http://www.w3.org/2002/07/owl#',rdf:'http://www.w3.org/1999/02/22-rdf-syntax-ns#',rdfs:'http://www.w3.org/2000/01/rdf-schema#',sys:'http://terminusdb.com/schema/sys#',vio:'http://terminusdb.com/schema/vio#',woql:'http://terminusdb.com/schema/woql#',xdd:'http://terminusdb.com/schema/xdd#',xsd:'http://www.w3.org/2001/XMLSchema#'},selected:[],system:system_descriptor{},transaction_objects:[...],update_guard:_6642,write_graph:branch_graph{branch_name:\""main\"",database_name:\""test570\"",organization_name:\""admin\"",repository_name:\""local\"",type:instance}},api_document:(...;...),_6592) at /home/astrochelonian/Documents/terminusdb/src/core/transaction/database.pl:220\n  [34] setup_call_catcher_cleanup(database:true,database:with_transaction_(...,...,_6834),_6812,database:true) at /usr/lib/swi-prolog/boot/init.pl:614\n  [30] '<meta-call>'('<garbage_collected>') <foreign>\n  [29] catch(routes:(...,...),error(missing_key('terminusdb:///schema#optional_field',...),context(_6936,_6938)),routes:do_or_die(...,...)) at /usr/lib/swi-prolog/boot/init.pl:532\n  [28] catch_with_backtrace('<garbage_collected>','<garbage_collected>','<garbage_collected>') at /usr/lib/swi-prolog/boot/init.pl:582\n\nNote: some frames are missing due to last-call optimization.\nRe-run your program in debug mode (:- debug.) to get more detail.\n\n"",
  ""api:status"":""api:server_error""
}%","to replicate # /bin / bash set -ex create the database xh ' http//adminroot@localhost6363 / api / db / admin / test570 ' < < eof { "" label""""l""""comment""""c "" } eof create a schema with an optional field and a lexical key curl -x post -h "" content - type application / json "" -u "" adminroot "" "" http//localhost6363 / api / document / admin / test570graph_type = schema&author = me&message = -PRON- "" --data - binary ' { "" @id "" "" doc "" "" @type "" "" class "" "" optional_field "" { "" @type "" "" optional "" "" @class "" "" xsdstring "" } "" @key "" { "" @type "" "" lexical "" "" @fields "" [ "" optional_field "" ] } } ' add something in the optional field curl -x post -h "" content - type application / json "" -u "" adminroot "" "" http//localhost6363 / api / document / admin / test570graph_type = instance&author = me&message = -PRON- "" --data - binary ' { "" @type "" "" doc "" "" optional_field "" "" string "" } ' omit the optional field curl -x post -h "" content - type application / json "" -u "" adminroot "" "" http//localhost6363 / api / document / admin / test570graph_type = instance&author = me&message = -PRON- "" --data - binary ' { "" @type "" "" doc "" } ' when the optional field be omit the follow error occur { "" apimessage""""error missing_key('terminusdb///schema#optional_field'json{'@id'_6198'@type''terminusdb///schema#doc'})\n [ 46 ] throw(error(missing_key('terminusdb///schema#optional_field')_6250))\n [ 44 ] catch('<garbage_collected>''<garbage_collected>''<garbage_collecte > ' ) at /usr / lib / swi - prolog / boot / initpl533\n [ 42 ] api_documentapi_insert_document_(instancetransaction_object{commit_infocommit_info{}descriptorbranch_descriptor{branch_name\""main\""repository_descriptor } inference_objects[]instance_objects[]parenttransaction_object{descriptor inference_objects[]instance_objects parent schema_object } schema_objects[]}<stream>(0x7f1388009760)_6332 ) at /home / astrochelonian / document / terminusdb / src / core / api / api_documentpl188\n [ 41 ] ' $ bags'findall_loop(_6448'<garbage_collected>'_6452 [ ] ) at /usr / lib / swi - prolog / boot / bagspl99\n [ 40 ] setup_call_catcher_cleanup('$bags''$new_findall_bag''$bags'findall_loop(_6506_6510[])_6488'$bags''$destroy_findall_bag ' ) at /usr / lib / swi - prolog / boot / initpl614\n [ 36 ] ' < meta - call>'('<garbage_collecte > ' ) < foreign>\n [ 35 ] databasewith_transaction_(query_context{all_witnessesfalseauthorization'terminusdb//system / datum / user / admin'bindings[]commit_infocommit_info{authormemessageme}default_collectionbranch_descriptor{branch_name\""main\""repository_descriptor } files[]filtertype_filter{types } prefixes_6718{'@base'\""terminusdb///data/\""'@schema'\""terminusdb///schema#\""'@type''context'api'http//terminusdbcom / schema / api#'owl'http//wwww3org/2002/07 / owl#'rdf'http//wwww3org/1999/02/22-rdf - syntax - ns#'rdfs'http//wwww3org/2000/01 / rdf - schema#'sys'http//terminusdbcom / schema / sys#'vio'http//terminusdbcom / schema / vio#'woql'http//terminusdbcom / schema / woql#'xdd'http//terminusdbcom / schema / xdd#'xsd'http//wwww3org/2001 / xmlschema#'}selected[]systemsystem_descriptor{}transaction_objects[]update_guard_6642write_graphbranch_graph{branch_name\""main\""database_name\""test570\""organization_name\""admin\""repository_name\""local\""typeinstance}}api_document()_6592 ) at /home / astrochelonian / document / terminusdb / src / core / transaction / databasepl220\n [ 34 ] setup_call_catcher_cleanup(databasetruedatabasewith_transaction_(_6834)_6812databasetrue ) at /usr / lib / swi - prolog / boot / initpl614\n [ 30 ] ' < meta - call>'('<garbage_collecte > ' ) < foreign>\n [ 29 ] catch(routes()error(missing_key('terminusdb///schema#optional_field')context(_6936_6938))routesdo_or_die ( ) ) at /usr / lib / swi - prolog / boot / initpl532\n [ 28 ] catch_with_backtrace('<garbage_collected>''<garbage_collected>''<garbage_collecte > ' ) at /usr / lib / swi - prolog / boot / initpl582\n\nnote some frame be miss due to last - call optimization\nre - run -PRON- program in debug mode ( - debug ) to get more detail\n\n "" "" apistatus""""apiserver_error "" } %",0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,570,2021-09-13T12:13:59Z,matko,Ensure Lexicals over optionals work,https://github.com/terminusdb/terminusdb/issues/570,"There's some uncertainty over what lexicals over optionals do right now, and if it is the right thing. Better get that checked.",2021-09-13T15:26:53Z,matko,https://github.com/terminusdb/terminusdb/issues/570#issuecomment-918308258,"So long story short, optional fields in lexical keys are treated exactly the same as the nonoptional variant. Their value is used to construct the lexical key (taking no special steps on account of its optional-ness), and if it's missing, a huge ugly error is thrown because the id building strategy expected it to be there.",so long story short optional field in lexical key be treat exactly the same as the nonoptional variant -PRON- value be use to construct the lexical key ( take no special step on account of -PRON- optional - ness ) and if -PRON- be miss a huge ugly error be throw because the -PRON- d building strategy expect -PRON- to be there,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,570,2021-09-13T12:13:59Z,matko,Ensure Lexicals over optionals work,https://github.com/terminusdb/terminusdb/issues/570,"There's some uncertainty over what lexicals over optionals do right now, and if it is the right thing. Better get that checked.",2021-09-13T15:32:46Z,AstroChelonian,https://github.com/terminusdb/terminusdb/issues/570#issuecomment-918313718,"Do we want to make it impossible for optional fields to have a key?


We need to generate something for lexical keys to have optional fields:
a)The lexical key has a value when the optional field exists
b) the lexical key does not exist if the optional field does not exist as well


@GavinMendelGleason any thoughts?",do -PRON- want to make -PRON- impossible for optional field to have a key -PRON- need to generate something for lexical key to have optional field a)the lexical key have a value when the optional field exist b ) the lexical key do not exist if the optional field do not exist as well @gavinmendelgleason any thought,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,570,2021-09-13T12:13:59Z,matko,Ensure Lexicals over optionals work,https://github.com/terminusdb/terminusdb/issues/570,"There's some uncertainty over what lexicals over optionals do right now, and if it is the right thing. Better get that checked.",2021-09-13T20:52:45Z,GavinMendelGleason,https://github.com/terminusdb/terminusdb/issues/570#issuecomment-918566856,"A variant of 2.
We should have the key formed with an out-of-band value to ensure that we get a valid lexical key which can not experience collisions whether the value is there or not.
The out of band value can be an unencoded '+' for the ""none"" condition, or we can encode the value with a ""Some+{value}"" prefix, where Some is literally anything (including nothing).","a variant of 2 -PRON- should have the key form with an out - of - band value to ensure that -PRON- get a valid lexical key which can not experience collision whether the value be there or not the out of band value can be an unencoded ' + ' for the "" none "" condition or -PRON- can encode the value with a "" some+{value } "" prefix where some be literally anything ( include nothing )",0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,570,2021-09-13T12:13:59Z,matko,Ensure Lexicals over optionals work,https://github.com/terminusdb/terminusdb/issues/570,"There's some uncertainty over what lexicals over optionals do right now, and if it is the right thing. Better get that checked.",2021-09-15T14:44:57Z,matko,https://github.com/terminusdb/terminusdb/issues/570#issuecomment-920085667,@AstroChelonian I'm fixing this today.,@astrochelonian -PRON- be fix this today,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,570,2021-09-13T12:13:59Z,matko,Ensure Lexicals over optionals work,https://github.com/terminusdb/terminusdb/issues/570,"There's some uncertainty over what lexicals over optionals do right now, and if it is the right thing. Better get that checked.",2021-09-15T21:22:00Z,matko,https://github.com/terminusdb/terminusdb/issues/570#issuecomment-920392117,"So one thing that would be nice is if people make an existing field optional or remove the optional restriction, any existing objects that did have the property set remain valid. So an out-of-band value for 'none' is probably best.",so one thing that would be nice be if people make an exist field optional or remove the optional restriction any exist object that do have the property set remain valid so an out - of - band value for ' none ' be probably good,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,571,2021-09-13T12:47:15Z,Cheukting,property name '_key' is drop during insert shcema,https://github.com/terminusdb/terminusdb/issues/571,"I insert this:
[{'@type': 'Class', '@id': 'Address', '@documentation': {'@comment': 'This is address', '@properties': {}}, '@subdocument': [], '@key': {'@type': 'Random'}, 'street': 'xsd:string', 'postal_code': 'xsd:string', 'country': 'Country'}, {'@type': 'Class', '@id': 'Contact', '@inherits': ['TaggedUnion'], '@documentation': {'@comment': 'TaggedUnion allow options for types', '@properties': {}}, '_key': {'type': 'Random'}, 'local_number': 'xsd:integer', 'international': 'xsd:string'}, {'@type': 'Class', '@id': 'Coordinate', '@abstract': [], '_key': {'type': 'Random'}, 'x': 'xsd:decimal', 'y': 'xsd:decimal'}, {'@type': 'Class', '@id': 'Country', '@documentation': {'@comment': ""This is Country.\nCountry is a class object in the schema. It's class attributes will be the properties of the object. Therefore, a Country object will have a name which is string and a list of alias names that is called 'also_know_as'"", '@properties': {}}, '_key': {'type': 'Hash', 'fields': ['name']}, 'name': 'xsd:string', 'also_know_as': {'@type': 'List', '@class': 'xsd:string'}}, {'@type': 'Class', '@id': 'Employee', '@inherits': ['Person'], '@documentation': {'@comment': 'Employee will inherits the attributes from Person', '@properties': {}}, '_key': {'type': 'Random'}, 'address_of': 'Address', 'contact_number': {'@type': 'Optional', '@class': 'xsd:string'}, 'managed_by': 'Employee', 'name': 'xsd:string', 'age': 'xsd:integer', 'friend_of': {'@type': 'Set', '@class': 'Person'}}, {'@type': 'Class', '@id': 'Location', '@inherits': ['Address', 'Coordinate'], '@documentation': {'@comment': 'Location is inherits from Address and Coordinate\nClass can have multiple inheritance. It will inherits both the attibutes from Address and Coordinate.', '@properties': {}}, '@subdocument': [], '@key': {'@type': 'Random'}, '@abstract': [], 'name': 'xsd:string', 'street': 'xsd:string', 'postal_code': 'xsd:string', 'country': 'Country', 'x': 'xsd:decimal', 'y': 'xsd:decimal'}, {'@type': 'Class', '@id': 'Person', '@documentation': {'@comment': 'This is a person\nCan store the explanation to the attributes in the docstring. Docstrings needs to be in numpydoc format.', '@properties': {'name': 'Name of the person.', 'age': 'Age of the person.'}}, '_key': {'type': 'Random'}, 'name': 'xsd:string', 'age': 'xsd:integer', 'friend_of': {'@type': 'Set', '@class': 'Person'}}, {'@type': 'Enum', '@id': 'Team', '@value': ['Information Technology', 'Marketing']}]

and got this back:
[{'@base': 'terminusdb:///data/', '@schema': 'terminusdb:///schema#', '@type': '@context'}, {'@documentation': {'@comment': 'This is address'}, '@id': 'Address', '@key': {'@type': 'Random'}, '@subdocument': [], '@type': 'Class', 'country': 'Country', 'postal_code': 'xsd:string', 'street': 'xsd:string'}, {'@documentation': {'@comment': 'TaggedUnion allow options for types'}, '@id': 'Contact', '@inherits': 'TaggedUnion', '@type': 'Class', 'international': 'xsd:string', 'local_number': 'xsd:integer'}, {'@abstract': [], '@id': 'Coordinate', '@type': 'Class', 'x': 'xsd:decimal', 'y': 'xsd:decimal'}, {'@documentation': {'@comment': ""This is Country.\nCountry is a class object in the schema. It's class attributes will be the properties of the object. Therefore, a Country object will have a name which is string and a list of alias names that is called 'also_know_as'""}, '@id': 'Country', '@type': 'Class', 'also_know_as': {'@class': 'xsd:string', '@type': 'List'}, 'name': 'xsd:string'}, {'@documentation': {'@comment': 'Employee will inherits the attributes from Person'}, '@id': 'Employee', '@inherits': 'Person', '@type': 'Class', 'address_of': 'Address', 'age': 'xsd:integer', 'contact_number': {'@class': 'xsd:string', '@type': 'Optional'}, 'friend_of': {'@class': 'Person', '@type': 'Set'}, 'managed_by': 'Employee', 'name': 'xsd:string'}, {'@abstract': [], '@documentation': {'@comment': 'Location is inherits from Address and Coordinate\nClass can have multiple inheritance. It will inherits both the attibutes from Address and Coordinate.'}, '@id': 'Location', '@inherits': ['Address', 'Coordinate'], '@key': {'@type': 'Random'}, '@subdocument': [], '@type': 'Class', 'country': 'Country', 'name': 'xsd:string', 'postal_code': 'xsd:string', 'street': 'xsd:string', 'x': 'xsd:decimal', 'y': 'xsd:decimal'}, {'@documentation': {'@comment': 'This is a person\nCan store the explanation to the attributes in the docstring. Docstrings needs to be in numpydoc format.', '@properties': {'age': 'Age of the person.', 'name': 'Name of the person.'}}, '@id': 'Person', '@type': 'Class', 'age': 'xsd:integer', 'friend_of': {'@class': 'Person', '@type': 'Set'}, 'name': 'xsd:string'}, {'@id': 'Team', '@type': 'Enum', '@value': ['Information Technology', 'Marketing']}]

seems _key is gone",2021-09-13T12:50:30Z,Cheukting,https://github.com/terminusdb/terminusdb/issues/571#issuecomment-918157835,Same apply for key_,same apply for key _,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,571,2021-09-13T12:47:15Z,Cheukting,property name '_key' is drop during insert shcema,https://github.com/terminusdb/terminusdb/issues/571,"I insert this:
[{'@type': 'Class', '@id': 'Address', '@documentation': {'@comment': 'This is address', '@properties': {}}, '@subdocument': [], '@key': {'@type': 'Random'}, 'street': 'xsd:string', 'postal_code': 'xsd:string', 'country': 'Country'}, {'@type': 'Class', '@id': 'Contact', '@inherits': ['TaggedUnion'], '@documentation': {'@comment': 'TaggedUnion allow options for types', '@properties': {}}, '_key': {'type': 'Random'}, 'local_number': 'xsd:integer', 'international': 'xsd:string'}, {'@type': 'Class', '@id': 'Coordinate', '@abstract': [], '_key': {'type': 'Random'}, 'x': 'xsd:decimal', 'y': 'xsd:decimal'}, {'@type': 'Class', '@id': 'Country', '@documentation': {'@comment': ""This is Country.\nCountry is a class object in the schema. It's class attributes will be the properties of the object. Therefore, a Country object will have a name which is string and a list of alias names that is called 'also_know_as'"", '@properties': {}}, '_key': {'type': 'Hash', 'fields': ['name']}, 'name': 'xsd:string', 'also_know_as': {'@type': 'List', '@class': 'xsd:string'}}, {'@type': 'Class', '@id': 'Employee', '@inherits': ['Person'], '@documentation': {'@comment': 'Employee will inherits the attributes from Person', '@properties': {}}, '_key': {'type': 'Random'}, 'address_of': 'Address', 'contact_number': {'@type': 'Optional', '@class': 'xsd:string'}, 'managed_by': 'Employee', 'name': 'xsd:string', 'age': 'xsd:integer', 'friend_of': {'@type': 'Set', '@class': 'Person'}}, {'@type': 'Class', '@id': 'Location', '@inherits': ['Address', 'Coordinate'], '@documentation': {'@comment': 'Location is inherits from Address and Coordinate\nClass can have multiple inheritance. It will inherits both the attibutes from Address and Coordinate.', '@properties': {}}, '@subdocument': [], '@key': {'@type': 'Random'}, '@abstract': [], 'name': 'xsd:string', 'street': 'xsd:string', 'postal_code': 'xsd:string', 'country': 'Country', 'x': 'xsd:decimal', 'y': 'xsd:decimal'}, {'@type': 'Class', '@id': 'Person', '@documentation': {'@comment': 'This is a person\nCan store the explanation to the attributes in the docstring. Docstrings needs to be in numpydoc format.', '@properties': {'name': 'Name of the person.', 'age': 'Age of the person.'}}, '_key': {'type': 'Random'}, 'name': 'xsd:string', 'age': 'xsd:integer', 'friend_of': {'@type': 'Set', '@class': 'Person'}}, {'@type': 'Enum', '@id': 'Team', '@value': ['Information Technology', 'Marketing']}]

and got this back:
[{'@base': 'terminusdb:///data/', '@schema': 'terminusdb:///schema#', '@type': '@context'}, {'@documentation': {'@comment': 'This is address'}, '@id': 'Address', '@key': {'@type': 'Random'}, '@subdocument': [], '@type': 'Class', 'country': 'Country', 'postal_code': 'xsd:string', 'street': 'xsd:string'}, {'@documentation': {'@comment': 'TaggedUnion allow options for types'}, '@id': 'Contact', '@inherits': 'TaggedUnion', '@type': 'Class', 'international': 'xsd:string', 'local_number': 'xsd:integer'}, {'@abstract': [], '@id': 'Coordinate', '@type': 'Class', 'x': 'xsd:decimal', 'y': 'xsd:decimal'}, {'@documentation': {'@comment': ""This is Country.\nCountry is a class object in the schema. It's class attributes will be the properties of the object. Therefore, a Country object will have a name which is string and a list of alias names that is called 'also_know_as'""}, '@id': 'Country', '@type': 'Class', 'also_know_as': {'@class': 'xsd:string', '@type': 'List'}, 'name': 'xsd:string'}, {'@documentation': {'@comment': 'Employee will inherits the attributes from Person'}, '@id': 'Employee', '@inherits': 'Person', '@type': 'Class', 'address_of': 'Address', 'age': 'xsd:integer', 'contact_number': {'@class': 'xsd:string', '@type': 'Optional'}, 'friend_of': {'@class': 'Person', '@type': 'Set'}, 'managed_by': 'Employee', 'name': 'xsd:string'}, {'@abstract': [], '@documentation': {'@comment': 'Location is inherits from Address and Coordinate\nClass can have multiple inheritance. It will inherits both the attibutes from Address and Coordinate.'}, '@id': 'Location', '@inherits': ['Address', 'Coordinate'], '@key': {'@type': 'Random'}, '@subdocument': [], '@type': 'Class', 'country': 'Country', 'name': 'xsd:string', 'postal_code': 'xsd:string', 'street': 'xsd:string', 'x': 'xsd:decimal', 'y': 'xsd:decimal'}, {'@documentation': {'@comment': 'This is a person\nCan store the explanation to the attributes in the docstring. Docstrings needs to be in numpydoc format.', '@properties': {'age': 'Age of the person.', 'name': 'Name of the person.'}}, '@id': 'Person', '@type': 'Class', 'age': 'xsd:integer', 'friend_of': {'@class': 'Person', '@type': 'Set'}, 'name': 'xsd:string'}, {'@id': 'Team', '@type': 'Enum', '@value': ['Information Technology', 'Marketing']}]

seems _key is gone",2021-09-13T12:53:15Z,Cheukting,https://github.com/terminusdb/terminusdb/issues/571#issuecomment-918159847,"To think of it, it may come to an error that I put in a non valid value for '_key' but it just drop scilencely",to think of -PRON- -PRON- may come to an error that i put in a non valid value for ' _ key ' but -PRON- just drop scilencely,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,571,2021-09-13T12:47:15Z,Cheukting,property name '_key' is drop during insert shcema,https://github.com/terminusdb/terminusdb/issues/571,"I insert this:
[{'@type': 'Class', '@id': 'Address', '@documentation': {'@comment': 'This is address', '@properties': {}}, '@subdocument': [], '@key': {'@type': 'Random'}, 'street': 'xsd:string', 'postal_code': 'xsd:string', 'country': 'Country'}, {'@type': 'Class', '@id': 'Contact', '@inherits': ['TaggedUnion'], '@documentation': {'@comment': 'TaggedUnion allow options for types', '@properties': {}}, '_key': {'type': 'Random'}, 'local_number': 'xsd:integer', 'international': 'xsd:string'}, {'@type': 'Class', '@id': 'Coordinate', '@abstract': [], '_key': {'type': 'Random'}, 'x': 'xsd:decimal', 'y': 'xsd:decimal'}, {'@type': 'Class', '@id': 'Country', '@documentation': {'@comment': ""This is Country.\nCountry is a class object in the schema. It's class attributes will be the properties of the object. Therefore, a Country object will have a name which is string and a list of alias names that is called 'also_know_as'"", '@properties': {}}, '_key': {'type': 'Hash', 'fields': ['name']}, 'name': 'xsd:string', 'also_know_as': {'@type': 'List', '@class': 'xsd:string'}}, {'@type': 'Class', '@id': 'Employee', '@inherits': ['Person'], '@documentation': {'@comment': 'Employee will inherits the attributes from Person', '@properties': {}}, '_key': {'type': 'Random'}, 'address_of': 'Address', 'contact_number': {'@type': 'Optional', '@class': 'xsd:string'}, 'managed_by': 'Employee', 'name': 'xsd:string', 'age': 'xsd:integer', 'friend_of': {'@type': 'Set', '@class': 'Person'}}, {'@type': 'Class', '@id': 'Location', '@inherits': ['Address', 'Coordinate'], '@documentation': {'@comment': 'Location is inherits from Address and Coordinate\nClass can have multiple inheritance. It will inherits both the attibutes from Address and Coordinate.', '@properties': {}}, '@subdocument': [], '@key': {'@type': 'Random'}, '@abstract': [], 'name': 'xsd:string', 'street': 'xsd:string', 'postal_code': 'xsd:string', 'country': 'Country', 'x': 'xsd:decimal', 'y': 'xsd:decimal'}, {'@type': 'Class', '@id': 'Person', '@documentation': {'@comment': 'This is a person\nCan store the explanation to the attributes in the docstring. Docstrings needs to be in numpydoc format.', '@properties': {'name': 'Name of the person.', 'age': 'Age of the person.'}}, '_key': {'type': 'Random'}, 'name': 'xsd:string', 'age': 'xsd:integer', 'friend_of': {'@type': 'Set', '@class': 'Person'}}, {'@type': 'Enum', '@id': 'Team', '@value': ['Information Technology', 'Marketing']}]

and got this back:
[{'@base': 'terminusdb:///data/', '@schema': 'terminusdb:///schema#', '@type': '@context'}, {'@documentation': {'@comment': 'This is address'}, '@id': 'Address', '@key': {'@type': 'Random'}, '@subdocument': [], '@type': 'Class', 'country': 'Country', 'postal_code': 'xsd:string', 'street': 'xsd:string'}, {'@documentation': {'@comment': 'TaggedUnion allow options for types'}, '@id': 'Contact', '@inherits': 'TaggedUnion', '@type': 'Class', 'international': 'xsd:string', 'local_number': 'xsd:integer'}, {'@abstract': [], '@id': 'Coordinate', '@type': 'Class', 'x': 'xsd:decimal', 'y': 'xsd:decimal'}, {'@documentation': {'@comment': ""This is Country.\nCountry is a class object in the schema. It's class attributes will be the properties of the object. Therefore, a Country object will have a name which is string and a list of alias names that is called 'also_know_as'""}, '@id': 'Country', '@type': 'Class', 'also_know_as': {'@class': 'xsd:string', '@type': 'List'}, 'name': 'xsd:string'}, {'@documentation': {'@comment': 'Employee will inherits the attributes from Person'}, '@id': 'Employee', '@inherits': 'Person', '@type': 'Class', 'address_of': 'Address', 'age': 'xsd:integer', 'contact_number': {'@class': 'xsd:string', '@type': 'Optional'}, 'friend_of': {'@class': 'Person', '@type': 'Set'}, 'managed_by': 'Employee', 'name': 'xsd:string'}, {'@abstract': [], '@documentation': {'@comment': 'Location is inherits from Address and Coordinate\nClass can have multiple inheritance. It will inherits both the attibutes from Address and Coordinate.'}, '@id': 'Location', '@inherits': ['Address', 'Coordinate'], '@key': {'@type': 'Random'}, '@subdocument': [], '@type': 'Class', 'country': 'Country', 'name': 'xsd:string', 'postal_code': 'xsd:string', 'street': 'xsd:string', 'x': 'xsd:decimal', 'y': 'xsd:decimal'}, {'@documentation': {'@comment': 'This is a person\nCan store the explanation to the attributes in the docstring. Docstrings needs to be in numpydoc format.', '@properties': {'age': 'Age of the person.', 'name': 'Name of the person.'}}, '@id': 'Person', '@type': 'Class', 'age': 'xsd:integer', 'friend_of': {'@class': 'Person', '@type': 'Set'}, 'name': 'xsd:string'}, {'@id': 'Team', '@type': 'Enum', '@value': ['Information Technology', 'Marketing']}]

seems _key is gone",2021-09-15T10:40:21Z,GavinMendelGleason,https://github.com/terminusdb/terminusdb/issues/571#issuecomment-919905687,"The key is pointing to an object which has no schema definition. This now results in an error rather than silent dropping so I will close.
'_key': {'type': 'Hash', 'fields': ['name']}",the key be point to an object which have no schema definition this now result in an error rather than silent dropping so i will close ' _ key ' { ' type ' ' hash ' ' field ' [ ' name ' ] },0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,572,2021-09-13T13:53:03Z,spl,ID capture in document insert,https://github.com/terminusdb/terminusdb/issues/572,"See #565
Coroutining magic!",,,,,,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,573,2021-09-13T14:14:59Z,matko,Allow disabling anonymous user,https://github.com/terminusdb/terminusdb/issues/573,"TerminusDB has an anonymous user which can be granted access to various resources, such as public databases. In some deployments, it makes no sense to have any sort of anonymous access. It'd be nice to be able to disable anonymous access for such deploys.",,,,,,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,574,2021-09-13T14:23:25Z,spl,`DELETE /api/db/admin/d` with `Content-Type: application/json` and empty request body hangs forever,https://github.com/terminusdb/terminusdb/issues/574,xh DELETE 'http://admin:root@localhost:6363/api/db/admin/d' Content-Type:application/json,2021-09-20T14:17:53Z,spl,https://github.com/terminusdb/terminusdb/issues/574#issuecomment-922971588,"This does not hang:
xh DELETE 'http://admin:root@localhost:6363/api/db/admin/d' Content-Type:application/json Content-Length:0",this do not hang xh delete ' http//adminroot@localhost6363 / api / db / admin / d ' content - typeapplication / json content - length0,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,574,2021-09-13T14:23:25Z,spl,`DELETE /api/db/admin/d` with `Content-Type: application/json` and empty request body hangs forever,https://github.com/terminusdb/terminusdb/issues/574,xh DELETE 'http://admin:root@localhost:6363/api/db/admin/d' Content-Type:application/json,2021-09-20T14:19:04Z,spl,https://github.com/terminusdb/terminusdb/issues/574#issuecomment-922972610,We should find out what the SWI-Prolog HTTP server request timeout is.,-PRON- should find out what the swi - prolog http server request timeout be,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,574,2021-09-13T14:23:25Z,spl,`DELETE /api/db/admin/d` with `Content-Type: application/json` and empty request body hangs forever,https://github.com/terminusdb/terminusdb/issues/574,xh DELETE 'http://admin:root@localhost:6363/api/db/admin/d' Content-Type:application/json,2021-10-04T14:32:34Z,spl,https://github.com/terminusdb/terminusdb/issues/574#issuecomment-933546471,@GavinMendelGleason says implementing a timeout would make it difficult for users. We have request timeouts with nginx on TerminusX. Won't change for now.,@gavinmendelgleason say implement a timeout would make -PRON- difficult for user -PRON- have request timeout with nginx on terminusx will not change for now,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,576,2021-09-13T18:17:41Z,spl,No auth reveals existence of organization,https://github.com/terminusdb/terminusdb/issues/576,"With the existing admin organization, the following POST to /api/db fails authorization:
xh 'http://localhost:6363/api/db/admin/dbt' label=l comment=c
HTTP/1.1 403 Forbidden
connection: keep-alive
content-length: 323
content-type: application/json
date: Mon, 13 Sep 2021 18:13:07 GMT

{
    ""action"": ""'@schema':'Action/create_database'"",
    ""api:message"": ""Access to 'terminusdb://system/data/Organization/admin' is not authorised with action '@schema':'Action/create_database' and auth anonymous"",
    ""api:status"": ""api:forbidden"",
    ""auth"": ""anonymous"",
    ""scope"": ""'terminusdb://system/data/Organization/admin'""
}
With an unknown org organization, the following POST to /api/db does not fail authorization but informs that the organization does not exist:
xh 'http://localhost:6363/api/db/org/dbt' label=l comment=c
HTTP/1.1 400 Bad Request
connection: keep-alive
content-length: 206
content-type: application/json
date: Mon, 13 Sep 2021 18:09:39 GMT

{
    ""@type"": ""api:DbCreateErrorResponse"",
    ""api:error"": {
        ""@type"": ""api:UnknownOrganization"",
        ""api:organization_name"": ""org""
    },
    ""api:message"": ""Organization org does not exist."",
    ""api:status"": ""api:failure""
}
The same can be repeated for DELETE:
xh DELETE 'http://localhost:6363/api/db/admin/dbt'
xh DELETE 'http://localhost:6363/api/db/org/dbt'",2021-09-22T12:06:21Z,spl,https://github.com/terminusdb/terminusdb/issues/576#issuecomment-924866681,"I'm pushing this down the line. I think it requires a discussion about authorization. I don't know what is intended by it in the big picture.
First, in this case, should anonymous even be allowed to create/delete a database? Perhaps we should do an authorization check for anonymous before even doing anything else. Of course, that still leaves non-anonymous users the ability to learn information like the existence of an organization.
Second, in the general case, after at assert_auth_action_scope and src/core/account/capabilities.pl, it seems like authorization works on the existence of specific scopes. So, here we are doing:
do_or_die(organization_name_uri(System_Context, Organization, Organization_Uri),
          error(unknown_organization(Organization), _)),
assert_auth_action_scope(System_Context, Auth, '@schema':'Action/create_database', Organization_Uri),
That tells the client:

if the organization exists and/or
if the client has the appropriate access to it.

But, if either of those is false, I think we should probably be telling the client only that the request is forbidden without revealing exactly why.
For POST, PUT, and DELETE, it might make sense to return 403 Forbidden for any operation with correct inputs but referring to entities which either do not exist or are not accessible. For GET, any operation with correct inputs but referring to entities which either do not exist are or not accessible should probably be 404.
The unfortunate aspect of the current fine-grained authorization checking is that the responses reveal the existence of scopes, and that information could potentially be used wrongly in some way.",-PRON- be push this down the line i think -PRON- require a discussion about authorization i do not know what be intend by -PRON- in the big picture first in this case should anonymous even be allow to create / delete a database perhaps -PRON- should do an authorization check for anonymous before even do anything else of course that still leave non - anonymous user the ability to learn information like the existence of an organization second in the general case after at assert_auth_action_scope and src / core / account / capabilitiespl -PRON- seem like authorization work on the existence of specific scope so here -PRON- be do do_or_die(organization_name_uri(system_context organization organization_uri ) error(unknown_organization(organization ) _ ) ) assert_auth_action_scope(system_context auth ' @schema''action / create_database ' organization_uri ) that tell the client if the organization exist and/or if the client have the appropriate access to -PRON- but if either of those be false i think -PRON- should probably be tell the client only that the request be forbid without reveal exactly why for post put and delete -PRON- may make sense to return 403 forbid for any operation with correct input but refer to entity which either do not exist or be not accessible for get any operation with correct input but refer to entity which either do not exist be or not accessible should probably be 404 the unfortunate aspect of the current fine - grain authorization checking be that the response reveal the existence of scope and that information could potentially be use wrongly in some way,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,577,2021-09-14T07:57:35Z,spl,true and false error even though field has type xsd:boolean in schema,https://github.com/terminusdb/terminusdb/issues/577,"Script:
#!/bin/bash
set -ex

# Create a database
xh 'http://admin:root@localhost:6363/api/db/admin/dbt' label=l comment=c

# Insert a schema
xh 'http://admin:root@localhost:6363/api/document/admin/dbt?author=a&message=m&graph_type=schema' <<EOF
{""@type"":""Class"",""@id"":""Bool"",""b"":""xsd:boolean""}
EOF

# Insert an instance
xh 'http://admin:root@localhost:6363/api/document/admin/dbt?author=a&message=m' <<EOF
{""@type"":""Bool"",""b"":false}
EOF

# Insert an instance
xh 'http://admin:root@localhost:6363/api/document/admin/dbt?author=a&message=m' <<EOF
{""@type"":""Bool"",""b"":true}
EOF
Output of last two commands:
HTTP/1.1 400 Bad Request
connection: keep-alive
content-length: 418
content-type: application/json
date: Tue, 14 Sep 2021 07:55:54 GMT

{
    ""api:message"": ""Type error for {\""@type\"":\""xsd:boolean\"",\""@value\"":false} which should be dict"",
    ""api:status"": ""api:failure"",
    ""system:witnesses"": [
        {
            ""@type"": ""vio:ViolationWithDatatypeObject"",
            ""vio:literal"": ""{\""@type\"":\""xsd:boolean\"",\""@value\"":false}"",
            ""vio:message"": ""Type error for {\""@type\"":\""xsd:boolean\"",\""@value\"":false} which should be dict"",
            ""vio:type"": ""dict""
        }
    ]
}
HTTP/1.1 400 Bad Request
connection: keep-alive
content-length: 415
content-type: application/json
date: Tue, 14 Sep 2021 07:55:54 GMT

{
    ""api:message"": ""Type error for {\""@type\"":\""xsd:boolean\"",\""@value\"":true} which should be dict"",
    ""api:status"": ""api:failure"",
    ""system:witnesses"": [
        {
            ""@type"": ""vio:ViolationWithDatatypeObject"",
            ""vio:literal"": ""{\""@type\"":\""xsd:boolean\"",\""@value\"":true}"",
            ""vio:message"": ""Type error for {\""@type\"":\""xsd:boolean\"",\""@value\"":true} which should be dict"",
            ""vio:type"": ""dict""
        }
    ]
}
This seems to be the same error as #515 (comment).",,,,,,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,578,2021-09-14T08:55:55Z,spl,Audit the API to make sure input names are URI-encoded,https://github.com/terminusdb/terminusdb/issues/578,"We should probably accept only URI-encoded names for things that appear in our URIs (organization name, database name, branch name, others?). Therefore, we should reject any names that are not URI-encoded before they are stored in the database.",2021-09-14T09:01:19Z,spl,https://github.com/terminusdb/terminusdb/issues/578#issuecomment-918956616,Is this completely handled by the SWI-Prolog route handler?,be this completely handle by the swi - prolog route handler,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,578,2021-09-14T08:55:55Z,spl,Audit the API to make sure input names are URI-encoded,https://github.com/terminusdb/terminusdb/issues/578,"We should probably accept only URI-encoded names for things that appear in our URIs (organization name, database name, branch name, others?). Therefore, we should reject any names that are not URI-encoded before they are stored in the database.",2021-09-15T14:02:34Z,GavinMendelGleason,https://github.com/terminusdb/terminusdb/issues/578#issuecomment-920047488,I believe so.,i believe so,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,578,2021-09-14T08:55:55Z,spl,Audit the API to make sure input names are URI-encoded,https://github.com/terminusdb/terminusdb/issues/578,"We should probably accept only URI-encoded names for things that appear in our URIs (organization name, database name, branch name, others?). Therefore, we should reject any names that are not URI-encoded before they are stored in the database.",2021-09-20T13:58:18Z,spl,https://github.com/terminusdb/terminusdb/issues/578#issuecomment-922952785,"We should look in (at least) these operations, since these use an input document with a location field.

 pull
 rebase
 reset

Look at uses of resolve_absolute_string_descriptor.",-PRON- should look in ( at least ) these operation since these use an input document with a location field pull rebase reset look at use of resolve_absolute_string_descriptor,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,579,2021-09-14T08:59:56Z,Cheukting,Document error responses,https://github.com/terminusdb/terminusdb/issues/579,Right now the error message (response) that is received from the backend is not documented and I cannot format and present it nicely in the Python client. It would be great if all the expected errors are documented so I can work on them in the client?,2021-09-15T15:19:48Z,GavinMendelGleason,https://github.com/terminusdb/terminusdb/issues/579#issuecomment-920116574,"We should presumably have a full schema for this, as with WOQL etc. and then documentation can be generated automatically. It's a fair bit of work but not terribly challenging.",-PRON- should presumably have a full schema for this as with woql etc and then documentation can be generate automatically -PRON- be a fair bit of work but not terribly challenging,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,580,2021-09-14T12:38:51Z,robertross,Passwordless Login,https://github.com/terminusdb/terminusdb/issues/580,"I'm always frustrated when I try to loginor signup to the dashboards and it says...
"" Wrong email or password. ""
Describe the solution you'd like
a passwordless identity confirmation process
Describe alternatives you've considered
A password-reset link feature or link...
Additional context
I was under the impression that I already had an account when I tried to register via the login screen but actually needed to signup but still was getting "" Wrong email or password. "" or other password related validation errors",2021-09-14T13:00:03Z,luke-feeney,https://github.com/terminusdb/terminusdb/issues/580#issuecomment-919126065,"Hello! Yes - this is a serious problem. We are about to deliver a new sign in and sign up flow that should make everything clearer, but waiting for designers to give us some screens. Annoying, but it is temporary!",hello yes - this be a serious problem -PRON- be about to deliver a new sign in and sign up flow that should make everything clear but wait for designer to give -PRON- some screen annoying but -PRON- be temporary,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,586,2021-09-15T09:38:07Z,GavinMendelGleason,Empty sets are not treated as empty,https://github.com/terminusdb/terminusdb/issues/586,Getting a violation of cardinality restriction on sets when I use an empty list.,2021-09-15T10:19:51Z,GavinMendelGleason,https://github.com/terminusdb/terminusdb/issues/586#issuecomment-919892044,This was reported in error!,this be report in error,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,587,2021-09-15T11:51:10Z,spl,Type error doesn't tell you where the problem is,https://github.com/terminusdb/terminusdb/issues/587,"The error explains the problem (excusing the use of dict, which is Prolog-specific) but doesn't tell you where the problem is.
#!/bin/bash
set -ex

xh 'http://admin:root@localhost:6363/api/db/admin/tdb' label=l comment=c

xh 'http://admin:root@localhost:6363/api/document/admin/tdb' graph_type==schema author==a message==m <<EOF
{""@id"":""Child"",""@type"":""Class"",""@key"":false}
EOF
HTTP/1.1 400 Bad Request
Connection: keep-alive
Content-Length: 299
Content-Type: application/json
Date: Wed, 15 Sep 2021 11:47:44 GMT

{
    ""api:message"": ""Type error for false which should be dict"",
    ""api:status"": ""api:failure"",
    ""system:witnesses"": [
        {
            ""@type"": ""vio:ViolationWithDatatypeObject"",
            ""vio:literal"": false,
            ""vio:message"": ""Type error for false which should be dict"",
            ""vio:type"": ""dict""
        }
    ]
}",2021-09-15T11:54:10Z,GavinMendelGleason,https://github.com/terminusdb/terminusdb/issues/587#issuecomment-919951611,That is indeed a very irritating message. We should be defensive about the type going into the key and report early if its not valid.,that be indeed a very irritating message -PRON- should be defensive about the type go into the key and report early if -PRON- not valid,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,587,2021-09-15T11:51:10Z,spl,Type error doesn't tell you where the problem is,https://github.com/terminusdb/terminusdb/issues/587,"The error explains the problem (excusing the use of dict, which is Prolog-specific) but doesn't tell you where the problem is.
#!/bin/bash
set -ex

xh 'http://admin:root@localhost:6363/api/db/admin/tdb' label=l comment=c

xh 'http://admin:root@localhost:6363/api/document/admin/tdb' graph_type==schema author==a message==m <<EOF
{""@id"":""Child"",""@type"":""Class"",""@key"":false}
EOF
HTTP/1.1 400 Bad Request
Connection: keep-alive
Content-Length: 299
Content-Type: application/json
Date: Wed, 15 Sep 2021 11:47:44 GMT

{
    ""api:message"": ""Type error for false which should be dict"",
    ""api:status"": ""api:failure"",
    ""system:witnesses"": [
        {
            ""@type"": ""vio:ViolationWithDatatypeObject"",
            ""vio:literal"": false,
            ""vio:message"": ""Type error for false which should be dict"",
            ""vio:type"": ""dict""
        }
    ]
}",2021-09-15T11:55:25Z,matko,https://github.com/terminusdb/terminusdb/issues/587#issuecomment-919952503,"Also, looks like this error is not following the proper format for api errors, where it specifies what response it is part of.",also look like this error be not follow the proper format for api error where -PRON- specify what response -PRON- be part of,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,587,2021-09-15T11:51:10Z,spl,Type error doesn't tell you where the problem is,https://github.com/terminusdb/terminusdb/issues/587,"The error explains the problem (excusing the use of dict, which is Prolog-specific) but doesn't tell you where the problem is.
#!/bin/bash
set -ex

xh 'http://admin:root@localhost:6363/api/db/admin/tdb' label=l comment=c

xh 'http://admin:root@localhost:6363/api/document/admin/tdb' graph_type==schema author==a message==m <<EOF
{""@id"":""Child"",""@type"":""Class"",""@key"":false}
EOF
HTTP/1.1 400 Bad Request
Connection: keep-alive
Content-Length: 299
Content-Type: application/json
Date: Wed, 15 Sep 2021 11:47:44 GMT

{
    ""api:message"": ""Type error for false which should be dict"",
    ""api:status"": ""api:failure"",
    ""system:witnesses"": [
        {
            ""@type"": ""vio:ViolationWithDatatypeObject"",
            ""vio:literal"": false,
            ""vio:message"": ""Type error for false which should be dict"",
            ""vio:type"": ""dict""
        }
    ]
}",2021-09-15T11:59:14Z,GavinMendelGleason,https://github.com/terminusdb/terminusdb/issues/587#issuecomment-919954848,It's because it's actually being translated from a generic prolog error at the very top level...,-PRON- be because -PRON- be actually be translate from a generic prolog error at the very top level,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,587,2021-09-15T11:51:10Z,spl,Type error doesn't tell you where the problem is,https://github.com/terminusdb/terminusdb/issues/587,"The error explains the problem (excusing the use of dict, which is Prolog-specific) but doesn't tell you where the problem is.
#!/bin/bash
set -ex

xh 'http://admin:root@localhost:6363/api/db/admin/tdb' label=l comment=c

xh 'http://admin:root@localhost:6363/api/document/admin/tdb' graph_type==schema author==a message==m <<EOF
{""@id"":""Child"",""@type"":""Class"",""@key"":false}
EOF
HTTP/1.1 400 Bad Request
Connection: keep-alive
Content-Length: 299
Content-Type: application/json
Date: Wed, 15 Sep 2021 11:47:44 GMT

{
    ""api:message"": ""Type error for false which should be dict"",
    ""api:status"": ""api:failure"",
    ""system:witnesses"": [
        {
            ""@type"": ""vio:ViolationWithDatatypeObject"",
            ""vio:literal"": false,
            ""vio:message"": ""Type error for false which should be dict"",
            ""vio:type"": ""dict""
        }
    ]
}",2021-09-20T13:50:13Z,spl,https://github.com/terminusdb/terminusdb/issues/587#issuecomment-922946198,We should drop the vio error handler and use the new and improved api_error handler.,-PRON- should drop the vio error handler and use the new and improved api_error handler,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,593,2021-09-15T20:35:31Z,GavinMendelGleason,Uncaught query error,https://github.com/terminusdb/terminusdb/issues/593,"Try searching using a type that doesn't exist:
plants = client.query_document({ '@type' : 'NuclearPowerPlant' })
Then you'll get a stack trace
{
    ""api:message"": ""Error: query_error(type_not_found(\""NuclearPowerPlant\""))\n  [40] throw(error(query_error(...),_6878))\n  [38] 'document/query':expand_query_document_(transaction_object{commit_info:commit_info{},descriptor:branch_descriptor{branch_name:\""capacity_factors\"",repository_descriptor: ...},inference_objects:[],instance_objects:[...],parent:transaction_object{descriptor: ...,inference_objects:[],instance_objects: ...,parent: ...,schema_objects: ...},schema_objects:[...]},'http://lib.terminusdb.com/nuclear#NuclearPowerPlant',_7008{'@type':\""NuclearPowerPlant\""},_6924) at /app/terminusdb/src/core/document/query.pl:147\n  [36] 'document/query':match_query_document_uri(transaction_object{commit_info:commit_info{},descriptor:branch_descriptor{branch_name:\""capacity_factors\"",repository_descriptor: ...},inference_objects:[],instance_objects:[...],parent:transaction_object{descriptor: ...,inference_objects:[],instance_objects: ...,parent: ...,schema_objects: ...},schema_objects:[...]},_7050,_7008{'@type':\""NuclearPowerPlant\""},_7054) at /app/terminusdb/src/core/document/query.pl:215\n  [35] solution_sequences:offset(0,api_document:match_query_document_uri(...,_7192,...,_7196)) at /usr/lib/swipl/library/solution_sequences.pl:241\n  [33] api_document:api_generate_documents_by_query('<garbage_collected>','User/auth0%7C613b70276972a400703b8078','TerminatorsX/nuclear/local/branch/capacity_factors',instance,true,true,_7244,_7008{'@type':\""NuclearPowerPlant\""},0,unlimited,_7252) at /app/terminusdb/src/core/api/api_document.pl:123\n  [32] '$apply':forall('<garbage_collected>',routes:json_write_with_header(...,_7310,...,false,...)) at /usr/lib/swipl/boot/apply.pl:52\n  [31] '<meta-call>'('<garbage_collected>') <foreign>\n  [30] catch(routes:(...,...),error(query_error(...),context(_7410,_7412)),routes:do_or_die(...,...)) at /usr/lib/swipl/boot/init.pl:532\n  [29] catch_with_backtrace('<garbage_collected>','<garbage_collected>','<garbage_collected>') at /usr/lib/swipl/boot/init.pl:582\n\nNote: some frames are missing due to last-call optimization.\nRe-run your program in debug mode (:- debug.) to get more detail.\n\n"",
    ""api:status"": ""api:server_error""
}",2021-09-16T12:30:12Z,GavinMendelGleason,https://github.com/terminusdb/terminusdb/issues/593#issuecomment-920858865,"I think perhaps all query_errors are still uncaught, is this true @matko ?",i think perhaps all query_error be still uncaught be this true @matko,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,594,2021-09-15T21:03:24Z,GavinMendelGleason,xsd:decimal round-trip dangers in @id,https://github.com/terminusdb/terminusdb/issues/594,"We have been punting on the dangers that exist in our current float marshalling. It can lead to strange problems when it is used in an ID.
Notably, the GeoCoordinate seems to have a name with more precision than the value returned.
            {
                ""@id"": ""NuclearPowerPlant/ASCO%20GR/location/GeoCoordinate/41.2008+0.5679"",
                ""@type"": ""GeoCoordinate"",
                ""latitude"": 41.2008,
                ""longitude"": 0.0
            },

And the full error:
terminusdb_client.errors.DatabaseError: Document was submitted with id 'http://lib.terminusdb.com/nuclear/NuclearPowerPlant/ASCO%20GR/location/GeoCoordinate/41.2008+0.5679', but id 'http://lib.terminusdb.com/nuclear/NuclearPowerPlant/ASCO%20GR/location/GeoCoordinate/41.2008+0.0' was generated
{
    ""@type"": ""api:ReplaceDocumentErrorResponse"",
    ""api:error"": {
        ""@type"": ""api:SubmittedIdDoesNotMatchGeneratedId"",
        ""api:document"": {
            ""@id"": ""NuclearPowerPlant/ASCO%20GR"",
            ""@type"": ""NuclearPowerPlant"",
            ""capacity"": {
                ""@id"": ""NuclearPowerPlant/ASCO%20GR/capacity/Quantity/http%3A%2F%2Flib.terminusdb.com%2Fnuclear%2FUnit%2FMWe+1990.5"",
                ""@type"": ""Quantity"",
                ""quantity"": 1990.5,
                ""unit"": ""Unit/MWe""
            },
            ""capacity_factor"": [
                {
                    ""@type"": ""AnnualCapacityFactor"",
                    ""capacity_factor"": 1103441.2842479602,
                    ""year"": ""2015""
                },
                {
                    ""@type"": ""AnnualCapacityFactor"",
                    ""capacity_factor"": 1087809.2827979228,
                    ""year"": ""2016""
                },
                {
                    ""@type"": ""AnnualCapacityFactor"",
                    ""capacity_factor"": 1144010.474877608,
                    ""year"": ""2017""
                }
            ],
            ""commissioning_year"": ""1983"",
            ""country"": ""Country/Spain"",
            ""gppd_idnr"": ""WRI1006213"",
            ""location"": {
                ""@id"": ""NuclearPowerPlant/ASCO%20GR/location/GeoCoordinate/41.2008+0.5679"",
                ""@type"": ""GeoCoordinate"",
                ""latitude"": 41.2008,
                ""longitude"": 0.0
            },
            ""name"": ""ASCO GR"",
            ""output"": [
                {
                    ""@id"": ""NuclearPowerPlant/ASCO%20GR/output/AnnualOutput/2015"",
                    ""@type"": ""AnnualOutput"",
                    ""output"": {
                        ""@id"": ""NuclearPowerPlant/ASCO%20GR/output/NuclearPowerPlant/ASCO%20GR/output/AnnualOutput/2015/output/Quantity/http%3A%2F%2Flib.terminusdb.com%2Fnuclear%2FUnit%2FGWh+15802.182"",
                        ""@type"": ""Quantity"",
                        ""quantity"": 15802.182,
                        ""unit"": ""Unit/GWh""
                    },
                    ""year"": ""2015""
                },
                {
                    ""@id"": ""NuclearPowerPlant/ASCO%20GR/output/AnnualOutput/2016"",
                    ""@type"": ""AnnualOutput"",
                    ""output"": {
                        ""@id"": ""NuclearPowerPlant/ASCO%20GR/output/NuclearPowerPlant/ASCO%20GR/output/AnnualOutput/2016/output/Quantity/http%3A%2F%2Flib.terminusdb.com%2Fnuclear%2FUnit%2FGWh+16029.262"",
                        ""@type"": ""Quantity"",
                        ""quantity"": 16029.262,
                        ""unit"": ""Unit/GWh""
                    },
                    ""year"": ""2016""
                },
                {
                    ""@id"": ""NuclearPowerPlant/ASCO%20GR/output/AnnualOutput/2017"",
                    ""@type"": ""AnnualOutput"",
                    ""output"": {
                        ""@id"": ""NuclearPowerPlant/ASCO%20GR/output/NuclearPowerPlant/ASCO%20GR/output/AnnualOutput/2017/output/Quantity/http%3A%2F%2Flib.terminusdb.com%2Fnuclear%2FUnit%2FGWh+15241.801"",
                        ""@type"": ""Quantity"",
                        ""quantity"": 15241.801,
                        ""unit"": ""Unit/GWh""
                    },
                    ""year"": ""2017""
                }
            ],
            ""owner"": ""ENDESA GENERACION S.A."",
            ""url"": ""http://www.ree.es/en/statistical-data-of-spanish-electrical-system/annual-report/preliminary-report-spanish-electricity-system-2015""
        },
        ""api:generated_id"": ""http://lib.terminusdb.com/nuclear/NuclearPowerPlant/ASCO%20GR/location/GeoCoordinate/41.2008+0.0"",
        ""api:submitted_id"": ""http://lib.terminusdb.com/nuclear/NuclearPowerPlant/ASCO%20GR/location/GeoCoordinate/41.2008+0.5679""
    },
    ""api:message"": ""Document was submitted with id 'http://lib.terminusdb.com/nuclear/NuclearPowerPlant/ASCO%20GR/location/GeoCoordinate/41.2008+0.5679', but id 'http://lib.terminusdb.com/nuclear/NuclearPowerPlant/ASCO%20GR/location/GeoCoordinate/41.2008+0.0' was generated"",
    ""api:status"": ""api:failure""
}",,,,,,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,599,2021-09-17T12:08:53Z,Cheukting,Access management to various objects,https://github.com/terminusdb/terminusdb/issues/599,I think it will be great if we have access restrictions to various data in the future if we have multiple access to data products in a Team. There may be sensitive data that should have to restrict access to only certain types of roles.,,,,,,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,601,2021-09-20T19:01:56Z,spl,Tests are failing due to missing links,https://github.com/terminusdb/terminusdb/issues/601,"CI tests are failing because the links below are not found. Can we remove the test dependencies on things that may come or go?

https://terminusdb.com/t/data/bike_tutorial.csv
https://terminusdb.com/t/data/bikeshare/2011-capitalbikeshare-tripdata.csv",,,,,,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,603,2021-09-21T07:25:18Z,spl,Stack trace error for duplicate @documentation,https://github.com/terminusdb/terminusdb/issues/603,"This was found while investigating #501.
Script:
#!/bin/bash
set -ex

xh 'http://admin:root@localhost:6363/api/db/admin/tdb' label=l comment=c

xh 'http://admin:root@localhost:6363/api/document/admin/tdb' graph_type==schema author==a message==m <<EOF
{
  ""@id"": ""D"",
  ""@type"": ""Class"",
  ""@documentation"": { },
  ""@documentation"": { }
}
EOF
Output of last command:
HTTP/1.1 500 Internal Server Error
Connection: keep-alive
Content-Length: 3077
Content-Type: application/json
Date: Tue, 21 Sep 2021 07:24:29 GMT

{
    ""api:message"": ""Error: duplicate_key('@documentation')\n  [46] dict_create(_6344,json,['@id'=\""D\"",...|...])\n  [43] utils:json_read_dict_stream(<stream>(0x7fde5e8e09a0),_6404) at /Users/leather/projects/terminusdb/terminusdb/src/core/util/utils.pl:900\n  [42] api_document:api_insert_document_(schema,transaction_object{commit_info:commit_info{},descriptor:branch_descriptor{branch_name:\""main\"",repository_descriptor: ...},inference_objects:[],instance_objects:[...],parent:transaction_object{descriptor: ...,inference_objects:[],instance_objects: ...,parent: ...,schema_objects: ...},schema_objects:[...]},<stream>(0x7fde5e8e09a0),_6446) at /Users/leather/projects/terminusdb/terminusdb/src/core/api/api_document.pl:174\n  [41] '$bags':findall_loop(_6562,api_document:api_insert_document_(schema,...,<stream>(0x7fde5e8e09a0),_6584),_6566,[]) at /usr/local/Cellar/swi-prolog/8.2.4/libexec/lib/swipl/boot/bags.pl:99\n  [40] setup_call_catcher_cleanup('$bags':'$new_findall_bag','$bags':findall_loop(_6636,...,_6640,[]),_6618,'$bags':'$destroy_findall_bag') at /usr/local/Cellar/swi-prolog/8.2.4/libexec/lib/swipl/boot/init.pl:614\n  [36] '<meta-call>'(api_document:(...;...)) <foreign>\n  [35] database:with_transaction_(query_context{all_witnesses:false,authorization:'terminusdb://system/data/User/admin',bindings:[],commit_info:commit_info{author:a,message:m},default_collection:branch_descriptor{branch_name:\""main\"",repository_descriptor: ...},files:[],filter:type_filter{types: ...},prefixes:_6830{'@base':\""terminusdb:///data/\"",'@schema':\""terminusdb:///schema#\"",'@type':'Context',api:'http://terminusdb.com/schema/api#',owl:'http://www.w3.org/2002/07/owl#',rdf:'http://www.w3.org/1999/02/22-rdf-syntax-ns#',rdfs:'http://www.w3.org/2000/01/rdf-schema#',sys:'http://terminusdb.com/schema/sys#',vio:'http://terminusdb.com/schema/vio#',woql:'http://terminusdb.com/schema/woql#',xdd:'http://terminusdb.com/schema/xdd#',xsd:'http://www.w3.org/2001/XMLSchema#'},selected:[],system:system_descriptor{},transaction_objects:[...],update_guard:_6776,write_graph:branch_graph{branch_name:\""main\"",database_name:\""tdb\"",organization_name:\""admin\"",repository_name:\""local\"",type:instance}},api_document:(...;...),_6734) at /Users/leather/projects/terminusdb/terminusdb/src/core/transaction/database.pl:220\n  [34] setup_call_catcher_cleanup(database:true,database:with_transaction_(...,...,_6976),_6954,database:true) at /usr/local/Cellar/swi-prolog/8.2.4/libexec/lib/swipl/boot/init.pl:614\n  [30] '<meta-call>'('<garbage_collected>') <foreign>\n  [29] catch(routes:(...,...),error(duplicate_key('@documentation'),context(...,_7078)),routes:do_or_die(...,...)) at /usr/local/Cellar/swi-prolog/8.2.4/libexec/lib/swipl/boot/init.pl:532\n  [28] catch_with_backtrace('<garbage_collected>','<garbage_collected>','<garbage_collected>') at /usr/local/Cellar/swi-prolog/8.2.4/libexec/lib/swipl/boot/init.pl:582\n\nNote: some frames are missing due to last-call optimization.\nRe-run your program in debug mode (:- debug.) to get more detail.\n\n"",
    ""api:status"": ""api:server_error""
}",2021-10-04T14:34:33Z,spl,https://github.com/terminusdb/terminusdb/issues/603#issuecomment-933548267,This is probably coming from the SWI-Prolog JSON conversion.,this be probably come from the swi - prolog json conversion,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,604,2021-09-21T14:44:12Z,matko,log messages in json,https://github.com/terminusdb/terminusdb/issues/604,"The default log format for the swipl http server is to log as prolog terms. This is a format literally no log manager out there is able to deal with.
I propose we write a new logging backend which instead logs in json format. json is standard enough that many log managers are able to deal with it by default, providing advanced search over fields in the logged json documents.
This would entail writing a replacement for http_log.pl. This is pretty doable since its interface to the rest of the http server is pretty detached. By not loading http_log.pl, its logging functionality simply won't happen. There are roughly two ways in which things get logged:

through a broadcast message. This is how the http server normally logs things. For each incoming request and each outgoing response, broadcast(http(...)) is called with a term specifying what happened. A listen(...) call can be used to set up a listener for this, which is then responsible for turning it into a proper log message.
through an explicit call to http_log. This currently takes a format string and parameters to insert into that format string. We need an alternative that instead takes a dictionary and prints a json message, then replace all existing calls with this new logging utility.

It may not be necessary that every log line is a json document. For example, the google cloud log aggregator is able to deal with log output that intersperses lines of json data with text lines. However, for the sake of searchability we should try to ensure that all our log output is json.",2021-09-27T13:17:30Z,matko,https://github.com/terminusdb/terminusdb/issues/604#issuecomment-927866155,Fixed in #607.,fix in # 607,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,609,2021-09-23T19:17:36Z,GavinMendelGleason,Odd Version report,https://github.com/terminusdb/terminusdb/issues/609,"CURL.md reports that I can get the version numbers from TerminusDB as follows:
$ curl -X GET ""http://127.0.0.1:6363/api/info"" -u ""admin:root""  -H ""Content-Type: application/json""
which on v10.0.3 results in:
{
  ""@type"":""api:InfoResponse"",
  ""api:info"": {
    ""authority"":""terminusdb://system/data/User/admin"",
    ""storage"": {""version"":""1""},
    ""terminusdb"": {""version"":""4.2.2""},
    ""terminusdb_store"": {""version"":""0.19.1""}
  },
  ""api:status"":""api:success""
}",2021-09-28T13:43:50Z,matko,https://github.com/terminusdb/terminusdb/issues/609#issuecomment-929223188,We'll have to remember to update this number after every release from now on.,-PRON- will have to remember to update this number after every release from now on,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,610,2021-09-23T19:35:27Z,GavinMendelGleason,Still experiencing issues with float canonicalisation,https://github.com/terminusdb/terminusdb/issues/610,"The following error on the Brokdorf power plant demonstrates the problem of assigning IDs which make use of floats.
Essentially we change precision in the round trip, leading to a correct URI, and incorrect retrieved value.
{
    ""@type"": ""api:ReplaceDocumentErrorResponse"",
    ""api:error"": {
        ""@type"": ""api:SubmittedIdDoesNotMatchGeneratedId"",
        ""api:document"": {
            ""@id"": ""NuclearPowerPlant/BROKDORF"",
            ""@type"": ""NuclearPowerPlant"",
            ""capacity"": {
                ""@id"": ""NuclearPowerPlant/BROKDORF/capacity/Quantity/http%3A%2F%2Flib.terminusdb.com%2Fnuclear%2FUnit%2FMWe+1480.0"",
                ""@type"": ""Quantity"",
                ""quantity"": 1480.0,
                ""unit"": ""Unit/MWe""
            },
            ""capacity_factor"": [
                {
                    ""@type"": ""AnnualCapacityFactor"",
                    ""capacity_factor"": 0.8111564389732198,
                    ""year"": ""2015""
                },
                {
                    ""@type"": ""AnnualCapacityFactor"",
                    ""capacity_factor"": 0.8437756849315069,
                    ""year"": ""2016""
                },
                {
                    ""@type"": ""AnnualCapacityFactor"",
                    ""capacity_factor"": 0.4227079476737011,
                    ""year"": ""2017""
                }
            ],
            ""commissioning_year"": ""1986"",
            ""country"": ""Country/Germany"",
            ""gppd_idnr"": ""WRI1005621"",
            ""location"": {
                ""@id"": ""NuclearPowerPlant/BROKDORF/location/GeoCoordinate/53.8506+9.3450"",
                ""@type"": ""GeoCoordinate"",
                ""latitude"": 53.8506,
                ""longitude"": 9.345
            },
            ""name"": ""BROKDORF"",
            ""output"": [
                {
                    ""@id"": ""NuclearPowerPlant/BROKDORF/output/AnnualOutput/2015"",
                    ""@type"": ""AnnualOutput"",
                    ""output"": {
                        ""@id"": ""NuclearPowerPlant/BROKDORF/output/NuclearPowerPlant/BROKDORF/output/AnnualOutput/2015/output/Quantity/http%3A%2F%2Flib.terminusdb.com%2Fnuclear%2FUnit%2FGWh+10516.481"",
                        ""@type"": ""Quantity"",
                        ""quantity"": 10516.481,
                        ""unit"": ""Unit/GWh""
                    },
                    ""year"": ""2015""
                },
                {
                    ""@id"": ""NuclearPowerPlant/BROKDORF/output/AnnualOutput/2016"",
                    ""@type"": ""AnnualOutput"",
                    ""output"": {
                        ""@id"": ""NuclearPowerPlant/BROKDORF/output/NuclearPowerPlant/BROKDORF/output/AnnualOutput/2016/output/Quantity/http%3A%2F%2Flib.terminusdb.com%2Fnuclear%2FUnit%2FGWh+10939.383"",
                        ""@type"": ""Quantity"",
                        ""quantity"": 10939.383,
                        ""unit"": ""Unit/GWh""
                    },
                    ""year"": ""2016""
                },
                {
                    ""@id"": ""NuclearPowerPlant/BROKDORF/output/AnnualOutput/2017"",
                    ""@type"": ""AnnualOutput"",
                    ""output"": {
                        ""@id"": ""NuclearPowerPlant/BROKDORF/output/NuclearPowerPlant/BROKDORF/output/AnnualOutput/2017/output/Quantity/http%3A%2F%2Flib.terminusdb.com%2Fnuclear%2FUnit%2FGWh+5480.324"",
                        ""@type"": ""Quantity"",
                        ""quantity"": 5480.324,
                        ""unit"": ""Unit/GWh""
                    },
                    ""year"": ""2017""
                }
            ],
            ""owner"": """",
            ""url"": ""https://www.iaea.org/PRIS/CountryStatistics/CountryStatisticsLandingPage.aspx""
        },
        ""api:generated_id"": ""http://lib.terminusdb.com/nuclear/NuclearPowerPlant/BROKDORF/location/GeoCoordinate/53.8506+9.345"",
        ""api:submitted_id"": ""http://lib.terminusdb.com/nuclear/NuclearPowerPlant/BROKDORF/location/GeoCoordinate/53.8506+9.3450""
    },
    ""api:message"": ""Document was submitted with id 'http://lib.terminusdb.com/nuclear/NuclearPowerPlant/BROKDORF/location/GeoCoordinate/53.8506+9.3450', but id 'http://lib.terminusdb.com/nuclear/NuclearPowerPlant/BROKDORF/location/GeoCoordinate/53.8506+9.345' was generated"",
    ""api:status"": ""api:failure""
}",2021-09-24T06:14:20Z,GavinMendelGleason,https://github.com/terminusdb/terminusdb/issues/610#issuecomment-926374125,Actually - I discovered this bug again on the production TerminusX server. I thought I fixed it in commit 8e83da1  Is this commit in TerminusX production @rrooij,actually - i discover this bug again on the production terminusx server i think i fix -PRON- in commit 8e83da1 be this commit in terminusx production @rrooij,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,611,2021-09-24T09:36:09Z,Cheukting,Almost like 'git blame' function,https://github.com/terminusdb/terminusdb/issues/611,If there is a simple API to retrieve the commit history of a document (e.g. storing the commit ids as metadata of a document) then there will be an increased sense of data ownership. I think it will be very convenient for client developers as well as update time can be retrieved easier this way.,,,,,,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,612,2021-09-26T18:32:08Z,GavinMendelGleason,Comment field should not be required in documentation.,https://github.com/terminusdb/terminusdb/issues/612,"The following schema is rejected as the ""@comment"" field does not exist.  This is overly picky - we shouldn't require a comment.  If the user does not supply a comment but does supply documentation on the properties, it would be good to allow this scenario (and it appears to be doing this).
  ""@type"": ""Class"",
  ""@id"": ""article"",
  ""@key"": {
    ""@type"": ""Random""
  },
  ""title"": {
    ""@class"": ""xsd:string"",
    ""@type"": ""Optional""
  },
  ""@documentation"": {
    ""@properties"": {
      ""abstract"": ""article abstract"",
      ""link"": ""link to article"",
      ""id"": ""storage id"",
      ""title"": ""article title""
    }
  },
  ""link"": {
    ""@class"": ""xdd:url"",
    ""@type"": ""Optional""
  },
  ""id"": {
    ""@class"": ""xsd:string"",
    ""@type"": ""Optional""
  },
  ""abstract"": {
    ""@class"": ""xdd:html"",
    ""@type"": ""Optional""
  }
}",,,,,,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,613,2021-09-27T21:45:58Z,Cheukting,"Strange error message when ""xsd:"" is missing",https://github.com/terminusdb/terminusdb/issues/613,"Document insert error message:
Type error for 1 which should be text
{
    ""api:message"": ""Type error for 1 which should be text"",
    ""api:status"": ""api:failure"",
    ""system:witnesses"": [
        {
            ""@type"": ""vio:ViolationWithDatatypeObject"",
            ""vio:literal"": ""1"",
            ""vio:message"": ""Type error for 1 which should be text"",
            ""vio:type"": ""text""
        }
    ]
}

while insert
{'@type': 'eu_daily', 'git_owner': 'covid19-eu-zh', 'git_repository': 'covid19-eu-data', 'git_url': 'https://api.github.com/repositories/244987370/git/blobs/b1585894ed9529ec6cee8e7fdf00d3f6603d810c', 'git_html_url': 'https://github.com/covid19-eu-zh/covid19-eu-data/blob/4c4038e4ec930e44edf4088532a9c86c6ad2a6c6/dataset/daily/ie/ie_covid19_2021-09-27_0_00.csv', 'git_path': 'dataset/daily/ie/ie_covid19_2021-09-27_0_00.csv', 'git_sha': 'b1585894ed9529ec6cee8e7fdf00d3f6603d810c', 'git_file_name': 'ie_covid19_2021-09-27_0_00.csv', 'git_last_modified': '2021-09-27T20:25:40.000000Z', '__sdc_row_number': 1, 'datetime': '2021-09-27T00:00:00.000000Z', 'date': '2021-09-27', 'cases_100k_pop': None, 'cases_lower': None, 'cases_upper': None, 'cases': 1694.0, 'country': 'IE', 'nuts_1': None, 'nuts_2': None, 'nuts_3': None, 'lau': 'Leitrim', 'percent': None, 'hospitalized': None, '@id': 'eu_daily/dataset/daily/ie/ie_covid19_2021-09-27_0_00.csv_1'}

FYI schema is:
{'@base': 'terminusdb:///data/', '@schema': 'terminusdb:///schema#', '@type': '@context'}, {'@id': 'eu_daily', '@type': 'Class', '__sdc_row_number': {'@class': 'integer', '@type': 'Optional'}, 'cases': {'@class': 'number', '@type': 'Optional'}, 'cases_100k_pop': {'@class': 'number', '@type': 'Optional'}, 'cases_lower': {'@class': 'integer', '@type': 'Optional'}, 'cases_upper': {'@class': 'integer', '@type': 'Optional'}, 'country': {'@class': 'string', '@type': 'Optional'}, 'date': {'@class': 'string', '@type': 'Optional'}, 'datetime': 'xsd:dateTime', 'deaths': {'@class': 'number', '@type': 'Optional'}, 'git_file_name': {'@class': 'string', '@type': 'Optional'}, 'git_html_url': {'@class': 'string', '@type': 'Optional'}, 'git_last_modified': 'xsd:dateTime', 'git_owner': {'@class': 'string', '@type': 'Optional'}, 'git_path': {'@class': 'string', '@type': 'Optional'}, 'git_repository': {'@class': 'string', '@type': 'Optional'}, 'git_sha': {'@class': 'string', '@type': 'Optional'}, 'git_url': {'@class': 'string', '@type': 'Optional'}, 'hospitalized': {'@class': 'number', '@type': 'Optional'}, 'intensive_care': {'@class': 'integer', '@type': 'Optional'}, 'lau': {'@class': 'string', '@type': 'Optional'}, 'nuts_1': {'@class': 'string', '@type': 'Optional'}, 'nuts_2': {'@class': 'string', '@type': 'Optional'}, 'nuts_3': {'@class': 'string', '@type': 'Optional'}, 'percent': {'@class': 'number', '@type': 'Optional'}, 'population': {'@class': 'number', '@type': 'Optional'}, 'quarantine': {'@class': 'integer', '@type': 'Optional'}, 'recovered': {'@class': 'integer', '@type': 'Optional'}, 'tests': {'@class': 'integer', '@type': 'Optional'}}

I have get same error while inserting other object (it's a loop in the program) except 1 will be 2, 3 and so on. I don't understand what that error message means.",2021-09-28T08:48:54Z,matko,https://github.com/terminusdb/terminusdb/issues/613#issuecomment-928987564,"It looks like a bug in the server code. Something is expecting a string for (I assume) the __sdc_row_number field, but a number is actually the right thing here.",-PRON- look like a bug in the server code something be expect a string for ( i assume ) the _ _ sdc_row_number field but a number be actually the right thing here,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,613,2021-09-27T21:45:58Z,Cheukting,"Strange error message when ""xsd:"" is missing",https://github.com/terminusdb/terminusdb/issues/613,"Document insert error message:
Type error for 1 which should be text
{
    ""api:message"": ""Type error for 1 which should be text"",
    ""api:status"": ""api:failure"",
    ""system:witnesses"": [
        {
            ""@type"": ""vio:ViolationWithDatatypeObject"",
            ""vio:literal"": ""1"",
            ""vio:message"": ""Type error for 1 which should be text"",
            ""vio:type"": ""text""
        }
    ]
}

while insert
{'@type': 'eu_daily', 'git_owner': 'covid19-eu-zh', 'git_repository': 'covid19-eu-data', 'git_url': 'https://api.github.com/repositories/244987370/git/blobs/b1585894ed9529ec6cee8e7fdf00d3f6603d810c', 'git_html_url': 'https://github.com/covid19-eu-zh/covid19-eu-data/blob/4c4038e4ec930e44edf4088532a9c86c6ad2a6c6/dataset/daily/ie/ie_covid19_2021-09-27_0_00.csv', 'git_path': 'dataset/daily/ie/ie_covid19_2021-09-27_0_00.csv', 'git_sha': 'b1585894ed9529ec6cee8e7fdf00d3f6603d810c', 'git_file_name': 'ie_covid19_2021-09-27_0_00.csv', 'git_last_modified': '2021-09-27T20:25:40.000000Z', '__sdc_row_number': 1, 'datetime': '2021-09-27T00:00:00.000000Z', 'date': '2021-09-27', 'cases_100k_pop': None, 'cases_lower': None, 'cases_upper': None, 'cases': 1694.0, 'country': 'IE', 'nuts_1': None, 'nuts_2': None, 'nuts_3': None, 'lau': 'Leitrim', 'percent': None, 'hospitalized': None, '@id': 'eu_daily/dataset/daily/ie/ie_covid19_2021-09-27_0_00.csv_1'}

FYI schema is:
{'@base': 'terminusdb:///data/', '@schema': 'terminusdb:///schema#', '@type': '@context'}, {'@id': 'eu_daily', '@type': 'Class', '__sdc_row_number': {'@class': 'integer', '@type': 'Optional'}, 'cases': {'@class': 'number', '@type': 'Optional'}, 'cases_100k_pop': {'@class': 'number', '@type': 'Optional'}, 'cases_lower': {'@class': 'integer', '@type': 'Optional'}, 'cases_upper': {'@class': 'integer', '@type': 'Optional'}, 'country': {'@class': 'string', '@type': 'Optional'}, 'date': {'@class': 'string', '@type': 'Optional'}, 'datetime': 'xsd:dateTime', 'deaths': {'@class': 'number', '@type': 'Optional'}, 'git_file_name': {'@class': 'string', '@type': 'Optional'}, 'git_html_url': {'@class': 'string', '@type': 'Optional'}, 'git_last_modified': 'xsd:dateTime', 'git_owner': {'@class': 'string', '@type': 'Optional'}, 'git_path': {'@class': 'string', '@type': 'Optional'}, 'git_repository': {'@class': 'string', '@type': 'Optional'}, 'git_sha': {'@class': 'string', '@type': 'Optional'}, 'git_url': {'@class': 'string', '@type': 'Optional'}, 'hospitalized': {'@class': 'number', '@type': 'Optional'}, 'intensive_care': {'@class': 'integer', '@type': 'Optional'}, 'lau': {'@class': 'string', '@type': 'Optional'}, 'nuts_1': {'@class': 'string', '@type': 'Optional'}, 'nuts_2': {'@class': 'string', '@type': 'Optional'}, 'nuts_3': {'@class': 'string', '@type': 'Optional'}, 'percent': {'@class': 'number', '@type': 'Optional'}, 'population': {'@class': 'number', '@type': 'Optional'}, 'quarantine': {'@class': 'integer', '@type': 'Optional'}, 'recovered': {'@class': 'integer', '@type': 'Optional'}, 'tests': {'@class': 'integer', '@type': 'Optional'}}

I have get same error while inserting other object (it's a loop in the program) except 1 will be 2, 3 and so on. I don't understand what that error message means.",2021-09-28T10:10:46Z,Cheukting,https://github.com/terminusdb/terminusdb/issues/613#issuecomment-929049054,"I found the error, it is because I missed the ""xsd:"" prefix","i find the error -PRON- be because i miss the "" xsd "" prefix",0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,614,2021-09-28T10:50:47Z,Francesca-Bit,interact with the _system database with a different user return an bad error,https://github.com/terminusdb/terminusdb/issues/614,"To Reproduce
Steps to reproduce the behavior:

Login with a user by the dashboard
go in the query build interface
run the follow query

using(""_system"").triple(""v:a"",""v:b"",""v:c"")
we got this error



<title>500 Internal server error</title>



Internal server error

Unknown message: compilation_error('Failure to compile term limit(10^^xsd:decimal,using(\'_system\',t(v(a),v(b),v(c))))')

SWI-Prolog httpd at terminusdb-server-deployment-6f44b8f457-9b8h4",2021-09-29T08:25:11Z,matko,https://github.com/terminusdb/terminusdb/issues/614#issuecomment-929954097,@GavinMendelGleason this is pretty weird. You have any leads?,@gavinmendelgleason this be pretty weird -PRON- have any lead,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,614,2021-09-28T10:50:47Z,Francesca-Bit,interact with the _system database with a different user return an bad error,https://github.com/terminusdb/terminusdb/issues/614,"To Reproduce
Steps to reproduce the behavior:

Login with a user by the dashboard
go in the query build interface
run the follow query

using(""_system"").triple(""v:a"",""v:b"",""v:c"")
we got this error



<title>500 Internal server error</title>



Internal server error

Unknown message: compilation_error('Failure to compile term limit(10^^xsd:decimal,using(\'_system\',t(v(a),v(b),v(c))))')

SWI-Prolog httpd at terminusdb-server-deployment-6f44b8f457-9b8h4",2021-09-29T08:29:07Z,GavinMendelGleason,https://github.com/terminusdb/terminusdb/issues/614#issuecomment-929957069,Should be easy to reproduce... let me have a look.,should be easy to reproduce let -PRON- have a look,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,615,2021-09-28T12:15:22Z,matko,Log authentication information,https://github.com/terminusdb/terminusdb/issues/615,"When a user is authenticated, it'd be good if we logged who they are and how they authenticated, so we can associate operations in the log with actual people.",2021-09-28T12:24:35Z,Francesca-Bit,https://github.com/terminusdb/terminusdb/issues/615#issuecomment-929152296,we can do this with the cloud-api,-PRON- can do this with the cloud - api,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,615,2021-09-28T12:15:22Z,matko,Log authentication information,https://github.com/terminusdb/terminusdb/issues/615,"When a user is authenticated, it'd be good if we logged who they are and how they authenticated, so we can associate operations in the log with actual people.",2021-09-28T13:07:26Z,matko,https://github.com/terminusdb/terminusdb/issues/615#issuecomment-929192341,"IMO we should do both.
For terminusdb we need to consider the standalone scenario as well as the scenario where terminusdb is deployed as part of a larger cloud solution. So terminusdb should log what it knows, cause there may not be a frontend like with terminusx.
Furthermore, due to bugs terminusdb and cloud-api may have different opinions on who is actually being authenticated, and that's exactly the sort of thing we need to be able to spot when digging in the logs.",imo -PRON- should do both for terminusdb -PRON- need to consider the standalone scenario as well as the scenario where terminusdb be deploy as part of a large cloud solution so terminusdb should log what -PRON- know cause there may not be a frontend like with terminusx furthermore due to bug terminusdb and cloud - api may have different opinion on who be actually be authenticate and that be exactly the sort of thing -PRON- need to be able to spot when dig in the log,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,616,2021-09-28T13:11:48Z,matko,Allow operation id to be submitted in a header for logging purposes,https://github.com/terminusdb/terminusdb/issues/616,"Currently in the json log format, we have an operation id which is meant to work with google cloud logging, associating all the log entries of a particular request together. However, it is very possible that terminusdb requests themselves are part of a larger cloud operation, such as in terminusx where multiple components together make up an operation. In this case, it'd be better if the operation id was not determined by terminusdb itself, but was submitted to it by the caller.
I propose we add a HTTP header, like X-Operation-ID, where an operation id can be submitted. The current operation-id which is really just a request id can move to its own field.
Other components using google's structured logging could then use the same operation id to group log entries accross different cloud bits.
@Francesca-Bit what do you think?",2021-09-30T01:44:04Z,matko,https://github.com/terminusdb/terminusdb/issues/616#issuecomment-930681230,"after the associated pull request is merged, X-Operation-ID can be used to submit an operation id. In this case, the server will assume it is part of a larger operation, and so it will not mark any log entry as the first or the last. If the header is not submitted, the old behavior still happens, meaning, an operation id will be generated from the server name and local request id, and first and last will be used to mark the first and last log record associated with a request.",after the associated pull request be merge x - operation - id can be use to submit an operation -PRON- d in this case the server will assume -PRON- be part of a large operation and so -PRON- will not mark any log entry as the first or the last if the header be not submit the old behavior still happen mean an operation -PRON- d will be generate from the server name and local request -PRON- d and first and last will be use to mark the first and last log record associate with a request,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,619,2021-09-28T20:49:42Z,Francesca-Bit,empty branch problems,https://github.com/terminusdb/terminusdb/issues/619,"Describe the bug


An empty branch does not create an empty schema, so if you try to add a new document in this branch you'll get the following error :
{
""@type"":""api:InsertDocumentErrorResponse"",
""api:error"": {
""@type"":""api:SchemaCheckFailure"",
""api:witnesses"": [ {""@type"":""context_not_found""} ]
},
""api:message"":""Schema check failure"",
""api:status"":""api:failure""
}


if you perform a query against an empty branch you'll get the following error message:


{
""api:message"":""Error: prefix_error('@base','User/auth0%7C614ceb4c2a17ef006a6c5951')\n  [35] throw(error(prefix_error('@base','User/auth0%7C614ceb4c2a17ef006a6c5951'),_23890))\n  [34] literals:prefixed_to_uri('User/auth0%7C614ceb4c2a17ef006a6c5951',_23940{api:'http://terminusdb.com/schema/api#',owl:'http://www.w3.org/2002/07/owl#',rdf:'http://www.w3.org/1999/02/22-rdf-syntax-ns#',rdfs:'http://www.w3.org/2000/01/rdf-schema#',sys:'http://terminusdb.com/schema/sys#',vio:'http://terminusdb.com/schema/vio#',woql:'http://terminusdb.com/schema/woql#',xdd:'http://terminusdb.com/schema/xdd#',xsd:'http://www.w3.org/2001/XMLSchema#'},'terminusdb://system/data/User/admin') at /app/terminusdb/src/core/triple/literals.pl:531\n  [32] woql_compile:assert_pre_flight_access(query_context{all_witnesses:false,authorization:'User/auth0%7C614ceb4c2a17ef006a6c5951',bindings:[],commit_info:_24094{},default_collection:branch_descriptor{branch_name:""ttttt"",repository_descriptor: ...},files:[],filter:type_filter{types: ...},prefixes:_23940{api:'http://terminusdb.com/schema/api#',owl:'http://www.w3.org/2002/07/owl#',rdf:'http://www.w3.org/1999/02/22-rdf-syntax-ns#',rdfs:'http://www.w3.org/2000/01/rdf-schema#',sys:'http://terminusdb.com/schema/sys#',vio:'http://terminusdb.com/schema/vio#',woql:'http://terminusdb.com/schema/woql#',xdd:'http://terminusdb.com/schema/xdd#',xsd:'http://www.w3.org/2001/XMLSchema#'},selected:[],system:transaction_object{descriptor: ...,inference_objects:[],instance_objects: ...,schema_objects: ...},transaction_objects:[...],update_guard:_24064,write_graph:branch_graph{branch_name:""ttttt"",database_name:""test"",organization_name:""hehjuejkjek"",repository_name:""local"",type:instance}},limit(10^^ ...,t(...,...,...))) at /app/terminusdb/src/core/query/woql_compile.pl:1020\n  [31] woql_compile:compile_query(limit(10^^ ...,t(...,...,...)),_24242,query_context{all_witnesses:false,authorization:'User/auth0%7C614ceb4c2a17ef006a6c5951',bindings:[],commit_info:_24094{},default_collection:branch_descriptor{branch_name:""ttttt"",repository_descriptor: ...},files:[],filter:type_filter{types: ...},prefixes:_23940{api:'http://terminusdb.com/schema/api#',owl:'http://www.w3.org/2002/07/owl#',rdf:'http://www.w3.org/1999/02/22-rdf-syntax-ns#',rdfs:'http://www.w3.org/2000/01/rdf-schema#',sys:'http://terminusdb.com/schema/sys#',vio:'http://terminusdb.com/schema/vio#',woql:'http://terminusdb.com/schema/woql#',xdd:'http://terminusdb.com/schema/xdd#',xsd:'http://www.w3.org/2001/XMLSchema#'},selected:[],system:transaction_object{descriptor: ...,inference_objects:[],instance_objects: ...,schema_objects: ...},transaction_objects:[...],update_guard:_24316,write_graph:branch_graph{branch_name:""ttttt"",database_name:""test"",organization_name:""hehjuejkjek"",repository_name:""local"",type:instance}},_24246) at /app/terminusdb/src/core/query/woql_compile.pl:614\n  [30] query_response:run_context_ast_jsonld_response(query_context{all_witnesses:false,authorization:'User/auth0%7C614ceb4c2a17ef006a6c5951',bindings:[],commit_info:_24094{},default_collection:branch_descriptor{branch_name:""ttttt"",repository_descriptor: ...},files:[],filter:type_filter{types: ...},prefixes:_23940{api:'http://terminusdb.com/schema/api#',owl:'http://www.w3.org/2002/07/owl#',rdf:'http://www.w3.org/1999/02/22-rdf-syntax-ns#',rdfs:'http://www.w3.org/2000/01/rdf-schema#',sys:'http://terminusdb.com/schema/sys#',vio:'http://terminusdb.com/schema/vio#',woql:'http://terminusdb.com/schema/woql#',xdd:'http://terminusdb.com/schema/xdd#',xsd:'http://www.w3.org/2001/XMLSchema#'},selected:[],system:transaction_object{descriptor: ...,inference_objects:[],instance_objects: ...,schema_objects: ...},transaction_objects:[...],update_guard:_24526,write_graph:branch_graph{branch_name:""ttttt"",database_name:""test"",organization_name:""hehjuejkjek"",repository_name:""local"",type:instance}},limit(10^^ ...,t(...,...,...)),_24476) at /app/terminusdb/src/core/query/query_response.pl:20\n  [28] ''('<garbage_collected>') \n  [27] catch(routes:(...,...),error(prefix_error('@base','User/auth0%7C614ceb4c2a17ef006a6c5951'),context(_24762,_24764)),routes:do_or_die(...,...)) at /usr/lib/swipl/boot/init.pl:532\n  [26] catch_with_backtrace('<garbage_collected>','<garbage_collected>','<garbage_collected>') at /usr/lib/swipl/boot/init.pl:582\n\nNote: some frames are missing due to last-call optimization.\nRe-run your program in debug mode (:- debug.) to get more detail.\n\n"",
""api:status"":""api:server_error""
}",2021-09-29T08:23:52Z,matko,https://github.com/terminusdb/terminusdb/issues/619#issuecomment-929953072,"It appears that the context is missing.
For newly created databases (which also start with an 'empty' branch), we create a fake initial commit containing just the context, which is replaced upon the very next commit. We probably forgot to do this for branches. I'll look into it.",-PRON- appear that the context be miss for newly create database ( which also start with an ' empty ' branch ) -PRON- create a fake initial commit contain just the context which be replace upon the very next commit -PRON- probably forget to do this for branch -PRON- will look into -PRON-,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,619,2021-09-28T20:49:42Z,Francesca-Bit,empty branch problems,https://github.com/terminusdb/terminusdb/issues/619,"Describe the bug


An empty branch does not create an empty schema, so if you try to add a new document in this branch you'll get the following error :
{
""@type"":""api:InsertDocumentErrorResponse"",
""api:error"": {
""@type"":""api:SchemaCheckFailure"",
""api:witnesses"": [ {""@type"":""context_not_found""} ]
},
""api:message"":""Schema check failure"",
""api:status"":""api:failure""
}


if you perform a query against an empty branch you'll get the following error message:


{
""api:message"":""Error: prefix_error('@base','User/auth0%7C614ceb4c2a17ef006a6c5951')\n  [35] throw(error(prefix_error('@base','User/auth0%7C614ceb4c2a17ef006a6c5951'),_23890))\n  [34] literals:prefixed_to_uri('User/auth0%7C614ceb4c2a17ef006a6c5951',_23940{api:'http://terminusdb.com/schema/api#',owl:'http://www.w3.org/2002/07/owl#',rdf:'http://www.w3.org/1999/02/22-rdf-syntax-ns#',rdfs:'http://www.w3.org/2000/01/rdf-schema#',sys:'http://terminusdb.com/schema/sys#',vio:'http://terminusdb.com/schema/vio#',woql:'http://terminusdb.com/schema/woql#',xdd:'http://terminusdb.com/schema/xdd#',xsd:'http://www.w3.org/2001/XMLSchema#'},'terminusdb://system/data/User/admin') at /app/terminusdb/src/core/triple/literals.pl:531\n  [32] woql_compile:assert_pre_flight_access(query_context{all_witnesses:false,authorization:'User/auth0%7C614ceb4c2a17ef006a6c5951',bindings:[],commit_info:_24094{},default_collection:branch_descriptor{branch_name:""ttttt"",repository_descriptor: ...},files:[],filter:type_filter{types: ...},prefixes:_23940{api:'http://terminusdb.com/schema/api#',owl:'http://www.w3.org/2002/07/owl#',rdf:'http://www.w3.org/1999/02/22-rdf-syntax-ns#',rdfs:'http://www.w3.org/2000/01/rdf-schema#',sys:'http://terminusdb.com/schema/sys#',vio:'http://terminusdb.com/schema/vio#',woql:'http://terminusdb.com/schema/woql#',xdd:'http://terminusdb.com/schema/xdd#',xsd:'http://www.w3.org/2001/XMLSchema#'},selected:[],system:transaction_object{descriptor: ...,inference_objects:[],instance_objects: ...,schema_objects: ...},transaction_objects:[...],update_guard:_24064,write_graph:branch_graph{branch_name:""ttttt"",database_name:""test"",organization_name:""hehjuejkjek"",repository_name:""local"",type:instance}},limit(10^^ ...,t(...,...,...))) at /app/terminusdb/src/core/query/woql_compile.pl:1020\n  [31] woql_compile:compile_query(limit(10^^ ...,t(...,...,...)),_24242,query_context{all_witnesses:false,authorization:'User/auth0%7C614ceb4c2a17ef006a6c5951',bindings:[],commit_info:_24094{},default_collection:branch_descriptor{branch_name:""ttttt"",repository_descriptor: ...},files:[],filter:type_filter{types: ...},prefixes:_23940{api:'http://terminusdb.com/schema/api#',owl:'http://www.w3.org/2002/07/owl#',rdf:'http://www.w3.org/1999/02/22-rdf-syntax-ns#',rdfs:'http://www.w3.org/2000/01/rdf-schema#',sys:'http://terminusdb.com/schema/sys#',vio:'http://terminusdb.com/schema/vio#',woql:'http://terminusdb.com/schema/woql#',xdd:'http://terminusdb.com/schema/xdd#',xsd:'http://www.w3.org/2001/XMLSchema#'},selected:[],system:transaction_object{descriptor: ...,inference_objects:[],instance_objects: ...,schema_objects: ...},transaction_objects:[...],update_guard:_24316,write_graph:branch_graph{branch_name:""ttttt"",database_name:""test"",organization_name:""hehjuejkjek"",repository_name:""local"",type:instance}},_24246) at /app/terminusdb/src/core/query/woql_compile.pl:614\n  [30] query_response:run_context_ast_jsonld_response(query_context{all_witnesses:false,authorization:'User/auth0%7C614ceb4c2a17ef006a6c5951',bindings:[],commit_info:_24094{},default_collection:branch_descriptor{branch_name:""ttttt"",repository_descriptor: ...},files:[],filter:type_filter{types: ...},prefixes:_23940{api:'http://terminusdb.com/schema/api#',owl:'http://www.w3.org/2002/07/owl#',rdf:'http://www.w3.org/1999/02/22-rdf-syntax-ns#',rdfs:'http://www.w3.org/2000/01/rdf-schema#',sys:'http://terminusdb.com/schema/sys#',vio:'http://terminusdb.com/schema/vio#',woql:'http://terminusdb.com/schema/woql#',xdd:'http://terminusdb.com/schema/xdd#',xsd:'http://www.w3.org/2001/XMLSchema#'},selected:[],system:transaction_object{descriptor: ...,inference_objects:[],instance_objects: ...,schema_objects: ...},transaction_objects:[...],update_guard:_24526,write_graph:branch_graph{branch_name:""ttttt"",database_name:""test"",organization_name:""hehjuejkjek"",repository_name:""local"",type:instance}},limit(10^^ ...,t(...,...,...)),_24476) at /app/terminusdb/src/core/query/query_response.pl:20\n  [28] ''('<garbage_collected>') \n  [27] catch(routes:(...,...),error(prefix_error('@base','User/auth0%7C614ceb4c2a17ef006a6c5951'),context(_24762,_24764)),routes:do_or_die(...,...)) at /usr/lib/swipl/boot/init.pl:532\n  [26] catch_with_backtrace('<garbage_collected>','<garbage_collected>','<garbage_collected>') at /usr/lib/swipl/boot/init.pl:582\n\nNote: some frames are missing due to last-call optimization.\nRe-run your program in debug mode (:- debug.) to get more detail.\n\n"",
""api:status"":""api:server_error""
}",2021-09-29T09:54:00Z,matko,https://github.com/terminusdb/terminusdb/issues/619#issuecomment-930025608,"One open problem here is that it is unclear which context object the branch should be initialized with. When inheriting from another branch, it's clear that their context object is used, but without any sort of shared history, nothing can really be known.
As an initial approach, just copying from main may work.",one open problem here be that -PRON- be unclear which context object the branch should be initialize with when inherit from another branch -PRON- be clear that -PRON- context object be use but without any sort of shared history nothing can really be know as an initial approach just copy from main may work,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,619,2021-09-28T20:49:42Z,Francesca-Bit,empty branch problems,https://github.com/terminusdb/terminusdb/issues/619,"Describe the bug


An empty branch does not create an empty schema, so if you try to add a new document in this branch you'll get the following error :
{
""@type"":""api:InsertDocumentErrorResponse"",
""api:error"": {
""@type"":""api:SchemaCheckFailure"",
""api:witnesses"": [ {""@type"":""context_not_found""} ]
},
""api:message"":""Schema check failure"",
""api:status"":""api:failure""
}


if you perform a query against an empty branch you'll get the following error message:


{
""api:message"":""Error: prefix_error('@base','User/auth0%7C614ceb4c2a17ef006a6c5951')\n  [35] throw(error(prefix_error('@base','User/auth0%7C614ceb4c2a17ef006a6c5951'),_23890))\n  [34] literals:prefixed_to_uri('User/auth0%7C614ceb4c2a17ef006a6c5951',_23940{api:'http://terminusdb.com/schema/api#',owl:'http://www.w3.org/2002/07/owl#',rdf:'http://www.w3.org/1999/02/22-rdf-syntax-ns#',rdfs:'http://www.w3.org/2000/01/rdf-schema#',sys:'http://terminusdb.com/schema/sys#',vio:'http://terminusdb.com/schema/vio#',woql:'http://terminusdb.com/schema/woql#',xdd:'http://terminusdb.com/schema/xdd#',xsd:'http://www.w3.org/2001/XMLSchema#'},'terminusdb://system/data/User/admin') at /app/terminusdb/src/core/triple/literals.pl:531\n  [32] woql_compile:assert_pre_flight_access(query_context{all_witnesses:false,authorization:'User/auth0%7C614ceb4c2a17ef006a6c5951',bindings:[],commit_info:_24094{},default_collection:branch_descriptor{branch_name:""ttttt"",repository_descriptor: ...},files:[],filter:type_filter{types: ...},prefixes:_23940{api:'http://terminusdb.com/schema/api#',owl:'http://www.w3.org/2002/07/owl#',rdf:'http://www.w3.org/1999/02/22-rdf-syntax-ns#',rdfs:'http://www.w3.org/2000/01/rdf-schema#',sys:'http://terminusdb.com/schema/sys#',vio:'http://terminusdb.com/schema/vio#',woql:'http://terminusdb.com/schema/woql#',xdd:'http://terminusdb.com/schema/xdd#',xsd:'http://www.w3.org/2001/XMLSchema#'},selected:[],system:transaction_object{descriptor: ...,inference_objects:[],instance_objects: ...,schema_objects: ...},transaction_objects:[...],update_guard:_24064,write_graph:branch_graph{branch_name:""ttttt"",database_name:""test"",organization_name:""hehjuejkjek"",repository_name:""local"",type:instance}},limit(10^^ ...,t(...,...,...))) at /app/terminusdb/src/core/query/woql_compile.pl:1020\n  [31] woql_compile:compile_query(limit(10^^ ...,t(...,...,...)),_24242,query_context{all_witnesses:false,authorization:'User/auth0%7C614ceb4c2a17ef006a6c5951',bindings:[],commit_info:_24094{},default_collection:branch_descriptor{branch_name:""ttttt"",repository_descriptor: ...},files:[],filter:type_filter{types: ...},prefixes:_23940{api:'http://terminusdb.com/schema/api#',owl:'http://www.w3.org/2002/07/owl#',rdf:'http://www.w3.org/1999/02/22-rdf-syntax-ns#',rdfs:'http://www.w3.org/2000/01/rdf-schema#',sys:'http://terminusdb.com/schema/sys#',vio:'http://terminusdb.com/schema/vio#',woql:'http://terminusdb.com/schema/woql#',xdd:'http://terminusdb.com/schema/xdd#',xsd:'http://www.w3.org/2001/XMLSchema#'},selected:[],system:transaction_object{descriptor: ...,inference_objects:[],instance_objects: ...,schema_objects: ...},transaction_objects:[...],update_guard:_24316,write_graph:branch_graph{branch_name:""ttttt"",database_name:""test"",organization_name:""hehjuejkjek"",repository_name:""local"",type:instance}},_24246) at /app/terminusdb/src/core/query/woql_compile.pl:614\n  [30] query_response:run_context_ast_jsonld_response(query_context{all_witnesses:false,authorization:'User/auth0%7C614ceb4c2a17ef006a6c5951',bindings:[],commit_info:_24094{},default_collection:branch_descriptor{branch_name:""ttttt"",repository_descriptor: ...},files:[],filter:type_filter{types: ...},prefixes:_23940{api:'http://terminusdb.com/schema/api#',owl:'http://www.w3.org/2002/07/owl#',rdf:'http://www.w3.org/1999/02/22-rdf-syntax-ns#',rdfs:'http://www.w3.org/2000/01/rdf-schema#',sys:'http://terminusdb.com/schema/sys#',vio:'http://terminusdb.com/schema/vio#',woql:'http://terminusdb.com/schema/woql#',xdd:'http://terminusdb.com/schema/xdd#',xsd:'http://www.w3.org/2001/XMLSchema#'},selected:[],system:transaction_object{descriptor: ...,inference_objects:[],instance_objects: ...,schema_objects: ...},transaction_objects:[...],update_guard:_24526,write_graph:branch_graph{branch_name:""ttttt"",database_name:""test"",organization_name:""hehjuejkjek"",repository_name:""local"",type:instance}},limit(10^^ ...,t(...,...,...)),_24476) at /app/terminusdb/src/core/query/query_response.pl:20\n  [28] ''('<garbage_collected>') \n  [27] catch(routes:(...,...),error(prefix_error('@base','User/auth0%7C614ceb4c2a17ef006a6c5951'),context(_24762,_24764)),routes:do_or_die(...,...)) at /usr/lib/swipl/boot/init.pl:532\n  [26] catch_with_backtrace('<garbage_collected>','<garbage_collected>','<garbage_collected>') at /usr/lib/swipl/boot/init.pl:582\n\nNote: some frames are missing due to last-call optimization.\nRe-run your program in debug mode (:- debug.) to get more detail.\n\n"",
""api:status"":""api:server_error""
}",2021-09-29T09:57:47Z,matko,https://github.com/terminusdb/terminusdb/issues/619#issuecomment-930028346,"Similarly, it's unclear if the new branch is supposed to be schemaless or not. Again, we can inherit from main, but imo it's a bit dodgy. Really this information should be coming in from the api.",similarly -PRON- be unclear if the new branch be suppose to be schemaless or not again -PRON- can inherit from main but imo -PRON- be a bit dodgy really this information should be come in from the api,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,619,2021-09-28T20:49:42Z,Francesca-Bit,empty branch problems,https://github.com/terminusdb/terminusdb/issues/619,"Describe the bug


An empty branch does not create an empty schema, so if you try to add a new document in this branch you'll get the following error :
{
""@type"":""api:InsertDocumentErrorResponse"",
""api:error"": {
""@type"":""api:SchemaCheckFailure"",
""api:witnesses"": [ {""@type"":""context_not_found""} ]
},
""api:message"":""Schema check failure"",
""api:status"":""api:failure""
}


if you perform a query against an empty branch you'll get the following error message:


{
""api:message"":""Error: prefix_error('@base','User/auth0%7C614ceb4c2a17ef006a6c5951')\n  [35] throw(error(prefix_error('@base','User/auth0%7C614ceb4c2a17ef006a6c5951'),_23890))\n  [34] literals:prefixed_to_uri('User/auth0%7C614ceb4c2a17ef006a6c5951',_23940{api:'http://terminusdb.com/schema/api#',owl:'http://www.w3.org/2002/07/owl#',rdf:'http://www.w3.org/1999/02/22-rdf-syntax-ns#',rdfs:'http://www.w3.org/2000/01/rdf-schema#',sys:'http://terminusdb.com/schema/sys#',vio:'http://terminusdb.com/schema/vio#',woql:'http://terminusdb.com/schema/woql#',xdd:'http://terminusdb.com/schema/xdd#',xsd:'http://www.w3.org/2001/XMLSchema#'},'terminusdb://system/data/User/admin') at /app/terminusdb/src/core/triple/literals.pl:531\n  [32] woql_compile:assert_pre_flight_access(query_context{all_witnesses:false,authorization:'User/auth0%7C614ceb4c2a17ef006a6c5951',bindings:[],commit_info:_24094{},default_collection:branch_descriptor{branch_name:""ttttt"",repository_descriptor: ...},files:[],filter:type_filter{types: ...},prefixes:_23940{api:'http://terminusdb.com/schema/api#',owl:'http://www.w3.org/2002/07/owl#',rdf:'http://www.w3.org/1999/02/22-rdf-syntax-ns#',rdfs:'http://www.w3.org/2000/01/rdf-schema#',sys:'http://terminusdb.com/schema/sys#',vio:'http://terminusdb.com/schema/vio#',woql:'http://terminusdb.com/schema/woql#',xdd:'http://terminusdb.com/schema/xdd#',xsd:'http://www.w3.org/2001/XMLSchema#'},selected:[],system:transaction_object{descriptor: ...,inference_objects:[],instance_objects: ...,schema_objects: ...},transaction_objects:[...],update_guard:_24064,write_graph:branch_graph{branch_name:""ttttt"",database_name:""test"",organization_name:""hehjuejkjek"",repository_name:""local"",type:instance}},limit(10^^ ...,t(...,...,...))) at /app/terminusdb/src/core/query/woql_compile.pl:1020\n  [31] woql_compile:compile_query(limit(10^^ ...,t(...,...,...)),_24242,query_context{all_witnesses:false,authorization:'User/auth0%7C614ceb4c2a17ef006a6c5951',bindings:[],commit_info:_24094{},default_collection:branch_descriptor{branch_name:""ttttt"",repository_descriptor: ...},files:[],filter:type_filter{types: ...},prefixes:_23940{api:'http://terminusdb.com/schema/api#',owl:'http://www.w3.org/2002/07/owl#',rdf:'http://www.w3.org/1999/02/22-rdf-syntax-ns#',rdfs:'http://www.w3.org/2000/01/rdf-schema#',sys:'http://terminusdb.com/schema/sys#',vio:'http://terminusdb.com/schema/vio#',woql:'http://terminusdb.com/schema/woql#',xdd:'http://terminusdb.com/schema/xdd#',xsd:'http://www.w3.org/2001/XMLSchema#'},selected:[],system:transaction_object{descriptor: ...,inference_objects:[],instance_objects: ...,schema_objects: ...},transaction_objects:[...],update_guard:_24316,write_graph:branch_graph{branch_name:""ttttt"",database_name:""test"",organization_name:""hehjuejkjek"",repository_name:""local"",type:instance}},_24246) at /app/terminusdb/src/core/query/woql_compile.pl:614\n  [30] query_response:run_context_ast_jsonld_response(query_context{all_witnesses:false,authorization:'User/auth0%7C614ceb4c2a17ef006a6c5951',bindings:[],commit_info:_24094{},default_collection:branch_descriptor{branch_name:""ttttt"",repository_descriptor: ...},files:[],filter:type_filter{types: ...},prefixes:_23940{api:'http://terminusdb.com/schema/api#',owl:'http://www.w3.org/2002/07/owl#',rdf:'http://www.w3.org/1999/02/22-rdf-syntax-ns#',rdfs:'http://www.w3.org/2000/01/rdf-schema#',sys:'http://terminusdb.com/schema/sys#',vio:'http://terminusdb.com/schema/vio#',woql:'http://terminusdb.com/schema/woql#',xdd:'http://terminusdb.com/schema/xdd#',xsd:'http://www.w3.org/2001/XMLSchema#'},selected:[],system:transaction_object{descriptor: ...,inference_objects:[],instance_objects: ...,schema_objects: ...},transaction_objects:[...],update_guard:_24526,write_graph:branch_graph{branch_name:""ttttt"",database_name:""test"",organization_name:""hehjuejkjek"",repository_name:""local"",type:instance}},limit(10^^ ...,t(...,...,...)),_24476) at /app/terminusdb/src/core/query/query_response.pl:20\n  [28] ''('<garbage_collected>') \n  [27] catch(routes:(...,...),error(prefix_error('@base','User/auth0%7C614ceb4c2a17ef006a6c5951'),context(_24762,_24764)),routes:do_or_die(...,...)) at /usr/lib/swipl/boot/init.pl:532\n  [26] catch_with_backtrace('<garbage_collected>','<garbage_collected>','<garbage_collected>') at /usr/lib/swipl/boot/init.pl:582\n\nNote: some frames are missing due to last-call optimization.\nRe-run your program in debug mode (:- debug.) to get more detail.\n\n"",
""api:status"":""api:server_error""
}",2021-09-29T10:27:30Z,GavinMendelGleason,https://github.com/terminusdb/terminusdb/issues/619#issuecomment-930050105,"I think the plan should be API information is optional, and main becomes the prototype object if it is not supplied. This gives us backward compatibility and we should later start asking the user what they want as we move forward.",i think the plan should be api information be optional and main become the prototype object if -PRON- be not supply this give -PRON- backward compatibility and -PRON- should later start ask the user what -PRON- want as -PRON- move forward,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,620,2021-09-29T07:56:29Z,Cheukting,Query document an option to return the number of result rather than the whole josn result,https://github.com/terminusdb/terminusdb/issues/620,I think it will be really helpful if I have a huge db and just want to know how many results I may get before really getting the result.,2021-09-29T08:22:39Z,matko,https://github.com/terminusdb/terminusdb/issues/620#issuecomment-929952126,"Would you want to be able to tell this for any document query? Or would it be enough to know that there's x amount of some type, or the total amount of documents of any type?",would -PRON- want to be able to tell this for any document query or would -PRON- be enough to know that there be x amount of some type or the total amount of document of any type,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,620,2021-09-29T07:56:29Z,Cheukting,Query document an option to return the number of result rather than the whole josn result,https://github.com/terminusdb/terminusdb/issues/620,I think it will be really helpful if I have a huge db and just want to know how many results I may get before really getting the result.,2021-09-29T08:45:56Z,Cheukting,https://github.com/terminusdb/terminusdb/issues/620#issuecomment-929969273,"I think if we can have it for any query would be great, as users can filter
anything but not just type. But I guess it's harder to implement?
…
On Wed, Sep 29, 2021 at 9:22 AM Matthijs van Otterdijk < ***@***.***> wrote:
 Would you want to be able to tell this for any document query? Or would it
 be enough to know that there's x amount of some type, or the total amount
 of documents of any type?

 —
 You are receiving this because you authored the thread.
 Reply to this email directly, view it on GitHub
 <#620 (comment)>,
 or unsubscribe
 <https://github.com/notifications/unsubscribe-auth/AG3N26J5KXD3GDNPETI7GI3UELEFTANCNFSM5E7GZKLA>
 .",i think if -PRON- can have -PRON- for any query would be great as user can filter anything but not just type but i guess -PRON- be hard to implement … on -PRON- d sep 29 2021 at 922 am matthijs van otterdijk < * * * @ * * * * * * > write would -PRON- want to be able to tell this for any document query or would -PRON- be enough to know that there be x amount of some type or the total amount of document of any type — -PRON- be receive this because -PRON- author the thread reply to this email directly view -PRON- on github < # 620 ( comment ) > or unsubscribe < https//githubcom / notification / unsubscribe - auth / ag3n26j5kxd3gdnpeti7gi3ueleftancnfsm5e7gzkla >,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,621,2021-09-29T09:19:55Z,matko,Custom roles scoped to an organization,https://github.com/terminusdb/terminusdb/issues/621,"All our roles are global. This means what we cannot really let organizations create custom roles, because every other organization would see these same roles pop up, in the same global namespace.
We should have a way to scope a role to a particular organization, possibly through an optional field on the role. That way, each organization can create their own roles that only they see.",2021-09-29T09:55:56Z,GavinMendelGleason,https://github.com/terminusdb/terminusdb/issues/621#issuecomment-930027055,"It may be that we can just point to some roles, with some default ones listed by default. If they construct new roles they can just go in the global pool?",-PRON- may be that -PRON- can just point to some role with some default one list by default if -PRON- construct new role -PRON- can just go in the global pool,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,622,2021-09-29T14:35:54Z,GavinMendelGleason,Elaboration needs to be slightly more robust in order to eat valid JSON-LD,https://github.com/terminusdb/terminusdb/issues/622,"When we ingest an object of the form:
{""@type"":""GovernmentOrganization"",
 ""@id"":""https://ontomatica.io/identifier/16040000001000131961"",
 ""name"":""US Department of Labor (DOL) Bureau of Labor Statistics (BLS)"",
 ""naics"":""926110"",
 ""url"":""https://www.dol.gov/"",
 ""logo"":{""@id"":""https://ontomatica.io/identifier/16040000001000132161""},
 ""address"":{""@id"":""https://ontomatica.io/identifier/14330101001000351961""},
 ""location"":{""@id"":""https://ontomatica.io/identifier/18000000001000351961""},
 ""brand"":{""@id"":""https://ontomatica.io/identifier/16040000001000132961""}}
We assume that the ""@id""'s will be bare, rather than decorated. We need to make it acceptable for them to be decorated as well (without a specified type).
{
    ""api:message"": ""Error: document_has_no_type(json{'@id':\""https://ontomatica.io/identifier/16020000001000012161\""})\n  [46] throw(error(document_has_no_type(...),_26696))\n  [44] catch(api_document:do_or_die(...,...),error(document_has_no_type(...),_26752),api_document:(...;...)) at /usr/lib/swipl/boot/init.pl:533\n  [42] api_document:api_insert_document_(instance,transaction_object{commit_info:commit_info{},descriptor:branch_descriptor{branch_name:\""main\"",repository_descriptor: ...},inference_objects:[],instance_objects:[...],parent:transaction_object{descriptor: ...,inference_objects:[],instance_objects: ...,parent: ...,schema_objects: ...},schema_objects:[...]},<stream>(0x55f5b1e4a100),_26810) at /app/terminusdb/src/core/api/api_document.pl:192\n  [41] '$bags':findall_loop(_26926,'<garbage_collected>',_26930,[]) at /usr/lib/swipl/boot/bags.pl:99\n  [40] setup_call_catcher_cleanup('$bags':'$new_findall_bag','$bags':findall_loop(_26984,...,_26988,[]),_26966,'$bags':'$destroy_findall_bag') at /usr/lib/swipl/boot/init.pl:614\n  [36] '<meta-call>'('<garbage_collected>') <foreign>\n  [35] database:with_transaction_(query_context{all_witnesses:false,authorization:'terminusdb://system/data/User/admin',bindings:[],commit_info:commit_info{author:admin,message:'Commit via python client 10.0.10'},default_collection:branch_descriptor{branch_name:\""main\"",repository_descriptor: ...},files:[],filter:type_filter{types: ...},prefixes:_27146{'@base':\""iri://afdsi/\"",'@schema':\""http://schema.org/\"",'@type':'Context',api:'http://terminusdb.com/schema/api#',owl:'http://www.w3.org/2002/07/owl#',rdf:'http://www.w3.org/1999/02/22-rdf-syntax-ns#',rdfs:'http://www.w3.org/2000/01/rdf-schema#',sys:'http://terminusdb.com/schema/sys#',vio:'http://terminusdb.com/schema/vio#',woql:'http://terminusdb.com/schema/woql#',xdd:'http://terminusdb.com/schema/xdd#',xsd:'http://www.w3.org/2001/XMLSchema#'},selected:[],system:system_descriptor{},transaction_objects:[...],update_guard:_27120,write_graph:branch_graph{branch_name:\""main\"",database_name:\""afdsi\"",organization_name:\""TerminatorsX\"",repository_name:\""local\"",type:instance}},api_document:(...;...),_27070) at /app/terminusdb/src/core/transaction/database.pl:220\n  [34] setup_call_catcher_cleanup(database:true,database:with_transaction_(...,...,_27312),_27290,database:true) at /usr/lib/swipl/boot/init.pl:614\n  [30] '<meta-call>'('<garbage_collected>') <foreign>\n  [29] catch(routes:(...,...),error(document_has_no_type(...),context(_27412,_27414)),routes:do_or_die(...,...)) at /usr/lib/swipl/boot/init.pl:532\n  [28] catch_with_backtrace('<garbage_collected>','<garbage_collected>','<garbage_collected>') at /usr/lib/swipl/boot/init.pl:582\n\nNote: some frames are missing due to last-call optimization.\nRe-run your program in debug mode (:- debug.) to get more detail.\n\n"",
    ""api:status"": ""api:server_error""
}",2021-10-04T14:16:13Z,spl,https://github.com/terminusdb/terminusdb/issues/622#issuecomment-933531288,"We currently accept this:
""logo"":""https://ontomatica.io/identifier/16040000001000132161"",
We should also accept:
""logo"":{""@id"":""https://ontomatica.io/identifier/16040000001000132161""},","-PRON- currently accept this "" logo""""https//ontomaticaio / identifier/16040000001000132161 "" -PRON- should also accept "" logo""{""@id""""https//ontomaticaio / identifier/16040000001000132161 "" }",0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,623,2021-09-29T14:56:21Z,GavinMendelGleason,Invalid datatype gives 500 and stack trace,https://github.com/terminusdb/terminusdb/issues/623,"Invalid datatype gives 500 and horrible message.
{
    ""api:message"": ""Error: not_a_valid_datatype([\""https://ontomatica.io/identifier/18010100001000162161\"",\""https://ontomatica.io/identifier/18010100001000162161\""],'http://www.w3.org/2001/XMLSchema#string')\n  [46] throw(error(not_a_valid_datatype(...,'http://www.w3.org/2001/XMLSchema#string'),_184238))\n  [44] catch('<garbage_collected>','<garbage_collected>','<garbage_collected>') at /usr/lib/swipl/boot/init.pl:533\n  [42] api_document:api_insert_document_(instance,transaction_object{commit_info:commit_info{},descriptor:branch_descriptor{branch_name:\""main\"",repository_descriptor: ...},inference_objects:[],instance_objects:[...],parent:transaction_object{descriptor: ...,inference_objects:[],instance_objects: ...,parent: ...,schema_objects: ...},schema_objects:[...]},<stream>(0x5594755a5c00),_184320) at /app/terminusdb/src/core/api/api_document.pl:192\n  [41] '$bags':findall_loop(_184436,'<garbage_collected>',_184440,[]) at /usr/lib/swipl/boot/bags.pl:99\n  [40] setup_call_catcher_cleanup('$bags':'$new_findall_bag','$bags':findall_loop(_184494,...,_184498,[]),_184476,'$bags':'$destroy_findall_bag') at /usr/lib/swipl/boot/init.pl:614\n  [36] '<meta-call>'('<garbage_collected>') <foreign>\n  [35] database:with_transaction_(query_context{all_witnesses:false,authorization:'terminusdb://system/data/User/admin',bindings:[],commit_info:commit_info{author:admin,message:'Commit via python client 10.0.10'},default_collection:branch_descriptor{branch_name:\""main\"",repository_descriptor: ...},files:[],filter:type_filter{types: ...},prefixes:_184656{'@base':\""https://ontomatica.io/identifier/\"",'@schema':\""http://schema.org/\"",'@type':'Context',api:'http://terminusdb.com/schema/api#',owl:'http://www.w3.org/2002/07/owl#',rdf:'http://www.w3.org/1999/02/22-rdf-syntax-ns#',rdfs:'http://www.w3.org/2000/01/rdf-schema#',sys:'http://terminusdb.com/schema/sys#',vio:'http://terminusdb.com/schema/vio#',woql:'http://terminusdb.com/schema/woql#',xdd:'http://terminusdb.com/schema/xdd#',xsd:'http://www.w3.org/2001/XMLSchema#'},selected:[],system:system_descriptor{},transaction_objects:[...],update_guard:_184630,write_graph:branch_graph{branch_name:\""main\"",database_name:\""afdsi\"",organization_name:\""TerminatorsX\"",repository_name:\""local\"",type:instance}},api_document:(...;...),_184580) at /app/terminusdb/src/core/transaction/database.pl:220\n  [34] setup_call_catcher_cleanup(database:true,database:with_transaction_(...,...,_184822),_184800,database:true) at /usr/lib/swipl/boot/init.pl:614\n  [30] '<meta-call>'('<garbage_collected>') <foreign>\n  [29] catch(routes:(...,...),error(not_a_valid_datatype(...,'http://www.w3.org/2001/XMLSchema#string'),context(_184924,_184926)),routes:do_or_die(...,...)) at /usr/lib/swipl/boot/init.pl:532\n  [28] catch_with_backtrace('<garbage_collected>','<garbage_collected>','<garbage_collected>') at /usr/lib/swipl/boot/init.pl:582\n\nNote: some frames are missing due to last-call optimization.\nRe-run your program in debug mode (:- debug.) to get more detail.\n\n"",
    ""api:status"": ""api:server_error""
}",,,,,,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,625,2021-09-30T01:14:53Z,matko,Merge http request log into a single line,https://github.com/terminusdb/terminusdb/issues/625,"Currently, HTTP requests are logged as two lines - one line when the request comes in, and a second line after the response has been sent.
This is somewhat atypical. It seems like google logs expect HTTP logs to be only a single line containing both the request and response information.
To support this, we need to save our logging information for the request for after the response, then send both as one record. An additional advantage of doing this is that we can calculate the request latency and include that in the log.",,,,,,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,627,2021-09-30T06:08:07Z,GavinMendelGleason,Schema Check Errors need Document Context,https://github.com/terminusdb/terminusdb/issues/627,"The current schema check is returning information about an auto-generated ID with no context about how it was generated or what part of the document it comes from. This is very confusing!
An example is below:
{
    ""@type"": ""api:InsertDocumentErrorResponse"",
    ""api:error"": {
        ""@type"": ""api:SchemaCheckFailure"",
        ""api:witnesses"": [
            {
                ""@type"": ""instance_not_of_class"",
                ""class"": ""iri://terminusdb.com/python#Expr"",
                ""instance"": ""iri://terminusdb.com/python/Name/989e6cbfd49a4ecf39abd7d1bc01dbf57804cbce6f0c585bba36d8dd71357e21""
            }
        ]
    },
    ""api:message"": ""Schema check failure"",
    ""api:status"": ""api:failure""
}
We should think about capturing the document context somehow - perhaps with a backward walk up to the most local document which points to this instance and then attempt to report the full document?",,,,,,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,628,2021-09-30T09:36:15Z,andtheWings,Allow lists and sets to be set as optional,https://github.com/terminusdb/terminusdb/issues/628,"Is your feature request related to a problem? Please describe.
Because list, set, and optional are all type families and because you can't assign multiple types to an object, it's currently not possible to make a set/list property optional.
Describe the solution you'd like
I am not a developer, but it seems like you'd either want to allow certain specific combinations of type families such as that above or perhaps make a separate boolean ""@optional"" keyword.
Additional context
For example, I currently can't set ""criteria"" in this schema definition to be optional.
{
    ""@id"": ""a_standard"",
    ""@inherits"": [
      ""described_entity"",
      ""named_entity""
    ],
    ""@key"": {
      ""@type"": ""Random""
    },
    ""@type"": ""Class"",
    ""criteria"": {
      ""@class"": ""criterion"",
      ""@type"": ""List""
    }
}",2021-09-30T10:02:14Z,GavinMendelGleason,https://github.com/terminusdb/terminusdb/issues/628#issuecomment-931177533,"Optional(List) is, in particular, really needed.
The Optional(Set) combination is actually redundant since sets can work with any cardinality including zero.
I haven't thought through all of the other possible combinations, but List of Set, Set of List, List of List and Set of Set all sound a little tricky and we'd have to make sure that the update semantics make sense.",optional(list ) be in particular really need the optional(set ) combination be actually redundant since set can work with any cardinality include zero i have not think through all of the other possible combination but list of set set of list list of list and set of set all sound a little tricky and -PRON- 'd have to make sure that the update semantic make sense,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,629,2021-09-30T09:39:36Z,Francesca-Bit,add multi user in the admin/organization,https://github.com/terminusdb/terminusdb/issues/629,"I need to add more that one user in the admin/organization
so I can access at the system database.
In this moment I can add an user in the admin/organization but I can not access to the system database",,,,,,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,631,2021-09-30T10:13:11Z,luke-feeney,UI Feedback,https://github.com/terminusdb/terminusdb/issues/631,"Is your feature request related to a problem? Please describe.
When i create a data product in the UI I don't get good feedback if I don't fill in one of the necessary fields.
Describe the solution you'd like
I would like the fields to either have perma * or to have a red outline when they are not complete but need to be completed.
Describe alternatives you've considered
I have considered continuing and that is what I did, but I still think it could be better
Additional context
This is where i am talking about:",,,,,,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,634,2021-09-30T13:44:04Z,chrisshortlaw,TerminusX Dashboard: Data Product Model Will not Display -,https://github.com/terminusdb/terminusdb/issues/634,"Describe the bug
After a schema update, Data Product Model on TerminusX Dashboard shows a black screen.
To Reproduce
Steps to reproduce the behavior:

Open TerminusX Dashboard
Select database
Click on 'Data Product Model'
See error

Expected behavior
Should display a graphical representation of the DB schema
Screenshots
Screen is black with no navbar or sidebar.

Info (please complete the following information):

OS: Windows 10 Pro 20H2
Browser: FireFox 92.0.1 (64-bit)
How did you run terminus-server"": TerminusX Dashboard
SWI Prolog version if you're using the manual installation: N/A

Additional context
Console logs two errors:



 TypeError: s is undefined  tdb-dashboard.min.js: 39486
    m tdb-dashboard.min.js:100711
    m tdb-dashboard.min.js:100690
    m tdb-dashboard.min.js:100686
    Nb tdb-dashboard.min.js:100725
    Nb tdb-dashboard.min.js:100730
    Nb tdb-dashboard.min.js:100731
    Yb tdb-dashboard.min.js:101232
    as tdb-dashboard.min.js:39544
    wl tdb-dashboard.min.js:40565
    unstable_runWithPriority tdb-dashboard.min.js:48243
    Vi tdb-dashboard.min.js:37667
    bl tdb-dashboard.min.js:40548
    rl tdb-dashboard.min.js:40102
    Gi tdb-dashboard.min.js:37697
    unstable_runWithPriority tdb-dashboard.min.js:48243
    Vi tdb-dashboard.min.js:37667
    Gi tdb-dashboard.min.js:37693
    qi tdb-dashboard.min.js:37685
    Xs tdb-dashboard.min.js:39961
    ga tdb-dashboard.min.js:38578
    t tdb-dashboard.min.js:99798
    c tdb-dashboard.min.js:47325
    _invoke tdb-dashboard.min.js:47307
    k tdb-dashboard.min.js:47363
    zv tdb-dashboard.min.js:99689
    a tdb-dashboard.min.js:99703



Uncaught (in promise) TypeError: s is undefined
    m tdb-dashboard.min.js:100711
    m tdb-dashboard.min.js:100690
    m tdb-dashboard.min.js:100686
    Nb tdb-dashboard.min.js:100725
    Nb tdb-dashboard.min.js:100730
    Nb tdb-dashboard.min.js:100731
    Yb tdb-dashboard.min.js:101232
    as tdb-dashboard.min.js:39544
    wl tdb-dashboard.min.js:40565
    unstable_runWithPriority tdb-dashboard.min.js:48243
    Vi tdb-dashboard.min.js:37667
    bl tdb-dashboard.min.js:40548
    rl tdb-dashboard.min.js:40102
    Gi tdb-dashboard.min.js:37697
    unstable_runWithPriority tdb-dashboard.min.js:48243
    Vi tdb-dashboard.min.js:37667
    Gi tdb-dashboard.min.js:37693
    qi tdb-dashboard.min.js:37685
    Xs tdb-dashboard.min.js:39961
    ga tdb-dashboard.min.js:38578
    t tdb-dashboard.min.js:99798
    c tdb-dashboard.min.js:47325
    _invoke tdb-dashboard.min.js:47307
    k tdb-dashboard.min.js:47363
    zv tdb-dashboard.min.js:99689
    a tdb-dashboard.min.js:997",2021-10-02T16:20:51Z,chrisshortlaw,https://github.com/terminusdb/terminusdb/issues/634#issuecomment-932778413,"Further Information:
Normal function returned when I corrected an error I had made in the Schema - I had a typo in a DocumentTemplate when referring to another Document (e.g 'Users' instead of 'User'). (This is using the Python Client).",further information normal function return when i correct an error i have make in the schema - i have a typo in a documenttemplate when refer to another document ( eg ' user ' instead of ' user ' ) ( this be use the python client ),0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,634,2021-09-30T13:44:04Z,chrisshortlaw,TerminusX Dashboard: Data Product Model Will not Display -,https://github.com/terminusdb/terminusdb/issues/634,"Describe the bug
After a schema update, Data Product Model on TerminusX Dashboard shows a black screen.
To Reproduce
Steps to reproduce the behavior:

Open TerminusX Dashboard
Select database
Click on 'Data Product Model'
See error

Expected behavior
Should display a graphical representation of the DB schema
Screenshots
Screen is black with no navbar or sidebar.

Info (please complete the following information):

OS: Windows 10 Pro 20H2
Browser: FireFox 92.0.1 (64-bit)
How did you run terminus-server"": TerminusX Dashboard
SWI Prolog version if you're using the manual installation: N/A

Additional context
Console logs two errors:



 TypeError: s is undefined  tdb-dashboard.min.js: 39486
    m tdb-dashboard.min.js:100711
    m tdb-dashboard.min.js:100690
    m tdb-dashboard.min.js:100686
    Nb tdb-dashboard.min.js:100725
    Nb tdb-dashboard.min.js:100730
    Nb tdb-dashboard.min.js:100731
    Yb tdb-dashboard.min.js:101232
    as tdb-dashboard.min.js:39544
    wl tdb-dashboard.min.js:40565
    unstable_runWithPriority tdb-dashboard.min.js:48243
    Vi tdb-dashboard.min.js:37667
    bl tdb-dashboard.min.js:40548
    rl tdb-dashboard.min.js:40102
    Gi tdb-dashboard.min.js:37697
    unstable_runWithPriority tdb-dashboard.min.js:48243
    Vi tdb-dashboard.min.js:37667
    Gi tdb-dashboard.min.js:37693
    qi tdb-dashboard.min.js:37685
    Xs tdb-dashboard.min.js:39961
    ga tdb-dashboard.min.js:38578
    t tdb-dashboard.min.js:99798
    c tdb-dashboard.min.js:47325
    _invoke tdb-dashboard.min.js:47307
    k tdb-dashboard.min.js:47363
    zv tdb-dashboard.min.js:99689
    a tdb-dashboard.min.js:99703



Uncaught (in promise) TypeError: s is undefined
    m tdb-dashboard.min.js:100711
    m tdb-dashboard.min.js:100690
    m tdb-dashboard.min.js:100686
    Nb tdb-dashboard.min.js:100725
    Nb tdb-dashboard.min.js:100730
    Nb tdb-dashboard.min.js:100731
    Yb tdb-dashboard.min.js:101232
    as tdb-dashboard.min.js:39544
    wl tdb-dashboard.min.js:40565
    unstable_runWithPriority tdb-dashboard.min.js:48243
    Vi tdb-dashboard.min.js:37667
    bl tdb-dashboard.min.js:40548
    rl tdb-dashboard.min.js:40102
    Gi tdb-dashboard.min.js:37697
    unstable_runWithPriority tdb-dashboard.min.js:48243
    Vi tdb-dashboard.min.js:37667
    Gi tdb-dashboard.min.js:37693
    qi tdb-dashboard.min.js:37685
    Xs tdb-dashboard.min.js:39961
    ga tdb-dashboard.min.js:38578
    t tdb-dashboard.min.js:99798
    c tdb-dashboard.min.js:47325
    _invoke tdb-dashboard.min.js:47307
    k tdb-dashboard.min.js:47363
    zv tdb-dashboard.min.js:99689
    a tdb-dashboard.min.js:997",2021-10-02T17:58:57Z,GavinMendelGleason,https://github.com/terminusdb/terminusdb/issues/634#issuecomment-932793937,"Thanks @chrisshortlaw
That further information is very useful. I suspect I ran into this same bug! We'll take a look on Monday.",thanks @chrisshortlaw that further information be very useful i suspect i run into this same bug -PRON- will take a look on monday,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,635,2021-09-30T15:25:22Z,GavinMendelGleason,Branch does not exist gives stack trace,https://github.com/terminusdb/terminusdb/issues/635,"Try to delete a non-existent branch and you get:
{'api:message': 'Error: branch_does_not_exist(""capacity_factors"")\n  [34] throw(error(branch_does_not_exist(""capacity_factors""),_11444))\n  [32] db_branch:branch_delete(\'<garbage_collected>\',\'User/auth0%7C613b70276972a400703b8078\',\'TerminatorsX/nuclear/local/branch/capacity_factors\') at /app/terminusdb/src/core/api/db_branch.pl:266\n  [31] \'<meta-call>\'(\'<garbage_collected>\') <foreign>\n  [30] catch(routes:(...,...),error(branch_does_not_exist(""capacity_factors""),context(_11582,_11584)),routes:do_or_die(...,...)) at /usr/lib/swipl/boot/init.pl:532\n  [29] catch_with_backtrace(\'<garbage_collected>\',\'<garbage_collected>\',\'<garbage_collected>\') at /usr/lib/swipl/boot/init.pl:582\n\nNote: some frames are missing due to last-call optimization.\nRe-run your program in debug mode (:- debug.) to get more detail.\n\n', 'api:status': 'api:server_error'}",,,,,,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,636,2021-09-30T18:13:28Z,GavinMendelGleason,optimize concurrency issue,https://github.com/terminusdb/terminusdb/issues/636,"Running optimize concurrently sometimes leads to missing branches.
It is as yet unclear which level the optimize is being run on which leads to the error. We need to try and recreate a race condition by firing many updates concurrently with optimizations.",,,,,,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,637,2021-10-01T15:00:08Z,matko,Upgrade to swipl 8.4,https://github.com/terminusdb/terminusdb/issues/637,swipl 8.4 has just been released. Let's try to upgrade to it.,2021-10-01T15:01:37Z,spl,https://github.com/terminusdb/terminusdb/issues/637#issuecomment-932306967,Do we continue testing against 8.2 and supporting it?,do -PRON- continue test against 82 and support -PRON-,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,637,2021-10-01T15:00:08Z,matko,Upgrade to swipl 8.4,https://github.com/terminusdb/terminusdb/issues/637,swipl 8.4 has just been released. Let's try to upgrade to it.,2021-10-01T15:08:00Z,matko,https://github.com/terminusdb/terminusdb/issues/637#issuecomment-932312992,"In my opinion, we should just support latest stable, and not be afraid to use its features. So we should also drop testing against 8.2. Unless you think there's a good reason to keep supporting 8.2. Right now I don't think there's any particular 8.4 feature we were waiting for.
Strangely, at least one test in the connect_handler test hangs indefinitely for me after upgrading. So some work will need to be done to do this upgrade.",in -PRON- opinion -PRON- should just support late stable and not be afraid to use -PRON- feature so -PRON- should also drop testing against 82 unless -PRON- think there be a good reason to keep support 82 right now i do not think there be any particular 84 feature -PRON- be wait for strangely at least one test in the connect_handler test hang indefinitely for -PRON- after upgrade so some work will need to be do to do this upgrade,1,1
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,637,2021-10-01T15:00:08Z,matko,Upgrade to swipl 8.4,https://github.com/terminusdb/terminusdb/issues/637,swipl 8.4 has just been released. Let's try to upgrade to it.,2021-10-01T15:33:25Z,spl,https://github.com/terminusdb/terminusdb/issues/637#issuecomment-932333742,"I don't mind dropping 8.2 support. TerminusDB is more of an application than a library, so we don't need to support users on multiple versions. I would like to be able to use 8.4 features, especially the error exit codes, which we are working around in our build for 8.2.",i do not mind drop 82 support terminusdb be more of an application than a library so -PRON- do not need to support user on multiple version i would like to be able to use 84 feature especially the error exit code which -PRON- be work around in -PRON- build for 82,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,637,2021-10-01T15:00:08Z,matko,Upgrade to swipl 8.4,https://github.com/terminusdb/terminusdb/issues/637,swipl 8.4 has just been released. Let's try to upgrade to it.,2021-10-04T13:13:18Z,spl,https://github.com/terminusdb/terminusdb/issues/637#issuecomment-933471355,Merge this when done: terminusdb/terminus_store_prolog#29,merge this when do terminusdb / terminus_store_prolog#29,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,637,2021-10-01T15:00:08Z,matko,Upgrade to swipl 8.4,https://github.com/terminusdb/terminusdb/issues/637,swipl 8.4 has just been released. Let's try to upgrade to it.,2021-10-04T14:38:28Z,spl,https://github.com/terminusdb/terminusdb/issues/637#issuecomment-933551885,Tests pass for me on macOS.,test pass for -PRON- on macos,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,640,2021-10-01T15:33:23Z,matko,AppImage fails to build,https://github.com/terminusdb/terminusdb/issues/640,"It looks like the appimage attempts to build the prolog module using make.sh, a script that has been removed long ago. The proper way to build now is make. If we wish to continue supporting AppImage, someone will have to fix this.
@rrooij do you know more?",,,,,,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,641,2021-10-03T21:06:15Z,zenkuro,TerminuxX page crash on entering set value,https://github.com/terminusdb/terminusdb/issues/641,"Describe the bug
All element dissapear from terminusx document page when I start entering value of a set field
To Reproduce
Steps to reproduce the behavior:

Create a document with string set field
Try to add text to it

Expected behavior
No idea... text should be entered, or some error should be showed that that will guide me on how to enter set fields
Screenshots
...
Info (please complete the following information):

OS: Fedora 33 Firefox 81
TerminusX online version


Additional context
...",,,,,,0,0
https://github.com/terminusdb/terminusdb,terminusdb,terminusdb,644,2021-10-04T13:50:45Z,spl,Rename command from `terminusdb` to `terminusdb-ctl`,https://github.com/terminusdb/terminusdb/issues/644,See terminusdb/terminusdb-client-python#242,,,,,,0,0
